<<setup-child, include = FALSE>>=
library(knitr)
source('./code/text_encoding.R')
# amsmath in preamble, whut?
set_parent("../style/preamble_mina.Rnw")
knitr::opts_chunk$set(cache=FALSE)
@

\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\lecturechapter{6}{Advanced Components of CNNs}
\lecture{Deeplearning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Lecture outline}
\tableofcontents
\end{frame}


\begin{vbframe}{Important Types of Convolutions}


\vspace*{0.2cm}

In this chapter, we discuss further advanced components of CNNs: 

    \begin{enumerate}
    \item Inception Modules 
    \item Skip Connections 
    \item Global Average Pooling
    \end{enumerate}
\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Inception Modules}

\begin{vbframe}{Inception modules}
    \begin{itemize}
        \item Problem setting: how do we choose the kernel size in each layer? 
        \item This is often an arbitrary decision.
        \item Solution: offer the model kernels of different sizes in each layer through which it can propagate information and let it decide, which one to use to which extent.
        \item Side-effect: massive parameter reduction allowing for deeper architectures.
        \item First proposed in \cite{26}.
    \end{itemize}
\framebreak
  \begin{figure}
    \centering
      \scalebox{0.75}{\includegraphics{plots/05_conv_variations/inception/incept_naive.png}}
    \caption{Naive inception module. The model can \enquote{choose} from kernels of different sizes.}
  \end{figure}
        \small{Idea: do several convolutions in parallel and concatenate the resulting feature maps in the depth dimension. This requires equal dimensions of the feature maps created by the parallel convolutions.Thus, same padding is used throughout the parallel convolutions.}
\framebreak 
    \begin{figure}
      \centering
      \scalebox{0.75}{\includegraphics{plots/05_conv_variations/inception/incep_ex.png}}
      \tiny{\\ credit : Stanford University}
      \caption{\footnotesize{Naive Inception module - Example}}
      
    \end{figure}
    \begin{itemize}
        \item \small{To allow for the bypass of information throughout one inception module, an 1x1 convolutional layer is also included.
        \item Max-pooling is used as it is ought to increase the robustness of the feature maps. The kernels are padded accordingly to yield feature maps of equal dimensions.}
    \end{itemize}
\framebreak
    \begin{figure}
      \centering
      \scalebox{0.80}{\includegraphics{plots/05_conv_variations/inception/incep_ex.png}}
      \tiny{\\ credit : Stanford University}
      \caption{\footnotesize{Naive Inception module - Example}}
    \end{figure}
    \begin{itemize}
        \item \small{Resulting feature map blocks are restricted to have the same dimensionality but can be of varying depth.
        \item The different feature maps are finally concatenated in the depth-dimension and fed to the next layer.}
    \end{itemize}
\framebreak
    \begin{figure}
        \centering
        \includegraphics[width=11cm]{plots/05_conv_variations/inception/googlenet.png}
        \caption{Inception modules are the integral part of the famous GoogLeNet (2014), one of the first very deep net architectures.}
    \end{figure}
\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% good blog post: https://hacktilldawn.com/2016/09/25/inception-modules-explained-and-implemented/
\begin{vbframe}{Inception modules}
  \begin{figure}
    \centering
    \includegraphics[width=8cm]{plots/05_conv_variations/inception/incept_naive.png}
    \caption{Naive inception module.}
  \end{figure}
    \begin{itemize}
        \item Problem: 3x3 and 5x5 convolutions are expensive operations, especially when executed on very deep input blocks such as many feature maps from the previous layer.
    \end{itemize}
\end{vbframe}

\begin{vbframe}{Inception modules}
  \begin{figure}
    \centering
    \includegraphics[width=8cm]{plots/05_conv_variations/inception/incept_dim_reduction.png}
    \caption{Dimensionality reduced inception module.}
  \end{figure}
    \begin{itemize}
        \item Solution: apply 1x1 convolutions beforehand to reduce the depth of the previous feature map.
    \end{itemize}
\end{vbframe}

\begin{vbframe}{Inception modules}
    \begin{itemize}
        \item Let us understand this with a little numerical example.
        \item Output dimensions of the previous layer: [28, 28, 192].
        \item Output dimensions of the 5x5 convolution from the inception module: [28, 28, 32].
        \item The 5x5 convolution has stride 1 and same padding.
        \item To improve speed, we first convolve the [28, 28, 192] input with 16 1x1 kernels which results in a [28, 28, 16] block. We then apply the 32 5x5 kernel convolution on this \enquote{thinner} block.
        \item Required operations:
        \begin{itemize}
            \item Naive: $5^2 \cdot 28^2 \cdot 192 \cdot 32 = 120.422.400$
            \item Improved version with 1x1 convolution and depth 16: $1^2 \cdot 28^2 \cdot 192 \cdot 16 + 5^2 \cdot 28^2 \cdot 16 \cdot 32 = 12.443.648$
        \end{itemize}
    \end{itemize}
\end{vbframe}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \begin{vbframe}{Separable convolutions}
%     \begin{itemize}
%         \item Problem setting: make convolution computationally more efficient.
%         \item Remember the sobel kernel from the previous lecture:
%             \begin{equation*}
%                     K_x = 
%                     \begin{bmatrix}
%                         +1 & 0 & -1 \\
%                         +2 & 0 & -2 \\
%                         +1 & 0 & -1 
%                     \end{bmatrix}
%             \end{equation*}
%         \item This 3x3 dimensional kernel can be replaced by the outer product of two 3x1 and 1x3 dimensional kernels:
%             \begin{equation*}
%                     \begin{bmatrix}
%                         +1 \\ 
%                         +2 \\
%                         +1   
%                     \end{bmatrix}* 
%                     \begin{bmatrix}
%                         +1 & 0 & -1   
%                     \end{bmatrix}
%             \end{equation*}
%         \item Convolving with both filters subsequently has a similar effect, reduces the amount of parameters to be stored and thus improves speed.
%     \end{itemize}
% \framebreak
% \begin{figure}
%       \centering
%       \scalebox{0.9}{\includegraphics{plots/05_conv_variations/inception/insep_1.png}}
%       \caption{\textit{Left}: Regular Inception module . \textit{Right}: Inception module where each 5x5 convolution is replaced by two 3x3 convolutions.}
%     Separable convolutions (also: factorized) find application in the inception net V4 \cite{28}. The authors find that it increases computational speed but suggest to use this trick in medium to late stage layers only.
%     \end{figure}
% \framebreak
% \begin{figure}
%       \centering
%       \scalebox{0.9}{\includegraphics{plots/05_conv_variations/inception/insep_2.png}}
%       \caption{\textit{Left}: Inception module from the previous slide. \textit{Right}: Inception module after the factorization of the $nxn$ convolutions (n = 3, here).}
%     \end{figure}
% % \framebreak
% %     \begin{figure}
% %         \centering
% %         \includegraphics[width=4cm]{plots/05_conv_variations/separable/separable.png}
% %         \footnotesize{\\credit: Szegedy et al. 2014}
% %         \caption{Seperable convolutions (also: factorized) find application in the inception net V4 \cite{28}. The authors find it to increase computational speed but suggest to use this trick in medium to late stage layers only.}
% %     \end{figure}
% \end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Skip connections}

\begin{vbframe}{Skip connections}
    \begin{itemize}
        \item Problem setting: theoretically, we could build infinitely deep architectures as the net should learn to pick the beneficial layers and skip those that do not improve the performance automatically.
        \item But: this skipping would imply learning an identity mapping $\xv = \mathcal{F}(\xv)$. It is very hard for a neural net to learn such a 1:1 mapping through the many non-linear activations in the architecture.
        \item Solution: offer the model explicitly the opportunity to skip certain layers if they are not useful.
        \item Introduced in \cite{29} and motivated by the observation that stacking evermore layers increases the test- as well as the train-error ($\neq$ overfitting).
    \end{itemize}
\framebreak
  \begin{figure}
    \centering
    \includegraphics[width=7cm]{plots/05_conv_variations/skip/residual_module.png}
    \caption{Skip connection/ residual learning module. The information flows through two layers and the identity function. Both streams of information are then element-wise summed and jointly activated.}
  \end{figure}
\framebreak
    \begin{itemize}
        \item Let $\mathcal{H}(\xv)$ be the optimal underlying mapping that should be learned by (parts of) the net.
        \item $\xv$ is the input in layer $l$ (can be raw data input or the output of a previous layer).
        \item $\mathcal{H}(\xv)$ is the output from layer $l$.
        \item Instead of fitting $\mathcal{H}(\xv)$, the net is ought to learn the residual mapping $\mathcal{F}(\xv):=\mathcal{H}(\xv)-\xv$ whilst $\xv$ is added via the identity mapping.
        \item Thus, $\mathcal{H}(\xv) = \mathcal{F}(\xv) + \xv$, as formulated on the previous slide.
        \item The model should only learn the \textbf{residual mapping} $\mathcal{F}(\xv)$ 
        \item Thus, the procedure is also referred to as \textbf{Residual Learning}.
    \end{itemize}
\framebreak
    \begin{itemize}
        \item The element-wise addition of the learned residuals $\mathcal{F}(\xv)$ and the identity-mapped data $\xv$ requires both to have the same dimensions.
        \item To allow for downsampling within $\mathcal{F}(\xv)$ (via pooling or valid-padded convolutions), the authors introduce a linear projection layer $W_s$ .
        \item $W_s$ ensures that $\xv$ is brought to the same dimensionality as $\mathcal{F}(\xv)$ such that:
        $$
            y = \mathcal{F}(\xv) + W_s\xv,
        $$
        \item $y$ is the output of the skip module and $W_s$ represents the weight matrix of the linear projection (\# rows of $W_s$ = dimensionality of $\mathcal{F}(\xv)$).
        \item This idea applies to fully connected layers as well as to convolutional layers.
    \end{itemize}
\end{vbframe}


\section{Global average pooling}

\begin{vbframe}{Global average pooling}
    \begin{itemize}
        \item Problem setting: tackle overfitting in the final fully connected layer.
        \begin{itemize}
        \item Classic pooling removes spatial information and is mainly used for dimension and parameter reduction.
        \item The elements of the final feature maps are connected to the output layer via a dense layer. This could require a huge number of weights increasing the danger of overfitting.
        \item Example: 256 feature maps of dim 100x100 connected to 10 output neurons lead to $25.6\times 10^6$ weights for the final dense layer.
        \end{itemize}
        \framebreak 

        \item Solution: 
        \begin{itemize}
            \item Average each final feature map to the element of one global average pooling (GAP) vector.
            \item Do not use pooling throughout the net.
            \item Example: 256 feature maps are now reduced to GAP-vector of length 256 yielding a final dense layer with 2560 weights.
        \end{itemize}
    \end{itemize}

    \begin{figure}
        \centering
        \includegraphics[width=11cm]{plots/05_conv_variations/gap/GAP.png}
        \small{\caption{Illustration of GAP \cite{25}. Each feature map representing one feature category averaged into one final vector. No pooling operations are applied throughout the net. The dimensionality of the input reduces solely due to the convolution operations. }}
    \end{figure}

\framebreak

    \begin{itemize}
        \item GAP preserves whole information from the single feature maps whilst decreasing the dimension.
        \item Mitigates the possibly \textbf{destructive} effect of pooling.
        \item Each element of the GAP output represents the activation of a certain feature on the input data.
        \item Acts as an additional regularizer on the final fully connected layer.
        \item Allows for interpretation of the model via Class Activation Maps (more on this later).
    \end{itemize}

\end{vbframe}

% \begin{vbframe}{Types: squeeze-and-excite}
%     \begin{itemize}
%         \item Problem setting: some feature maps from one convolutional block can be more important than others. Still, we force the net to weight them equally. 
%         \item Solution: offer the model a way to weight the channels of each feature map block individually.
%         \item Intuition: some features (car wheel vs. straight line) might be more important than others for the final prediction. Allow the model, to weight them according to their importance.
%         \item Most basic variant: add scalar parameter to each feature map and let the model adapt its relevance throughout training.
%         \item Easily applicable to any convolutional layer.
%         \item Proposed by \cite{27}.
%     \end{itemize}
% \framebreak
%     \begin{figure}
%     \centering
%     \includegraphics[width=11cm]{plots/05_conv_variations/se/se.png}
%     \caption{Illustration of SE as in \cite{27}: in the \textbf{squeeze}-step the feature map U with depth C is transformed to a vector of length C via GAP. Then, a simple 2-layer dense net is used to add non-linearity and output the \textbf{excited} vector of length C. The initial feature map is now multiplied with this vector and the output is fed into the next layer.}
%     \end{figure}
% \framebreak
%     \begin{itemize}
%         \item The authors include a little bit more complexity in the following steps:
%         \begin{enumerate}
%             \item \textbf{Squeeze} feature map $U$ of depth $C$ to vector of the same length via GAP.
%             \item Run a 2-layer dense net on this vector to yield another, \textbf{excited} vector of length $C$ to introduce non-linearity in the process.
%             \item Multiply the initial feature map block by this scalar vector.
%             \item Feed the scaled block $\tilde X$ to the subsequent convolutional layer.
%         \end{enumerate}
%     \end{itemize}
% \end{vbframe}

\begin{vbframe}{Class activation mapping}
    \begin{itemize}
        \item We want to understand the decision-making of a net, e.g. \textbf{why does it classify image X as a cat?}
        \item Simplest method based on GAP was introduced in \cite{32}.
        \item Idea: 
        \begin{itemize}
            \item the final GAP vector stores the activation of each feature map category that was learnt throughout the net.
            \item the dense layer that connects the output classes with the GAP vector stores information about how much each feature contributes to each class.
            \item exploit this information to show which parts of the input image would be activated for each class.
        \end{itemize}
    \end{itemize}
\framebreak
    \begin{figure}
    \centering
    \includegraphics[width=11cm]{plots/05_conv_variations/cam/cam_scheme.png}
    \caption{Illustration of the class activation mapping. The activated regions from the feature maps are summed up weighted by their connection strength with the final output classes and upsampled back to the dimension of the input image. No max-pooling is applied throughout the architecture, the downsampling is due to the CNN layers. }
    \end{figure}
\framebreak
    \begin{enumerate}
        \item Train a net with GAP pooling end-to-end.
        \item Run a forward-pass with the image $i$ you would like to understand.
        \item Take the final $l$ feature maps $f_1, ..., f_l$ for this input.
        \item Get the \textit{feature weights} $w_{j1}, ...,w_{jl}$ that connect the GAP layer with the final class output $j$ that you would like to interpret (e.g. terrier).
        \item Create the \textbf{class activation map} (CAM) for class $j$ on input image $i$:
        $$
            \text{CAM}_{j, i} = \sum_{k = 1}^{l}w_{jk} * f_k
        $$
        \item Normalize the values such that $\text{CAM}_{j, i} \in [0, 1]$.
        \item In case of valid convolutions, the resulting CAM will be smaller than the input image. Linear upsampling is then used to map it back to the input dimension.
        \item Overlay the input image with the CAM and interpret the activation.
    \end{enumerate}
% \framebreak
%     \begin{itemize}
%         \item There exists a variety of extensions such as:
%         \begin{itemize}
%             \item 
%         \end{itemize}
%     \end{itemize}
% \framebreak
%     \begin{itemize}
%         \item Zeiler et al (2013) with occluders: https://arxiv.org/abs/1311.2901
%         \item Grad-CAM blog: https://github.com/jacobgil/pytorch-grad-cam/blob/master/README.md
%         \item Gradient-based Localization (Grad-CAM) Paper: https://arxiv.org/abs/1610.02391
%     \end{itemize}
\end{vbframe}



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{Filter Visualization}
%     \begin{itemize}
%         \item keras post https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html
%         \item explain mathematically
%     \end{itemize}
%     \begin{figure}
%         \centering
%         \includegraphics[width=10cm]{plots/other/visualization.png}
%         \caption{Visualizing and Understanding Convolutional Networks (Zeiler \& Fergus (2013))}
%     \end{figure}
% \end{vbframe}
% 
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{Transfer Learning example}
%     \begin{itemize}
%         \item use trained model, freeze and fine-tune the last layers
%         \item practical example
%     \end{itemize}
% \end{vbframe}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}
\frametitle{References}
\footnotesize{
\begin{thebibliography}{99}

% \bibitem[Ronneberger et al., 2015]{12} Olaf Ronneberger, Philipp Fischer, Thomas Brox (2015)
% \newblock U-Net: Convolutional Networks for Biomedical Image Segmentation
% \newblock \emph{\url{http://arxiv.org/abs/1505.04597}}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Zhou et. al , 2016]{25} B. Zhou, Khosla, A., Labedriza, A., Oliva, A. and A. Torralba (2016)
\newblock Deconvolution and Checkerboard Artifacts
\newblock \emph{\url{http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Szegedy et. al , 2014]{26} Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke and Andrew Rabinovich (2014)
\newblock Going deeper with convolutions
\newblock \emph{\url{https://arxiv.org/abs/1409.4842}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \bibitem[Jie Hu et. al , 2014]{27} Jie Hu, Shen, Li and Gang Sun (2017)
% \newblock Squeeze-and-Excitation Networks
% \newblock \emph{\url{https://arxiv.org/abs/1709.01507}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \bibitem[Szegedy Christian et. al , 2015]{28} Christian Szegedy, Vanhoucke, Vincent, Sergey, Ioffe, Shlens, Jonathan and Wojna Zbigniew (2015)
% \newblock Rethinking the Inception Architecture for Computer Vision
% \newblock \emph{\url{https://arxiv.org/abs/1512.00567}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[He et. al , 2015]{29} Kaiming He, Zhang, Xiangyu, Ren, Shaoqing, and Jian Sun (2015)
\newblock Deep Residual Learning for Image Recognition
\newblock \emph{\url{https://arxiv.org/abs/1512.03385}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Zhou et. al, 2016]{32} Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva and Antonio Torralba (2016)
\newblock Learning Deep Features for Discriminative Localization
\newblock \emph{\url{http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\end{thebibliography}
}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\endlecture

