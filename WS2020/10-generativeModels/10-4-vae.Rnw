<<setup-child, include = FALSE>>=
    library(knitr)
set_parent("../style/preamble_mina.Rnw")
knitr::opts_chunk$set(cache=TRUE)

@

\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\vphi}{\bm{\phi}}
\lecturechapter{10}{Variational Autoencoder (VAE)}
\lecture{Deeplearning}

\begin{frame}
\frametitle{Lecture outline}
\tableofcontents
\end{frame}



\begin{frame}
\frametitle{Variational Autoencoder (VAE)}

The classic DAG problem: How do we efficiently learn $p_{\thetab}(\mathbf{z}| \xv )$?
    
    \vspace{3mm}
    Idea: introduce an \textbf{inference model $q_{\vphi }(\mathbf{z}| \xv)$}  that learns to approximate the posterior $p_{\thetab}(\mathbf{z}| \xv )$
    
    \hspace{1.5cm}\includegraphics[width=0.7\framewidth]{plots/VAE-full.png}


Independently proposed by:
    \begin{itemize}
\item Kingma and Welling, \emph{Auto-Encoding Variational Bayes}, ICLR 2014
\item Rezende, Mohamed and Wierstra, \emph{Stochastic back-propagation and variational inference in deep latent Gaussian models.} ICML 2014
\end{itemize}

%\begin{itemize}
%\item To do inference one would like to calculate $p(\mathbf{z}| \xv ) = \frac{p(\xv| \mathbf{z})p_{\thetab}(\mathbf{z})}{p(\xv)}$.
%
%\pause
%\item But $p_{\thetab}(\xv)=\int p_{\thetab}(\xv|\mathbf{z}) p_{\thetab}(\mathbf{z}) d\mathbf{z}$ is intractable.
%\end{itemize}

%An VAE contains two directed graphical models
%An VAE  consists of directed graphical models

%\vspace{0.7cm}
%\only<1-3>{
    %\hspace{2,3cm}
    %\includegraphics[width=0.7\framewidth]{VAE}
    %\vspace{0.7cm}
    %with distributions
    %with distribution
    %\vspace{-0.4cm}
    %\begin{align}
    % p(\xVec, \hVec)  &= p(\xVec|\hVec_1) p(\hVec_1|\hVec_2) ... p(\hVec_L) \nonumber
    %\end{align}
    %\pause
    %\vspace{-0.4cm}
    %\begin{itemize}
    %\item To do inference one would like to calculate $p(\vec h| \xv ) = \frac{p(\xv| \vec h)p(\vec h)}{p(\xv)}$.
    %
    %\pause
    %\item But $p(\xv)=\int p(\xv|\vec h) p(\vec h) d\vec h$ is intractable.
    %\end{itemize}
    %}
%\only<4>{\hspace{2,3cm} \includegraphics[width=0.7\framewidth]{VAE-full}\\
    
    %with distributions
    %\vspace{-0.4cm}
    %with distribution
    %\begin{align}
    % p(\xVec, \hVec)  &= p(\xVec|\hVec) p(\hVec)  \nonumber \\
    %q(\hVec | \xVec) &= q(\hVec|\xVec) q(\hVec|\hVec_1) ... %q(\hVec_L | \hVec_{L-1}) \nonumber
    % \nonumber
    %\end{align}}

%\small{[Kingma and Welling, Auto-Encoding Variational Bayes, ICLR 2014]}

%\small{[Rezende, Mohamed and Wierstra, Stochastic back-propagation and variational inference in deep latent Gaussian models. ICML 2014]}

\end{frame}


\begin{frame}
\frametitle{VAE-Parameter Fitting: Variational Lower Bound}

%Since $\log p(\xv_s)$ is intractable, optimization is often based on
%\textbf{variational lower bound} (or \textbf{Evidence Lower BOund (ELBO)})
%\textbf{Evidence Lower BOund (ELBO)}
%of log-likelihood for training example $\xv_s$
    \begin{itemize}
\item $\log p_{\thetab}(\xv)$ is intractable.
\item But we can compute a \textbf{variational lower bound}:
%\textbf{Evidence Lower BOund (ELBO)}
\end{itemize}
%\pause
\only<1>{
    \begin{align*}
    \log p_{\thetab}(\xv) &= \log\int p_{\thetab}(\xv,\mathbf{z}) d\mathbf{z} =  \log \int  q_{\vphi}(\mathbf{z}|\xv) \frac{p_{\thetab}(\xv,\mathbf{z})}{q_{\vphi}(\mathbf{z}|\xv)} d\mathbf{z} \\
    &\color{white}{\geq  \int  q(\vec h|\xv_s)  \log \frac{p(\xv_s,\vec h)}{q(\vec h|\xv_s)} d\vec h }\\
    &\color{white}{\geq \int  q(\vec h|\xv_s)  \big( \log p(\xv_s|\vec h) + \log p(\vec h) - \log q(\vec h|\xv_s) \big)d\vec h} \\
    & \color{white}{mathbb{E}_{q(\vec h|\xv_s)}\big[ \log p(\xv_s|\vec h) \big] - KL\big[ q(\vec h|\xv_s) ||  p(\vec h) \big] }
    \end{align*}}
\only<2>{
    \begin{align*}
    \log p_{\thetab}(\xv) &= \log\int p_{\thetab}(\xv,\mathbf{z}) d\mathbf{z} =  \log \int  q_{\vphi}(\mathbf{z}|\xv) \frac{p_{\thetab}(\xv,\mathbf{z})}{q_{\vphi}(\mathbf{z}|\xv)} d\mathbf{z} \\
    &\color{black}{\geq  \int  q_{\vphi}(\mathbf{z}|\xv)  \log \frac{p_{\thetab}(\xv,\mathbf{z})}{q_{\vphi}(\mathbf{z}|\xv)} d\mathbf{z} }\\
    &\color{white}{= \int  q(\vec h|\xv_s)  \big( \log p(\xv_s|\vec h) + \log p(\vec h) - \log q(\vec h|\xv_s) \big)d\vec h} \\
    & \color{white}{mathbb{E}_{q(\vec h|\xv_s)}\big[ \log p(\xv_s|\vec h) \big] - KL\big[ q(\vec h|\xv_s) ||  p(\vec h) \big] }
    \end{align*}
    \begin{block}{Jensen's inequality}
    Let $f$ be a concave function  and $\mathbf{x}$ an integrable random variable. Then it holds:
        $f(\E{[\mathbf{x}]})  \geq \E{[f(\mathbf{x})]}$.
    \end{block}
    
    
}
\only<3>{
    \begin{align*}
    \log p_{\thetab}(\xv) &= \log\int p_{\thetab}(\xv,\mathbf{z}) d\mathbf{z} =  \log \int  q_{\vphi}(\mathbf{z}|\xv) \frac{p_{\thetab}(\xv,\mathbf{z})}{q_{\vphi}(\mathbf{z}|\xv)} d\mathbf{z} \\
    &\color{black}{\geq  \int  q_{\vphi}(\mathbf{z}|\xv)  \log \frac{p_{\thetab}(\xv,\mathbf{z})}{q_{\vphi}(\mathbf{z}|\xv)} d\mathbf{z} }\\
    %&\color{black}{= \int  q_{\vphi}(\mathbf{z}|\xv)  \big( \log p_{\thetab}(\xv|\mathbf{z}) + \log p_{\thetab}(\mathbf{z}) - \log q_{\vphi}(\mathbf{z}|\xv) \big)d\mathbf{z}} \\
    &\color{black}{= \int  q_{\vphi}(\mathbf{z}|\xv)  \big( \log p_{\thetab}(\xv|\mathbf{z}) + \log \frac{p_{\thetab}(\mathbf{z})}{ q_{\vphi}(\mathbf{z}|\xv)} \big)d\mathbf{z}} \\
    & \color{white}{mathbb{E}_{q(\vec h|\xv_s)}\big[ \log p(\xv_s|\vec h) \big] - KL\big[ q(\vec h|\xv_s) ||  p(\vec h) \big] }
    \end{align*}
    %Todo: Box mit Logarithmus-Regeln einbauen!
}
\only<4>{
    \begin{align*}
    \log p_{\thetab}(\xv) &= \log\int p_{\thetab}(\xv,\mathbf{z}) d\mathbf{z} =  \log \int  q_{\vphi}(\mathbf{z}|\xv) \frac{p_{\thetab}(\xv,\mathbf{z})}{q_{\vphi}(\mathbf{z}|\xv)} d\mathbf{z} \\
    &\color{black}{\geq  \int  q_{\vphi}(\mathbf{z}|\xv)  \log \frac{p_{\thetab}(\xv,\mathbf{z})}{q_{\vphi}(\mathbf{z}|\xv)} d\mathbf{z} }\\
    &\color{black}{= \int  q_{\vphi}(\mathbf{z}|\xv)  \big( \log p_{\thetab}(\xv|\mathbf{z}) + \log \frac{p_{\thetab}(\mathbf{z})}{ q_{\vphi}(\mathbf{z}|\xv)} \big)d\mathbf{z}} \\
    & \color{black}{=\mathbb{E}_{q_{\vphi}(\mathbf{z}|\xv)}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big] - KL\big[ q_{\vphi}(\mathbf{z}|\xv) ||  p_{\thetab}(\mathbf{z}) \big] }
    \end{align*}}
\only<5>{
    \begin{align*}
    \log p_{\thetab}(\xv) &= \log\int p_{\thetab}(\xv,\mathbf{z}) d\mathbf{z} =  \log \int  q_{\vphi}(\mathbf{z}|\xv) \frac{p_{\thetab}(\xv,\mathbf{z})}{q_{\vphi}(\mathbf{z}|\xv)} d\mathbf{z} \\
    &\color{black}{\geq  \int  q_{\vphi}(\mathbf{z}|\xv)  \log \frac{p_{\thetab}(\xv,\mathbf{z})}{q_{\vphi}(\mathbf{z}|\xv)} d\mathbf{z} }\\
    &\color{black}{= \int  q_{\vphi}(\mathbf{z}|\xv)  \big( \log p_{\thetab}(\xv|\mathbf{z}) + \log \frac{p_{\thetab}(\mathbf{z})}{ q_{\vphi}(\mathbf{z}|\xv)} \big)d\mathbf{z}} \\
    & \color{black}{=\underbrace{\mathbb{E}_{q_{\vphi}(\mathbf{z}|\xv)}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big] - KL\big[ q_{\vphi}(\mathbf{z}|\xv) ||  p_{\thetab}(\mathbf{z}) \big] }_{:=ELBO(\thetab, \vphi, \xv)} }
    \end{align*}}

\end{frame}


\begin{frame}
\frametitle{VAE-Parameter Fitting: Variational Lower Bound}

\only<1>{
    \begin{equation*}
    ELBO(\thetab, \vphi, \xv) = \mathbb{E}_{q_{\vphi}(\mathbf{z}|\xv)}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big] - KL\big[ q_{\vphi}(\mathbf{z}|\xv) ||  p_{\thetab}(\mathbf{z}) \big]
    \end{equation*}
    %\begin{itemize}
    %\item \color{white}{First term resembles reconstruction loss}
    %\item \color{white}{Second  term penalizes encoder for deviating from prior}
    %\end{itemize}
}
\only<2>{
    \begin{equation*}
    ELBO(\thetab, \vphi, \xv) = \mathbb{E}_{q_{\vphi}(\mathbf{z}|\xv)}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big] - KL\big[ q_{\vphi}(\mathbf{z}|\xv) ||  p_{\thetab}(\mathbf{z}) \big]
    \end{equation*}
    \begin{itemize}
    \item Also known as \textbf{Evidence Lower BOund (ELBO)}.
    %\item \color{black}{First term resembles reconstruction loss}
    %\item \color{white}{Second  term penalizes encoder for deviating from prior}
    \end{itemize}
}
\only<3>{
    \begin{equation*}
    ELBO(\thetab, \vphi, \xv) = \color{red}{\mathbb{E}_{q_{\vphi}(\mathbf{z}|\xv)}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big]} \color{black}{- KL\big[ q_{\vphi}(\mathbf{z}|\xv) ||  p_{\thetab}(\mathbf{z}) \big]}
    \end{equation*}
    \begin{itemize}
    \item Also known as \textbf{Evidence Lower BOund (ELBO)}.
    \item \color{black}{First term resembles reconstruction loss}.
    %\item \color{white}{Second  term penalizes encoder for deviating from prior}
    \end{itemize}
}
\only<4>{
    \begin{equation*}
    ELBO(\thetab, \vphi, \xv) = \color{black}{\mathbb{E}_{q_{\vphi}(\mathbf{z}|\xv)}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big]} \color{red}{- KL\big[ q_{\vphi}(\mathbf{z}|\xv) ||  p_{\thetab}(\mathbf{z}) \big]}
    \end{equation*}
    \begin{itemize}
    \item Also known as \textbf{Evidence Lower BOund (ELBO)}.
    \item \color{black}{First term resembles reconstruction loss}.
    \item \color{black}{Second  term penalizes encoder for deviating from prior}.
    \end{itemize}
}
\only<5>{
    \begin{equation*}
    ELBO(\thetab, \vphi, \xv) = \mathbb{E}_{q_{\vphi}(\mathbf{z}|\xv)}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big] - KL\big[ q_{\vphi}(\mathbf{z}|\xv) ||  p_{\thetab}(\mathbf{z}) \big]
    \end{equation*}
    \begin{itemize}
    \item Also known as \textbf{Evidence Lower BOund (ELBO)}.
    \item \color{black}{First term resembles reconstruction loss.}
    \color{black}{ \item Second  term penalizes encoder for deviating from prior.}
    \item It can be shown that
    %$\log p(\xv_s) = ELBO(q(\vec h|\xv_s)) + KL\big[ q(\vec h|\xv_s) ||  p(\vec h|\xv ) \big]$
        $ELBO(\thetab, \vphi, \xv) = \log p_{\thetab}(\xv) -  KL\big[ q_{\vphi}(\mathbf{z}|\xv) ||  p_{\thetab}(\mathbf{z}|\xv ) \big]$\\
    $\Rightarrow$ by maximizing the ELBO we maximize $p_{\thetab}(\xv)$ and minimize $KL\big[ q_{\vphi}(\mathbf{z}|\xv) ||  p_{\thetab}(\mathbf{z}|\xv ) \big]$.
    \end{itemize}
}


\end{frame}

%
%\frame{
    %\frametitle{VAE-Parameter Fitting: Variational Lower Bound}
    %
    %Todo:?
        %Add illustrative slides for objective suggested by Sharath
    %
    %}

\frame{
    \frametitle{VAE-Model Definition}
    
    %Idea: Use a NN to learn a mapping from some latent variable $\mathbf{z}$ to a complicated distribution on $\xv$.
    
    %$$p(\xv) = \int p(\xv, \mathbf{z}) d\mathbf{z} = \int p(\xv| \vec %z) p_{\thetab}(\mathbf{z}) d\mathbf{z}$$
            %where $p_{\thetab}(\mathbf{z})$ is a some simple distribution and $ p(\xv| \mathbf{z})=g(\mathbf{z})$
            %$$p(\xv, \mathbf{z}) = p(\xv| \mathbf{z}) p_{\thetab}(\mathbf{z})$$
            
            Idea:
            \begin{itemize}
        \item Set $p_{\thetab}(\mathbf{z})$ to some simple distribution. % e.g. $\mathcal{N}(1,0)$.
        \item Parametrize inference model and generative model with  neural networks $f(\xv, \vphi)$ and $g(\mathbf{z}, \thetab)$.
        \end{itemize}
        
        
        \begin{figure}
        \centering
        \includegraphics[width=5cm]{plots/VAE-inferenceModel.png}
        \includegraphics[width=5cm]{plots/VAE-generativeModel2.png}
        \end{figure}
        
}

\frame{
    \frametitle{VAE-Model Definition}
    
    %Idea: Use a NN to learn a mapping from some latent variable $\mathbf{z}$ to a complicated distribution on $\xv$.
    
    %$$p(\xv) = \int p(\xv, \mathbf{z}) d\mathbf{z} = \int p(\xv| \vec %z) p_{\thetab}(\mathbf{z}) d\mathbf{z}$$
            %where $p_{\thetab}(\mathbf{z})$ is a some simple distribution and $ p(\xv| \mathbf{z})=g(\mathbf{z})$
            %$$p(\xv, \mathbf{z}) = p(\xv| \mathbf{z}) p_{\thetab}(\mathbf{z})$$
            
            
            \only<1>{
                %\vspace{-0.8cm}
                Usually:
                    \begin{itemize}
                \item $f(\xv, \vphi)= \left( \bm{\mu}_{\mathbf{z}}(\xv), \bm{\sigma}_{\mathbf{z}}(\xv) \right)$ and $q_{\vphi}(\mathbf{z}| \xv) = \mathcal{N}(\mathbf{z}; \bm{\mu}_{\mathbf{z}}(\xv), \bm{\sigma}^2_{\mathbf{z}}(\xv))$
                    %\item \color{white}{$p_{\thetab}(\mathbf{z})= \mathcal{N}(\mathbf{z}; 1,0)$}
                %\item \color{white}{$g(\mathbf{z})= \left( \mu_{\xv}(\mathbf{z}), \sigma_{\xv}(\mathbf{z}) \right)$ and $p_{\thetab}(\mathbf{z}| \xv) = \mathcal{N}(\mathbf{z}; \bm{\mu}_{\xv}(\mathbf{z}), \bm{\sigma}_{\xv}(\mathbf{z}))$}
                \end{itemize}
                
                \vspace{1.3cm}
                \begin{figure}
                \hspace{-6cm}\includegraphics[width=5cm]{plots/VAE-inferenceModelGaussian.png}
                %\includegraphics[width=6cm]{VAE-generativeModelGaussian.png}
                \end{figure}
            }
        
        \only<2>{
            Usually:
                \begin{itemize}
            \item $f(\xv, \vphi)= \left( \bm{\mu}_{\mathbf{z}}(\xv), \bm{\sigma}_{\mathbf{z}}(\xv) \right)$ and $q_{\vphi}(\mathbf{z}| \xv) = \mathcal{N}(\mathbf{z}; \bm{\mu}_{\mathbf{z}}(\xv), \bm{\sigma}^2_{\mathbf{z}}(\xv))$
                \item $p_{\thetab}(\mathbf{z})= \mathcal{N}(\mathbf{z}; 0,1)$
                    \pause
                \item $g(\mathbf{z}, \thetab)= \left( \bm{\mu}_{\xv}(\mathbf{z}), \bm{\sigma}_{\xv}(\mathbf{z}) \right)$ and $p_{\thetab}(\xv| \mathbf{z}) = \mathcal{N}(\xv; \bm{\mu}_{\xv}(\mathbf{z}), \bm{\sigma}^2_{\xv}(\mathbf{z}))$
                    \end{itemize}
                
                
                \begin{figure}
                \centering
                \includegraphics[width=5cm]{plots/VAE-inferenceModelGaussian.png}
                \includegraphics[width=6cm]{plots/VAE-generativeModelGaussian.png}
                \end{figure}
        }
        
}





\begin{frame}
\frametitle{VAE-Parameter Fitting: reparameterization Trick}
\begin{itemize}
\item Goal: Learn parameters $\vphi$ and $\thetab$ by maximizing %the ELBO (lower bound of $\log p_{\thetab}(\xv)$) by gradient ascent.
\begin{equation*}
ELBO(\thetab, \vphi, \xv) = \mathbb{E}_{q_{\vphi}(\mathbf{z}|\xv)}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big] - KL\big[ q_{\vphi}(\mathbf{z}|\xv) ||  p_{\thetab}(\mathbf{z}) \big]
\end{equation*}
based on gradient ascent.
\item Idea: Approximate first term
\begin{align*}
\mathbb{E}_{q_{\vphi}(\mathbf{z}|\xv)}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big] &= \mathbb{E}_{\mathbf{z} \sim \mathcal{N}(\mathbf{z}; \bm{\mu}_{\mathbf{z}}(\xv), \bm{\sigma}_{\mathbf{z}}(\xv))}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big]\\
&\approx \frac{1}{L} \sum_{l=1}^{L} \log p_{\thetab}(\xv|\mathbf{z}^{(l)})
\end{align*}
\item  Problem:
    %Given $\mathbf{z} \sim q_{\vphi}(\mathbf{z}| \xv)$,
Given this average,
how should one take derivatives
%(a function of) $\mathbf{z}$
    %$ q_{\vphi}(\mathbf{z}| \xv)$
    w.r.t.~$\vphi$?
    %\item Solution:
    % For $ q_{\vphi}(\vec h| \xv) =\mathcal{N}(\bm{\mu}, \bm{\sigma})$, $\vphi=(\bm{\mu}, \bm{\sigma})$ \\ reparametrize: $\alert{\vec h= \bm{\mu} + \bm{\sigma} \odot \bm{\epsilon}}$, with $\alert{\bm{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{1})}$
    %\item and let $(\bm{\mu}, \bm{\sigma})=f(\xv)$ be a function given by a neural network.
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{VAE-Parameter Fitting: reparameterization Trick}

\begin{block}{Recall: Linear transformation of a normal random variable}
\begin{center}
Let $\bm{\epsilon}$ be standard normally distributed, i.e. $\bm{\epsilon} \sim\mathcal{N}(\bm{\epsilon}; \mathbf{0}, \mathbf{1})$. Then for $\mathbf{z} = \bm{\epsilon} \cdot \bm{\sigma} + \bm{\mu} $ it holds: $\mathbf{z}\sim \mathcal{N}(\mathbf{z}; \bm{\mu}, \bm{\sigma}^2)$
    \end{center}
\end{block}
\only<2>{
    Back to our problem:
        \begin{align*}
    \mathbb{E}_{q_{\vphi}(\mathbf{z}|\xv)}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big] &= \mathbb{E}_{\mathbf{z} \sim \mathcal{N}(\mathbf{z}; \bm{\mu}_{\mathbf{z}}(\xv), \bm{\sigma}_{\mathbf{z}}(\xv))}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big]\\
    &\approx \frac{1}{L} \sum_{l=1}^{L} \log p_{\thetab}(\xv|\mathbf{z}^{(l)})
    \end{align*}
    Solution: Define \color{red}{$\mathbf{z} = \bm{\epsilon} \cdot \bm{\sigma}_{\mathbf{z}}(\xv )+ \bm{\mu}_{\mathbf{z}}(\xv ) $} \color{black}{ where} \color{red}{$\bm{\epsilon} \sim\mathcal{N}(\bm{\epsilon}; \mathbf{0}, \mathbf{1})$. }
}
\only<3>{
    Back to our problem:
        \begin{align*}
    \mathbb{E}_{q_{\vphi}(\mathbf{z}|\xv)}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big] &= \mathbb{E}_{\color{red}{\bm{\epsilon} \sim\mathcal{N}(\bm{\epsilon}; \mathbf{0}, \mathbf{1})}}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big]\\
    &\approx \frac{1}{L} \sum_{l=1}^{L} \log p_{\thetab}(\xv|\mathbf{z}^{(l)})
    \end{align*}
    Solution: Define \color{red}{$\mathbf{z} = \bm{\epsilon} \cdot \bm{\sigma}_{\mathbf{z}}(\xv )+ \bm{\mu}_{\mathbf{z}}(\xv ) $} \color{black}{ where} \color{red}{$\bm{\epsilon} \sim\mathcal{N}(\bm{\epsilon}; \mathbf{0}, \mathbf{1})$. }
}
\only<4>{
    Back to our problem:
        \begin{align*}
    \mathbb{E}_{q_{\vphi}(\mathbf{z}|\xv)}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big] &= \mathbb{E}_{\color{red}{\bm{\epsilon} \sim\mathcal{N}(\bm{\epsilon}; \mathbf{0}, \mathbf{1})}}\big[ \log p_{\thetab}(\xv|\color{red}{\mathbf{z} = \bm{\epsilon} \cdot \bm{\sigma}_{\mathbf{z}}(\xv )+ \bm{\mu}_{\mathbf{z}}(\xv ) }\color{black}{) \big]}\\
        &\approx \frac{1}{L} \sum_{l=1}^{L} \log p_{\thetab}(\xv|\mathbf{z}^{(l)})
        \end{align*}
        Solution: Define \color{red}{$\mathbf{z} = \bm{\epsilon} \cdot \bm{\sigma}_{\mathbf{z}}(\xv )+ \bm{\mu}_{\mathbf{z}}(\xv ) $}\color{black}{ where} \color{red}{$\bm{\epsilon} \sim\mathcal{N}(\bm{\epsilon}; \mathbf{0}, \mathbf{1})$. }
}
\only<5>{
    Back to our problem:
        \begin{align*}
    \mathbb{E}_{q_{\vphi}(\mathbf{z}|\xv)}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big] &= \mathbb{E}_{\color{red}{\bm{\epsilon} \sim\mathcal{N}(\bm{\epsilon}; \mathbf{0}, \mathbf{1})}}\big[ \log p_{\thetab}(\xv|\color{red}{\mathbf{z} = \bm{\epsilon} \cdot \bm{\sigma}_{\mathbf{z}}(\xv )+ \bm{\mu}_{\mathbf{z}}(\xv ) }\color{black}{) \big]}\\
        &\approx \frac{1}{L} \sum_{l=1}^{L} \log p_{\thetab}(\xv|\color{red}{\mathbf{z}^{(l)} = \bm{\epsilon}^{(l)} \cdot \bm{\sigma}_{\mathbf{z}}(\xv )+ \bm{\mu}_{\mathbf{z}}(\xv ) } )
        \end{align*}
        Solution: Define \color{red}{$\mathbf{z} = \bm{\epsilon} \cdot \bm{\sigma}_{\mathbf{z}}(\xv )+ \bm{\mu}_{\mathbf{z}}(\xv ) $}\color{black}{ where} \color{red}{$\bm{\epsilon} \sim\mathcal{N}(\bm{\epsilon}; \mathbf{0}, \mathbf{1})$. }
}

\end{frame}



\frame{
    \frametitle{VAE-Training with Backpropagation}
    
    Due to the reparameterization trick, we can simultaneously train both the generative model  $p_{\thetab}(\xv| \mathbf{z})$  and the inference model $q_{\vphi}(\mathbf{z}| \xv)$   by maximizing the ELBO based on gradient ascent and backpropagation.
    
    
    
    \begin{figure}
    \centering
    \includegraphics[width=6cm]{plots/VAE-BackProp.png}
    \end{figure}
    
}

\begin{frame}{Illustration of forward pass}
  \begin{figure}
    \centering
    \scalebox{1}{\includegraphics{plots/vae_forwardpass.png}}
    \tiny{\\ credit: Lillian Weng}
    \caption{\footnotesize Illustration of the forward pass in a VAE. For a given $\xv$, the encoder network outputs the mean(s) $\bm{\mu}_{\mathbf{z}}(\xv)$ and standard deviation(s) $\bm{\sigma}_{\mathbf{z}}(\xv)$ of $\mathcal{N}(\mathbf{z}; \bm{\mu}_{\mathbf{z}}(\xv), \bm{\sigma}^2_{\mathbf{z}}(\xv))$. From this, a vector $\mathbf{z}$ is sampled and fed to the decoder network which gives us $\xv'$. If $\xv$ is an image, $\xv'$ is commonly interpreted as the reconstructed image. }
    \end{figure}
\end{frame}

\frame{
    \frametitle{Latent Variables learned by a VAE}
    
    \begin{figure}
    \centering
    \scalebox{0.4}{\includegraphics{plots/LatentVariables-FF.png}}
    \caption{\footnotesize Images generated by a VAE are superimposed on the latent vectors that generated them. In the two-dimensional latent space of the VAE, the first dimension encodes the position of the face and the second dimension encodes the expression. Therefore, starting at any point in the latent space, if we move along either axis, the corresponding property will change in the generated image.    (Goodfellow et al.,  2016)}
    \end{figure}
    
}

\frame{
    \frametitle{Latent Variables learned by a VAE}
    
    \begin{figure}
    \centering
    \scalebox{0.5}{\includegraphics{plots/vae_mnist_n.png}}
    \caption{\footnotesize Images generated by a VAE are superimposed on the latent vectors that generated them. The two-dimensional latent space of the VAE captures much of the variation present in MNIST. Different regions in the latent space correspond to different digits in the generated images. (Goodfellow et al.,  2016)}
    \end{figure}
    
}


% \frame{
%     \frametitle{VAE with deep encoder/decoder: Components collapse}
%     
%     \begin{figure}
%     \centering
%     \includegraphics[width=12cm]{plots/VAE-componentsCollaps1.png}
%     \end{figure}
%     
% }
% 
% 
% \frame{
%     \frametitle{VAE with deeper encoder/decoder: More components collapse}
%     
%     \begin{figure}
%     \centering
%     \includegraphics[width=12cm]{plots/VAE-componentsCollaps2.png}
%     \end{figure}
%     
%     
% }




\frame{
    \frametitle{Samples from a vanilla VAE}
    
    % \begin{figure}
    % \centering
    % \includegraphics[width=5cm]{plots/SamplesVAE-Faces.png}
    % %\hspace{0.3cm}
    % \includegraphics[width=6cm]{plots/SamplesVAE-ImageNet.png}
    % \caption{\footnotesize Samples generated by a VAE trained on }
    % \end{figure}
    % %\vspace{-1cm}
    % \texttt{ \small{ Labeled Faces in the Wild (LFW)}    \;\;\;\;\;\;\;\;\;\;\;\ \small{ImageNet (small)}}
    % %
    
    \begin{figure}
      \centering
      \scalebox{0.9}{\includegraphics{plots/vae_faces.png}}
      \tiny{\\ Credit : Wojciech Mormul}
      \caption{\footnotesize Samples generated by a VAE that was trained on images of people's faces.}
    \end{figure}
}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}
\frametitle{References}
\footnotesize{
\begin{thebibliography}{99}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Weng, 2014]{5} Lilian Weng (2018)
\newblock From Autoencoder to Beta-VAE
\newblock \emph{\url{https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{thebibliography}
}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlecture


