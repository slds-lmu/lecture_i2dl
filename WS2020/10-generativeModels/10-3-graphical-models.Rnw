<<setup-child, include = FALSE>>=
    library(knitr)
set_parent("../style/preamble_mina.Rnw")
knitr::opts_chunk$set(cache=TRUE)

@

\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
    
    
\lecturechapter{10}{Probabilistic Graphical Models \& Directed Generative Models}
\lecture{Deeplearning}

\begin{frame}
\frametitle{Lecture outline}
\tableofcontents
\end{frame}


\section{Probabilistic graphical models}



\begin{frame}{Graphical models}
    
    %\bigskip
    
    \begin{center}
    \begin{minipage}{.9\textwidth}
    \begin{block}{}
    \begin{center}
    \textbf{Probabilistic graphical models}
    describe probability distributions
    by mapping conditional dependence and independence properties
    between random variables
    on a graph structure.
    \end{center}
    \end{block}
    \end{minipage}
    \end{center}

\end{frame}



\begin{frame}{Why again graphical models?}
    
    \begin{minipage}{.62\textwidth}
    \begin{enumerate}\itemsep2ex
    \item Graphical models visualize the structure of a probabilistic
    model; they help to develop, understand and motivate probabilistic models.
    %\item Die Graphstruktur macht wichtige Eigenschaften der Verteilung
    % deutlich, insbesondere bedingte Abhängigkeiten und Unabhängigkeiten.
    \pause
    \item Complex computations (e.g., marginalization)
    can derived efficiently
    using algorithms exploiting the graph structure.
    \end{enumerate}
    \end{minipage}
    \hfill\begin{minipage}{.33\textwidth}
    \phantom{mini}
    
    \hfill\includegraphics[width=.95\textwidth]{plots/FigureDice}
    \end{minipage}

\end{frame}

\begin{frame}{Graphical models: Example}

  \begin{figure}
    \centering
    \scalebox{0.9}{\includegraphics{plots/studnet.png}}
    \tiny{\\Credit: Daphne Koller}
    \caption{\footnotesize A graphical model representing five variables and their (in-)dependencies along with the corresponding marginal and conditional distributions. The variable 'Grade', for example, is affected by 'Difficulty' (of the exam) and 'Intelligence' (of the student). This is captured in the corresponding conditional distribution. 'Letter' refers to a letter of recommendation. In this model, 'Letter' is conditionally independent of 'Difficulty' and 'Intelligence', given 'Grade'.}
  \end{figure}

\end{frame}


\subsection{Latent Variables}

% \frame{
%     \frametitle{Latent variables: Motivation}
%     
%     \mode<beamer>{\begin{center}
%         \only<1>{\includegraphics[width=7cm]{plots/ImCrossO.pdf}}\only<2>{\includegraphics[width=7cm]{plots/ImHeartR.pdf}}\only<3>{\includegraphics[width=7cm]{plots/ImStarO.pdf}}\only<4>{\includegraphics[width=7cm]{plots/ImCrossR.pdf}}\only<5>{\includegraphics[width=7cm]{plots/ImStarB.pdf}}\only<6>{\includegraphics[width=7cm]{plots/ImStarR.pdf}}
%         \end{center}}
%     \mode<handout>{\begin{center}
%         \includegraphics[width=7cm]{plots/ImCrossO.pdf}
%         \end{center}}
%     
%     \begin{center}
%     $200\times 200$ pixels \quad$\rightarrow$\quad
%     $2^{40000}-1$ parameters?
%         \end{center}
% }

\begin{frame}{Latent variables: Motivation}
  \begin{figure}
    \centering
      \scalebox{0.8}{\includegraphics{plots/latvar_motiv.png}}
\caption{\footnotesize A simple illustration of the relevance of latent variables. Here, six 200 x 200 pixel images are shown where each pixel is either black or white. Naively, the probability distribution over the space of all such images would need $2^{40000}-1$ parameters to fully specify. However, we see that the images have three main factors of variation : object type (shape), position and size. This suggests that the actual number of parameters required might be significantly fewer. }
 \end{figure}
\end{frame}


 
\frame{
    \frametitle{Latent variables}
    \begin{itemize}
    \item Additional nodes, which do not directly correspond
    to observations, allow to describe complex distributions
    over the visible variables by means of simple conditional distributions.
    \item The corresponding random variables are called \emph{hidden} or
    \emph{latent} variables.
    \end{itemize}
    
    \begin{center}
    \begin{figure}
    \includegraphics[width=5cm]{plots/MyFigure8-8}
    \caption{\footnotesize 'Object', 'position' and 'size' are the latent variables behind an image.}
    \end{figure}
    \end{center}
    
}



\section{Directed generative models}


\frame{
    \frametitle{Directed generative models}
    
    \textbf{Goal:} Learn to generate $\xv$ from some latent variables $\mathbf{z}$
    
    $$p_{\thetab}(\xv) = \int p_{\thetab}(\xv, \mathbf{z}) d\mathbf{z} = \int p_{\thetab}(\xv| \mathbf{z}) p_{\thetab}(\mathbf{z}) d\mathbf{z}$$
        
        %Todo: h zu z machen, theta einbauen moeglich?!!!
        \begin{figure}
    %\hspace{-3cm}
    \includegraphics[width=4cm]{plots/VAE.png}
    \includegraphics[width=7cm]{plots/zToxManifold.png}
    \tiny{Image from: Ward, A. D., Hamarneh, G.: \textbf{3D Surface Parameterization Using Manifold Learning for Medial Shape Representation}, Conference on Image Processing, Proc. of SPIE Medical Imaging, 2007}
    \caption{\footnotesize \textit{Left}: An illustration of a directed generative model. \textit{Right}: A mapping (represented by $g$) from the 2D latent space to the 3D space of observed variables.}
    \end{figure}
    
    
    
}

\frame{
    \frametitle{Directed generative models}
    
     The latent variables $\mathbf{z}$ must be learned from the data (which only contains the observed variables $\xv$).
        
        \begin{itemize}
    \item %For inference we need
    The posterior  is given by $p_{\thetab}(\mathbf{z}| \xv ) = \frac{p_{\thetab}(\xv| \mathbf{z})p_{\thetab}(\mathbf{z})}{p_{\thetab}(\xv)}$.
    %
    % \pause
    \item But $p_{\thetab}(\xv)=\int p_{\thetab}(\xv|\mathbf{z}) p_{\thetab}(\mathbf{z}) d\mathbf{z}$ is intractable and common algorithms (such as Expectation Maximization) cannot be used.
    \end{itemize}
    
    \pause
    
    The classic DAG problem: How do we efficiently learn $p_{\thetab}(\mathbf{z}| \xv )$?
    %An VAE contains two directed graphical models
    %An VAE  consists of directed graphical models
    
    %\vspace{0.7cm}
    %\only<1-3>{\hspace{2,3cm
        %\hspace{5.4cm}
        \includegraphics[width=0.37\framewidth]{plots/VAE.png}
        
        We will see two approaches to this problem:
            \begin{itemize}
        \item \textbf{Variational Autoencoders (VAEs)}
        \item \textbf{Generative Adversarial Networks (GANs)}
        \end{itemize}
        
    }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlecture
