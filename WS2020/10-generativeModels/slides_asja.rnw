<<setup-child, include = FALSE>>=
library(knitr)
set_parent("../style/preamble_temp.Rnw")
knitr::opts_chunk$set(cache=TRUE)

@


\lecturechapter{10}{Generative Models}
\lecture{Deeplearning}



%\frame{
%\frametitle{Recall: Unsupervised Learning}

%\begin{itemize}
%\item
%%Training data: training data consists of unlabeled input points. TODO:
%Training data: A set of
%%unlabeled
%i.i.d.~examples $x_1, x_2,\dots, x_n \sim p_{\text{data}}$.
%%sampled from unknown distribution.
%\item  Task: find and describe intrinsic structure in the data.
%\end{itemize}
%
%\vspace{0.5cm}
%\hspace{2cm} clustering \hspace{4cm} density fitting / \\
%\hspace{5cm} learning a generative model
%\vspace{-1cm}
%\begin{figure}
%\centering
%\includegraphics[width=5cm]{plots/clustering.png}
%\includegraphics[width=6cm]{plots/kerneldist.png}
%\end{figure}
%}


\begin{frame} {Recall: Unsupervised Deep Learning}

  There are two main goals of \textbf{unsupervised deep learning learning}:
  \begin{itemize}
  \vspace{4mm}
    \item \textbf{Representation Learning}
      \begin{itemize}
       % \item Learning the right features for the task.
       \item i.e.~manifold learning, feature learning, etc.
       \item Can be done by an autoencoder.
        \item Applications examples:
          \begin{itemize}
            \item dimensionality reduction / data compression
            \item transfer learning / semi-supervised learning
          \end{itemize}
      \end{itemize}
      \vspace{5mm}
    \item \textbf{Generative Models / Density Estimation}
      \begin{itemize}
        \item Given a training set $S = (\mathbf{x}^{(1)}, \mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)})$ where each $\mathbf{x}^{(i)} \sim p_{\text{data}}(\mathbf{x})$, the goal is to estimate $p_{\text{data}}(\mathbf{x})$.
        \item Applications :
          \begin{itemize}
            \item generating music, videos, volumetric models for 3D printing, synthetic data for learning algorithms, outlier identification, images denoising, inpainting etc.
          \end{itemize}
      \end{itemize}
  \end{itemize}
\end{frame}



\frame{
\frametitle{Density fitting / Learning a generative model}

Given  $x^{(1)}, x^{(2)},\dots, x^{(n)} \sim p_{\text{data}}$ learn a model
 $p_{\vec \theta}$ of $p_{data}$.

\only<1>{
\begin{figure}
\hspace{-5cm}\includegraphics[width=5cm]{plots/sample.png}
%\includegraphics[width=6cm]{sample-withGaussian.png}
\end{figure}
}
\pause
\only<2>{
\begin{figure}
\centering
\includegraphics[width=5cm]{plots/sample.png}
\includegraphics[width=5cm]{plots/sample-withGaussian.png}
\end{figure}
}



}


\frame{
\frametitle{Why generative models?}

A  generative model $p_{\vec \theta}$
%is a probabilistic model  of the data generating distribution $p_{data}$.
%It
can be used for

\begin{itemize}
\item sampling / data generation
\item outlier detection
\item missing feature extraction
\item image denoising/ reconstruncion
\item representation learning
\item planing in reinforcement learning
\item ...
\end{itemize}

}

\begin{frame} {Application Example: Generating Images from Text}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/text_to_img.png}}
      \tiny{\\Source : Zhang et al (2017)}
  \end{figure}
\end{frame}

\begin{frame} {Application Example: Semantic Labels --> Images}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/sem_to_img.png}}
      \tiny{\\Source : Wang et al (2017)}
  \end{figure}
\end{frame}

\begin{frame} {Application Example: Image Inpainting}
  \vspace{8mm}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/inpainting.png}}
      \tiny{\\Source : Demir et al (2018)}
  \end{figure}
\end{frame}

\begin{frame} {Application Example: Image generation}
None of these are real!
  \vspace{8mm}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/fake_celeb.png}}
      \tiny{\\Source : Karras et al (2018)}
  \end{figure}
\end{frame}

\section{Background}

%\frame{\frametitle{Outline}\tableofcontents}
% \AtBeginSubsection[]
% {
%   \begin{frame}
%       \frametitle{Outline}
%       %\tableofcontents[currentsection,currentsubsection]
%       \tableofcontents[currentsection]
%   \end{frame}
%}

\frame{
\frametitle{Maximum Likelihood}
%\vspace{-1cm}
\only<1>{
\begin{figure}
\includegraphics[width=5cm]{plots/sample.png}
%\includegraphics[width=6cm]{sample-withGaussian.png}
\end{figure}
}
%\vspace{-1cm}
\only<2-3>{
\begin{figure}
%\includegraphics[width=5cm]{sample.png}
\includegraphics[width=5cm]{plots/sample-whichGaussian1.png}
\end{figure}
}
\only<4>{
\begin{figure}
%\includegraphics[width=5cm]{sample.png}
\includegraphics[width=5cm]{plots/sample-whichGaussian2.png}
\end{figure}
}
\only<5>{
\begin{figure}
%\includegraphics[width=5cm]{sample.png}
\includegraphics[width=5cm]{plots/sample-whichGaussian3.png}
\end{figure}
}
%\vspace{-1cm}

%We assume that the data-underlying distribution is Gaussian, that is we use a model
We choose the model distribution $p_{\theta}$ to be Gaussian, that is
\only<1>{
$$p_{\vec \theta}(x^{(i)})= \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{1}{2} \frac{(x^{(i)}-\mu)^2}{\sigma^2}\right)$$
}
\only<2-5>{
$$p_{\vec \theta}(x^{(1)},\dots, x^{(n)})
= \prod_{i=1}^{n} p_{\vec \theta}(x^{(i)})
= \prod_{i=1}^{n}  \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{1}{2} \frac{(x^{(i)}-\mu)^2}{\sigma^2}\right)$$
}
%\only<3-6>{
%$$p_{\vec \theta}(x_1,\dots, x_n)
%= \prod_{i=1}^{n} p_{\vec \theta}(x_i)
%=  \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^n \exp\left(\sum_{i=1}^{n} -\frac{1}{2} \frac{(x_i-\mu)^2}{\sigma^2}\right)$$
%}

\only<3-5>{
Given the samples $x^{(1)}, \dots, x^{(n)}$, how should we estimate $\vec \theta =\{\mu, \sigma^2\}$?
}
}

\frame{
\frametitle{Recall: Maximum Likelihood}


The \textbf{likelihood function} is given by

$$L(\vec \theta | x^{(1)},\dots, x^{(n)})= \prod_{i=1}^{n} p_{\vec \theta}(x^{(i)})
%=  \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^n \exp\left(\sum_{i=1}^{n} -\frac{1}{2} \frac{(x^{(i)}-\mu)^2}{\sigma^2}\right)
=\prod_{i=1}^{n}  \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{1}{2} \frac{(x^{(i)}-\mu)^2}{\sigma^2}\right) \enspace.
$$

To maximize it, we  often consider the  \textbf{log-likelihood}

\begin{align*}
%l (\vec \theta | x^{(1)},\dots, x^{(n)})&=
\log L(\vec \theta | x^{(1)},\dots, x^{(n)}) &= \log \prod_{i=1}^{n} p_{\vec \theta}(x^{(i)})
%=  \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^n \exp\left(\sum_{i=1}^{n} -\frac{1}{2} \frac{(x^{(i)}-\mu)^2}{\sigma^2}\right)
= \sum_{i=1}^{n} \log p_{\vec \theta}(x^{(i)}) \\
&=  \log \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right) -\frac{1}{2}  \sum_{i=1}^{n} \frac{(x^{(i)}-\mu)^2}{\sigma^2}\enspace.
\end{align*}
}


\frame{
\frametitle{Recall: Maximum Likelihood}

Setting derivatives equal to zero yields
$$
\frac{\partial \log L(\vec \theta | x^{(1)},\dots, x^{(n)}) }{\partial \mu} = \frac{1}{\sigma^2  } \big( \sum_{i=1}^n x^{(i)} -n \mu \big)
$$
and
$$
\frac{\partial \log L(\vec \theta | x^{(1)},\dots, x^{(n)}) }{\partial \sigma} = \frac{1}{2\sigma^2  } \big(  \frac{1}{\sigma^2  } \sum_{i=1}^n (x^{(i)} -\mu)^2 -n \big)  \enspace.
$$
Leading to
$$
\hat \mu = \frac{1}{n} \sum_{i=1}^n x^{(i)}    \text{\;\;\;\;and\;\;\;\;}  \hat \sigma = \frac{1}{n} \sum_{i=1}^n (x^{(i)}- \hat \mu)^2  \enspace.
$$
}

\begin{frame}
\frametitle{Notes on maximum likelihood learning}
\begin{itemize}\itemsep2ex
\item For a  model $p$ with visible variables $\vec x$ and
  hidden variables $\vec z$, the likelihood computation involves
\[
p(\vec x^{(i)}\,|\,\vec\theta) = \sum_{\vec z} p(\vec x^{(i)},\vec z\,|\,\vec\theta)\enspace.
\]
This is difficult, especially because of the sum which prevents the logarithm to
act directly on  the joint distribution. \pause
\item If we  can not find the maximum likelihood parameters analytically (i.e.~by setting the derivative to zero) one can  maximize the likelihood via SGD or related algorithms.
\item If $p_{\text{data}}$ is the true distribution underlying $S$, maximizing the
  logarithmic likelihood function corresponds to minimizing an
  empirical estimate of the Kullback-Leibler divergence
  $\KL(p_{\text{data}}\,\|\,p)$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Kullback-Leibler divergence}
Kullback-Leibler (KL) divergence between two distribution $p$ and $p_{\text{data}}$
over $\vec x$  is
%\smallskip
\begin{itemize}%\normalsize\itemsep2ex
\item a (non-symmetric) measure of
  difference between
% distributions,
$p$ and $p_{\text{data}}$,
\item always positive, zero iff the distributions are the same,
\end{itemize}\pause
and defined as
\begin{align*}
\KL(p_{\text{data}}\,\|\,p)
&= -\sum_{\vec x} p_{\text{data}}(\vec
x)\ln\frac{p(\vec x)}{p_{\text{data}}(\vec x)} \\
 &= -\fub{\sum_{\vec x} p_{\text{data}}(\vec
x)\ln p(\vec x)}{can be approximated by $\frac{1}{\ell}\sum_{n=1}^{\ell} \ln p(\xb_n)$}  +\fub{\sum_{\vec x} p_{\text{data}}(\vec
x)\ln      {p_{\text{data}}(\vec x)}}{{independent of $p$}}
\end{align*}
 (sum turns to integral for
  continuous random variables).
\end{frame}


\section{Probabilistic graphical models}



\frame{
\frametitle{Graphical models}

%\bigskip

\begin{center}
\begin{minipage}{.9\textwidth}
\begin{block}{}
\begin{center}
\textbf{Probabilistic graphical models}
describe probability distributions
by mapping conditional dependence and independence properties
between random variables
on a graph structure.
\end{center}
\end{block}
\end{minipage}
\end{center}
}

\frame{
\frametitle{Why again graphical models?}

\begin{minipage}{.62\textwidth}
\begin{enumerate}\itemsep2ex
\item Graphical models visualize the structure of a probabilistic
  model; they help to develop, understand and motivate probabilistic models.
%\item Die Graphstruktur macht wichtige Eigenschaften der Verteilung
 % deutlich, insbesondere bedingte Abhängigkeiten und Unabhängigkeiten.
 \pause
\item Complex computations (e.g., marginalization)
can derived efficiently
using algorithms exploiting the graph structure.
\end{enumerate}
\end{minipage}
\hfill\begin{minipage}{.33\textwidth}
\phantom{mini}

\hfill\includegraphics[width=.95\textwidth]{plots/FigureDice}
\end{minipage}
}




%\subsection{Latent Variables}

\frame{
\frametitle{Latent variables: Motivation}

\mode<beamer>{\begin{center}
\only<1>{\includegraphics[width=7cm]{plots/ImCrossO.pdf}}\only<2>{\includegraphics[width=7cm]{plots/ImHeartR.pdf}}\only<3>{\includegraphics[width=7cm]{plots/ImStarO.pdf}}\only<4>{\includegraphics[width=7cm]{plots/ImCrossR.pdf}}\only<5>{\includegraphics[width=7cm]{plots/ImStarB.pdf}}\only<6>{\includegraphics[width=7cm]{plots/ImStarR.pdf}}
\end{center}}
\mode<handout>{\begin{center}
\includegraphics[width=7cm]{plots/ImCrossO.pdf}
\end{center}}

\begin{center}
$200\times 200$ pixels \quad$\rightarrow$\quad
$2^{40000}-1$ parameters?
\end{center}
}



\frame{
\frametitle{Latent variables}
\begin{itemize}
\item Additional nodes, which do not directly correspond
to observations, allow to describe complex distributions
over the visible variables by means of simple conditional distributions.
\item The corresponding random variables are called \emph{hidden} or
  \emph{latent} variables.
\end{itemize}

\begin{center}
\includegraphics[width=5cm]{plots/MyFigure8-8}
\end{center}

}



\section{Directed generative models}


 \frame{
\frametitle{Directed generative models}

Learn to generate $\vec x$ from some latent variables $\vec z$.

$$p_{\vec \theta}(\vec x) = \int p_{\vec \theta}(\vec x, \vec z) d\vec z = \int p_{\vec \theta}(\vec x| \vec z) p_{\vec \theta}(\vec z) d\vec z$$

%Todo: h zu z machen, theta einbauen moeglich?!!!
\begin{figure}
%\hspace{-3cm}
\includegraphics[width=4cm]{plots/VAE.png}
\includegraphics[width=7cm]{plots/zToxManifold.png}
\end{figure}

\tiny{Image from: Ward, A. D., Hamarneh, G.: \textbf{3D Surface Parameterization Using Manifold Learning for Medial Shape Representation}, Conference on Image Processing, Proc. of SPIE Medical Imaging, 2007}

}

 \frame{
\frametitle{Directed generative models}

The classic DAG problem: Where does $\vec z$ come from?

\begin{itemize}
\item %For inference we need
The posterior  is given by $p_{\vec \theta}(\vec z| \vec x ) = \frac{p_{\vec \theta}(\vec x| \vec z)p_{\vec \theta}(\vec z)}{p_{\vec \theta}(\vec x)}$.
%
\pause
\item But $p_{\vec \theta}(\vec x)=\int p_{\vec \theta}(\vec x|\vec z) p_{\vec \theta}(\vec z) d\vec z$ is intractable.
\end{itemize}

%An VAE contains two directed graphical models
%An VAE  consists of directed graphical models

%\vspace{0.7cm}
%\only<1-3>{\hspace{2,3cm
%\hspace{5.4cm}
\includegraphics[width=0.37\framewidth]{plots/VAE.png}

We will see two approaches to this problems:
\begin{itemize}
\item \textbf{Variational Autoencoders (VAEs)}
\item \textbf{Generative Adversarial Networks (GANs)}
\end{itemize}

}


\section{Variational Autoencoder (VAE)}


 \begin{frame}
\frametitle{Variational Autoencoder (VAE)}

The classic DAG problem: Where does $\vec z$ come from?

Idea: introduce an \textbf{inference model $q_{\vec \phi }(\vec z| \vec x)$}  that learns to approximate the posterior   $p_{\vec \theta}(\vec z| \vec x )$

\hspace{1.5cm}\includegraphics[width=0.7\framewidth]{plots/VAE-full.png}


Independently proposed by:
\begin{itemize}
\item Kingma and Welling, \emph{Auto-Encoding Variational Bayes}, ICLR 2014
\item Rezende, Mohamed and Wierstra, \emph{Stochastic back-propagation and variational inference in deep latent Gaussian models.} ICML 2014
\end{itemize}

%\begin{itemize}
%\item To do inference one would like to calculate $p(\vec z| \vec x ) = \frac{p(\vec x| \vec z)p(\vec z)}{p(\vec x)}$.
%
%\pause
%\item But $p_{\vec \theta}(\vec x)=\int p_{\vec \theta}(\vec x|\vec z) p_{\vec \theta}(\vec z) d\vec z$ is intractable.
%\end{itemize}

%An VAE contains two directed graphical models
%An VAE  consists of directed graphical models

%\vspace{0.7cm}
%\only<1-3>{
%\hspace{2,3cm}
%\includegraphics[width=0.7\framewidth]{VAE}
%\vspace{0.7cm}
%with distributions
%with distribution
%\vspace{-0.4cm}
%\begin{align}
% p(\xVec, \hVec)  &= p(\xVec|\hVec_1) p(\hVec_1|\hVec_2) ... p(\hVec_L) \nonumber
%\end{align}
%\pause
%\vspace{-0.4cm}
%\begin{itemize}
%\item To do inference one would like to calculate $p(\vec h| \vec x ) = \frac{p(\vec x| \vec h)p(\vec h)}{p(\vec x)}$.
%
%\pause
%\item But $p(\vec x)=\int p(\vec x|\vec h) p(\vec h) d\vec h$ is intractable.
%\end{itemize}
%}
%\only<4>{\hspace{2,3cm} \includegraphics[width=0.7\framewidth]{VAE-full}\\

%with distributions
%\vspace{-0.4cm}
%with distribution
%\begin{align}
% p(\xVec, \hVec)  &= p(\xVec|\hVec) p(\hVec)  \nonumber \\
%q(\hVec | \xVec) &= q(\hVec|\xVec) q(\hVec|\hVec_1) ... %q(\hVec_L | \hVec_{L-1}) \nonumber
% \nonumber
%\end{align}}

%\small{[Kingma and Welling, Auto-Encoding Variational Bayes, ICLR 2014]}

%\small{[Rezende, Mohamed and Wierstra, Stochastic back-propagation and variational inference in deep latent Gaussian models. ICML 2014]}

\end{frame}


\begin{frame}
\frametitle{VAE-Parameter Fitting: Variational Lower Bound}

%Since $\log p(\vec x_s)$ is intractable, optimization is often based on
%\textbf{variational lower bound} (or \textbf{Evidence Lower BOund (ELBO)})
%\textbf{Evidence Lower BOund (ELBO)}
%of log-likelihood for training example $\vec x_s$
\begin{itemize}
\item $\log p(\vec x)$ is intractable.
\item But we can compute a \textbf{variational lower bound}
%\textbf{Evidence Lower BOund (ELBO)}
\end{itemize}
%\pause
\only<1>{
\begin{align*}
\log p_{\vec \theta}(\vec x) &= \log\int p_{\vec \theta}(\vec x,\vec z) d\vec z =  \log \int  q_{\vec \phi}(\vec z|\vec x) \frac{p_{\vec \theta}(\vec x,\vec z)}{q_{\vec \phi}(\vec z|\vec x)} d\vec z \\
&\color{white}{\geq  \int  q(\vec h|\vec x_s)  \log \frac{p(\vec x_s,\vec h)}{q(\vec h|\vec x_s)} d\vec h }\\
&\color{white}{\geq \int  q(\vec h|\vec x_s)  \big( \log p(\vec x_s|\vec h) + \log p(\vec h) - \log q(\vec h|\vec x_s) \big)d\vec h} \\
& \color{white}{mathbb{E}_{q(\vec h|\vec x_s)}\big[ \log p(\vec x_s|\vec h) \big] - KL\big[ q(\vec h|\vec x_s) ||  p(\vec h) \big] }
\end{align*}}
\only<2>{
\begin{align*}
\log p_{\vec \theta}(\vec x) &= \log\int p_{\vec \theta}(\vec x,\vec z) d\vec z =  \log \int  q_{\vec \phi}(\vec z|\vec x) \frac{p_{\vec \theta}(\vec x,\vec z)}{q_{\vec \phi}(\vec z|\vec x)} d\vec z \\
&\color{black}{\geq  \int  q_{\vec \phi}(\vec z|\vec x)  \log \frac{p_{\vec \theta}(\vec x,\vec z)}{q_{\vec \phi}(\vec z|\vec x)} d\vec z }\\
&\color{white}{= \int  q(\vec h|\vec x_s)  \big( \log p(\vec x_s|\vec h) + \log p(\vec h) - \log q(\vec h|\vec x_s) \big)d\vec h} \\
& \color{white}{mathbb{E}_{q(\vec h|\vec x_s)}\big[ \log p(\vec x_s|\vec h) \big] - KL\big[ q(\vec h|\vec x_s) ||  p(\vec h) \big] }
\end{align*}
\begin{block}{Jensen’s inequality}
Let $f$ be a concave function  and $\mathbf{x}$ an integrable random variable. Then it holds:
$f(\E{[\mathbf{x}]})  \geq \E{[f(\mathbf{x})]}$.
\end{block}


}
\only<3>{
\begin{align*}
\log p_{\vec \theta}(\vec x) &= \log\int p_{\vec \theta}(\vec x,\vec z) d\vec z =  \log \int  q_{\vec \phi}(\vec z|\vec x) \frac{p_{\vec \theta}(\vec x,\vec z)}{q_{\vec \phi}(\vec z|\vec x)} d\vec z \\
&\color{black}{\geq  \int  q_{\vec \phi}(\vec z|\vec x)  \log \frac{p_{\vec \theta}(\vec x,\vec z)}{q_{\vec \phi}(\vec z|\vec x)} d\vec z }\\
%&\color{black}{= \int  q_{\vec \phi}(\vec z|\vec x)  \big( \log p_{\vec \theta}(\vec x|\vec z) + \log p_{\vec \theta}(\vec z) - \log q_{\vec \phi}(\vec z|\vec x) \big)d\vec z} \\
&\color{black}{= \int  q_{\vec \phi}(\vec z|\vec x)  \big( \log p_{\vec \theta}(\vec x|\vec z) + \log \frac{p_{\vec \theta}(\vec z)}{ q_{\vec \phi}(\vec z|\vec x)} \big)d\vec z} \\
& \color{white}{mathbb{E}_{q(\vec h|\vec x_s)}\big[ \log p(\vec x_s|\vec h) \big] - KL\big[ q(\vec h|\vec x_s) ||  p(\vec h) \big] }
\end{align*}
%Todo: Box mit Logarithmus-Regeln einbauen!
}
\only<4>{
\begin{align*}
\log p_{\vec \theta}(\vec x) &= \log\int p_{\vec \theta}(\vec x,\vec z) d\vec z =  \log \int  q_{\vec \phi}(\vec z|\vec x) \frac{p_{\vec \theta}(\vec x,\vec z)}{q_{\vec \phi}(\vec z|\vec x)} d\vec z \\
&\color{black}{\geq  \int  q_{\vec \phi}(\vec z|\vec x)  \log \frac{p_{\vec \theta}(\vec x,\vec z)}{q_{\vec \phi}(\vec z|\vec x)} d\vec z }\\
&\color{black}{= \int  q_{\vec \phi}(\vec z|\vec x)  \big( \log p_{\vec \theta}(\vec x|\vec z) + \log \frac{p_{\vec \theta}(\vec z)}{ q_{\vec \phi}(\vec z|\vec x)} \big)d\vec z} \\
& \color{black}{=\mathbb{E}_{q_{\vec \phi}(\vec z|\vec x)}\big[ \log p_{\vec \theta}((\vec x|\vec z) \big] - KL\big[ q_{\vec \phi}(\vec z|\vec x) ||  p_{\vec \theta}(\vec z) \big] }
\end{align*}}
\only<5>{
\begin{align*}
\log p_{\vec \theta}(\vec x) &= \log\int p_{\vec \theta}(\vec x,\vec z) d\vec z =  \log \int  q_{\vec \phi}(\vec z|\vec x) \frac{p_{\vec \theta}(\vec x,\vec z)}{q_{\vec \phi}(\vec z|\vec x)} d\vec z \\
&\color{black}{\geq  \int  q_{\vec \phi}(\vec z|\vec x)  \log \frac{p_{\vec \theta}(\vec x,\vec z)}{q_{\vec \phi}(\vec z|\vec x}) d\vec z }\\
&\color{black}{= \int  q_{\vec \phi}(\vec z|\vec x)  \big( \log p_{\vec \theta}(\vec x|\vec z) + \log \frac{p_{\vec \theta}(\vec z)}{ q_{\vec \phi}(\vec z|\vec x)} \big)d\vec z} \\
& \color{black}{=\underbrace{\mathbb{E}_{q_{\vec \phi}(\vec z|\vec x)}\big[ \log p_{\vec \theta}(\vec x|\vec z) \big] - KL\big[ q_{\vec \phi}((\vec z|\vec x) ||  p_{\vec \theta}(\vec z) \big] }_{:=ELBO(\vec \theta, \vec \phi, \vec x)} }
\end{align*}}

\end{frame}


\begin{frame}
\frametitle{VAE-Parameter Fitting: Variational Lower Bound}

\only<1>{
\begin{equation*}
ELBO(\vec \theta, \vec \phi, \vec x) = \mathbb{E}_{q_{\vec \phi}(\vec z|\vec x)}\big[ \log p_{\vec \theta}(\vec x|\vec z) \big] - KL\big[ q_{\vec \phi}(\vec z|\vec x) ||  p_{\vec \theta}(\vec z) \big]
\end{equation*}
%\begin{itemize}
%\item \color{white}{First term resembles reconstruction loss}
%\item \color{white}{Second  term penalizes encoder for deviating from prior}
%\end{itemize}
}
\only<2>{
\begin{equation*}
ELBO(\vec \theta, \vec \phi, \vec x) = \mathbb{E}_{q_{\vec \phi}(\vec z|\vec x)}\big[ \log p_{\vec \theta}(\vec x|\vec z) \big] - KL\big[ q_{\vec \phi}(\vec z|\vec x) ||  p_{\vec \theta}(\vec z) \big]
\end{equation*}
\begin{itemize}
\item Also known as \textbf{Evidence Lower BOund (ELBO)}.
%\item \color{black}{First term resembles reconstruction loss}
%\item \color{white}{Second  term penalizes encoder for deviating from prior}
\end{itemize}
}
\only<3>{
\begin{equation*}
ELBO(\vec \theta, \vec \phi, \vec x) = \color{red}{\mathbb{E}_{q_{\vec \phi}(\vec z|\vec x)}\big[ \log p_{\vec \theta}(\vec x|\vec z) \big]} \color{black}{- KL\big[ q_{\vec \phi}(\vec z|\vec x) ||  p_{\vec \theta}(\vec z) \big]}
\end{equation*}
\begin{itemize}
\item Also known as \textbf{Evidence Lower BOund (ELBO)}.
\item \color{black}{First term resembles reconstruction loss}.
%\item \color{white}{Second  term penalizes encoder for deviating from prior}
\end{itemize}
}
\only<4>{
\begin{equation*}
ELBO(\vec \theta, \vec \phi, \vec x) = \color{black}{\mathbb{E}_{q_{\vec \phi}(\vec z|\vec x)}\big[ \log p_{\vec \theta}(\vec x|\vec z) \big]} \color{red}{- KL\big[ q_{\vec \phi}(\vec z|\vec x) ||  p_{\vec \theta}(\vec z) \big]}
\end{equation*}
\begin{itemize}
\item Also known as \textbf{Evidence Lower BOund (ELBO)}.
\item \color{black}{First term resembles reconstruction loss}.
\item \color{black}{Second  term penalizes encoder for deviating from prior}.
\end{itemize}
}
\only<5>{
\begin{equation*}
ELBO(\vec \theta, \vec \phi, \vec x) = \mathbb{E}_{q_{\vec \phi}(\vec z|\vec x)}\big[ \log p_{\vec \theta}(\vec x|\vec z) \big] - KL\big[ q_{\vec \phi}(\vec z|\vec x) ||  p_{\vec \theta}(\vec z) \big]
\end{equation*}
\begin{itemize}
\item Also known as \textbf{Evidence Lower BOund (ELBO)}.
\item \color{black}{First term resembles reconstruction loss.}
\color{black}{ \item Second  term penalizes encoder for deviating from prior.}
\item It holds
%$\log p(\vec x_s) = ELBO(q(\vec h|\vec x_s)) + KL\big[ q(\vec h|\vec x_s) ||  p(\vec h|\vec x ) \big]$
$ELBO(\vec \theta, \vec \phi, \vec x) = \log p_{\vec \theta}(\vec x) -  KL\big[ q_{\vec \phi}(\vec z|\vec x) ||  p_{\vec \theta}(\vec z|\vec x ) \big]$\\
$\Rightarrow$ by maximizing the ELBO we maximize $p_{\vec \theta}(\vec x)$ and minimize $KL\big[ q_{\vec \phi}(\vec z|\vec x) ||  p_{\vec \theta}(\vec z|\vec x ) \big]$.
\end{itemize}
}


\end{frame}

%
%\frame{
%\frametitle{VAE-Parameter Fitting: Variational Lower Bound}
%
%Todo:?
%Add illustrative slides for objective suggested by Sharath
%
%}

 \frame{
\frametitle{VAE-Model Definition}

%Idea: Use a NN to learn a mapping from some latent variable $\vec z$ to a complicated distribution on $\vec x$.

%$$p(\vec x) = \int p(\vec x, \vec z) d\vec z = \int p(\vec x| \vec %z) p(\vec z) d\vec z$$
%where $p(\vec z)$ is a some simple distribution and $ p(\vec x| \vec z)=g(\vec z)$
%$$p(\vec x, \vec z) = p(\vec x| \vec z) p(\vec z)$$

Idea:
\begin{itemize}
\item Set $p_{\vec \theta}(\vec z)$ to some simple distribution. % e.g. $\mathcal{N}(1,0)$.
\item Parametrize inference model and generative model with  neural networks $f(\vec x, \vec \phi)$ and $g(\vec z, \vec \theta)$.
\end{itemize}


\begin{figure}
\centering
\includegraphics[width=5cm]{plots/VAE-inferenceModel.png}
\includegraphics[width=5cm]{plots/VAE-generativeModel2.png}
\end{figure}

}

\frame{
\frametitle{VAE-Model Definition}

%Idea: Use a NN to learn a mapping from some latent variable $\vec z$ to a complicated distribution on $\vec x$.

%$$p(\vec x) = \int p(\vec x, \vec z) d\vec z = \int p(\vec x| \vec %z) p(\vec z) d\vec z$$
%where $p(\vec z)$ is a some simple distribution and $ p(\vec x| \vec z)=g(\vec z)$
%$$p(\vec x, \vec z) = p(\vec x| \vec z) p(\vec z)$$


\only<1>{
%\vspace{-0.8cm}
Usually:
\begin{itemize}
\item $f(\vec x, \vec \phi)= \left( \mu_{\vec x}(\vec x), \sigma_{\vec z}(\vec x) \right)$ and $q_{\vec \phi}(\vec z| \vec x) = \mathcal{N}(\vec z; \vec \mu_{\vec z}(\vec x), \vec \sigma^2_{\vec z}(\vec x))$
%\item \color{white}{$p_{\vec \theta}(\vec z)= \mathcal{N}(\vec z; 1,0)$}
%\item \color{white}{$g(\vec z)= \left( \mu_{\vec x}(\vec z), \sigma_{\vec x}(\vec z) \right)$ and $p_{\vec \theta}(\vec z| \vec x) = \mathcal{N}(\vec z; \vec \mu_{\vec x}(\vec z), \vec \sigma_{\vec x}(\vec z))$}
\end{itemize}

\vspace{1.3cm}
\begin{figure}
\hspace{-6cm}\includegraphics[width=5cm]{plots/VAE-inferenceModelGaussian.png}
%\includegraphics[width=6cm]{VAE-generativeModelGaussian.png}
\end{figure}
}

\only<2>{
Usually:
\begin{itemize}
\item $f(\vec x, \vec \phi)= \left( \mu_{\vec z}(\vec x), \sigma_{\vec z}(\vec x) \right)$ and $q_{\vec \phi}(\vec z| \vec x) = \mathcal{N}(\vec z; \vec \mu_{\vec z}(\vec x), \vec \sigma^2_{\vec z}(\vec x))$
\item $p_{\vec \theta}(\vec z, \vec \theta)= \mathcal{N}(\vec z; 0,1)$
\pause
\item $g(\vec z)= \left( \mu_{\vec x}(\vec z), \sigma_{\vec x}(\vec z) \right)$ and $p_{\vec \theta}(\vec x| \vec z) = \mathcal{N}(\vec x; \vec \mu_{\vec x}(\vec z), \vec \sigma^2_{\vec x}(\vec z))$
\end{itemize}


\begin{figure}
\centering
\includegraphics[width=5cm]{plots/VAE-inferenceModelGaussian.png}
\includegraphics[width=6cm]{plots/VAE-generativeModelGaussian.png}
\end{figure}
}

}





 \begin{frame}
\frametitle{VAE-Parameter Fitting: Reparametrization Trick}
\begin{itemize}
\item Goal: Learn parameters $\vec \phi$ and $\vec \theta$ by maximizing %the ELBO (lower bound of $\log p_{\vec \theta}(\vec x)$) by gradient ascent.
\begin{equation*}
ELBO(\vec \theta, \vec \phi, \vec x) = \mathbb{E}_{q_{\vec \phi}(\vec z|\vec x)}\big[ \log p_{\vec \theta}(\vec x|\vec z) \big] - KL\big[ q_{\vec \phi}(\vec z|\vec x) ||  p_{\vec \theta}(\vec h) \big]
\end{equation*}
based on gradient ascent.
\item Idea: Approximate first term
\begin{align*}
\mathbb{E}_{q_{\vec \phi}(\vec z|\vec x)}\big[ \log p_{\vec \theta}(\vec x|\vec z) \big] &= \mathbb{E}_{\vec z \sim \mathcal{N}(\vec z; \vec \mu_{\vec z}(\vec x), \vec \sigma_{\vec z}(\vec x))}\big[ \log p_{\vec \theta}(\vec x|\vec z) \big]\\
&\approx \frac{1}{L} \sum_{l=1}^{L} \log p_{\vec \theta}(\vec x|\vec z_l)
\end{align*}
\item  Problem:
%Given $\vec z \sim q_{\vec \phi}(\vec z| \vec x)$,
Given this average,
how should one take derivatives
%(a function of) $\vec z$
%$ q_{\vec \phi}(\vec z| \vec x)$
w.r.t.~$\vec \phi$?
%\item Solution:
% For $ q_{\vec \phi}(\vec h| \vec x) =\mathcal{N}(\vec \mu, \vec \sigma)$, $\vec \phi=(\vec \mu, \vec \sigma)$ \\ reparametrize: $\alert{\vec h= \vec \mu + \vec \sigma \odot \vec \epsilon}$, with $\alert{\vec \epsilon \sim \mathcal{N}(\vec 0, \vec 1)}$
%\item and let $(\vec \mu, \vec \sigma)=f(\vec x)$ be a function given by a neural network.
\end{itemize}

\end{frame}


 \begin{frame}
\frametitle{VAE-Parameter Fitting: Reparametrization Trick}

\begin{block}{Recall: Linear transformation of a normal random variable}
\begin{center}
Let $\vec \epsilon$ be standard normally distributed, i.e. $\vec \epsilon \sim\mathcal{N}(\vec \epsilon; \vec 0, \vec 1)$. Then for $\vec z = \vec \epsilon \cdot \vec \sigma + \vec \mu $ it holds: $\vec z\sim \mathcal{N}(\vec z; \vec \mu, \vec \sigma^2)$
\end{center}
\end{block}
\only<2>{
Back to our problem:
\begin{align*}
\mathbb{E}_{q_{\vec \phi}(\vec z|\vec x)}\big[ \log p_{\vec \theta}(\vec x|\vec z) \big] &= \mathbb{E}_{\vec z \sim \mathcal{N}(\vec z; \vec \mu_{\vec z}(\vec x), \vec \sigma_{\vec z}(\vec x))}\big[ \log p_{\vec \theta}(\vec x|\vec z) \big]\\
&\approx \frac{1}{L} \sum_{l=1}^{L} \log p_{\vec \theta}(\vec x|\vec z_l)
\end{align*}
Solution: Define \color{red}{$\vec z = \vec \epsilon \cdot \vec \sigma_{\vec z}(\vec x )+ \vec \mu_{\vec z}(\vec x ) $} \color{black}{ where} \color{red}{$\vec \epsilon \sim\mathcal{N}(\vec \epsilon; \vec 0, \vec 1)$. }
}
\only<3>{
Back to our problem:
\begin{align*}
\mathbb{E}_{q_{\vec \phi}(\vec z|\vec x)}\big[ \log p_{\vec \theta}(\vec x|\vec z) \big] &= \mathbb{E}_{\color{red}{\vec \epsilon \sim\mathcal{N}(\vec \epsilon; \vec 0, \vec 1)}}\big[ \log p_{\vec \theta}(\vec x|\vec z) \big]\\
&\approx \frac{1}{L} \sum_{l=1}^{L} \log p_{\vec \theta}(\vec x|\vec z_l)
\end{align*}
Solution: Define \color{red}{$\vec z = \vec \epsilon \cdot \vec \sigma_{\vec z}(\vec x )+ \vec \mu_{\vec z}(\vec x ) $} \color{black}{ where} \color{red}{$\vec \epsilon \sim\mathcal{N}(\vec \epsilon; \vec 0, \vec 1)$. }
}
\only<4>{
Back to our problem:
\begin{align*}
\mathbb{E}_{q_{\vec \phi}(\vec z|\vec x)}\big[ \log p_{\vec \theta}(\vec x|\vec z) \big] &= \mathbb{E}_{\color{red}{\vec \epsilon \sim\mathcal{N}(\vec \epsilon; \vec 0, \vec 1)}}\big[ \log p_{\vec \theta}(\vec x|\color{red}{\vec z = \vec \epsilon \cdot \vec \sigma_{\vec z}(\vec x )+ \vec \mu_{\vec z}(\vec x ) }\color{black}{) \big]}\\
&\approx \frac{1}{L} \sum_{l=1}^{L} \log p_{\vec \theta}(\vec x|\vec z_l)
\end{align*}
Solution: Define \color{red}{$\vec z = \vec \epsilon \cdot \vec \sigma_{\vec z}(\vec x )+ \vec \mu_{\vec z}(\vec x ) $}\color{black}{ where} \color{red}{$\vec \epsilon \sim\mathcal{N}(\vec \epsilon; \vec 0, \vec 1)$. }
}
\only<5>{
Back to our problem:
\begin{align*}
\mathbb{E}_{q_{\vec \phi}(\vec z|\vec x)}\big[ \log p_{\vec \theta}(\vec x|\vec z) \big] &= \mathbb{E}_{\color{red}{\vec \epsilon \sim\mathcal{N}(\vec \epsilon; \vec 0, \vec 1)}}\big[ \log p_{\vec \theta}(\vec x|\color{red}{\vec z = \vec \epsilon \cdot \vec \sigma_{\vec z}(\vec x )+ \vec \mu_{\vec z}(\vec x ) }\color{black}{) \big]}\\
&\approx \frac{1}{L} \sum_{l=1}^{L} \log p_{\vec \theta}(\vec x|\color{red}{\vec z_l = \vec \epsilon_l \cdot \vec \sigma_{\vec z}(\vec x )+ \vec \mu_{\vec z}(\vec x ) } )
\end{align*}
Solution: Define \color{red}{$\vec z = \vec \epsilon \cdot \vec \sigma_{\vec z}(\vec x )+ \vec \mu_{\vec z}(\vec x ) $}\color{black}{ where} \color{red}{$\vec \epsilon \sim\mathcal{N}(\vec \epsilon; \vec 0, \vec 1)$. }
}

\end{frame}



\frame{
\frametitle{VAE-Training with Backpropagation}

Due to the reparametrization trick, we can simultaneously train both the generative model  $p_{\vec \theta}(\vec x| \vec z)$  and the inference model $q_{\vec \phi}(\vec z| \vec x)$   by maximizing the ELBO based on gradient asscent and backpropagation.



\begin{figure}
\centering
\includegraphics[width=6cm]{plots/VAE-BackProp.png}
\end{figure}

}


\frame{
\frametitle{Latent Variables learned by a VAE}

\begin{figure}
\centering
\includegraphics[width=5cm]{plots/LatentVariables-FF.png}
\includegraphics[width=6cm]{plots/LatentVariables-MNIST.png}
\caption{Goodfellow et al.,  2016}
\end{figure}

}


\frame{
\frametitle{VAE with deep encoder/decoder: Components collapse}

\begin{figure}
\centering
\includegraphics[width=12cm]{plots/VAE-componentsCollaps1.png}
\end{figure}

}


\frame{
\frametitle{VAE with deeper encoder/decoder: More components collapse}

\begin{figure}
\centering
\includegraphics[width=12cm]{plots/VAE-componentsCollaps2.png}
\end{figure}


}




\frame{
\frametitle{Samples from a vanilla VAE}

\begin{figure}
\centering
\includegraphics[width=5cm]{plots/SamplesVAE-Faces.png}
%\hspace{0.3cm}
\includegraphics[width=6cm]{plots/SamplesVAE-ImageNet.png}
\end{figure}
%\vspace{-1cm}
\texttt{ \small{ Labeled Faces in the Wild (LFW)}    \;\;\;\;\;\;\;\;\;\;\;\ \small{ImageNet (small)}}
%
}


\section{Generative Adversarial Networks (GANs)}

\begin{frame} {Generative Adversarial Networks (GANs)}

Generative adversarial networks
\begin{itemize}
\item define the generative model as in VAEs,
\item but approach problem of learning a directed generative model $p(\mathbf{x}| \mathbf{z})$ from a totally different perspective.

\end{itemize}
  \begin{figure}
\includegraphics[width=0.37\framewidth]{plots/VAE.png}
\includegraphics[width=5cm]{plots/VAE-generativeModel2.png}

 \end{figure}

\end{frame}

\begin{frame} {What's a GAN?}
  \begin{figure}
    \centering
      \scalebox{0.75}{\includegraphics{plots/gan.png}}
      \tiny{\\Source : dl4j}
  \end{figure}
  \begin{itemize}
    \item In its simplest form, a GAN consists of two deep neural networks:
      \begin{itemize}
        \item the generator
        \item the discriminator
      \end{itemize}
    \item The generator is fed a random noise vector which it transforms into a fake sample in a given domain.
    \item The discriminator is fed both real and fake samples and outputs a number between 0 and 1 indicating the probability of the input being real.
  \end{itemize}
\end{frame}

\begin{frame} {What's a GAN?}
  \begin{figure}
    \centering
      \scalebox{0.5}{\includegraphics{plots/gan.png}}
      \tiny{\\Source : dl4j}
  \end{figure}
  \begin{itemize}
    \item The goal of the generator is to fool the discriminator into thinking that the synthesized samples are real
    \vspace{4mm}
    \item The goal of the discriminator is to accurately recognize real samples and not be fooled by the generator.
    \vspace{4mm}
    \item This sets off an arms race. As the generator gets better at producing realistic samples, the discriminator is forced to get better at detecting the fake samples which in turn forces the generator to get even better at producing realistic samples and so on.
  \end{itemize}
\end{frame}

\begin{frame} {GANs : Fake currency illustration}
\vspace{15mm}
  The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistinguishable from the genuine articles. \\
\hspace{45mm} -Ian Goodfellow
\end{frame}


\begin{frame} {Minimax Loss for GANs}
  \begin{tcolorbox}
  $\min \limits_G \max \limits_D V(D,G) = \E_{\mathbf{x}\sim p_{\text{data}(\mathbf{x})}}[\log D(\mathbf{x})] + \E_{\mathbf{z}\sim p(\mathbf{z})}[\log (1 - D(G(\mathbf{z})))]$
  \end{tcolorbox}
   \begin{itemize}
  \vspace{6mm}
      \item $p_{\text{data}(\mathbf{x})}$ is our target, the data distribution.
  \vspace{6mm}
  \item Recall, that we defined the generator to be neural network mapping a latend random vector $\mathbf{z}$ to generated samples $G(\mathbf{z})$. Thus even if the generator is a determinisic function, we have random outputs, i.e. variability. 
      %\item Because a neural network (the generator) is a deterministic function, we feed a latent random vector $\mathbf{z}$ to the generator to induce variability in its outputs.
  \vspace{6mm}
      \item $p(\mathbf{z})$ is usually a uniform distribution or an isotropic Gaussian. It is typically fixed and not adapted during training.

   \end{itemize}
\end{frame}

\begin{frame} {Minimax Loss for GANs}
  \begin{tcolorbox}
    $\min \limits_G \max \limits_D V(D,G) = \E_{\mathbf{x}\sim p_{\text{data}(\mathbf{x})}}[\log D(\mathbf{x})] + \E_{\mathbf{z}\sim p(\mathbf{z})}[\log (1 - D(G(\mathbf{z})))]$
  \end{tcolorbox}
  \begin{itemize}
  \vspace{6mm}
    \item $G(\mathbf{z})$ is the output of the generator for a given state $\mathbf{z}$ of the latent variables.
  \vspace{6mm}
      \item $D(\mathbf{x})$ is the output of the discriminator for a real sample $\mathbf{x}$.
  \vspace{6mm}
      \item $D(G(\mathbf{z}))$ is the output of the discriminator for a fake sample $G(\mathbf{z})$ synthesized by the generator.
  \end{itemize}
\end{frame}


\begin{frame} {Minimax Loss for GANs}
  \begin{tcolorbox}
    $\min \limits_G \max \limits_D V(D,G) = \E_{\mathbf{x}\sim p_{\text{data}(\mathbf{x})}}[\log D(\mathbf{x})] + \E_{\mathbf{z}\sim p(\mathbf{z})}[\log (1 - D(G(\mathbf{z})))]$
  \end{tcolorbox}
  \begin{itemize}
    \item Roughly speaking, $\E_{\mathbf{x}\sim p_{\text{data}(\mathbf{x})}}[\log D(\mathbf{x})]$ is the log-probability of correctly classifying real data points as real. 
  \vspace{2mm}
    \item $\E_{\mathbf{z}\sim p(\mathbf{z})}[\log (1 - D(G(\mathbf{z})))]$ is the log-probability of correctly classifying fake samples as fake.
  \vspace{2mm}
    \item Therefore, with each gradient update, the discriminator tries to push $D(\mathbf{x})$ toward 1 and $D(G(\mathbf{z})))$ toward 0. This is the same as maximizing V(D,G).
  \vspace{2mm}
    \item The generator, on the other hand, only has control over $D(G(\mathbf{z}))$ and tries to push that toward 1 with each gradient update. This is the same as minimizing V(D,G).
  \end{itemize}
\end{frame}

\begin{frame} {GAN training : Pseudocode}
  \begin{algorithm}[H]
  \footnotesize
    \caption{Minibatch stochastic gradient descent training of GANs. The number of steps ,$k$ to apply to the discriminatoris a hyperparameter}
    \begin{algorithmic}[1]
      \For{number of training iterations}
        \For{k steps}
          \State Sample minibatch of $m$ noise samples $\{\mathbf{z}^{(1)} \ldots \mathbf{z}^{(m)}$\} from the noise prior $p_g(\mathbf{z})$
          \State Sample minibatch of $m$ examples $\{\mathbf{x}^{(1)} \ldots \mathbf{x}^{(m)}$\} from the data \item[]
 generating distribution $p_{\text{data}}(\mathbf{x})$.
          \State Update the discriminator by ascending its stochastic gradient: \item[]
  \hspace{2.5 cm}          $\nabla_{{\theta}_d} \frac {1}{m} \sum \limits_{i=1} \limits^{m} \left [ \log D(\mathbf{x}^{(i)}) + \log (1 - D(G(\mathbf{z}^{(i)}))) \right]$
            % + \log (1 - D(G(\mathbf{z}^{(i)})))}\right]$
        \EndFor
        \State Sample minibatch of $m$ noise samples $\{\mathbf{z}^{(1)} \ldots \mathbf{z}^{(m)}$\} from the noise prior $p_g(\mathbf{z})$
        \State Update the generator by descending its stochastic gradient: \item[]
   \hspace{2.5 cm}       $\nabla_{{\theta}_g} \frac {1}{m} \sum \limits_{i=1} \limits^{m} \log (1 - D(G(\mathbf{z}^{(i)})))$
      \EndFor
    \end{algorithmic}
  \end{algorithm}
\end{frame}

\begin{frame} {GAN training : Illustration}
  \begin{figure}
    \centering
      \scalebox{0.75}{\includegraphics{plots/gan_training.png}}
      \tiny{\\Source : Pascual et al (2017)}
  \end{figure}
  \begin{itemize}
    \item For $k$ steps, G's parameters are frozen and one performs \textbf{gradient ascent} on D to increase it's accuracy.
    \item Finally, D's parameters are frozen and one performs \textbf{gradient descent} on G to increase it's generation performance. %/ to make D misclassify.
    \item Note, that G gets to peek at D's internals (from the back-propogated errors) but D doesn't get to peek at G.
  \end{itemize}
\end{frame}

\begin{frame} {Adversarial Training}
  \begin{itemize}
  \vspace{12mm}
    \item So, GANs have intuitive appeal and produce state-of-the-art results but the reason they're such an important breakthrough is that they introduce a whole new paradigm to training deep neural networks: Adversarial Training.
    \vspace{4mm}
    \item Compared to plain-vanilla gradient descent, %we'll see throughout the lecture that
    adversarial training is a whole different beast; one that we haven't figured out how to tame (yet).
    \vspace{4mm}
    \item To illustrate this, let's look at a simple example.
  \end{itemize}
\end{frame}

\begin{frame} {Adversarial Training: A Toy Example}
  \begin{figure}
    \centering
      \scalebox{0.3}{\includegraphics{plots/adv_xy.png}}
  \end{figure}
  \begin{itemize}
    \item Consider the function $f(x,y) = xy$, where $x$ and $y$ are both scalars.
    \item Player A can control $x$ and Player B can control $y$
    \item The loss:
      \begin{itemize}
        \item Player A : $L_{A}(x,y) = xy$
        \item Player B : $L_{B}(x,y) = -xy$
      \end{itemize}
    \item This can be rewritten as $L(x,y) = \min \limits_x \max \limits_y xy$
    \item What we have here is a simple zero-sum game with its characteristic minimax loss
  \end{itemize}
\end{frame}

\begin{frame} {Possible behaviour \#1: Convergence}
  \begin{figure}
    \centering
      \scalebox{0.3}{\includegraphics{plots/adv_xy.png}}
  \end{figure}
  \begin{itemize}
    \item The partial derivatives of the losses are:
     \begin{equation*}
       \frac {\partial{L_{A}}}{\partial x} = y \text{ , }
       \frac {\partial{L_{B}}}{\partial y} = -x
     \end{equation*}
    \item In adversarial training, both players perform gradient descent on their respective losses.
    \item To perform simultaneous gradient descent, we update $x$ with $x - \alpha \cdot y$ and $y$ with $y + \alpha \cdot x$ simultaneously in one iteration, where $\alpha$ is the learning rate.
  \end{itemize}
\end{frame}

\begin{frame} {Possible behaviour \#1: Convergence}
  \begin{figure}
    \centering
      \scalebox{0.3}{\includegraphics{plots/adv_xy.png}}
  \end{figure}
  \begin{itemize}
    \item In order for simultaneous gradient descent to converge to a fixed point, both gradients have to be simultaneously 0.
    \item They are both (simultaneously) zero only for the point (0,0).
    \item This is a saddle point of the function $f(x,y) = xy$.
    \begin{itemize}
       \item The fixed point for a minimax game is typically a saddle point.
      \item Such a fixed point is an example of a Nash equilibrium.
    \end{itemize}
    \item In adversarial training, convergence to a fixed point is \textbf{NOT} guaranteed.
  \end{itemize}
\end{frame}

\begin{frame} {Possible behaviour \#2: Chaotic behaviour}
  \begin{figure}
    \centering
      \scalebox{0.6}{\includegraphics{plots/sim_grad.png}}
      \tiny{\\Credit : Lilian Weng}
      \caption{\footnotesize A simulation of our example for updating x to minimize xy and updating y to minimize -xy. The learning rate eta = 0.1. With more iterations, the oscillation grows more and more unstable.}
  \end{figure}
  \begin{itemize}
    \item Once $x$ and $y$ have different signs, every following gradient update causes huge oscillation and the instability gets worse in time, as shown in the figure.
  \end{itemize}
\end{frame}


\begin{frame} {Possible behaviour \#3: Cycles}
  \begin{figure}
    \centering
      \scalebox{0.3}{\includegraphics{plots/adv_cycle.png}}
      \tiny{\\Credit : Goodfellow}
      \caption{\footnotesize Simultaneous gradient descent with an infinitesimal step size can result in a circular orbit in the parameter space}
  \end{figure}
  \begin{itemize}
    \item A Discrete Example: A never-ending game of Rock-Paper-Scissors where player A chooses 'Rock' $\rightarrow$ player B chooses 'Paper' $\rightarrow$ A chooses 'Scissors' $\rightarrow$ B chooses 'Rock' $\rightarrow$ ...
   % \item In GAN training, this happens when the generator keeps switching the category of the samples generated without a noticeable improvement in image quality for any category.
   \item  \textbf{Moral:} Adversarial training is highly unpredictable. It can go in circles or become chaotic.
  \end{itemize}
\end{frame}


\begin{frame} {Non-stationary loss surface}
   \begin{itemize}
     \item Once again, it is extremely important to note that from the perspective of one of the players, the loss surface changes every time the other person makes a move.
  \vspace{2mm}
     \item This is in stark contrast to the (full batch) gradient descent case where the loss surface is stationary no matter how many iterations of gradient descent are performed.
%  \vspace{2mm}
   %  \item It's \textit{somewhat} analogous to a game of football played by two players where the goalposts for a given player changes position every time the opponent kicks the ball and vice versa!
%   \vspace{2mm}
 %   \item The only way this game ends(\textit{if} it ends, that is) is if the ball goes through both players' goalposts simultaneously.
%   \vspace{2mm}
%    \item This is what a saddle point represents. It is simultaneously a minimum for the first player and a maximum for the second player.

   \end{itemize}
 \end{frame}

\begin{frame} {Illustration of Convergence}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/illus_conv_one.png}}
      \tiny{\\Credit : Mark Chang}
  \end{figure}
\end{frame}

\begin{frame} {Illustration of Convergence : Final Step}
  \begin{figure}
    \centering
      \scalebox{0.9}{\includegraphics{plots/illus_conv_two.png}}
      \tiny{\\Credit : Mark Chang}
  \end{figure}
  Such convergence is not guaranteed, however.
\end{frame}

%\begin{frame} {Toy Example : Estimating a 1D Gaussian}
%  \vspace{10mm}
%  \begin{itemize}
%    \item Our target distribution is a simple Gaussian with mean 4 and standard deviation of 0.5.
%    \begin{figure}
%    \centering
%      \scalebox{0.66}{\includegraphics{plots/toy_gaussian.png}}
%      \tiny{\\Source : Aylien}
%  \end{figure}
%  \end{itemize}
%\end{frame}
%
%\begin{frame} {Toy Example : Estimating a 1D Gaussian}
%  \begin{itemize}
%    \item Our generator
%      \begin{itemize}
%        \item Takes a scalar noise input
%        \item Has only a single hidden layer with a softplus activation
%        \item The output layer has only a single neuron with a linear activation
%      \end{itemize}
%  \vspace{4mm}
%    \item Our discriminator
%      \begin{itemize}
%        \item Has 3 hidden layers with 'tanh' activations
%        \item The output layer once again has a single neuron with a sigmoid activation
%      \end{itemize}
%  \vspace{4mm}
%    \item Loss : Non-saturating Loss
%  \vspace{4mm}
%    \item Optimizer : Gradient Descent with exponential learning rate decay
%  \end{itemize}
%\end{frame}
%
%\begin{frame} {Toy Example : Estimating a 1D Gaussian}
%
%  \begin{itemize}
%  \item After training, the the two distributions look like this:
%  \begin{figure}
%    \centering
%      \scalebox{0.75}{\includegraphics{plots/gan_gaussian_post.png}}
%      \tiny{\\Source : Aylien}
%  \end{figure}
%  \item This makes sense intuitively.If the generator just produces the mean value of the real data in this simple example, then it is going to be quite likely to fool the discriminator.
%  \end{itemize}
%\end{frame}



\begin{frame} {Divergence measures}
  \begin{itemize}
    \item Recall that the goal of generative modeling is to learn $p_{\text{data}}(\mathbf{x})$.
    \vspace{2mm}
    \item In order to understand the differences between different generative models, it is sometimes helpful to consider them from the angle of \textbf{divergence measures}.
    \vspace{2mm}
    \item A divergence measure quantifies the distance between two distributions. It is a measure of how different one distribution is from another.
    \vspace{2mm}
    \item There are many different divergence measures that one can use here (such as the Kullback-Liebler divergence).
    \vspace{2mm}
    \item One thing that all such measures have in common is that they are 0 if and only if the two distributions are equal to each other (otherwise, they are all positive).
  \end{itemize}
\end{frame}

\begin{frame} {Divergence measures}
  \begin{itemize}
    \small{\item One approach to training generative models, then, is to explicitly minimize the distance between $p_{\text{data}}(\vec x)$ and the model distribution $p(\vec x)$ according to some divergence measure.
    \vspace{2mm}
    \item If our generator has the capacity to model $p_{\text{data}}(\vec x)$ perfectly, the choice of divergence doesn't matter much because they all achieve their minimum (that is,0) when $p(\vec x) = p_{\text{data}}(\vec x)$.
    \vspace{2mm}
    \item However, it is not likely that that the generator, which is  parametrized by the weights of a neural network, is capable of perfectly modelling an arbitrary $ p_{\text{data}}(\vec x)$.
    \vspace{2mm}
    \item In such a scenario, the choice of divergence measure matters, because the parameters that miniminize the various divergence measures differ.}
  \end{itemize}
\end{frame}

\begin{frame} {Implicit Divergence measure of GANs}
  \begin{itemize}
   \item GANs do not explicitly minimize any divergence measure.
   \item However, (under some assumptions!) optimizing the minimax loss is equivalent to implicitly minimizing a divergence measure.
    \item That is, if the optimal discriminator is found in every iteration,  the generator minimizes %a divergence measure between distributions known as
    the  \textbf{Jensen-Shannon divergence  (JSD)} (theorem and proof are given by the original GAN paper (Goodfellow et al, 2014)):
    \begin{align*}
      JS(p_{\text{data}}|p) & =  \frac{1}{2} KL(p_{\text{data}}||\frac{p_{\text{data}}+p}{2}) + \frac{1}{2}  KL(p || \frac {p_{\text{data}} + p}{2}) \\
        KL(p_{\text{data}}|p) & = E_{\vec x \sim p_{\text{data}}(\vec x)}[log \frac {p_{\text{data}}(\vec x)}{p(\vec x)}]
    \end{align*}
 % \vspace{2mm}
%    \item A generator with sufficient capacity successfully learns the target distribution
 % \vspace{2mm}
 %   \item Otherwise, it exhibits "mode-seeking" behaviour.
%  \vspace{2mm}
%    \item Researchers originally speculated that this behaviour is the reason GANs produce stunningly realistic samples.
  \end{itemize}
\end{frame}


\begin{frame} {Optimal Discriminator}

  For G fixed, the optimal discriminator D is:
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics[width=7cm]{plots/opt_discriminator.png}}
      \tiny{\\Credit : Mark Chang}
  \end{figure}
  \begin{itemize}
  \item  Note: The optimal solution is almost never found in practice, since the discriminator has a finite capacity and is trained on a finite amount of data.
  \item Therefore, the assumption needed to guarantee that the generator minimizes the JSD does usually not hold in practice.
  \end{itemize}
\end{frame}

%
\begin{frame} {Trade-offs made by some common divergences}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/div_tradeoffs.png}}
      \tiny{\\Source : Theis et al 2016}
      \caption{\small : An isotropic Gaussian distribution was fit to data drawn from a mixture of Gaussians by either minimizing Kullback-Leibler divergence (KLD), maximum mean discrepancy (MMD), or
Jensen-Shannon divergence (JSD). The different fits demonstrate different tradeoffs made by the
three measures of distance between distributions.}
  \end{figure}
\end{frame}


\begin{frame} {Trade-offs made by some common divergences}

  \begin{figure}
    \centering
      \scalebox{0.8}{\includegraphics{plots/implicit_div.png}}
      \tiny{\\Credit : Aiden NIbali}
  \end{figure}
  \begin{itemize}
    \item In this simplified 1-dimensional example, $p_{\text{data}}(x)$ is a bimodal distribution, but $p(x)$ only has the modelling capacity of a single Gaussian.
    \vspace{2mm}
    \item Therefore, based on the divergence measure, $p(x)$ can either fit a single mode really well, i.e.~be `mode-seeking´(e.g.~JSD), or attempt to cover both modes, i.e. be mode-covering´(e.g.~KLD).
  \end{itemize}
\end{frame}

\begin{frame} {Non-Saturating Loss}
  \begin{figure}
    \centering
      \scalebox{0.85}{\includegraphics{plots/ns_loss.png}}
      \tiny{\\Credit : Daniel Seita}
  \end{figure}
  \begin{itemize}
    \item It was discovered that a relatively strong discriminator could completely dominate the generator.
    \item %The reason for this is that
   When optimizing the minimax loss, as the discriminator gets good at identifying fake images, i.e.~as $D(G(\mathbf{z}))$ approaches 0, the gradient with respect to the generator parameters vanishes.

  \end{itemize}
\end{frame}

\begin{frame} {Non-Saturating Loss}
  \begin{figure}
    \centering
      \scalebox{0.85}{\includegraphics{plots/ns_loss.png}}
      \tiny{\\Credit : Daniel Seita}
  \end{figure}
  \begin{itemize}
    \item Solution: Use a non-saturating generator loss instead:  $J^{(G)} = - \frac{1}{2} \E_{\vec z \sim p(\vec z)} [\log D(G(\mathbf{x}))]$
    \item In contrast to the minimax loss, when the discriminator gets good at identifying fake images, the magnitude of the gradient of $J^{(G)}$ increases and the generator is able to learn to produce better images in successive iterations.
  \end{itemize}
\end{frame}
\begin{frame} {Other loss functions}

Various minimax losses for GAN training with different properties have been proposed:

  \vspace{10mm}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/other_losses.png}}
      \tiny{\\Source : Lucic et al 2016}
  \end{figure}
\end{frame}


\begin{frame} {Explosive growth in the number of (named) GAN papers}

Understanding and improving GAN training is a very active area of research.

  \begin{figure}
    \centering
      \scalebox{0.8}{\includegraphics{plots/named_gans.png}}
      \tiny{\\Credit : hindupuravinash}
  \end{figure}
\end{frame}



\begin{frame} {Conditional GANs: Motivation}
  \begin{itemize}
    \item In an ordinary GAN, the only thing that is fed to the generator are the latent variables $\mathbf{z}$.
    \item A conditional GAN allows you to condition the generative model on additional variables. %have more control over the samples produced by the generator. This makes it very easy to work with multiple modalities.
    \item E.g. a generator conditioned on text input (in addition to $\mathbf{z}$) can be trained to generate the image described by the text.
  \end{itemize}
\end{frame}

\begin{frame} {Conditional GANs : Architecture}
  \begin{figure}
    \centering
      \scalebox{0.75}{\includegraphics{plots/cgan_arch.png}}
      \tiny{\\Credit : Guim Perarnau}
  \end{figure}
  \begin{itemize}
    \item In a conditional GAN, additional information in the form of vector $\vec y$  is fed to both the generator and the discriminator.
    \item $\vec z$  can then encode all  variations in $\vec z$ that are not encoded by $\vec y$.
    \item E.g.~ $\vec y$ could encode the class of a hand-written number (from 0 to 9). Then,  $\vec z$ could encode  the style of the number (size, weight, rotation, etc).
  \end{itemize}
\end{frame}

%\begin{frame} {Conditional GANs : Loss}
%\end{frame}

\begin{frame} {Conditional GANs: Example}
  \vspace{10mm}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/cgan_mnist.png}}
      \tiny{\\Source : Mirza et al 2014}
  \end{figure}
\end{frame}

\begin{frame} {Conditional GANs: More Examples}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/congan.png}}
       \tiny{\\Source : Isola et al 2016}
 \end{figure}
\end{frame}


\begin{frame} {More Generative Models}
  \begin{itemize}
   \vspace{8mm}
   \item Today, we learned about to kinds of (directed) generative models:
      \begin{itemize}
        \item Variational Autoencoders (VAEs)
        \item Generative Adversarial Networks (GANs).
      \end{itemize}
   \item There are more interesting generative models, e.g.:
      \begin{itemize}
        \item autoregressive models
        \item restricted Boltzmann machines.
      \end{itemize}
    \item Note:
      \begin{itemize}
        \item It's important to bear in mind that generative models are not a solved problem.
        \item There are many interesting hybrid models that combine two or more of these approaches.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Acknowledgements}

Big thanks to
\begin{itemize}
\item \textbf{Christian Igel} (University of Copenhagen) for providing his material on graphical models
\item \textbf{Aaron Courville} (Univerite de Montreal) for providing his material on VAEs
\end{itemize}
which helped creating this lecture.

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}
\frametitle{References}
\footnotesize{
\begin{thebibliography}{99}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Zhang et al., 2017]{1} Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris Metaxas (2017)
\newblock StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks
\newblock \emph{\url{https://arxiv.org/abs/1612.03242}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Wang et al., 2017]{2} Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, Bryan Catanzaro (2017)
\newblock High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs
\newblock \emph{\url{https://arxiv.org/abs/1711.11585}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Demir et al., 2018]{3} Ugur Demir, Gozde Unal (2018)
\newblock Patch-Based Image Inpainting with Generative Adversarial Networks
\newblock \emph{\url{https://arxiv.org/abs/1803.07422}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Karras et al., 2018]{4} Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen (2018)
\newblock Progressive Growing of GANs for Improved Quality, Stability, and Variation
\newblock \emph{\url{https://arxiv.org/abs/1710.10196}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Goodfellow et al., 2014]{5} Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio (2014)
\newblock Generative Adversarial Networks
\newblock \emph{\url{https://arxiv.org/abs/1406.2661}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Pascual et al., 2017]{6} Santiago Pascual, Antonio Bonafonte, Joan Serra (2017)
\newblock SEGAN: Speech Enhancement Generative Adversarial Network
\newblock \emph{\url{https://arxiv.org/abs/1703.09452}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Goodfellow, 2016]{7} Ian Goodfellow (2016)
\newblock NIPS 2016 Tutorial: Generative Adversarial Networks
\newblock \emph{\url{https://arxiv.org/abs/1701.00160}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Weng, 2017]{8} Lilian Weng (2017)
\newblock From GAN to WGAN
\newblock \emph{\url{https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Chang, 2016]{9} Mark Chang (2016)
\newblock Generative Adversarial Networks
\newblock \emph{\url{https://www.slideshare.net/ckmarkohchang/generative-adversarial-networks}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Theis et al., 2016]{10} Lucas Theis, Aaron van den Oord, Matthias Bethge (2016)
\newblock A note on the evaluation of generative models
\newblock \emph{\url{https://arxiv.org/abs/1511.01844}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Nibali, 2016]{11} Aiden Nibali (2016)
\newblock The GAN objective, from practice to theory and back again
\newblock \emph{\url{https://aiden.nibali.org/blog/2016-12-21-gan-objective/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Mirza, 2014]{12} Mehdi Mirza, Simon Osindero (2014)
\newblock Conditional Generative Adversarial Nets
\newblock \emph{\url{https://arxiv.org/abs/1411.1784}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Isola et al., 2016]{13} Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros (2016)
\newblock Image-to-Image Translation with Conditional Adversarial Networks
\newblock \emph{\url{https://arxiv.org/abs/1611.07004}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Perarnau, 2017]{14} Guim Perarnau (2017)
\newblock Fantastic GANs and where to find them
\newblock \emph{\url{https://guimperarnau.com/blog/2017/03/Fantastic-GANs-and-where-to-find-them}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{thebibliography}
}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlecture
