<<setup-child, include = FALSE>>=
    library(knitr)
source('./code/helper_conv.R')
# amsmath in preamble, whut?
set_parent("../style/preamble_mina.Rnw")
knitr::opts_chunk$set(cache=FALSE)
@
    
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-nn}

\lecturechapter{5}{Variants of convolutions}
\lecture{Deeplearning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Lecture outline}
\tableofcontents
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Padding}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{

\frametitle{Padding}

  \begin{itemize}

    \only<1-3>{\item \enquote{Valid} convolution without padding.}
    \only<1>{\item[] Exactly what we just did is called valid convolution. Suppose we have an input of size $5 \times 5$ and a filter of size $2$. }
    \only<2>{\item[] The filter is only allowed to move inside of the input space.}
    \only<3>{\item[] That will inevitably reduce the output dimensions.}
    
  \end{itemize}

  \center
  \only<1>{\scalebox{0.92}{\includegraphics{plots/05_conv_variations/valid/valid_conv_2.jpg}}}%
  \only<2>{\scalebox{1.05}{\includegraphics{plots/05_conv_variations/valid/valid_conv_3.jpg}}}%
  %\only<3>{\includegraphics[width=10cm]{plots/05_conv_variations/valid/valid2.png}}%
  %\only<4>{\includegraphics[width=10cm]{plots/05_conv_variations/valid/valid3.png}}%
  \only<3>{\scalebox{0.92}{\includegraphics{plots/05_conv_variations/valid/valid_conv_1n.png}}}%
  
  \only<3>{\item[] In general, for an input of size $i \:(\times \:i)$ and filter size $k \:(\times \:k)$, the size $o \:(\times \:o)$ of the output feature map is:
    $$ o=  i-k + 1 $$ }
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{

\frametitle{Padding}

  \begin{itemize}

    \only<1-4>{\item Convolution with \enquote{same} padding.}
    \only<1>{\item[] Suppose the following situation: an input with dimensions $5x5$ and a filter with size $3$.}
    \only<2>{\item[] We would like to obtain an output with the same dimensions as the input.}
    \only<3>{\item[] Hence, we apply a technique called zero padding. That is to say \enquote{pad} zeros around the input:}
    \only<4>{\item[] That always works! We just have to adjust the zeros according to the input dimensions and filter size (ie. one, two or more rows).}

  \end{itemize}

  \center
  \only<1>{\includegraphics[width=11cm]{plots/05_conv_variations/same/same0.png}}%
  \only<2>{\includegraphics[width=11cm]{plots/05_conv_variations/same/same1.png}}%
  \only<3>{\includegraphics[width=11cm]{plots/05_conv_variations/same/same2.png}}%
  \only<4>{\includegraphics[width=11cm]{plots/05_conv_variations/same/same7.png}}%

}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{

\frametitle{Padding and Network Depth}

\begin{figure}
\center
  \scalebox{0.58}{\includegraphics{plots/zeropadding.png}}
\caption{\small{\enquote{Valid} versus \enquote{same} convolution. \emph{Top} : Without padding, the width of the feature map shrinks rapidly to 1 after just three convolutional layers (filter width of 6 shown in each layer). This limits how deep the network can be made. {Bottom} : With zero padding (shown as solid circles), the feature map can remain the same size after each convolution which means the network can be made arbitrarily deep. (Goodfellow, \emph{et al.}, 2016, ch.~9)}}
\end{figure}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Strides}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frame{

\frametitle{Strides}

  \begin{itemize}

    \only<1-3>{\item Stepsize \enquote{strides} of our filter (stride = 2 shown below).}

  \end{itemize}

  \center
  \only<1>{\scalebox{0.7}{\includegraphics{plots/05_conv_variations/strides/strides0.png}}}%
  \only<2>{\scalebox{0.7}{\includegraphics{plots/05_conv_variations/strides/strides1.png}}}%
  %\only<3>{\includegraphics[width=9cm]{plots/05_conv_variations/strides/strides2.png}}%
  %\only<4>{\includegraphics[width=9cm]{plots/05_conv_variations/strides/strides3.png}}%
  \only<3>{\scalebox{0.7}{\includegraphics[width=9cm]{plots/05_conv_variations/strides/strides4.png}}}%
  
  \only<3>{\item[] In general, when there is no padding, for an input of size $i$, filter size $k$ and stride $s$, the size $o$ of the output feature map is:
    $$ o=\left\lfloor\frac{i-k}{s}\right\rfloor+ 1 $$ }

}

\frame{

\frametitle{Strides and downsampling}

\begin{figure}
\center
\includegraphics[width=.5\textwidth]{plots/stride.png}
\caption{A strided convolution is equivalent to a convolution without strides followed by downsampling (Goodfellow, \emph{et al.}, 2016, ch.~9).}
\end{figure}


}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\section{Pooling}

\frame{
\frametitle{Max Pooling}

  
  
  \center
  \only<1>{\includegraphics[width=10cm]{plots/08_pooling/pool1.png}}%
  \only<2>{\includegraphics[width=10cm]{plots/08_pooling/pool3.png}}%
  \only<3>{\includegraphics[width=10cm]{plots/08_pooling/pool6.png}}%
  %\only<4>{\includegraphics[width=10cm]{plots/08_pooling/pool8.png}}%
  
  \begin{itemize}
    \only<1>{\item We've seen how convolutions work, but there is one other operation we need to understand.}
    \only<1>{\item We want to downsample the feature map but optimally lose no information.}
    \only<2>{\item Applying the max pooling operation, we simply look for the maximum value at each spatial location.}
    \only<2>{\item That is 8 for the first location.}
    \only<2>{\item  Due to the filter of size 2 we halve the dimensions of the original feature map and obtain downsampling.}
    \only<3>{\item The final pooled feature map has entries 8, 6, 9 and 3.
    \item Popular pooling functions: max and (weighted) average.}
    %\only<4>{\item Blurring the image by randomly changing pixel entries by either +1 or -1 will only marginally change activations.}
    %\only<4>{\item Next to the dimension and thus parameter reduction, pooling can increase the robustness of the net.}
  \end{itemize}
}

\frame{
\frametitle{Average Pooling}

  \center
  \only<1>{\includegraphics[width=10cm]{plots/08_pooling/avgpool0.png}}%
  \only<2>{\includegraphics[width=10cm]{plots/08_pooling/avgpool1.png}}%
  \only<3>{\includegraphics[width=10cm]{plots/08_pooling/avgpool2.png}}%
  \only<4>{\includegraphics[width=10cm]{plots/08_pooling/avgpool3.png}}%
  
  \begin{itemize}
    \only<1>{\item We've seen how max pooling work, but there are exists other pooling operation.}
    \only<1>{\item Similar to max pooling, we downsample the feature map but optimally lose no information.}
    \only<2>{\item Applying the average pooling operation, we simply look for the mean/average value at each spatial location.}
    \only<2>{\item That is 8 for the first location.}
    \only<2>{\item  Due to the filter of size 2 we halve the dimensions of the original feature map and obtain downsampling.}
    \only<3>{\item The final pooled feature map has entries 8, 6, 9 and 3.
    \item Popular pooling functions: max and (weighted) average.}
    %\only<4>{\item Blurring the image by randomly changing pixel entries by either +1 or -1 will only marginally change activations.}
    %\only<4>{\item Next to the dimension and thus parameter reduction, pooling can increase the robustness of the net.}
  \end{itemize}
}


\frame{
\frametitle{Comparision of Max and Average Pooling}

  \center
  \only<1>{\includegraphics[width=10cm]{plots/08_pooling/comparepool.png}}
  
  \begin{itemize}
    \only<1>{\item Shortcomings of Max & Average Pooling using Toy Image (source: Williams and Li, ICLR 2018).}
    \only<1>{\item Similar to max pooling, we downsample the feature map but optimally lose no information.}
    \only<1>{\item Applying the average pooling operation, we simply look for the mean/average value at each spatial location.}
  \end{itemize}
}


\frame{

\frametitle{Invariance to small translation}

\begin{figure}
\center
\scalebox{0.51}{\includegraphics{plots/maxpooling.png}}
\caption{\footnotesize{Max pooling introduces invariance to small translations of the input. (\emph{Top}) A view of the middle of the output of a convolutional layer. The bottom row shows outputs of the nonlinearity. The top row shows the outputs of max pooling, with a stride of \textbf{one} pixel between pooling regions and a pooling region width of three pixels. (\emph{Bottom}) A view of the same network, after the input has been shifted to the right by one pixel. Every value in the bottom row has changed, but only half of the values in the top row have changed, because the max pooling units are sensitive only to the maximum value in the neighborhood, not its exact location (Goodfellow, \emph{et al.}, 2016, ch.~9).}}
\end{figure}


}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}
\frametitle{References}
\footnotesize{
\begin{thebibliography}{99}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Ian Goodfellow et al., 2016]{1} Ian Goodfellow, Yoshua Bengio and Aaron Courville (2016)
\newblock Deep Learning
\newblock \emph{\url{http://www.deeplearningbook.org/}}


\end{thebibliography}
}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\endlecture
