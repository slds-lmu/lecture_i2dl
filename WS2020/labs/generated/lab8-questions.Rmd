---
title: "Lab 8"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE, echo = TRUE)
knitr::opts_knit$set(root.dir = '.')
```

## Exercise 1
NB: it is impractical to do this exercise on your laptop, even if you have a GPU. You are advised to work on [Google Colab](https://colab.research.google.com/github/IRkernel/IRkernel/blob/master/example-notebooks/Demo.ipynb).

In this exercise we would like to build a classification system for 120 different breeds of dogs, based on commonplace pictures. The data is available at \url{https://www.kaggle.com/c/dog-breed-identification/data} (permanent, requires an account) or \url{https://www.dropbox.com/s/lb8bqhm24isza27/dog-breed-identification.zip?dl=0} (temporary, no login needed). Download it and unzip it, then put the contents in a folder named "data" in the same directory of this R notebook.

```{r}
dst = "./dog-breed-identification.zip"

if(!file.exists(dst)) {
  url = "https://www.dropbox.com/s/lb8bqhm24isza27/dog-breed-identification.zip?dl=1"
  download.file(url, dst)
}

if(!dir.exists("data")) {
  unzip(dst, exdir="data")
}
```

This dataset is composed of 10222 pictures in different resolutions and aspect rations. The smallest classes are Briad and Eskimo dog with only 66 images each, whereas the biggest class is the Scottish deerhound with 126 images.

Here are some sample images along with the relative label:

```{r, out.width="0.75\\textwidth", fig.align = "center"}
library(magick)

data = read.csv("data/labels.csv", stringsAsFactors = FALSE)

par(mfrow = c(3,3), mar=c(1, 0, 0, 0))
for (i in 1:9) {
  img = image_resize(image_read(paste0("data/train/", data$id[i], ".jpg")),
                     geometry = "255x255")
  plot(image_annotate(img, data$breed[i], size = 30, color = "red"))
}
```


This is a challenging problem since there are many classes and only few instances per class. Moreover, the images contain a lot of details that do not help to identify the dogs, but allow a network to easily memorize the data. We will first try to naively approach the problem and directly train a CNN on this dataset. After convincing ourselves that this is not going to work, we will use a pre-trained VGG16 (i.e., trained successfully on some other dataset) and fine-tune it to our data.

But first, we will re-organize the images by reserving a subset of the training images for validation, and creating a directory for every breed of dog.


```{r}
library(data.table)

data = fread("data/labels.csv")

breeds = unique(data$breed)
if(!dir.exists("data/train_sorted")) {
  dir.create("data/train_sorted")
  dir.create("data/val_sorted")
  
  for(dog in breeds) {
    dir.create(paste0("data/train_sorted/", dog))
    dir.create(paste0("data/val_sorted/", dog))
    ids = data[breed == dog]$id
    sp = ceiling(length(ids) / 5)
    for (id in ids[1:sp])
      file.copy(paste0("data/train/", id, ".jpg"),
                paste0("data/val_sorted/", dog, "/"))
    for (id in ids[(sp+1):length(ids)])
      file.copy(paste0("data/train/", id, ".jpg"),
                paste0("data/train_sorted/", dog, "/"))
  } 
}
```


### Data preparation
As this dataset is fairly small, we can generate more synthetic images by applying random transformations to the images we have. This technique is called _data augmentation_, and it can greatly help in reducing overfitting on small datasets.

Keras provides a function called `image_data_generator` that does this for us; please read [its documentation](https://keras.rstudio.com/reference/image_data_generator.html). Every time a new batch of data is requested, the augmentations are randomly applied on-the-fly; this saves a lot of memory, at the price of larger computational resources needed.

We now want to randomly perform the following transformations to each image:

 - Re-scale the pixel values to be between 0 and 1 (they are now between 0 and 255)
 - Flip horizontally
 - Rotation of at most 30 degrees,
 - Change brightness by at most 50%
 - Stretch horizontally and vertically by at most 20%
 - Zoom in/out by at most 20%
 - Mirror the image to fill possible blanks created by the transformations (parameter: `fill_mode`)

```{r}
library(keras)

train_data_generator = image_data_generator(
  # TODO insert the parameters as specified above
)
```


This generator can be coupled with another utility function called `flow_images_from_directory`. This function automatically loads images from the given directory, assuming that each of the diffent classes is in its own directory (hence the re-organization we previously did). It will also resize the images to a given size, create batches of suitable size, and so on. Again, read the [documentation](https://keras.rstudio.com/reference/flow_images_from_directory.html) for its complete functionality.

We do not use random augmentation for the validation images except for centering and scaling. Why?

```{r}
data_source_train = flow_images_from_directory(
  # TODO load the training images from the `train_sorted` directory,
  # apply the transformations declared earlier and resize them to 224x224
)

data_source_validation = flow_images_from_directory(
  # TODO load the validation images from the `val_sorted` directory,
  # resize them to 224x224 and rescale the pixel values to [0, 1]
)
```


Here are some examples of how the augmented images look:

```{r out.width = "0.5\\textwidth"}
x = data_source_train[["next"]]()[[1]]
for(i in 1:4) {
  img = x[i,,,]
  r = img[,,1]
  g = img[,,2]
  b = img[,,3]
  col = rgb(r, g, b)
  dim(col) = dim(r)
  grid::grid.newpage()
  grid::grid.raster(col, interpolate=FALSE)
}
```


### Define a Network
After preparing the data we define a network architecture. There are a lot of possible architectures; a good start might be a slightly smaller version of the famous VGG16 architecture. It consists of 4 blocks of 2 convolutional layers followed by one max pooling step, then two fully connected layers of size 512 are used, for a total of around 5 million weight parameters.

Global average pooling is used instead of flattening to reduce the number of parameters of the network. It takes the average of every input channel, so that a tensor of shape 14x14x512 results in a vector of 512 elements, each of which is the average of the corresponding 14x14 slice.

```{r}
model = keras_model_sequential() %>%
  # Block 1
  layer_conv_2d(filters=64, kernel_size=c(3,3), padding="same", activation="relu",
                input_shape=c(224, 224, 3)) %>%
  layer_conv_2d(filters=64, kernel_size=c(3,3), padding="same", activation="relu") %>%
  layer_max_pooling_2d(pool_size=c(2,2), stride=c(2,2)) %>%
  
  # Block 2
  layer_conv_2d(filters=128, kernel_size=c(3,3), padding="same", activation="relu") %>%
  layer_conv_2d(filters=128, kernel_size=c(3,3), padding="same", activation="relu") %>%
  layer_max_pooling_2d(pool_size=c(2,2), stride=c(2,2)) %>%
  
  # Block 3
  layer_conv_2d(filters=256, kernel_size=c(3,3), padding="same", activation="relu") %>%
  layer_conv_2d(filters=256, kernel_size=c(3,3), padding="same", activation="relu") %>%
  layer_max_pooling_2d(pool_size=c(2,2), stride=c(2,2)) %>%
  
  # Block 4
  layer_conv_2d(filters=512, kernel_size=c(3,3), padding="same", activation="relu") %>%
  layer_conv_2d(filters=512, kernel_size=c(3,3), padding="same", activation="relu") %>%
  layer_max_pooling_2d(pool_size=c(2,2), stride=c(2,2)) %>%
  
  # Flatten by global averaging
  layer_global_average_pooling_2d() %>%
  layer_dense(512, activation="relu") %>%
  layer_dense(512, activation="relu") %>%
  
  # Output for 120 Classes
  layer_dense(120, activation="softmax")
```

```{r}
summary(model)
```

### Train the network
After the network is defined, we need to `compile` it to configure it for training, and fit it using the function `fit_generator` ([documentation here](https://keras.rstudio.com/reference/fit_generator.html)). This function is used when the dataset is not available in memory (e.g., because it does not fit, or because we are augmenting images on the fly), but we can provide a function that generates the data batch-by-batch, like we are doin now.

```{r, eval = FALSE}
compile(
  # TODO compile the model with appropriate settings
)

steps_per_epoch = floor(8127 / 32)
validation_steps = floor(2095 / 32)

history = fit_generator(
  # TODO fit the model for 100 epochs,
  # periodically evaluating it on the validation data.
  # if you do not have the time, you can run less than 100 epochs.
)
```

```{r, echo=FALSE}
plot(readRDS("normal.rds"))
```


## Using a pretrained network
Even with the aid of data augmentation, the network overfits badly; this can be explained by the fact that the images are quite diverse in relation to the size of the training set. Data augmentation can only bring you so far, and even with the aid of regularization the task would be difficult. 

One popular trick to overcome this difficulty, known as _pre-training_, is to use another CNN that has been trained on a different, larger dataset, for a related task. Most of the weights of this network are then frozen (i.e., will not be updated), and the last few layers (the "head") are replaced with new, freshly re-initialized ones and learned from scratch. After the network has converged, the weights are unfrozen, and fine-tuned again. What is the rationale behind freezing and unfreezing the weights?

In Keras, this is done with the following steps:

 1. Download the network architecture without its head.
 2. Add a custom head appropriate for the new task and define this as a new Keras model.
 3. Freeze the weights of all layers except the new head.
 4. Train the network
 5. Unfreeze the weights
 6. Train the network again.

Do not forget to read the documentation for the functions you do not yet know!

```{r}
# Download Network
vgg_headless = application_vgg16(
  include_top = FALSE,
  weights = "imagenet",
  input_shape = c(224, 224, 3))

# Define the new head
vgg = vgg_headless$output %>%
  # TODO insert global average pooling, two dense layers
  # of 512 units with relu, and one with 120 for classification

# Create the model
vgg_pretrained = keras_model(inputs = vgg_headless$input, outputs = vgg)

# freeze the weights of the first 19 layers
freeze_weights(vgg_pretrained, from = 1, to = 19)

summary(vgg_pretrained)
```

Now the pretrained model can be fine tuned to the new task

```{r, eval = FALSE}
compile(
  # TODO compile new the model
)

history = fit_generator(
  # TODO fit the model
)
```

```{r, echo = FALSE}
plot(readRDS("pretrained.rds"))
```

As you can see, the results are much better now, and would keep improving if we had trained for longer.

## Exercise 2
This exercise is about the receptive field of convolutional neural networks. For our purposes, the the receptive field of a neuron in layer $L$ contains the features in a preceding layer $\ell$ that affect the output of said neuron, with $\ell=0$ being the input to the network. In other words, changing any value in a neuron's receptive field will change the output of that neuron. By going backwards from layer $L$, convolutions and pooling operations enlarge the receptive field of neurons at layer $L$, so that the deeper the network, the larger the receptive field of neurons at the end ofthe network. 

Let $\textbf{y}_\ell\in\mathbb{R}^{n_\ell}$ be the output of layer $\ell$ (and $\textbf{y}_0$ the input), that is obtained with a one-dimensional convolution or pooling operation from $\textbf{y}_{\ell-1}$ with a kernel of size $k_\ell$ and stride $s_\ell$. Define $r_\ell$ to be the size of the receptive field in the $\ell$-th layer of a neuron in layer $L$, i.e. the minimum width of the region that contains the elements in $\textbf{y}_\ell$ that affect a generic element in $\textbf{y}_L$. Note that this region can contain gaps, i.e. neurons that do not affect the output of the neuron in layer $L$, if they are in between neurons that do affect it.

Show that $r_{\ell-1}$ can be computed from $r_\ell$ as follows:

$$
r_{\ell-1}=s_\ell\cdot r_\ell+k_\ell-s_\ell
$$
You can consider padding to be infinite, or, equivalently, focus on the neurons in the middle of the layer, without analyzing what happens near the borders. Hint: consider the case $k_\ell=1$ first.

Then solve the recurrence to show that:

$$
r_0=\sum_{\ell=1}^L\left(
\left(k_\ell-1\right)\prod_{i=1}^{\ell-1}s_i
\right)+1
$$

with the base case being $r_L=1$.

Compute the receptive field size of the pre-trained VGG16 architecture we used above, right before the global average pooling layer.

Now suppose to have a dilation of $d_\ell\geq 1$ at every layer. What is the new formula for $r_0$?

What is the most effective way to increase the size of the receptive field of a neural network?


