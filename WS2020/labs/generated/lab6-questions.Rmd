---
title: "Lab 6"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE, echo = TRUE)
```

Welcome to the sixth lab. Today, we will see what are the signs of overfitting, and how to avoid it using regularization. Then, we will analyze convergence of gradient descent on quadratic surfaces, and apply the intuition we gain on a practical example, comparing gradient descent with and without momentum.

## Exercise 1 - Regularization
In this exercise, we will look at a couple of different methods to regularize a neural network in order to prevent overfitting.

Recall that the capacity of a neural network is determined by the total number of trainable weights. Neural networks that are very "big" require a correspondingly large amount of training data in order to not overfit. Therefore, if you find that the network overfits, the best solution to the problem is to simply get more training data.

If this is not possible, you can either:

 1. Reduce the size of the network by removing layers or decreasing the number of neurons in the layers, or,  
 2. Regularize the network 

Plotting the validation loss is a simple way to determine whether the network has overfit. During the first few epochs of training, both the training loss and the validation loss tend to decrease in tandem but after a certain point, the validation loss starts to increase while the training loss continues to decrease. It is at this point that the network begins to overfit.

### Prepare the data

We'll once again be working with the IMDB dataset. Let's prepare the data using code from the previous tutorial.

```{r}
library(keras)

imdb = dataset_imdb(num_words = 10000)
train_data = imdb$train$x
train_labels = imdb$train$y
test_data = imdb$test$x
test_labels = imdb$test$y

vectorize_sequences = function(sequences, dimension = 10000) {
  # Create an all-zero matrix of shape (len(sequences), dimension)
  results = matrix(0, nrow = length(sequences), ncol = dimension)
  for (i in 1:length(sequences)) {
    # Sets specific indices of results[i] to 1s
    results[i, sequences[[i]]] = 1
  }
  results
}

# Our vectorized training data
x_train = vectorize_sequences(train_data)
# Our vectorized test data
x_test = vectorize_sequences(test_data)

# Our vectorized labels
y_train = as.numeric(train_labels)
y_test = as.numeric(test_labels)
```

### Train multiple networks

In order to get a feel for the overfitting behaviour of neural networks, we will train 3 different architectures and observe the training and validation losses.

Create a model with two hidden layers, each with 16 units and relu activation. Check the shape of `x_train` to know the input shape for the first layer. How many units and which activation should the last layer have, considering that we are doing binary classification?

```{r}
original_model = (
  # TODO create the network according to the specifications above
)

compile(
  original_model,
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
```

Our second model will be similar to the first but it will be much smaller; reduce the number of neurons in the hidden layers from 16 to 4, and keep everything else unchanged.

```{r}
smaller_model = (
  # TODO create a smaller model according to the specifications above
)

compile(
  smaller_model, 
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

```


### Train both networks
Train the first network using the `fit` function for 20 epochs using a batch size of 512; remember to pass `x_test` and `y_test` as validation data.

```{r}
original_hist = fit(
  # TODO fit the original model
)
```

Now train the second network using the `fit` function for 20 epochs using a batch size of 512. Once again, remember to pass `x_test` and `y_test` as validation data.

```{r}
smaller_model_hist = fit(
  # TODO fit the smaller model
)
```
  
### Plot the losses

To compare the losses we will write an R function that takes a named list of loss series and plots it:

```{r}
library(ggplot2)
library(tidyr)

plot_training_losses = function(losses) {
  loss_names = names(losses)
  losses = as.data.frame(losses)
  losses$epoch = seq_len(nrow(losses))
  losses %>% 
    gather(model, loss, loss_names[[1]], loss_names[[2]]) %>% 
    ggplot(aes(x = epoch, y = loss, colour = model)) +
    geom_point()
}

plot_training_losses(list(
  orig=original_hist$metrics$val_loss,
  small=smaller_model_hist$metrics$val_loss
))
```

As you can see, the smaller network starts overfitting later than the original one and its performance degrades much more slowly once it starts overfitting.

### Third model
Now we build a third neural network that is even bigger than the original network. If the previous plot is any indication, this new network should overfit even worse than the original model.

```{r}
bigger_model = (
  # TODO build a network with 512 units in the two hidden layers
)

compile(
  # TODO compile this network with the same settings we used previously
)
```

Let's train this network:

```{r}
bigger_model_hist = fit(
  bigger_model,
  x_train, y_train,
  batch_size = 512,
  epochs = 20,
  validation_data = list(x_test, y_test),
  verbose = 0
)
```

Here's how the bigger network fares compared to the reference one:

```{r}
plot_training_losses(list(
  orig=original_hist$metrics$val_loss,
  big=bigger_model_hist$metrics$val_loss
))
```

The bigger network starts overfitting almost right away, after just one epoch, and overfits much more severely. Its validation loss is also more noisy.

Let's plot the training losses:

```{r}
plot_training_losses(list(
  orig=original_hist$metrics$loss,
  big=bigger_model_hist$metrics$loss
))
```


As you can see, the bigger network gets its training loss near zero very quickly. The more capacity the network has, the quicker it will be able to model the training data (resulting in a low training loss), but the more susceptible it is to overfitting (resulting in a large difference between the training and validation loss).

### Adding weight regularization
Recall that weight regularization constrains the learning algorithm by penalizing large weights. This reduces the effective capacity of the network and helps prevent overfitting.

There are two main flavours of weight regularization:

 - L1 regularization, where the cost added is proportional to the absolute value of the weights coefficients.
 - L2 regularization, where the cost added is proportional to the square of the value of the weights coefficients. L2 regularization is also called _weight decay_ in the context of neural networks.

Regularizing a network in Keras is extremely easy. All you need to do is pass a weight regularizer object as the argument to the `kernel_regularizer` parameter of a layer. Keras has 3 built-in regularizers: `regularizer_l1` , `regularizer_l2` and `regularizer_l1_l2`, whose documentation is [here](https://keras.rstudio.com/reference/regularizer_l1.html).

Modify the original model and add weight decay with coefficient 0.001 to both hidden layers.

```{r}
l2_model = (
  # TODO modify the original model to have L2 regularization with coefficient 0.001
)

compile(
  # TODO compile the model with the usual configuration
)
```

Because the penalty term is only added during training (that is, it is not added when evaluating the model on the test set), the training error of this regularized model will be much higher.

```{r}
l2_model_hist = fit(
  l2_model,
  x_train, y_train,
  batch_size = 512,
  epochs = 20,
  validation_data = list(x_test, y_test),
  verbose = 0
)
```

Now, plot the validation losses of the original model (in `original_hist`), and of its regularized counterpart (in `l2_model_hist`).

```{r}
plot_training_losses(list(
  # TODO plot the validation losses
))
```


As you can see, the regularized model does not overfit as much, even though both models have the same number of parameters. Feel free to play with the regularization strength to get a feel on how different settings affect learning. When is regularization preventing the network from learning anything at all? When is regularization so weak it does not make a difference?

### Dropout regularization
Dropout is a very popular technique to regularize neural nets. It works by randomly turning off (or "dropping out")  the input/hidden neurons in a network. This means that every neuron is trained on a different set of examples. Note that dropout is only added during training time (the entire network is used to evaluate the model on the test set or to make predictions). At test time we scale down the output by the dropout rate to account for the fact that all neurons were used for the prediction. Normally, dropout is not applied to the inputs.

In Keras, dropout is implemented as its own separate layer: [`layer_dropout`](https://keras.rstudio.com/reference/layer_dropout.html), that takes as input the probability to _drop_ units. To apply dropout to a layer, place a `layer_dropout` after it while stacking layers.

```{r}
dpt_model = (
  # TODO create a copy of the original model using dropout with probability 0.5
)

compile(
  dpt_model,
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
```

```{r}
dpt_model_hist =  fit(
  # TODO train the dropout-regularized model with the usual settings
)
```

```{r}
plot_training_losses(list(
  # TODO plot the validation losses of dpt_model and original_model
))
```

Once again, we see a marked improvement in the new model.

### Early Stopping
Previously, we were training the network and checking _after training_ when it started to overfit. Keras provides a way to do this automatically, interrupting training when the model is starting to overfit, with the use of [_callbacks_](https://keras.rstudio.com/reference/index.html#section-callbacks). A callback contains methods that are called at several points during the training process: at the beginning and end of each epoch, before and after feeding a new batch to the network, and at the beginning and end of training itself. Keras provides a few useful callbacks; read the documentation to find out what you can do with them. You can also write custom callbacks, more info on this [here](https://keras.rstudio.com/reference/KerasCallback.html).

We will now re-train the dropout-regularized model, and make use of [`callback_early_stopping`](https://keras.rstudio.com/reference/callback_early_stopping.html) to interrupt training after the validation loss has stopped improving. In particular, the parameter `patience` indicates how many epochs to wait for an improvement of the validation loss. If there is no improvement for more than `patience` epochs, training is interrupted. Moreover, the setting the parameter `restore_best_weights` to `TRUE` will make sure that the network's parameters will be restored to the best performing on the validation set.


```{r}
dpt_model_early = (
  keras_model_sequential() %>% 
    layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>% 
    layer_dropout(0.5) %>%
    layer_dense(units = 16, activation = "relu") %>% 
    layer_dropout(0.5) %>%
    layer_dense(units = 1, activation = "sigmoid")
)

compile(
  dpt_model_early,
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

dpt_model_early_hist = fit(
  dpt_model_early,
  x_train, y_train,
  batch_size = 512,
  epochs = 20,
  validation_data = list(x_test, y_test),
  callbacks = list(
    # TODO use the early stopping callback
  ),
  verbose = 0
)
```

```{r}
# pad to the same length with NA's
pad = length(dpt_model_hist$metrics$val_loss)-length(dpt_model_early_hist$metrics$val_loss)
plot_training_losses(list(
  early=c(dpt_model_early_hist$metrics$val_loss, rep(NA, pad)),
  dpt=dpt_model_hist$metrics$val_loss
))
```

As you can see, the early stopping callback worked, and the model was trained for only seven epochs. Now, evaluate this model on the test data:

```{r}
# TODO compute and print the loss on the test samples
```

As you can see, the loss is close to the lowest loss in the graph.

The take-home message for this exercise is: Large neural networks can easily overfit, especially with small training sets. This means that the network learns spurious patterns that are present in the training data and, therefore, fails to generalize to unseen examples. In such a scenario, your options are: 

 1. Get more training data
 2. Reduce the size of the network
 3. Regularize the network

## Exercise 2
Consider an error function of the form:

$$
E=\frac 1 2 \lambda_1 x_1^2+\frac 1 2 \lambda_2 x_2^2
$$

With $\lambda_1\geq 0$ and $\lambda_2\geq 0$. First, show that the global minimum of $E$ is at $x_1=x_2=0$, then find the matrix $\textbf{H}$ such that $E=1/2 \cdot \textbf{x}^T\textbf{H}\textbf{x}$. Show that the two eigenvectors $\textbf{u}_1$ and $\textbf{u}_2$ of this matrix are axis-aligned, and have $\lambda_1$ and $\lambda_2$ as eigenvalues.

Note that any vector $\textbf{x}$ can be expressed as

$$
\textbf{x}=\sum_i \alpha_i\textbf{u}_i
$$
where $\alpha_i$ is the distance from the origin to $\textbf{x}$ along the $i$-th axis (assuming the eigenvectors have unit length). Now find the gradient of $E$ with respect to $\textbf{x}$, and express it in terms of $\alpha_i$, $\lambda_i$ and $\textbf{u}_i$. 

Then, use this gradient to perform one step of gradient descent, i.e. compute

$$
\textbf{x}^\prime=\textbf{x}-\eta\nabla_{\textbf{x}}E
$$

And show that

$$
\alpha^\prime_i=(1-\eta\lambda_i)\alpha_i
$$

Which means that the distances from the origin to the current location evolve independently for each axis, and at every step the distance along the direction $\textbf{u}_i$ is multiplied by $(1-\eta\lambda_i)$. After $T$ steps, we have

$$
\alpha^{(T)}_i=(1-\eta\lambda_i)^T\alpha^{(0)}_i
$$
So that, as long as $|1-\eta\lambda_i|<1$ for every $i$, $\textbf{x}^{(T)}$ converges to the origin as $T$ goes to infinity.

Now, find the largest learning rate that guarantees convergence along all directions, and show that, when using this learning rate, the slowest direction of convergence is along the eigenvector with the smallest eigenvalue. Also show that the rate of convegence along this direction is:

$$
\left(1-2\frac{\lambda_{\text{min}}}{\lambda_{\text{max}}}\right)
$$

Where $\lambda_{\text{min}}$ and $\lambda_{\text{max}}$ are the smallest and largest eigenvalues of $\textbf{H}$.

This exercise shows that the largest eigenvalue determines the maximum learning rate, and that the relationship between smallest and largest eigenvalues determines the speed of convergence. Note that the ratio $\lambda_{\text{max}}/\lambda_{\text{min}}$ is known as the _condition number_ of $\textbf{H}$, and plays an important role in numerical analysis. Matrices with large condition number make optimization algorithms slower and/or more imprecise.



## Exercise 3
In this exercise we play a bit with the quadratic error surfaces that we analyzed in the previous exercise. We will apply apply the insights we got, and test different forms of gradient descent. The purpose is to get an intuitive feeling for how these things work, and for this some playful interaction is required from your side.

Remember that the error function was:

$$
E=\frac 1 2 \lambda_1 x_1^2+\frac 1 2 \lambda_2 x_2^2
$$

We first create functions to compute $E$ and its gradient:

```{r}
lambda1 = 1
lambda2 = 10

make_function = function(lambda1, lambda2) {
  function(p) {
    lambda1 * p[1]^2 / 2 + lambda2 * p[2]^2 / 2  
  }
}

func = make_function(lambda1 = 1, lambda2 = 10)


make_gradient = function(lambda1, lambda2) {
  function(p) c(
  # TODO compute the two components of the gradient of E at p
  )
}

grad = make_gradient(lambda1 = 1, lambda2 = 10)
```

And we visualize a contour plot of the surface:

```{r}
make_contours = function(lambda1, lambda2) {
  halfres = 100
  data = as.matrix(expand.grid(-halfres:halfres,-halfres:halfres)) / halfres * 5
  data.frame(x = data[,1], y = data[,2], z = apply(data, 1, func))
}

contours = make_contours(lambda1 = 1, lambda2 = 10)

library(ggplot2)
ggplot(contours) + 
  geom_contour(aes(x, y, z=z))
```

We now create a vanilla gradient descent optimizer that returns all points visited during the process:

```{r}
gradient_descent_optimizer = function(x0, max_steps, lrate) {
  point = x0
  sapply(1:(max_steps + 1), function(i) {
    old = point
    # TODO modify `point` performing one step of gradient descent
    # remember to use the '<<-' operator to assign the new value to `point`
    old
  })
}

hist_slow = gradient_descent_optimizer(c(4, 4), 10, 0.05)
hist_fast = gradient_descent_optimizer(c(4, 4), 10, 0.15)
```

And a function that plots several traces together, so that we can compare them:

```{r}
plot_histories = function(histories) {
  histories = do.call("rbind", lapply(names(histories), function(name) {
    hh = get(name, histories)
    data.frame(x = hh[1,], y = hh[2,], name = rep(name, length(hh)))
  }))

  ggplot() +
    geom_contour(aes(contours$x, contours$y, z = contours$z)) +
    geom_line(aes(histories$x, histories$y, colour = histories$name))
}

plot_histories(list(
  fast = hist_fast,
  slow = hist_slow
))
```

Now, recall from the previous exercise that the learning rate cannot be larger than $1-2\lambda_{\text{min}}/\lambda_{\text{max}}$. Compute this upper bound for the example here, use it to optimize the error starting from $\textbf{x}=|4,4|^T$, and plot the resulting trajectory. What can you notice? Try to slightly reduce it and increase it, and verify that when it is larger than the upper bound, the procedure diverges.

```{r}
max_learning_rate = (
  # TODO compute the maximum learning rate possible in this case
)

plot_histories(list(
  boundary = gradient_descent_optimizer(c(4, 4), 10, max_learning_rate)
))
```

Now try to change the eigenvalues so as to increase the condition number, and verify that convergence becomes slower as the condition number increases:

```{r}
func = make_function(lambda1 = 1, lambda2 = 30)
grad = make_gradient(lambda1 = 1, lambda2 = 30)
contours = make_contours(lambda1 = 1, lambda2 = 30)

max_learning_rate = (
  # TODO compute the maximum learning rate
)

sgd_history = gradient_descent_optimizer(c(4, 4), 10, max_learning_rate)

plot_histories(list(
  boundary = sgd_history
))
```

Finally, modify the optimizer to use momentum, and verify that convergence becomes faster:

```{r}
momentum_optimizer = function(x0, max_steps, lrate, momentum) {
  point = x0
  velocity = c(0, 0)
  history = sapply(1:(max_steps + 1), function(i) {
    old = point
    # TODO modify `point` performing one step of gradient descent
    old
  })
}

momentum = (
  # TODO try several values for the momentum
)
momentum_history = momentum_optimizer(c(4, 4), 10, max_learning_rate, momentum)


plot_histories(list(
  sgd = sgd_history,
  momentum = momentum_history
))
```

Now explore the convergence behavior as momentum and learning rate change. Does momentum bring any improvement when the condition number is one (i.e. the eigenvalues are identical)?

If you have time to spare, try to implement the rmsprop optimizer and/or the adam optimizer, and play with them, too.