---
title: "Lab 9"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE, echo = TRUE)
```

## Exercise 1
In this exercise, we first train a logistic regression classifier on a subset of the MNIST dataset, containing only the digits zero and one. Then, we look for adversarial examples that can fool this classifier.

### Training logistic regression
First of all, we download the dataset (using Keras's utility), discard the samples we do not need, normalize the inputs, turn them into vectors, add a bias term, and encode the labels to $\pm 1$:

```{r}
library(keras)

mnist = dataset_mnist()

train_mask = (mnist$train$y == 1) | (mnist$train$y == 2)
train_x = mnist$train$x[train_mask,,] / 255.0
dim(train_x) <- c(nrow(train_x), 28 * 28)
train_x = cbind(train_x, rep(1, nrow(train_x)))
train_y = 2 * mnist$train$y[train_mask] - 3

test_mask = (mnist$test$y == 1) | (mnist$test$y == 2)
test_x = mnist$test$x[test_mask,,] / 255.0
dim(test_x) <- c(nrow(test_x), 28 * 28)
test_x = cbind(test_x, rep(1, nrow(test_x)))
test_y = 2 * mnist$test$y[test_mask] - 3
```

We now implement and train logistic regression, with loss function

$$
\mathcal{L}(y, f(\textbf{x}|\theta))=\log\left(1+\exp\left(-y\cdot \theta^T\textbf{x}\right)\right)
$$

```{r}
train_logreg = function(data_x, data_y, max_steps, lrate, batch_size) {
  theta = rnorm(ncol(data_x), sd=0.001)
  
  losses = sapply(1:(max_steps + 1), function(i) {
    batch_idx = sample(nrow(data_x), batch_size)
    
    batch_x = data_x[batch_idx,]
    batch_y = data_y[batch_idx]
    
    # TODO perform the forward pass, compute predictions and loss
    
    # TODO perform the backward pass, compute the gradient of theta
    
    # TODO update theta with one step of gradient descent
    
    loss  # return the loss
  })
  
  list(theta = theta, losses = losses)
}


hist = train_logreg(
  train_x, train_y, max_steps = 200, lrate = 0.1, batch_size = 128
)
theta = hist$theta
plot(hist$losses)
```

We then compute the accuracy on the test set:

```{r}
# compute test accuracy
test_predictions = ifelse(test_x %*% theta > 0, 1, -1)[, 1]
mean(test_predictions == test_y)
```

### Randomly searching adversarial examples
Now that the classifier is trained, we can construct adversarial examples. The first strategy we try is to randomly generate vectors of a given length, and check whether they result in a different classification.

First, let's select an example that is classified correctly:

```{r}
predict_class = function(x) {
  # TODO compute the class of x (+1 or -1)
}

# choose an example that is correctly classified
repeat {
  idx = floor(runif(1, 0, nrow(train_x)))
  
  sample = train_x[idx,]
  label = train_y[idx]

  if(predict_class(sample) == label) {
    break
  }
}

library(grid)
grid.raster(matrix(tail(sample, -1), nrow = 28), interpolate = FALSE)
```

Next, we write a function that perturbs this sample with a vector whose elements are all $\pm\epsilon$, and checks whether the class is changed:

```{r}
perturb_and_check = function(eps) {
  delta = (
    # TODO create a random vector with elements +/- eps
  )
  
  prediction = (
    # TODO perturb the sample and predict its class
  )

  ifelse(prediction == label, 1, -1)
}
```

Now, we try different values for `eps`. For each of them, we generate one thousand different perturbations, and compute the proportion that result in a change of class:

```{r}
count = 1000
eps_range = c(1e-1, 2e-1, 5e-1, 1e0, 2e0, 5e0, 1e1, 2e1, 5e1, 1e2, 2e2, 5e2)

p_wrong = sapply(eps_range, function(eps) {
  sum(sapply(1:count, function(i) {
    ifelse(perturb_and_check(eps) > 0, 0, 1)
  })) / count
})

p_wrong
```

We can also plot this:

```{r}
library(ggplot2)
ggplot() +
  geom_point(aes(x = eps_range, y = p_wrong)) +
  scale_x_log10() +
  annotation_logticks() +
  xlab("Half-width of box") +
  ylab("Proportion of mis-classified examples")
```

Clearly, there are no points with a different classification up to a certain distance from the original point. The specific range depends, obviously, on the point in question. Further than that, and the proportion of points with a different class grows larger and larger, until it stabilizes to a value smaller than one. Such range can be thought of as the distance of the point to the decision boundary. If you now imagine a hyper-cube centered on this point, the proportion of mis-classified examples is (an estimation of) the amount of surface of this cube that is on the other side of the separating hyper-plane. This intuition is valid regardless of the specific classification model employed, with the only difference that the decision boundary can be arbitrarily more complex than a hyper-plane.

### Creating an adversarial example via gradient ascent
Now, we can try to look for adversarial examples more intelligently. Specifically, we want to find a vector $\delta^*$ such that

$$
\delta^*=\text{argmax}_\delta \left[\mathcal{L}(y, f(\textbf{x}))-\mathcal{L}(y, f(\textbf{x}+\delta))\right]
$$

and $\delta\in\mathcal{B}^\infty_\epsilon$, with $f(x)$ being our trained logistic regression classifier. The function above can be maximized with gradient ascent, by slightly modifying the training procedure we employed earlier. In particular, note that we can use values for epsilon for which a random perturbation has practically no chance to result in a change of class.

```{r}
make_adversarial = function(theta, sample, true_class, eps, max_steps, lrate) {
  delta = rnorm(length(sample))
  
  wanted_class = -1 * true_class
  
  losses = sapply(1:(max_steps + 1), function(i) {
    
    # TODO perturb the sample with delta and compute the linear part of the model
    
    loss_true = log(1 + exp(-true_class * linear[,1]))
    loss_wanted = log(1 + exp(-wanted_class * linear[,1]))
    
    # TODO compute the gradient of delta with respect to loss_true 
    
    # TODO compute the gradient of delta with respect to loss_wanted
    
    # TODO update delta with one step of gradient ascent

    # TODO normalize delta to have elements +/- eps
    
    loss_true - loss_wanted  # return difference in loss
  })
  
  list(delta = delta, losses = losses)
}

hist = make_adversarial(
  theta,
  sample = sample,
  true_class = label,
  eps = 0.2,
  max_steps = 100,
  lrate = 0.1
)
delta = hist$delta
plot(hist$losses)
```


The label changes when the new loss becomes positive. We can now check that the class was indeed changed:


```{r}
c(
  predict_class(sample),
  predict_class(sample + delta)
)
```

We can also visualize the resulting perturbation vector: 

```{r}
dd = (delta - min(delta)) / (max(delta) - min(delta))
grid.raster(matrix(tail(dd, -1), nrow = 28), interpolate = FALSE)
```

and the perturbed sample:

```{r}
ss = sample + delta
dd = (ss - min(ss)) / (max(ss) - min(ss))
grid.raster(matrix(tail(dd, -1), nrow = 28), interpolate = FALSE)
```

Can you interpret these images?


## Exercise 2
Suppose that the adversarial examples for a logistic regression classifier are generated in $\mathcal{B}^2_\epsilon(\textbf{x})$ instead of $\mathcal{B}^\infty_\epsilon(\textbf{x})$. Show that the adversarial risk becomes

$$
\mathcal{L}(y,f(\textbf{x}|\theta))=\Psi\left(y(\theta^T\textbf{x})-\epsilon||\theta||_2\right)
$$


