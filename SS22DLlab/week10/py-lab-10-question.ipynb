{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "800f0988",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab 10\n",
    "\n",
    "Hüseyin Anil Gündüz\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81e061c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T17:30:11.069729Z",
     "iopub.status.busy": "2022-01-25T17:30:11.066228Z",
     "iopub.status.idle": "2022-01-25T17:30:12.738346Z",
     "shell.execute_reply": "2022-01-25T17:30:12.737676Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "from math import ceil\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "from torch import nn, Tensor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.datasets import IMDB\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "set_matplotlib_formats('png', 'pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9651572",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "In this exercise, we are going to revise the sentiment classifier for IMDB reviews we\n",
    "developed in a previous lab. Earlier, we encoded each review as a single \"bag-of-words\"\n",
    "vector which had one element for each word in our dictionary set to one if that word was\n",
    "found in the review, zero otherwise. This allowed us to use a simple fully-connected\n",
    "neural network but, on the flip side, we lost all information contained in the ordering\n",
    "and of the words and possible multiple repetitions. Recurrent neural networks, however,\n",
    "are able to process reviews directly. Let's see how!\n",
    "\n",
    "The first step is to load the data and preprocess it like it in exercise 6, so if you\n",
    "still remember what we did there, feel free to skip this part.\n",
    "For brevity, we only use the 10000 most common words\n",
    "and truncate reviews to 250 words, but if you can use a GPU then feel free to\n",
    "use the full length reviews and all words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45b14024",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T17:30:12.742470Z",
     "iopub.status.busy": "2022-01-25T17:30:12.741893Z",
     "iopub.status.idle": "2022-01-25T17:30:31.444059Z",
     "shell.execute_reply": "2022-01-25T17:30:31.444548Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_iterator, test_iterator = IMDB();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bffc65d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T17:30:31.458162Z",
     "iopub.status.busy": "2022-01-25T17:30:31.457558Z",
     "iopub.status.idle": "2022-01-25T17:30:37.400870Z",
     "shell.execute_reply": "2022-01-25T17:30:37.400327Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Feed iterator to list\n",
    "train_x = []\n",
    "train_y = []\n",
    "test_x = []\n",
    "test_y = []\n",
    "\n",
    "for label, line in train_iterator:\n",
    "    train_x.append(line)\n",
    "    train_y.append(label)\n",
    "\n",
    "for label, line in test_iterator:\n",
    "    test_x.append(line)\n",
    "    test_y.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc17cc06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T17:30:37.450772Z",
     "iopub.status.busy": "2022-01-25T17:30:37.413844Z",
     "iopub.status.idle": "2022-01-25T17:30:40.118781Z",
     "shell.execute_reply": "2022-01-25T17:30:40.119291Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize sentences.\n",
    "\n",
    "def tokenize(data_list: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Tokenize a list of strings.\n",
    "\n",
    "    :param data_list: A list of strings.\n",
    "    :return: A list where each entry is a list including the tokenized elements.\n",
    "    \"\"\"\n",
    "    token_list: List[List[str]] = []\n",
    "    for data_string in data_list:\n",
    "        # Remove punctuation.\n",
    "        data_string = data_string.translate(str.maketrans('', '', string.punctuation))\n",
    "        # Split by space.\n",
    "        token_list.append(data_string.split())\n",
    "    return token_list\n",
    "\n",
    "\n",
    "train_x = tokenize(train_x)\n",
    "test_x = tokenize(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a96b5a0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T17:30:40.194581Z",
     "iopub.status.busy": "2022-01-25T17:30:40.158380Z",
     "iopub.status.idle": "2022-01-25T17:30:44.369469Z",
     "shell.execute_reply": "2022-01-25T17:30:44.368843Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Count-vectorize sentences.\n",
    "class CountVectorizer:\n",
    "    def __init__(self):\n",
    "        self.vec_to_str_map: Dict[int, str] = {}\n",
    "        self.str_to_vec_map: Dict[str, int] = {}\n",
    "\n",
    "    def fit(self, token_list: List[str]) -> None:\n",
    "        # The `Counter` object from the `collections` library gives us efficient counting\n",
    "        # in large lists out of box.\n",
    "        cnt = Counter(token_list)\n",
    "        sorted_cnt = sorted(cnt.items(), key=lambda item: item[1], reverse=True)\n",
    "        sorted_words = [key for key, val in sorted_cnt]\n",
    "\n",
    "        # Python does not know a bidirectional mapping by default.\n",
    "        # We trick a bit by simply creating two dicts, but note that this is inefficient.\n",
    "        self.str_to_vec_map = {sorted_words[i]: i + 1 for i in range(len(sorted_words))}\n",
    "        self.vec_to_str_map = {i + 1: sorted_words[i] for i in range(len(sorted_words))}\n",
    "\n",
    "    def transform_to_vec(self, token_list: List[str]) -> List[int]:\n",
    "        return [self.str_to_vec_map.get(word) for word in token_list]\n",
    "\n",
    "    def transform_to_str(self, token_list: List[int]) -> List[str]:\n",
    "        return [self.vec_to_str_map.get(rank) for rank in token_list]\n",
    "\n",
    "\n",
    "train_words = [word for word_list in train_x for word in word_list]\n",
    "test_words = [word for word_list in test_x for word in word_list]\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "counter = count_vectorizer.fit(train_words)\n",
    "\n",
    "train_x = [count_vectorizer.transform_to_vec(word_list) for word_list in train_x]\n",
    "test_x = [count_vectorizer.transform_to_vec(word_list) for word_list in test_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffc29682",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T17:30:44.377457Z",
     "iopub.status.busy": "2022-01-25T17:30:44.376841Z",
     "iopub.status.idle": "2022-01-25T17:30:45.868699Z",
     "shell.execute_reply": "2022-01-25T17:30:45.868104Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Discard words that are not in the top 10000\n",
    "# Truncate sequences to a length of 250\n",
    "# Remove Nones\n",
    "\n",
    "def filter_word_ranks(\n",
    "        word_list: List[Optional[int]],\n",
    "        max_rank: int = 10000,\n",
    "        max_seq_len: int = 250\n",
    ") -> List[int]:\n",
    "    output = []\n",
    "    seq_len = 0\n",
    "    for word_rank in word_list:\n",
    "        if seq_len >= max_seq_len:\n",
    "            return output\n",
    "        elif word_rank is None:\n",
    "            continue\n",
    "        elif word_rank <= max_rank:\n",
    "            output.append(word_rank)\n",
    "            seq_len += 1\n",
    "    return output\n",
    "\n",
    "\n",
    "train_x = [filter_word_ranks(word_list) for word_list in train_x]\n",
    "test_x = [filter_word_ranks(word_list) for word_list in test_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e76476d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T17:30:45.875914Z",
     "iopub.status.busy": "2022-01-25T17:30:45.875326Z",
     "iopub.status.idle": "2022-01-25T17:30:45.877222Z",
     "shell.execute_reply": "2022-01-25T17:30:45.877691Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Encode labels to binary targets\n",
    "train_y = [1 if label == 'pos' else 0 for label in train_y]\n",
    "test_y = [1 if label == 'pos' else 0 for label in test_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87e2300",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, each review is a vector of numbers, each corresponding to a different word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba859157",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T17:30:45.882183Z",
     "iopub.status.busy": "2022-01-25T17:30:45.881591Z",
     "iopub.status.idle": "2022-01-25T17:30:45.884871Z",
     "shell.execute_reply": "2022-01-25T17:30:45.884331Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 1595, 8, 35, 70, 434, 1191, 80, 4, 32, 1, 7739, 9, 3423, 10, 55, 10, 13, 89, 654, 7, 7978, 8, 91, 559, 9, 30, 89, 10, 13, 31, 835, 60, 10, 129, 787, 5, 3578, 11, 783, 2044, 110, 2, 341, 4, 94, 1165, 3150, 8, 59, 61, 5, 66, 11, 17, 12, 14, 114, 6, 6504, 187, 2, 199, 4239, 500, 1612, 777, 5011, 34, 481, 5, 847, 330, 72, 67, 41, 128, 121, 826, 72, 481, 5, 1139, 39, 5, 255, 47, 439, 4, 701, 21, 51, 1, 895, 196, 41, 809, 1008, 1328, 141, 15, 1, 2757, 969, 3, 1672, 1328, 7, 1, 2535, 2532, 121, 194, 2193, 7740, 3, 1999, 4, 41, 64, 5012, 21, 2549, 72, 43, 423, 16, 39, 500, 1774, 8513, 3, 1052, 12, 216, 1116, 71, 41, 8, 6, 9, 1831, 154, 614, 11, 13, 1165, 8972, 2069, 1, 423, 3, 1048, 139, 22, 164, 3, 243, 194, 65, 107, 37, 24, 333, 36, 47, 6972, 88, 5058, 441, 70, 357, 162, 10, 1635, 7, 677, 423, 3, 1048, 22, 2, 723, 7, 4239, 496, 380, 5218, 5059, 64, 1564, 5, 48, 175, 522, 305, 2092, 61, 423, 139, 7, 23, 3817, 12, 8, 84, 1, 902, 17, 1, 189, 9, 100, 423, 613, 7, 1, 19, 6, 613, 17, 1618, 5555, 250, 73, 44, 5, 1579, 82, 3, 93, 291, 5, 27, 613, 7, 8972, 2245, 7, 949, 8, 6, 2, 48, 19, 17, 278, 1786, 5, 2093]\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3825e99",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Even though RNNs can process sequences of arbitrary length, all sequences in the same\n",
    "batch must be of the same length, while sequences in different batches can have different\n",
    "length. In this case, however, we pad all sequences to the same length as this makes for\n",
    "much simpler code.\n",
    "PyTorch provides a function to do so for you called `pad_sequence` (read the documentation!).\n",
    "Hint: It might be good to set the argument `batch_first` to `True.\n",
    "Beforehand, we need to convert the data to tensors. Let's also define our device and push\n",
    "the newly created padded tensor to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcd93812",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T17:30:45.889548Z",
     "iopub.status.busy": "2022-01-25T17:30:45.888960Z",
     "iopub.status.idle": "2022-01-25T17:30:45.960964Z",
     "shell.execute_reply": "2022-01-25T17:30:45.960423Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "# TODO: Define the device you want to use.\n",
    ")\n",
    "\n",
    "train_x = [torch.tensor(word_list, dtype=torch.int) for word_list in train_x]\n",
    "test_x = [torch.tensor(word_list, dtype=torch.int) for word_list in test_x]\n",
    "\n",
    "train_x = (\n",
    "# TODO: Pad the sequences and push the result to your device.\n",
    ")\n",
    "\n",
    "test_x = (\n",
    "# TODO: Pad the sequences and push the result to your device.\n",
    ")\n",
    "\n",
    "train_y = torch.tensor(train_y, dtype=torch.float, device=device)\n",
    "test_y = torch.tensor(test_y, dtype=torch.float, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1e3b355",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T17:30:45.966161Z",
     "iopub.status.busy": "2022-01-25T17:30:45.965573Z",
     "iopub.status.idle": "2022-01-25T17:30:46.015050Z",
     "shell.execute_reply": "2022-01-25T17:30:46.014525Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e44032e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The data is now an array of shape `(num_samples x seq_len)`.\n",
    "A PyTorch RNN with `batch_first=True` option expects the input to be of shape\n",
    "`(num_samples x seq_len x features)`. Although we have a univariate timeseries, we still\n",
    "need to add this additional last dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dd6efbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T17:30:46.042074Z",
     "iopub.status.busy": "2022-01-25T17:30:46.041332Z",
     "iopub.status.idle": "2022-01-25T17:30:46.069498Z",
     "shell.execute_reply": "2022-01-25T17:30:46.069963Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_x = (\n",
    "# TODO: Add the feature dimension.\n",
    ")\n",
    "\n",
    "test_x = (\n",
    "# TODO: Add the feature dimension.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2329a69",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally let's create our `IMDBDataset` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c187f4cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T17:30:46.094391Z",
     "iopub.status.busy": "2022-01-25T17:30:46.093538Z",
     "iopub.status.idle": "2022-01-25T17:30:46.121611Z",
     "shell.execute_reply": "2022-01-25T17:30:46.122078Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data: Tensor, labels: Tensor):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> Tuple:\n",
    "        return self.data.shape\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Tensor, Tensor]:\n",
    "# TODO: Return the correct review and label for the index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bb2783",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we define our sequential model. The first layer is an _Embedding_ layer that\n",
    "associates a vector of numbers to each word in the vocabulary. These numbers are updated\n",
    "during training just like all other weights in the network. Crucially, thanks to this\n",
    "embedding layer we do not have to one-hot-encode the reviews but we can use the word\n",
    "indices directly, making the process much more efficient.\n",
    "\n",
    "Note the parameter `padding_idx`: this indicates that zeros in the input sequences are\n",
    "used for padding (verify that this is the case!). Internally, this is used by the RNN to\n",
    "ignore padding tokens, preventing them from contributing to the gradients (read more in\n",
    "the user guide, [link](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html/)!)\n",
    "\n",
    "We also make use of a `nn.Module` container, where we will define our model. This\n",
    "gives us more flexibility in the flow of the network. Here we add the model blocks\n",
    "as class attributes and define a forward pass, which is enough for the autograd engine.\n",
    "\n",
    "The shapes and dimensions of tensors can now be a bit tricky. It helps if you print\n",
    "the resulting shape of each transformation to console and investigate what happened!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9148edf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T17:30:46.148274Z",
     "iopub.status.busy": "2022-01-25T17:30:46.129225Z",
     "iopub.status.idle": "2022-01-25T17:30:46.178521Z",
     "shell.execute_reply": "2022-01-25T17:30:46.177646Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        # A class that inherits from nn.Module needs to call the constructor from the\n",
    "        # parent class\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=10001,\n",
    "            embedding_dim=64,\n",
    "            padding_idx=0\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=64,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(in_features=32, out_features=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # The output needs to be reshaped or otherwise we have a dimension too much.\n",
    "        x = self.embedding(x).squeeze(2)\n",
    "\n",
    "        # The LSTM module gives a variety of outputs. Please refer to the official\n",
    "        # docs for a detailed description. Here `hidden` contains the final hidden states\n",
    "        # from the last layer for every sample in the batch.\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "\n",
    "        # We need to extract the last hidden state\n",
    "        y_score = self.fc(hidden[-1])\n",
    "        y_hat = self.sigmoid(y_score).squeeze(-1)\n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106ca9b3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the next step, we once again need our beloved training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a8de19b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T17:30:46.205842Z",
     "iopub.status.busy": "2022-01-25T17:30:46.203845Z",
     "iopub.status.idle": "2022-01-25T17:30:46.233866Z",
     "shell.execute_reply": "2022-01-25T17:30:46.234320Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train(\n",
    "        model: nn.Module,\n",
    "        loss: nn.Module,\n",
    "        optimizer: Optimizer,\n",
    "        train_dataset: Dataset,\n",
    "        test_dataset: Dataset,\n",
    "        epochs: int,\n",
    "        batch_size: int\n",
    ") -> Dict:\n",
    "    metrics: Dict = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_loss': [],\n",
    "        'test_acc': [],\n",
    "    }\n",
    "\n",
    "    num_train_batches = ceil(len(train_dataset) / batch_size)\n",
    "    num_test_batches = ceil(len(test_dataset) / batch_size)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size)\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        total_loss = 0\n",
    "        num_correct = 0\n",
    "\n",
    "        ################################################################################\n",
    "        # TRAINING LOOP\n",
    "        ################################################################################\n",
    "\n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "\n",
    "# TODO: Add forward pass + batch loss, backpropagation and apply gradients\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('TRAINING BATCH:\\t({:5} / {:5})\\tLOSS:\\t{:.3f}'\n",
    "                      .format(batch_idx, num_train_batches, float(batch_loss)),\n",
    "                      end='\\r')\n",
    "\n",
    "            total_loss += float(batch_loss)\n",
    "            num_correct += int(torch.sum(torch.where(y_hat > 0.5, 1, 0) == y))\n",
    "\n",
    "        ep_train_loss = total_loss / len(train_dataset)\n",
    "        ep_train_acc = num_correct / len(train_dataset)\n",
    "\n",
    "        total_loss = 0\n",
    "        num_correct = 0\n",
    "\n",
    "        ################################################################################\n",
    "        # TEST LOOP\n",
    "        ################################################################################\n",
    "\n",
    "        for batch_idx, (x, y) in enumerate(test_loader):\n",
    "\n",
    "            with torch.no_grad():\n",
    "# TODO: Do a forward pass and get the batch loss\n",
    "\n",
    "            if batch_idx % 50 == 0:\n",
    "                print('TEST BATCH:\\t({:5} / {:5})\\tLOSS:\\t{:.3f}'\n",
    "                      .format(batch_idx, num_test_batches, float(batch_loss)), end='\\r')\n",
    "\n",
    "            total_loss += float(batch_loss)\n",
    "            num_correct += int(torch.sum(torch.where(y_hat > 0.5, 1, 0) == y))\n",
    "\n",
    "        ep_test_loss = total_loss / len(test_dataset)\n",
    "        ep_test_acc = num_correct / len(test_dataset)\n",
    "\n",
    "        metrics['train_loss'].append(ep_train_loss)\n",
    "        metrics['train_acc'].append(ep_train_acc)\n",
    "        metrics['test_loss'].append(ep_test_loss)\n",
    "        metrics['test_acc'].append(ep_test_acc)\n",
    "\n",
    "        print('EPOCH:\\t{:5}\\tTRAIN LOSS:\\t{:.3f}\\tTRAIN ACCURACY:\\t{:.3f}\\tTEST LOSS:\\t'\n",
    "              '{:.3f}\\tTEST ACCURACY:\\t{:.3f}'\n",
    "              .format(ep, ep_train_loss, ep_train_acc, ep_test_loss, ep_test_acc))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72275d6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We declare model, optimizer, datasets, loss, epochs, batch size and then start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "453565dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T17:30:46.241452Z",
     "iopub.status.busy": "2022-01-25T17:30:46.240247Z",
     "iopub.status.idle": "2022-01-25T17:30:46.292794Z",
     "shell.execute_reply": "2022-01-25T17:30:46.292166Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "model = (\n",
    "# Initialize the model and push it to your device.\n",
    ")\n",
    "\n",
    "optimizer = (\n",
    "# TODO: Define an optimizer.\n",
    ")\n",
    "\n",
    "loss = (\n",
    "# TODO: Define the matching loss function.\n",
    ")\n",
    "\n",
    "train_dataset = (\n",
    "# TODO: Define the train dataset.\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "# TODO: Define the train dataset.\n",
    ")\n",
    "\n",
    "metrics = train(model, loss, optimizer, train_dataset, test_dataset, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4023198d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T17:30:46.300486Z",
     "iopub.status.busy": "2022-01-25T17:30:46.299884Z",
     "iopub.status.idle": "2022-01-25T17:30:46.350566Z",
     "shell.execute_reply": "2022-01-25T17:30:46.350040Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_training_progress_plot(\n",
    "        train_losses: List[float],\n",
    "        train_accs: List[float],\n",
    "        val_losses: List[float],\n",
    "        val_accs: List[float],\n",
    ") -> None:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 2))\n",
    "\n",
    "    ax1.set_title('Loss')\n",
    "    ax1.plot(train_losses, label='Train Loss')\n",
    "    ax1.plot(val_losses, label='Test Loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.set_title('Accuracy')\n",
    "    ax2.plot(train_accs, label='Train Accuracy')\n",
    "    ax2.plot(val_accs, label='Test Accuracy')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.legend()\n",
    "\n",
    "\n",
    "get_training_progress_plot(\n",
    "    metrics['train_loss'],\n",
    "    metrics['train_acc'],\n",
    "    metrics['test_loss'],\n",
    "    metrics['test_acc'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d3977d",
   "metadata": {},
   "source": [
    "The model seems to be learning more easily than the simple baseline we created time ago,\n",
    "which had an accuracy of 85-88% on the test data.\n",
    "Let it train for longer and tune the\n",
    "architecture above to reach as high accuracy as possible! (note that evaluating on the\n",
    "same data that you used for early stopping is cheating).\n",
    "\n",
    "\n",
    "## Exercise 2\n",
    "\n",
    "In this exercise, we are going to build a model that is able to sum two numbers, each given as a sequence\n",
    "of images of handwritten digits. The network will first use a convolutional encoder to transform each\n",
    "digit into a feature vector. These feature vectors will then be processed by a LSTM that will produce as\n",
    "output each digit of the sum.\n",
    "\n",
    "### Dataset\n",
    "We are now going to create a synthetic dataset using images from MNIST.\n",
    "\n",
    "First, we define some auxiliary functions.\n",
    "We need a function that converts an integer to a padded tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c50d6fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T17:30:46.375398Z",
     "iopub.status.busy": "2022-01-25T17:30:46.374476Z",
     "iopub.status.idle": "2022-01-25T17:30:46.403988Z",
     "shell.execute_reply": "2022-01-25T17:30:46.403476Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def convert_int_to_vector(num: int, length: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Take an integer and convert it to a vector.\n",
    "\n",
    "    Example: 123 with a length of 3 returns a tensor with [1, 2, 3].\n",
    "    5 with a length of 3 returns [0, 0, 5]\n",
    "    \"\"\"\n",
    "# TODO: Fill the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ae3c64",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then, we need a function that generates our training labels.\n",
    "The result of the function should be a dictionary that contains 3 tensors (first numbers, second numbers, sum of first + second) of shape `(num_samples, max_length)`.\n",
    "We need the summands for drawing matching images later, while the latter is our actual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26a498e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T17:30:46.428270Z",
     "iopub.status.busy": "2022-01-25T17:30:46.427685Z",
     "iopub.status.idle": "2022-01-25T17:30:46.458451Z",
     "shell.execute_reply": "2022-01-25T17:30:46.457782Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_labels(num_samples: int, max_length: int) -> Dict:\n",
    "    \"\"\"\n",
    "    Generate random numbers, whose sum does not exceed maximum length.\n",
    "\n",
    "    We will pad numbers that are less than max_length with zeros.\n",
    "    \"\"\"\n",
    "\n",
    "    num_1s = []\n",
    "    num_2s = []\n",
    "    sums = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "\n",
    "        # Ensure the sum always has at most max_len digits\n",
    "        num_1 = torch.randint(10**max_length // 2 - 1, (1,))\n",
    "        num_2 = torch.randint(10**max_length // 2 - 1, (1,))\n",
    "\n",
    "        num_1s.append(convert_int_to_vector(int(num_1), max_length))\n",
    "        num_2s.append(convert_int_to_vector(int(num_2), max_length))\n",
    "\n",
    "# TODO: Add the two numbers and save the result as padded tensor\n",
    "\n",
    "    return {\n",
    "        'num_1': torch.stack(num_1s),\n",
    "        'num_2': torch.stack(num_2s),\n",
    "        'sum': torch.stack(sums)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1705d0c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For our training, we need our `Dataset` object. Here, we will also draw the images to create our input tensors. One image is of shape `(1 x 28 x 28)`.\n",
    "Thus, a constructed input tensor is of shape `(max_length x 1 x 28 x 28)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a012168",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T17:30:46.483236Z",
     "iopub.status.busy": "2022-01-25T17:30:46.464557Z",
     "iopub.status.idle": "2022-01-25T17:30:46.511743Z",
     "shell.execute_reply": "2022-01-25T17:30:46.512204Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class NumberMNIST(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            max_length: int = 3,\n",
    "            train: bool = True\n",
    "    ) -> None:\n",
    "        mnist_base = MNIST('.data', train=train, download=True)\n",
    "        mnist_base.data = mnist_base.data.float() / 255\n",
    "\n",
    "        self.max_length = max_length\n",
    "        # We choose 20k samples for training and 5k for testing.\n",
    "        self.num_samples = 20000 if train else 5000\n",
    "        self.digit_idxs = NumberMNIST._generate_digit_groups(mnist_base.targets)\n",
    "\n",
    "        self.labels = generate_labels(self.num_samples, self.max_length)\n",
    "\n",
    "        self.num_1s = torch.zeros(self.num_samples, self.max_length, 1, 28, 28)\n",
    "        self.num_2s = torch.zeros(self.num_samples, self.max_length, 1, 28, 28)\n",
    "\n",
    "        for i in range(self.num_samples):\n",
    "\n",
    "            imgs = []\n",
    "            for num_1_digit in self.labels['num_1'][i]:\n",
    "                # Get corresponding index group\n",
    "                digit_idxs = self.digit_idxs[int(num_1_digit)]\n",
    "                # Sample a random index from the digit class\n",
    "                rand_idx = digit_idxs[torch.randint(len(digit_idxs), (1, ))]\n",
    "                # Obtain image for the sampled index\n",
    "                imgs.append(mnist_base.data[rand_idx])\n",
    "            # Add images to main tensor.\n",
    "            self.num_1s[i] = torch.stack(imgs)\n",
    "\n",
    "# TODO: Repeat the procedure for the second number\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_digit_groups(targets: Tensor) -> Dict:\n",
    "        \"\"\"Separates the dataset in groups based on the label. Returns a Dict with indices.\"\"\"\n",
    "        res = {}\n",
    "        for i in range(10):\n",
    "            idxs = (targets == i).nonzero().squeeze(-1)\n",
    "            res.update({i: idxs})\n",
    "        return res\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> Tuple:\n",
    "        return self.data.shape\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.num_1s)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "# TODO: Given an index return a dictionary with images\n",
    "# for the first and second digit (Keys: 'num_1' and 'num_2') & sum as label (Key: 'label')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a978142",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's initialize our datasets and see if everything works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cc407e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T17:30:46.536766Z",
     "iopub.status.busy": "2022-01-25T17:30:46.536172Z",
     "iopub.status.idle": "2022-01-25T17:30:46.563834Z",
     "shell.execute_reply": "2022-01-25T17:30:46.563313Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = NumberMNIST(train=True, max_length=3)\n",
    "test_dataset = NumberMNIST(train=False, max_length=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d760e396",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T17:30:46.588219Z",
     "iopub.status.busy": "2022-01-25T17:30:46.587627Z",
     "iopub.status.idle": "2022-01-25T17:30:46.617138Z",
     "shell.execute_reply": "2022-01-25T17:30:46.617603Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def plot_digits(num_1: Tensor, num_2: Tensor) -> None:\n",
    "    grid_img = make_grid(torch.cat([num_1, num_2]), nrow=3)\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    plt.show()\n",
    "\n",
    "idx = int(torch.randint(len(test_dataset), (1,)))\n",
    "sample = test_dataset[idx]\n",
    "\n",
    "print('Sum:', [int(i) for i in sample['label']])\n",
    "plot_digits(sample['num_1'], sample['num_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bf40f5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The model\n",
    "\n",
    "Let's now see how to create the model.\n",
    "\n",
    "This network will have two inputs, one for each number. The numbers have three digits, each of which is an image of size 1 x 28 x 28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9fec123",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T17:30:46.644427Z",
     "iopub.status.busy": "2022-01-25T17:30:46.643630Z",
     "iopub.status.idle": "2022-01-25T17:30:46.672587Z",
     "shell.execute_reply": "2022-01-25T17:30:46.672023Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class AdditionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.latent_dim = 128\n",
    "\n",
    "        # The network will use the same convolutional encoder for all digits in both numbers.\n",
    "        # Let us first define this encoder as its own submodule, a normal CNN:\n",
    "\n",
    "        self.digit_encoder = nn.Sequential(\n",
    "# TODO: Add some convolutional and pooling layers as you see fit.\n",
    "# Note: We will encode each digit on its own. Expect an input one single grayscale mnist image.\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "\n",
    "        # Our second model in this szenario will be a bidrectional LSTM.\n",
    "        # The input for this model are the concatenated latent vectors that we obtained from\n",
    "        # the digit encoder.\n",
    "        # For flexibility, we do not use a sequential but have the final layers as single attributes in this module class.\n",
    "\n",
    "        # Let's also apply a bit of dropout to prevent overfitting too much.\n",
    "        self.dropout = (\n",
    "# TODO: Add a dropout layer.\n",
    "        )\n",
    "\n",
    "        self.lstm = (\n",
    "# TODO: Add a bidirectional LSTM.\n",
    "        )\n",
    "\n",
    "        # Finally, we add a fully connected layer as output.\n",
    "        # Note that the input size of the linear layer should be twice the hidden size\n",
    "        # of the LSTM (bidirectional).\n",
    "        self.fc = (\n",
    "# TODO: Add an output linear layer.\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, num_1: Tensor, num_2: Tensor) -> Tensor:\n",
    "        # Note: num_1 and num_2 are of shape (batch_size x max_length x 1 x 28 x 28)\n",
    "        batch_size = num_1.shape[0]\n",
    "        max_length = num_1.shape[1]\n",
    "\n",
    "        enc_1 = torch.zeros(batch_size, max_length, self.latent_dim, device=num_1.device)\n",
    "        enc_2 = torch.zeros(batch_size, max_length, self.latent_dim, device=num_1.device)\n",
    "# TODO: Encode each digit of the batched tensors with the encoder.\n",
    "# TODO: Fill enc_1 and enc_2 with the results\n",
    "\n",
    "        # After we apply the CNN to both numbers, we need to \"merge\" the two sequence of vectors.\n",
    "        # There are several options here, here we choose to concatenate the two tensor in each time-step\n",
    "        # to produce a single tensor of shape (batch_size, max_len, latent_dim * 2).\n",
    "        enc_total = (\n",
    "# TODO: Concat enc_1 and enc_2 as described above\n",
    "        )\n",
    "\n",
    "        # Now, we pass the total encoded tensor through the dropout, lstm and output layer.\n",
    "        enc_total = self.dropout(enc_total)\n",
    "\n",
    "        # We obtain all hidden states from each timestep resulting in a tensor of shape (batch_size, max_length, lstm_hidden_dim)\n",
    "        out, _ = self.lstm(enc_total)\n",
    "        # Due to broadcasting, we can feed this tensor directly to the fully connected layer.\n",
    "        out = self.fc(out)\n",
    "        # Our loss function does accept a tensor of shape (batch_size, num_classes, max_length)\n",
    "        # So we reshape before returning the network output.\n",
    "        return out.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395d2d58",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can mainly reuse the training loop from the exercise before, but we need to change the computation of the accuracy and dataloading.\n",
    "Let's initialize our modules and start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "733dc17d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T17:30:46.679777Z",
     "iopub.status.busy": "2022-01-25T17:30:46.675522Z",
     "iopub.status.idle": "2022-01-25T17:30:46.726424Z",
     "shell.execute_reply": "2022-01-25T17:30:46.727046Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "# TODO: Define the device you want to use.\n",
    ")\n",
    "\n",
    "def train(\n",
    "        model: AdditionModel,\n",
    "        loss: nn.Module,\n",
    "        optimizer: Optimizer,\n",
    "        train_dataset: Dataset,\n",
    "        test_dataset: Dataset,\n",
    "        epochs: int,\n",
    "        batch_size: int\n",
    ") -> Dict:\n",
    "    metrics: Dict = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_loss': [],\n",
    "        'test_acc': [],\n",
    "    }\n",
    "\n",
    "    num_train_batches = ceil(len(train_dataset) / batch_size)\n",
    "    num_test_batches = ceil(len(test_dataset) / batch_size)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size)\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        total_loss = 0\n",
    "        num_correct = 0\n",
    "\n",
    "        ################################################################################\n",
    "        # TRAINING LOOP\n",
    "        ################################################################################\n",
    "\n",
    "        for batch_idx, sample in enumerate(train_loader):\n",
    "\n",
    "            num_1 = sample['num_1'].to(device)\n",
    "            num_2 = sample['num_2'].to(device)\n",
    "            y = sample['label'].to(device)\n",
    "\n",
    "\n",
    "# TODO: Add forward pass + batch loss, backpropagation and apply gradients\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('TRAINING BATCH:\\t({:5} / {:5})\\tLOSS:\\t{:.3f}'\n",
    "                      .format(batch_idx, num_train_batches, float(batch_loss)),\n",
    "                      end='\\r')\n",
    "\n",
    "            total_loss += float(batch_loss)\n",
    "\n",
    "            num_correct += int(torch.sum(torch.all(torch.eq(torch.argmax(y_hat, dim=1), y), dim=1)))\n",
    "\n",
    "        ep_train_loss = total_loss / len(train_dataset)\n",
    "        ep_train_acc = num_correct / len(train_dataset)\n",
    "\n",
    "        total_loss = 0\n",
    "        num_correct = 0\n",
    "\n",
    "        ################################################################################\n",
    "        # TEST LOOP\n",
    "        ################################################################################\n",
    "\n",
    "        for batch_idx, sample in enumerate(test_loader):\n",
    "            num_1 = sample['num_1'].to(device)\n",
    "            num_2 = sample['num_2'].to(device)\n",
    "            y = sample['label'].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "# TODO: Do a forward pass and get the batch loss\n",
    "\n",
    "            if batch_idx % 50 == 0:\n",
    "                print('TEST BATCH:\\t({:5} / {:5})\\tLOSS:\\t{:.3f}'\n",
    "                      .format(batch_idx, num_test_batches, float(batch_loss)), end='\\r')\n",
    "\n",
    "            total_loss += float(batch_loss)\n",
    "\n",
    "            num_correct += int(torch.sum(torch.all(torch.eq(torch.argmax(y_hat, dim=1), y), dim=1)))\n",
    "\n",
    "        ep_test_loss = total_loss / len(test_dataset)\n",
    "        ep_test_acc = num_correct / len(test_dataset)\n",
    "\n",
    "        metrics['train_loss'].append(ep_train_loss)\n",
    "        metrics['train_acc'].append(ep_train_acc)\n",
    "        metrics['test_loss'].append(ep_test_loss)\n",
    "        metrics['test_acc'].append(ep_test_acc)\n",
    "\n",
    "        print('EPOCH:\\t{:5}\\tTRAIN LOSS:\\t{:.3f}\\tTRAIN ACCURACY:\\t{:.3f}\\tTEST LOSS:\\t'\n",
    "              '{:.3f}\\tTEST ACCURACY:\\t{:.3f}'\n",
    "              .format(ep, ep_train_loss, ep_train_acc, ep_test_loss, ep_test_acc))\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b32314b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's initialize our modules and start training!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9f33496",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T17:30:46.751275Z",
     "iopub.status.busy": "2022-01-25T17:30:46.732702Z",
     "iopub.status.idle": "2022-01-25T17:30:46.780410Z",
     "shell.execute_reply": "2022-01-25T17:30:46.779814Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "model = (\n",
    "# Initialize the model and push it to your device.\n",
    ")\n",
    "\n",
    "optimizer = (\n",
    "# TODO: Define an optimizer.\n",
    ")\n",
    "\n",
    "loss = (\n",
    "# TODO: Define the matching loss function.\n",
    ")\n",
    "\n",
    "metrics = train(model, loss, optimizer, train_dataset, test_dataset, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64873552",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T17:30:46.785136Z",
     "iopub.status.busy": "2022-01-25T17:30:46.783193Z",
     "iopub.status.idle": "2022-01-25T17:30:46.833631Z",
     "shell.execute_reply": "2022-01-25T17:30:46.833046Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "get_training_progress_plot(\n",
    "    metrics['train_loss'],\n",
    "    metrics['train_acc'],\n",
    "    metrics['test_loss'],\n",
    "    metrics['test_acc'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348217c9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It is amazing what we achieved with such a small (for the standard of deep learning) model and dataset!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
