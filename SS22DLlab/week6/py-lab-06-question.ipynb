{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f5ee16",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab 6\n",
    "\n",
    "Hüseyin Anil Gündüz\n",
    "\n",
    "Welcome to the sixth lab.\n",
    "\n",
    "In this lab, we will finally use PyTorch as a deep learning framework.\n",
    "We will see what are the signs of overfitting, and how to avoid it using regularization.\n",
    "Then, we will analyze convergence of gradient descent on quadratic\n",
    "surfaces, and apply the intuition we gain on a practical example,\n",
    "comparing gradient descent with and without momentum.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "725e1508",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:17.445625Z",
     "iopub.status.busy": "2021-11-23T09:18:17.432082Z",
     "iopub.status.idle": "2021-11-23T09:18:18.042390Z",
     "shell.execute_reply": "2021-11-23T09:18:18.042649Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "import random\n",
    "import string\n",
    "from typing import Union, List, Tuple, Dict\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.optim import RMSprop, Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.datasets import IMDB\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats('png', 'pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0346746",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "In this exercise, we will learn the basic usage of PyTorch,\n",
    "a popular deep learning library.\n",
    "We already utilized PyTorch in the exercises before but only to construct matrices and\n",
    "perform operations on them.\n",
    "\n",
    "The networks that we will use are small enough that they can run on your personal\n",
    "computer, but, if you need a GPU, you can try Jupyter on Google Colab\n",
    "(click [here](https://colab.research.google.com/notebooks/intro.ipynb)).\n",
    "At the beginning of your session, get a GPU by clicking on \"Runtime\", then \"Change runtime type\",\n",
    "then choose \"GPU\" as hardware accelerator.\n",
    "\n",
    "### Loading and preparing the dataset\n",
    "\n",
    "The dataset that we will be working with is the IMDB dataset,\n",
    "which is included in PyTorch (or more precisely in `torchtext`).\n",
    "It contains 50,000 reviews that are highly polarized,\n",
    "that is, they are unambiguously either 'positive' or 'negative'.\n",
    "When the data is loaded, the training and test sets will contain 25,000 reviews each.\n",
    "In both sets, half of the reviews are positive and half are negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba9f4f9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:18.045077Z",
     "iopub.status.busy": "2021-11-23T09:18:18.044761Z",
     "iopub.status.idle": "2021-11-23T09:18:29.830897Z",
     "shell.execute_reply": "2021-11-23T09:18:29.830582Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_iterator, test_iterator = IMDB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa0e1b5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The `IMDB` object returns two iterators, which allows to iterate over the data.\n",
    "However, for us it is more convenient to just have everything directly, so we load\n",
    "the whole dataset into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdb67753",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:29.836160Z",
     "iopub.status.busy": "2021-11-23T09:18:29.835739Z",
     "iopub.status.idle": "2021-11-23T09:18:33.374532Z",
     "shell.execute_reply": "2021-11-23T09:18:33.374231Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_x: List = []\n",
    "train_y: List = []\n",
    "test_x: List = []\n",
    "test_y: List = []\n",
    "\n",
    "for label, line in train_iterator:\n",
    "    if len(line) < 25000:\n",
    "        train_x.append(line)\n",
    "        train_y.append(label)\n",
    "\n",
    "for label, line in test_iterator:\n",
    "    if len(line) < 25000:\n",
    "        test_x.append(line)\n",
    "        test_y.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33acbab5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's check a random review!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1de9da5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:33.377202Z",
     "iopub.status.busy": "2021-11-23T09:18:33.376833Z",
     "iopub.status.idle": "2021-11-23T09:18:33.379332Z",
     "shell.execute_reply": "2021-11-23T09:18:33.379000Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boasting an all-star cast so impressive that it almost seems like the \"Mad Mad Mad Mad World\" of horror pictures, \"The Sentinel\" (1977) is nevertheless an effectively creepy film centering on the relatively unknown actress Cristina Raines. In this one, she plays a fashion model, Alison Parker, who moves into a Brooklyn Heights brownstone that is (and I don't think I'm giving away too much at this late date) very close to the gateway of Hell. And as a tenant in this building, she suffers far worse conditions than leaky plumbing and the occasional water bug, to put it mildly! Indeed, the scene in which Alison encounters her noisy upstairs neighbor is truly terrifying, and should certainly send the ice water coursing down the spines of most viewers. Despite many critics' complaints regarding Raines' acting ability, I thought she was just fine, more than ably holding her own in scenes with Ava Gardner, Burgess Meredith, Arthur Kennedy, Chris Sarandon and Eli Wallach. The picture builds to an effectively eerie conclusion, and although some plot points go unexplained, I was left feeling more than satisfied. As the book \"DVD Delirium\" puts it, \"any movie with Beverly D'Angelo and Sylvia Miles as topless cannibal lesbians in leotards can't be all bad\"! On a side note, yesterday I walked over to 10 Montague Terrace in Brooklyn Heights to take a look at the Sentinel House. Yes, it's still there, and although shorn of its heavy coat of ivy and lacking a blind priest/nun at the top-floor window, looks much the same as it did in this picture. If this house really does sit atop the entrance to Hell, I take it that Hell is...the Brooklyn Queens Expressway. But we New Yorkers have known THAT for some time!\n"
     ]
    }
   ],
   "source": [
    "print(train_x[random.randint(0, len(train_x) - 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9e8099",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This looks nice but this is not a good representation for neural network training.\n",
    "Thus, we apply some preprocessing in the next steps:\n",
    "\n",
    "1. Tokenization.\n",
    "2. Count-vectorization.\n",
    "3. Convert to \"bag of words\" vectors.\n",
    "4. Convert string labels to binary.\n",
    "5. Create a PyTorch dataset.\n",
    "\n",
    "We start with removing punctuation and isolating single words. This is a very basic\n",
    "approach for tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d0363df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:33.464073Z",
     "iopub.status.busy": "2021-11-23T09:18:33.427850Z",
     "iopub.status.idle": "2021-11-23T09:18:34.899615Z",
     "shell.execute_reply": "2021-11-23T09:18:34.899245Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'rented', 'I', 'AM', 'CURIOUSYELLOW', 'from', 'my', 'video', 'store', 'because', 'of', 'all', 'the', 'controversy', 'that', 'surrounded', 'it', 'when', 'it', 'was', 'first', 'released', 'in', '1967', 'I', 'also', 'heard', 'that', 'at', 'first', 'it', 'was', 'seized', 'by', 'US', 'customs', 'if', 'it', 'ever', 'tried', 'to', 'enter', 'this', 'country', 'therefore', 'being', 'a', 'fan', 'of', 'films', 'considered', 'controversial', 'I', 'really', 'had', 'to', 'see', 'this', 'for', 'myselfbr', 'br', 'The', 'plot', 'is', 'centered', 'around', 'a', 'young', 'Swedish', 'drama', 'student', 'named', 'Lena', 'who', 'wants', 'to', 'learn', 'everything', 'she', 'can', 'about', 'life', 'In', 'particular', 'she', 'wants', 'to', 'focus', 'her', 'attentions', 'to', 'making', 'some', 'sort', 'of', 'documentary', 'on', 'what', 'the', 'average', 'Swede', 'thought', 'about', 'certain', 'political', 'issues', 'such', 'as', 'the', 'Vietnam', 'War', 'and', 'race', 'issues', 'in', 'the', 'United', 'States', 'In', 'between', 'asking', 'politicians', 'and', 'ordinary', 'denizens', 'of', 'Stockholm', 'about', 'their', 'opinions', 'on', 'politics', 'she', 'has', 'sex', 'with', 'her', 'drama', 'teacher', 'classmates', 'and', 'married', 'menbr', 'br', 'What', 'kills', 'me', 'about', 'I', 'AM', 'CURIOUSYELLOW', 'is', 'that', '40', 'years', 'ago', 'this', 'was', 'considered', 'pornographic', 'Really', 'the', 'sex', 'and', 'nudity', 'scenes', 'are', 'few', 'and', 'far', 'between', 'even', 'then', 'its', 'not', 'shot', 'like', 'some', 'cheaply', 'made', 'porno', 'While', 'my', 'countrymen', 'mind', 'find', 'it', 'shocking', 'in', 'reality', 'sex', 'and', 'nudity', 'are', 'a', 'major', 'staple', 'in', 'Swedish', 'cinema', 'Even', 'Ingmar', 'Bergman', 'arguably', 'their', 'answer', 'to', 'good', 'old', 'boy', 'John', 'Ford', 'had', 'sex', 'scenes', 'in', 'his', 'filmsbr', 'br', 'I', 'do', 'commend', 'the', 'filmmakers', 'for', 'the', 'fact', 'that', 'any', 'sex', 'shown', 'in', 'the', 'film', 'is', 'shown', 'for', 'artistic', 'purposes', 'rather', 'than', 'just', 'to', 'shock', 'people', 'and', 'make', 'money', 'to', 'be', 'shown', 'in', 'pornographic', 'theaters', 'in', 'America', 'I', 'AM', 'CURIOUSYELLOW', 'is', 'a', 'good', 'film', 'for', 'anyone', 'wanting', 'to', 'study', 'the', 'meat', 'and', 'potatoes', 'no', 'pun', 'intended', 'of', 'Swedish', 'cinema', 'But', 'really', 'this', 'film', 'doesnt', 'have', 'much', 'of', 'a', 'plot']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(data_list: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Tokenize a list of strings.\n",
    "\n",
    "    :param data_list: A list of strings.\n",
    "    :return: A list where each entry is a list including the tokenized elements.\n",
    "    \"\"\"\n",
    "    token_list: List[List[str]] = []\n",
    "    for data_string in data_list:\n",
    "        # Remove punctuation.\n",
    "        data_string = data_string.translate(str.maketrans('', '', string.punctuation))\n",
    "        # Split by space.\n",
    "        token_list.append(data_string.split())\n",
    "    return token_list\n",
    "\n",
    "train_x = tokenize(train_x)\n",
    "test_x = tokenize(test_x)\n",
    "\n",
    "print(train_x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43664840",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we will count all the words and rank them based on their occurrence.\n",
    "For example, if \"film\" is the second most word, it will be encoded as `2`.\n",
    "This process is also called count-vectorization. We also need to save mappings, so that\n",
    "we can translate between this and the text representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "387e34a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:34.944626Z",
     "iopub.status.busy": "2021-11-23T09:18:34.929152Z",
     "iopub.status.idle": "2021-11-23T09:18:38.622816Z",
     "shell.execute_reply": "2021-11-23T09:18:38.622083Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CountVectorizer:\n",
    "    def __init__(self):\n",
    "        self.vec_to_str_map: Dict[int, str] = {}\n",
    "        self.str_to_vec_map: Dict[str, int] = {}\n",
    "\n",
    "    def fit(self, token_list: List[str]) -> None:\n",
    "        # The `Counter` object from the `collections` library gives us efficient counting\n",
    "        # in large lists out of box.\n",
    "        cnt = Counter(token_list)\n",
    "        sorted_cnt = sorted(cnt.items(), key=lambda item: item[1], reverse=True)\n",
    "        sorted_words = [key for key, val in sorted_cnt]\n",
    "\n",
    "        # Python does not know a bidirectional mapping by default.\n",
    "        # We trick a bit by simply creating two dicts, but note that this is inefficient.\n",
    "        self.str_to_vec_map = {sorted_words[i]: i + 1 for i in range(len(sorted_words))}\n",
    "        self.vec_to_str_map = {i + 1: sorted_words[i] for i in range(len(sorted_words))}\n",
    "\n",
    "    def transform_to_vec(self, token_list: List[str]) -> List[int]:\n",
    "        return [self.str_to_vec_map.get(word) for word in token_list]\n",
    "\n",
    "    def transform_to_str(self, token_list: List[int]) -> List[str]:\n",
    "        return [self.vec_to_str_map.get(rank) for rank in token_list]\n",
    "\n",
    "train_words = [word for word_list in train_x for word in word_list]\n",
    "test_words = [word for word_list in test_x for word in word_list]\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "counter = count_vectorizer.fit(train_words)\n",
    "\n",
    "train_x = [count_vectorizer.transform_to_vec(word_list) for word_list in train_x]\n",
    "test_x = [count_vectorizer.transform_to_vec(word_list) for word_list in test_x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0eb944",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A sentence is now a list of integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce488d36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:38.627826Z",
     "iopub.status.busy": "2021-11-23T09:18:38.627208Z",
     "iopub.status.idle": "2021-11-23T09:18:38.630092Z",
     "shell.execute_reply": "2021-11-23T09:18:38.629614Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 1594, 8, 13839, 41841, 35, 70, 434, 1191, 80, 4, 32, 1, 7739, 9, 3423, 10, 55, 10, 13, 89, 654, 7, 7974, 8, 91, 559, 9, 30, 89, 10, 13, 22082, 31, 835, 10585, 60, 10, 129, 787, 5, 3577, 11, 783, 2044, 110, 2, 341, 4, 94, 1165, 3150, 8, 59, 61, 5, 66, 11, 17, 14800, 12, 14, 114, 6, 6504, 187, 2, 199, 4238, 500, 1611, 777, 5011, 34, 481, 5, 847, 330, 72, 67, 41, 128, 121, 826, 72, 481, 5, 1139, 39, 13005, 5, 255, 47, 439, 4, 699, 21, 51, 1, 895, 36302, 196, 41, 809, 1007, 1328, 141, 15, 1, 2757, 969, 3, 1672, 1328, 7, 1, 2535, 2532, 121, 194, 2193, 7740, 3, 1999, 20904, 4, 23443, 41, 64, 5012, 21, 2549, 72, 43, 424, 16, 39, 500, 1774, 8512, 3, 1052, 10586, 12, 216, 1116, 71, 41, 8, 13839, 41841, 6, 9, 1831, 154, 614, 11, 13, 1165, 8972, 2068, 1, 424, 3, 1048, 139, 22, 164, 3, 243, 194, 65, 107, 37, 24, 333, 36, 47, 6971, 88, 5058, 441, 70, 26993, 357, 162, 10, 1635, 7, 677, 424, 3, 1048, 22, 2, 723, 10818, 7, 4238, 495, 380, 15929, 5218, 5059, 64, 1564, 5, 48, 175, 522, 305, 2092, 61, 424, 139, 7, 23, 3817, 12, 8, 84, 13840, 1, 902, 17, 1, 189, 9, 100, 424, 613, 7, 1, 19, 6, 613, 17, 1618, 5554, 250, 73, 44, 5, 1579, 82, 3, 93, 291, 5, 27, 613, 7, 8972, 2245, 7, 949, 8, 13839, 41841, 6, 2, 48, 19, 17, 278, 1785, 5, 2093, 1, 4518, 3, 20905, 68, 5682, 1423, 4, 4238, 495, 101, 59, 11, 19, 152, 25, 74, 4, 2, 114]\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cad1f90",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can convert it back using the fitted `CountVectorizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6f8766d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:38.634317Z",
     "iopub.status.busy": "2021-11-23T09:18:38.633700Z",
     "iopub.status.idle": "2021-11-23T09:18:38.636416Z",
     "shell.execute_reply": "2021-11-23T09:18:38.636889Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'rented', 'I', 'AM', 'CURIOUSYELLOW', 'from', 'my', 'video', 'store', 'because', 'of', 'all', 'the', 'controversy', 'that', 'surrounded', 'it', 'when', 'it', 'was', 'first', 'released', 'in', '1967', 'I', 'also', 'heard', 'that', 'at', 'first', 'it', 'was', 'seized', 'by', 'US', 'customs', 'if', 'it', 'ever', 'tried', 'to', 'enter', 'this', 'country', 'therefore', 'being', 'a', 'fan', 'of', 'films', 'considered', 'controversial', 'I', 'really', 'had', 'to', 'see', 'this', 'for', 'myselfbr', 'br', 'The', 'plot', 'is', 'centered', 'around', 'a', 'young', 'Swedish', 'drama', 'student', 'named', 'Lena', 'who', 'wants', 'to', 'learn', 'everything', 'she', 'can', 'about', 'life', 'In', 'particular', 'she', 'wants', 'to', 'focus', 'her', 'attentions', 'to', 'making', 'some', 'sort', 'of', 'documentary', 'on', 'what', 'the', 'average', 'Swede', 'thought', 'about', 'certain', 'political', 'issues', 'such', 'as', 'the', 'Vietnam', 'War', 'and', 'race', 'issues', 'in', 'the', 'United', 'States', 'In', 'between', 'asking', 'politicians', 'and', 'ordinary', 'denizens', 'of', 'Stockholm', 'about', 'their', 'opinions', 'on', 'politics', 'she', 'has', 'sex', 'with', 'her', 'drama', 'teacher', 'classmates', 'and', 'married', 'menbr', 'br', 'What', 'kills', 'me', 'about', 'I', 'AM', 'CURIOUSYELLOW', 'is', 'that', '40', 'years', 'ago', 'this', 'was', 'considered', 'pornographic', 'Really', 'the', 'sex', 'and', 'nudity', 'scenes', 'are', 'few', 'and', 'far', 'between', 'even', 'then', 'its', 'not', 'shot', 'like', 'some', 'cheaply', 'made', 'porno', 'While', 'my', 'countrymen', 'mind', 'find', 'it', 'shocking', 'in', 'reality', 'sex', 'and', 'nudity', 'are', 'a', 'major', 'staple', 'in', 'Swedish', 'cinema', 'Even', 'Ingmar', 'Bergman', 'arguably', 'their', 'answer', 'to', 'good', 'old', 'boy', 'John', 'Ford', 'had', 'sex', 'scenes', 'in', 'his', 'filmsbr', 'br', 'I', 'do', 'commend', 'the', 'filmmakers', 'for', 'the', 'fact', 'that', 'any', 'sex', 'shown', 'in', 'the', 'film', 'is', 'shown', 'for', 'artistic', 'purposes', 'rather', 'than', 'just', 'to', 'shock', 'people', 'and', 'make', 'money', 'to', 'be', 'shown', 'in', 'pornographic', 'theaters', 'in', 'America', 'I', 'AM', 'CURIOUSYELLOW', 'is', 'a', 'good', 'film', 'for', 'anyone', 'wanting', 'to', 'study', 'the', 'meat', 'and', 'potatoes', 'no', 'pun', 'intended', 'of', 'Swedish', 'cinema', 'But', 'really', 'this', 'film', 'doesnt', 'have', 'much', 'of', 'a', 'plot']\n"
     ]
    }
   ],
   "source": [
    "print(count_vectorizer.transform_to_str(train_x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c38938",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Before we feed the reviews to the network, we need to convert them from sequences of\n",
    "integers to \"bag of words\" vectors. For example, turning the sequence (3,5,9)\n",
    "into a 10 dimensional vector gives us (0,0,1,0,1,0,0,0,1,0),\n",
    "which has a 1 in the positions 3, 5 and 9 and zeros everywhere else.\n",
    "\n",
    "We only keep the 10,000 most common words, which will be the size of the input vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f32ae263",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:38.653460Z",
     "iopub.status.busy": "2021-11-23T09:18:38.641402Z",
     "iopub.status.idle": "2021-11-23T09:18:38.681325Z",
     "shell.execute_reply": "2021-11-23T09:18:38.681720Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_index_vector(sequence: List[int], size: int = 10000) -> List[int]:\n",
    "# TODO: Encode each sequence to a binary vector as described above.\n",
    "# For now we rely on plain python, thus a \"binary vector\" is only a list of ints.\n",
    "# Note: The size argument specifies the maximal number of words in the index vector.\n",
    "# Note 2: Also take care of potential `None` values.\n",
    "\n",
    "train_x = [get_index_vector(count_vector) for count_vector in train_x]\n",
    "test_x = [get_index_vector(count_vector) for count_vector in test_x]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9fbf73",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lastly we need to convert the labels to a fitting format as well.\n",
    "As we only have a binary outcome, our label will be 1 for positive and\n",
    "0 for negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38573607",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:38.688884Z",
     "iopub.status.busy": "2021-11-23T09:18:38.688250Z",
     "iopub.status.idle": "2021-11-23T09:18:38.690401Z",
     "shell.execute_reply": "2021-11-23T09:18:38.689995Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_y = [1 if label == 'pos' else 0 for label in train_y]\n",
    "test_y = [1 if label == 'pos' else 0 for label in test_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091a48e8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now have everything ready to built a Pytorch `Dataset` object,\n",
    "which is recommended for the training process.\n",
    "\n",
    "We also define `device`. This indicates the location where we would like to process data.\n",
    "By default tensors are on the CPU. If we have a GPU, we could set `device` to `cuda` to\n",
    "utilize GPU power. If we have set a device, we can then push a tensor to the desired location\n",
    "by calling `<tensor>.to(device)`. Tensors can also be created directly on the device\n",
    "by specifying the `device` argument on tensor initialization.\n",
    "In this setting device is set to `cuda` if a GPU is\n",
    "available, otherwise we'll just use the cpu.\n",
    "\n",
    "Pushing the whole dataset to GPU is often not a possibility due to memory constraints,\n",
    "but in this exercise the samll vectorized IMDB data will only consume around 3GB VRAM.\n",
    "We could also save memory by reducing the size of the input vector or using sparse tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1f78da6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:38.703239Z",
     "iopub.status.busy": "2021-11-23T09:18:38.694036Z",
     "iopub.status.idle": "2021-11-23T09:18:38.718972Z",
     "shell.execute_reply": "2021-11-23T09:18:38.719287Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data: Union[List, Tuple], labels: List, device: torch.device):\n",
    "        self.data = torch.tensor(data, dtype=torch.float, device=device)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float, device=device)\n",
    "\n",
    "    # We don't need this method necessarily,\n",
    "    # but it is usually good to have direct access to the dimensions of the dataset.\n",
    "    @property\n",
    "    def shape(self) -> Tuple:\n",
    "        return self.data.shape\n",
    "\n",
    "    # The `__len__` method should return the number of samples in the dataset.\n",
    "    # This will later on be used automatically by the data loader.\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    # Each Dataset needs to implement the `__get_item__` method.\n",
    "    # The method gets an index and should return the corresponding items.\n",
    "    # For example index = 5 should return the 5th review and its matching label.\n",
    "    def __getitem__(self, idx: int) -> Tuple[Tensor, Tensor]:\n",
    "# TODO: Return the correct review and label for the index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9542c1c6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let’s also create a validation set to monitor the (generalization) performance of the model during training,\n",
    "by randomly taking the 10,000 samples of the training data and the corresponding labels.\n",
    "The new training set should contain only the remaining 15,000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4634710",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:38.730732Z",
     "iopub.status.busy": "2021-11-23T09:18:38.722669Z",
     "iopub.status.idle": "2021-11-23T09:18:38.746777Z",
     "shell.execute_reply": "2021-11-23T09:18:38.747098Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get random indices for training and validation split\n",
    "shuffled_indices = list(range(len(train_x)))\n",
    "random.shuffle(shuffled_indices)\n",
    "\n",
    "train_idxs = shuffled_indices[:15000]\n",
    "val_idxs = shuffled_indices[15000:]\n",
    "\n",
    "# Plain python does not know multi index selection\n",
    "# We can still use `itemgetter` from the `operator` module to achieve what we want.\n",
    "val_x = itemgetter(*val_idxs)(train_x)\n",
    "val_y = itemgetter(*val_idxs)(train_y)\n",
    "\n",
    "train_x = itemgetter(*train_idxs)(train_x)\n",
    "train_y = itemgetter(*train_idxs)(train_y)\n",
    "\n",
    "# We can now finally initialize our PyTorch datasets.\n",
    "train_dataset = IMDBDataset(train_x, train_y, device)\n",
    "val_dataset = IMDBDataset(val_x, val_y, device)\n",
    "test_dataset = IMDBDataset(test_x, test_y, device)\n",
    "\n",
    "print('Training\\t Shape: {}'.format(train_dataset.shape))\n",
    "print('Validation\\tShape: {}'.format(val_dataset.shape))\n",
    "print('Test\\t\\tShape: {}'.format(test_dataset.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8327ea",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Our data is now ready to be fed to a neural network.\n",
    "\n",
    "Let's remove the preprocessed data lists to free some RAM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad52c957",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:38.750665Z",
     "iopub.status.busy": "2021-11-23T09:18:38.750180Z",
     "iopub.status.idle": "2021-11-23T09:18:38.775236Z",
     "shell.execute_reply": "2021-11-23T09:18:38.774873Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "del train_x\n",
    "del train_y\n",
    "del val_x\n",
    "del val_y\n",
    "del test_x\n",
    "del test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e669b5b2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Building the network\n",
    "When deciding on an architecture for a neural network with fully connected layers,\n",
    "the two key things to consider are:\n",
    "\n",
    " - The number of hidden layers in the network\n",
    " - The number of neurons in each of the hidden layers\n",
    "\n",
    "Increasing the depth of the network (that is, adding layers) or increasing the number\n",
    "of neurons in a given layer will increase the capacity of the network and allow it\n",
    "to learn more complex non-linear decision boundaries.\n",
    "However, making the network too large can lead to overfitting.\n",
    "\n",
    "In practice, deciding on an architecture is an iterative process where\n",
    "many different networks are trained in order to find a good setting for the\n",
    "hyperparameters. For this exercise, however, we will use a simple feedforward neural\n",
    "network with two fully-connected hidden layers consisting of 16 neurons each,\n",
    "and a single output neuron which outputs the probability of the review being 'positive'.\n",
    "\n",
    "In PyTorch the `nn` module holds all building blocks that we need.\n",
    "The `Sequential` module allows us to specify a neural network layer by layer.\n",
    "A fully connected layer is available as `Linear`.\n",
    "Simiarly we can import the activation functions as `ReLU` and `Sigmoid`.\n",
    "We need to push the model to our `device` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3112d1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:38.787906Z",
     "iopub.status.busy": "2021-11-23T09:18:38.787457Z",
     "iopub.status.idle": "2021-11-23T09:18:38.803993Z",
     "shell.execute_reply": "2021-11-23T09:18:38.803608Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_size = train_dataset.shape[1]\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=input_size, out_features=16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=16, out_features=16),\n",
    "    nn.ReLU(),\n",
    "# TODO: Add the output neuron and activation.\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1cc0fb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "So, we've built our model. Before we can train the network, however, we must specify:\n",
    "\n",
    "1. The loss function to use (mean squared error, cross entropy, etc) (Info: [here](https://pytorch.org/docs/stable/nn.html#loss-functions))\n",
    "2. The optimizer (SGD, Adam, RMSProp, etc.) (Info: [here](https://pytorch.org/docs/stable/optim.html))\n",
    "3. Any metrics (such as accuracy) used to measure the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc3fb445",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:38.816002Z",
     "iopub.status.busy": "2021-11-23T09:18:38.815559Z",
     "iopub.status.idle": "2021-11-23T09:18:38.832148Z",
     "shell.execute_reply": "2021-11-23T09:18:38.831573Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss = (\n",
    "# TODO: Add the appropriate loss for binary classification.\n",
    ")\n",
    "\n",
    "optimizer = (\n",
    "# TODO: Add the RMSprop optimizer.\n",
    "# Note: You also need the specify the parameters we want to optimize.\n",
    "# You get these by calling `model.parameters()`.\n",
    ")\n",
    "\n",
    "def get_accuracy(prediction: Tensor, target: Tensor) -> float:\n",
    "    return float(torch.sum(((prediction > 0.5) == target).float()) / len(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecae91e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Fit\n",
    "We wish to train the network for 20 epochs with batches of size 512.\n",
    "In contrast to e.g. Keras, in the basic PyTorch approach we need to implement our\n",
    "training routine mostly from scratch. Luckily PyTorch provides a `DataLoader` utility,\n",
    "which allows to easily sample from our `Datasets`.\n",
    "On the downside we still need to manage metrics on our own.\n",
    "\n",
    "The `train` function below implements one version of a training loop.\n",
    "The outer `for` loop is for the number of training epochs. The inner `for` loop\n",
    "iterates over the whole dataset with the help of the `Dataloader`.\n",
    "The output of the dataloader is dependend on batch size and the definition of the\n",
    "provided `Dataset`. In our case we specified in the `__get_item__` function of\n",
    "`IMDBDataset` that a  tuple with one training sample and label should be returned.\n",
    "The dataloader does batching and collating automatically. This means in the backend\n",
    "the loader utilizes our `__get_item__` method but presents us batched results.\n",
    "In other words, for a batch size of 512 the loader gives us a tuple of a 512 x 10000\n",
    "sample matrix and a label vector of length 512, which we directly unpack into `x` and `y`.\n",
    "\n",
    "The actual training logic is straightforward. First, we need to do a forward pass and\n",
    "compute the loss. The optimization itself follows three steps.\n",
    "\n",
    "1. Make sure all gradients of the parameter tensors are zeroed or None (`optim.zero_grad()`)\n",
    "2. Backpropagate the error (`loss.backward()`)\n",
    "3. Apply the gradients using the optimizer (`optim.step`)\n",
    "\n",
    "The validation loop follows the exact same principles, but obviously we don't do any\n",
    "optimization steps. The `torch.no_grad()` context manager implies that no gradients\n",
    "are calculated and no results are stashed on the computation. This makes a significant\n",
    "difference in speed in larger models.\n",
    "\n",
    "You may notice that we have set the model in training (`model.train()`) and evaluation\n",
    "(`model.eval()`) mode. For our simple model this doesn't make a difference, but for some\n",
    "layers like `Dropout` or `BatchNormalization` this setting triggers different policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f1a5f7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:38.847147Z",
     "iopub.status.busy": "2021-11-23T09:18:38.846404Z",
     "iopub.status.idle": "2021-11-23T09:18:38.863439Z",
     "shell.execute_reply": "2021-11-23T09:18:38.863048Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    loss: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    train_dataset: Dataset,\n",
    "    val_dataset: Dataset,\n",
    "    batch_size: int,\n",
    "    epochs: int\n",
    ") -> Dict:\n",
    "\n",
    "    # Define a dict with room for metrics that will be populated during training.\n",
    "    metrics: Dict = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "    }\n",
    "\n",
    "    # The loader allows to shuffle the training data on the fly.\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "\n",
    "        batch_losses = []\n",
    "        predictions = []\n",
    "        targets = []\n",
    "\n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "\n",
    "            ############################################\n",
    "            # TRAINING LOGIC\n",
    "            ############################################\n",
    "            # Set the model to training mode\n",
    "            model.train()\n",
    "\n",
    "            # Forward pass through the model\n",
    "            y_hat = model(x).squeeze()\n",
    "            # Obtain the loss\n",
    "            batch_loss = loss(y_hat, y)\n",
    "\n",
    "# TODO: Add backpropagation of the loss and apply the gradients via the optimizer\n",
    "\n",
    "            ############################################\n",
    "\n",
    "            batch_losses.append(batch_loss)\n",
    "            predictions.append(y_hat.detach())\n",
    "            targets.append(y)\n",
    "\n",
    "\n",
    "        ep_train_loss = float(torch.mean(torch.stack(batch_losses)))\n",
    "        ep_train_acc = get_accuracy(torch.cat(predictions), torch.cat(targets))\n",
    "\n",
    "        batch_losses.clear()\n",
    "        predictions.clear()\n",
    "        targets.clear()\n",
    "\n",
    "        for batch_idx, (x, y) in enumerate(val_loader):\n",
    "\n",
    "            ############################################\n",
    "            # VALIDATION LOGIC\n",
    "            ############################################\n",
    "            # Set the model to evaluation mode\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "# TODO: Do a forward pass and get the batch loss\n",
    "            ############################################\n",
    "\n",
    "            batch_losses.append(batch_loss)\n",
    "            predictions.append(y_hat.detach())\n",
    "            targets.append(y)\n",
    "\n",
    "        ep_val_loss = float(torch.mean(torch.stack(batch_losses)))\n",
    "        ep_val_acc = get_accuracy(torch.cat(predictions), torch.cat(targets))\n",
    "\n",
    "        metrics['train_loss'].append(ep_train_loss)\n",
    "        metrics['train_acc'].append(ep_train_acc)\n",
    "        metrics['val_loss'].append(ep_val_loss)\n",
    "        metrics['val_acc'].append(ep_val_acc)\n",
    "\n",
    "        print('EPOCH:\\t{:5}\\tTRAIN LOSS:\\t{:.3f}\\tTRAIN ACCURACY:\\t{:.2f}'\n",
    "              '\\tVAL LOSS:\\t {:.5f}\\tVAL ACCURACY:\\t {:.2f}'\n",
    "              .format(ep, ep_train_loss, ep_train_acc, ep_val_loss, ep_val_acc), end='\\r')\n",
    "\n",
    "    return metrics\n",
    "\n",
    "metrics = train(\n",
    "    model=model,\n",
    "    loss=loss,\n",
    "    optimizer=optimizer,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    epochs=20,\n",
    "    batch_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6f5eed",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's visualize the training progress.\n",
    "We can utilize the `metrics` that are returned from our `train` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06cdcaa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:38.875575Z",
     "iopub.status.busy": "2021-11-23T09:18:38.875122Z",
     "iopub.status.idle": "2021-11-23T09:18:38.890966Z",
     "shell.execute_reply": "2021-11-23T09:18:38.890551Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_training_progress_plot(\n",
    "        train_losses: List[float],\n",
    "        train_accs: List[float],\n",
    "        val_losses: List[float],\n",
    "        val_accs: List[float],\n",
    ") -> None:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 2))\n",
    "\n",
    "    ax1.set_title('Loss')\n",
    "    ax1.plot(train_losses, label='Train Loss')\n",
    "    ax1.plot(val_losses, label='Val Loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.set_title('Accuracy')\n",
    "    ax2.plot(train_accs, label='Train Accuracy')\n",
    "    ax2.plot(val_accs, label='Val Accuracy')\n",
    "    ax2.legend()\n",
    "\n",
    "get_training_progress_plot(\n",
    "    metrics['train_loss'],\n",
    "    metrics['train_acc'],\n",
    "    metrics['val_loss'],\n",
    "    metrics['val_acc'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83949db8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As expected, the training loss decreases with each epoch (and training accuracy increases).\n",
    "However, the validation loss decreases initially and then begins to increase after\n",
    "the first few epochs epochs. Therefore, the network has overfit.\n",
    "\n",
    "### Evaluate\n",
    "Let's evaluate the performance of the model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "929ecd26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:38.903994Z",
     "iopub.status.busy": "2021-11-23T09:18:38.903560Z",
     "iopub.status.idle": "2021-11-23T09:18:38.917970Z",
     "shell.execute_reply": "2021-11-23T09:18:38.918281Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate(model: nn.Module, test_dataset: Dataset, batch_size: int = 512) -> Dict:\n",
    "    batch_losses: List = []\n",
    "    predictions: List = []\n",
    "    targets: List = []\n",
    "\n",
    "    for x, y in DataLoader(test_dataset, batch_size):\n",
    "# TODO: Do a forward pass and get the batch loss\n",
    "        ############################################\n",
    "\n",
    "        batch_losses.append(batch_loss)\n",
    "        predictions.append(y_hat.detach())\n",
    "        targets.append(y)\n",
    "\n",
    "    eval_loss = float(torch.mean(torch.stack(batch_losses)))\n",
    "    eval_acc = get_accuracy(torch.cat(predictions), torch.cat(targets))\n",
    "\n",
    "    return {'test_loss': eval_loss, 'test_acc': eval_acc}\n",
    "\n",
    "eval_metrics = evaluate(model, test_dataset)\n",
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8803d403",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Our simple model does reasonably well. It achieves an accuracy of around 85-88%.\n",
    "\n",
    "### Predict\n",
    "Finally, to generate the likelihood of the reviews being positive, we only need to\n",
    "forward data through our fitted model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bec39c76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:38.930551Z",
     "iopub.status.busy": "2021-11-23T09:18:38.930133Z",
     "iopub.status.idle": "2021-11-23T09:18:38.944577Z",
     "shell.execute_reply": "2021-11-23T09:18:38.944885Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_x = test_dataset.data[:10]\n",
    "test_y = test_dataset.labels[:10]\n",
    "predictions = model(test_x)\n",
    "\n",
    "for i in range(len(test_x)):\n",
    "    print('{} | TRUE: {} | PRED {:.2e}'.format(i, int(test_y[i]), float(predictions[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f013589",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now play around with the code by adding and deleting layers, changing the hidden activation, optimizer, learning rate, batch-size, etc.\n",
    "\n",
    "### Conclusion\n",
    "Here's what you should take away from this example:\n",
    "\n",
    " - You usually need to do quite a bit of preprocessing on your raw data in order to be\n",
    "able to feed it -- as tensors -- into a neural network.\n",
    " - Stacks of dense layers with `ReLU` activations can solve a wide range of problems\n",
    "(including sentiment classification), and you'll likely use them frequently.\n",
    " - As they get better on their training data, neural networks eventually start\n",
    "_overfitting_ and end up obtaining increasingly worse results on data they have never\n",
    "seen before. Be sure to always monitor performance on data that is outside of the training set.\n",
    "\n",
    "\n",
    "## Exercise 2\n",
    "In this exercise, we will look at a couple of different methods to regularize a neural\n",
    "network in order to prevent overfitting.\n",
    "\n",
    "Plotting the validation loss is a simple way to determine whether the network has\n",
    "overfit. During the first few epochs of training, both the training loss and the\n",
    "validation loss tend to decrease in tandem but after a certain point, the validation\n",
    "loss starts to increase while the training loss continues to decrease.\n",
    "It is at this point that the network begins to overfit.\n",
    "\n",
    "### Training multiple networks\n",
    "\n",
    "In order to get a feel for the overfitting behaviour of neural networks,\n",
    "we will train 3 different architectures and observe the training and validation losses.\n",
    "\n",
    "Create the first model with two hidden layers, each with 16 units and ReLU activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58ef13f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:38.956123Z",
     "iopub.status.busy": "2021-11-23T09:18:38.947765Z",
     "iopub.status.idle": "2021-11-23T09:18:38.972101Z",
     "shell.execute_reply": "2021-11-23T09:18:38.971732Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_size = train_dataset.shape[1]\n",
    "\n",
    "original_model = (\n",
    "# TODO: Create the network according to the specifications above and push it to `device`.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe34f03",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Our second model will be similar to the first but it will be much smaller.\n",
    "Reduce the number of neurons in the hidden layers from 16 to 4, and keep everything\n",
    "else unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "293108e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:38.975843Z",
     "iopub.status.busy": "2021-11-23T09:18:38.975269Z",
     "iopub.status.idle": "2021-11-23T09:18:39.000121Z",
     "shell.execute_reply": "2021-11-23T09:18:38.999696Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "smaller_model = (\n",
    "# TODO: Create the network according to the specifications above and push it to `device`.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277b358e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now train both networks network using our `train` function for 20 epochs using a\n",
    "batch size of 512. Remember to use a separate validation dataset.\n",
    "We also need to initialize a new optimizer for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f696d262",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:39.004340Z",
     "iopub.status.busy": "2021-11-23T09:18:39.003769Z",
     "iopub.status.idle": "2021-11-23T09:18:39.028575Z",
     "shell.execute_reply": "2021-11-23T09:18:39.028150Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training original model...')\n",
    "original_model_metrics = train(\n",
    "# TODO: Fill in the correct parameters for the function\n",
    ")\n",
    "\n",
    "print('Training smaller model...')\n",
    "smaller_model_metrics = train(\n",
    "# TODO: Fill in the correct parameters for the function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf811f6b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Plotting the losses\n",
    "\n",
    "Let's compare the losses over the course of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8de6e087",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:39.042154Z",
     "iopub.status.busy": "2021-11-23T09:18:39.033356Z",
     "iopub.status.idle": "2021-11-23T09:18:39.058163Z",
     "shell.execute_reply": "2021-11-23T09:18:39.057738Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def compare_losses_plot(\n",
    "        first_losses: List[float],\n",
    "        second_losses: List[float],\n",
    "        first_loss_label: str,\n",
    "        second_loss_label: str\n",
    ") -> None:\n",
    "    plt.plot(first_losses, label=first_loss_label)\n",
    "    plt.plot(second_losses, label=second_loss_label)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "compare_losses_plot(\n",
    "    original_model_metrics['val_loss'],\n",
    "    smaller_model_metrics['val_loss'],\n",
    "    'original',\n",
    "    'smaller'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732c0ed7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As you can see, the smaller network starts overfitting later than the original one and\n",
    "its performance degrades much more slowly once it starts overfitting.\n",
    "\n",
    "### Third model\n",
    "Now we build a third neural network that is even bigger than the original network.\n",
    "If the previous plot is any indication, this new network should overfit even worse\n",
    "than the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75425a2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:39.070860Z",
     "iopub.status.busy": "2021-11-23T09:18:39.070426Z",
     "iopub.status.idle": "2021-11-23T09:18:39.085487Z",
     "shell.execute_reply": "2021-11-23T09:18:39.085129Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bigger_model = (\n",
    "# TODO: Create a bigger network with 2 hidden layers of size 512 and push it to `device`.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0521ec",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's train this network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3178d9d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:39.097782Z",
     "iopub.status.busy": "2021-11-23T09:18:39.096998Z",
     "iopub.status.idle": "2021-11-23T09:18:39.113894Z",
     "shell.execute_reply": "2021-11-23T09:18:39.113470Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training bigger model...')\n",
    "bigger_model_metrics = train(\n",
    "# TODO: Fill in the correct parameters for the function\n",
    "# TODO: Also use the `Adam` optimizer if there are convergence issues.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec139de",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here's how the bigger network fares compared to the reference one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9a2226f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:39.126438Z",
     "iopub.status.busy": "2021-11-23T09:18:39.126011Z",
     "iopub.status.idle": "2021-11-23T09:18:39.141573Z",
     "shell.execute_reply": "2021-11-23T09:18:39.141890Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compare_losses_plot(\n",
    "    original_model_metrics['val_loss'],\n",
    "    bigger_model_metrics['val_loss'],\n",
    "    'original',\n",
    "    'bigger'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df504913",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The bigger network starts overfitting almost right away, after just one epoch\n",
    "and overfits much more severely. Its validation loss is also more noisy.\n",
    "\n",
    "Let's plot the training losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51f10de5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:39.154725Z",
     "iopub.status.busy": "2021-11-23T09:18:39.154299Z",
     "iopub.status.idle": "2021-11-23T09:18:39.170967Z",
     "shell.execute_reply": "2021-11-23T09:18:39.170511Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compare_losses_plot(\n",
    "    original_model_metrics['train_loss'],\n",
    "    bigger_model_metrics['train_loss'],\n",
    "    'original',\n",
    "    'bigger'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76236d2a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As you can see, the bigger network gets its training loss near zero very quickly.\n",
    "The more capacity the network has, the quicker it will be able to model the training\n",
    "data (resulting in a low training loss), but the more susceptible it is to overfitting\n",
    "(resulting in a large difference between the training and validation loss).\n",
    "\n",
    "### Adding weight regularization\n",
    "\n",
    "Regularizing a model in PyTorch can be done over the additional `weight_decay`\n",
    "argument in the optimizer function. By setting a coefficient there, the linked\n",
    "parameters will have a L2 penalty.\n",
    "\n",
    "In the following we redeclare the original model, set an optimizer with\n",
    "`weight_decay` coefficient of 0.001 and fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af038d6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:39.183608Z",
     "iopub.status.busy": "2021-11-23T09:18:39.183171Z",
     "iopub.status.idle": "2021-11-23T09:18:39.199026Z",
     "shell.execute_reply": "2021-11-23T09:18:39.198595Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "regularized_model = nn.Sequential(\n",
    "        nn.Linear(in_features=input_size, out_features=16),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=16, out_features=16),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=16, out_features=1),\n",
    "        nn.Sigmoid()\n",
    ").to(device)\n",
    "\n",
    "regularized_model_metrics = train(\n",
    "    model=regularized_model,\n",
    "    loss=nn.BCELoss(),\n",
    "    optimizer=RMSprop(regularized_model.parameters(), weight_decay=0.001),\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    epochs=20,\n",
    "    batch_size=512\n",
    ")\n",
    "\n",
    "compare_losses_plot(\n",
    "    original_model_metrics['val_loss'],\n",
    "    regularized_model_metrics['val_loss'],\n",
    "    'original',\n",
    "    'regularized'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaf43c8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As you can see, the regularized model does not overfit as much,\n",
    "even though both models have the same number of parameters.\n",
    "Feel free to play with the regularization strength to get a feel on how different\n",
    "settings affect learning. When is regularization preventing the network from\n",
    "learning anything at all? When is regularization so weak it does not make a difference?\n",
    "\n",
    "### Dropout regularization\n",
    "Dropout is a very popular technique to regularize neural nets.\n",
    "It works by randomly turning off (or \"dropping out\")  the input/hidden neurons in a\n",
    "network. This means that every neuron is trained on a different set of examples.\n",
    "Note that dropout is, in most cases, only used during training time.\n",
    "At test time, all units are used with their activations scaled down by the dropout rate\n",
    "to account for the fact that all neurons were used for the prediction.\n",
    "Normally, dropout is not applied to the inputs.\n",
    "\n",
    "In Keras, dropout is implemented as its own separate layer (`Dropout`) that takes as\n",
    "input the probability to _drop_ units. To apply dropout to a layer, place a `Dropout` after\n",
    "it while stacking layers. The dropout will be ignored if the model is in `eval` mode.\n",
    "Luckily, we already set the correct modes for training and evaluation in our `train`\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59723698",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:39.203435Z",
     "iopub.status.busy": "2021-11-23T09:18:39.202843Z",
     "iopub.status.idle": "2021-11-23T09:18:39.228164Z",
     "shell.execute_reply": "2021-11-23T09:18:39.227743Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dropout_model = nn.Sequential(\n",
    "# TODO: Keep the structure of the original model,\n",
    "# but add dropout after the hidden layers with prob=0.5\n",
    ").to(device)\n",
    "\n",
    "dropout_model_metrics = train(\n",
    "    model=dropout_model,\n",
    "    loss=nn.BCELoss(),\n",
    "    optimizer=RMSprop(dropout_model.parameters()),\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    epochs=20,\n",
    "    batch_size=512\n",
    ")\n",
    "\n",
    "compare_losses_plot(\n",
    "    original_model_metrics['val_loss'],\n",
    "    dropout_model_metrics['val_loss'],\n",
    "    'original',\n",
    "    'dropout'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d11f5a0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Once again, we see a marked improvement in the new model.\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "Previously, we were training the network and checking _after training_ when it started\n",
    "to overfit. But another very popular method to regularize a network is to stop\n",
    "training earlier than the specified number of epochs, by checking when the validation\n",
    "loss starts to increase.\n",
    "\n",
    "There are no out of box utilities to achieve this kind of behavior, so we will adjust\n",
    "the training loop accordingly.\n",
    "We also add the `patience` argument to the function.\n",
    "`patience` indicates how many epochs to wait for an improvement of the validation loss.\n",
    "If there is no improvement for more than `patience` epochs, training is interrupted.\n",
    "\n",
    "NOTE: We only implement a very naive method of early stopping. Usually you would\n",
    "need to roll back the weights to the epoch, which had the last improvement (early\n",
    "stopping epoch minus patience). However, checkpointing and caching weights is out\n",
    "of scope of this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5be365d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:39.241573Z",
     "iopub.status.busy": "2021-11-23T09:18:39.232581Z",
     "iopub.status.idle": "2021-11-23T09:18:39.257730Z",
     "shell.execute_reply": "2021-11-23T09:18:39.257311Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    loss: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    train_dataset: Dataset,\n",
    "    val_dataset: Dataset,\n",
    "    batch_size: int,\n",
    "    epochs: int,\n",
    "    early_stopping: bool = False,\n",
    "    patience: int = 2,\n",
    ") -> Dict:\n",
    "\n",
    "    # Define a dict with room for metrics that will be populated during training.\n",
    "    metrics: Dict = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "    }\n",
    "\n",
    "    # Track how often in a row no improvements happen\n",
    "    early_stopping_strikes = 0\n",
    "\n",
    "    # The loader allows to shuffle the training data on the fly.\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "\n",
    "        batch_losses = []\n",
    "        predictions = []\n",
    "        targets = []\n",
    "\n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "            model.train()\n",
    "\n",
    "            y_hat = model(x).squeeze()\n",
    "            batch_loss = loss(y_hat, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_losses.append(batch_loss)\n",
    "            predictions.append(y_hat.detach())\n",
    "            targets.append(y)\n",
    "\n",
    "\n",
    "        ep_train_loss = float(torch.mean(torch.stack(batch_losses)))\n",
    "        ep_train_acc = get_accuracy(torch.cat(predictions), torch.cat(targets))\n",
    "\n",
    "        batch_losses.clear()\n",
    "        predictions.clear()\n",
    "        targets.clear()\n",
    "\n",
    "        for batch_idx, (x, y) in enumerate(val_loader):\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_hat = model(x).squeeze()\n",
    "                batch_loss = loss(y_hat, y)\n",
    "\n",
    "            batch_losses.append(batch_loss)\n",
    "            predictions.append(y_hat.detach())\n",
    "            targets.append(y)\n",
    "\n",
    "        ep_val_loss = float(torch.mean(torch.stack(batch_losses)))\n",
    "        ep_val_acc = get_accuracy(torch.cat(predictions), torch.cat(targets))\n",
    "\n",
    "        metrics['train_loss'].append(ep_train_loss)\n",
    "        metrics['train_acc'].append(ep_train_acc)\n",
    "        metrics['val_loss'].append(ep_val_loss)\n",
    "        metrics['val_acc'].append(ep_val_acc)\n",
    "\n",
    "        print('EPOCH:\\t{:5}\\tTRAIN LOSS:\\t{:.3f}\\tTRAIN ACCURACY:\\t{:.2f}'\n",
    "              '\\tVAL LOSS:\\t {:.5f}\\tVAL ACCURACY:\\t {:.2f}'\n",
    "              .format(ep, ep_train_loss, ep_train_acc, ep_val_loss, ep_val_acc), end='\\r')\n",
    "\n",
    "        ###############################################################\n",
    "        # EARLY STOPPING\n",
    "        ###############################################################\n",
    "        if early_stopping and ep > 1:\n",
    "            if metrics['val_loss'][-2] <= metrics['val_loss'][-1]:\n",
    "                early_stopping_strikes += 1\n",
    "            else:\n",
    "                early_stopping_strikes = 0\n",
    "\n",
    "# TODO: Abort training when `early_stopping_strikes` has reached `patience`.\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d54526",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's train the dropout model with early stopping and patience of 2.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "acdab4ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:39.272948Z",
     "iopub.status.busy": "2021-11-23T09:18:39.261435Z",
     "iopub.status.idle": "2021-11-23T09:18:39.294352Z",
     "shell.execute_reply": "2021-11-23T09:18:39.293788Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "early_dropout_model = nn.Sequential(\n",
    "        nn.Linear(in_features=input_size, out_features=16),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(in_features=16, out_features=16),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(in_features=16, out_features=1),\n",
    "        nn.Sigmoid()\n",
    ").to(device)\n",
    "\n",
    "early_dropout_model_metrics = train(\n",
    "    model=early_dropout_model,\n",
    "    loss=nn.BCELoss(),\n",
    "    optimizer=RMSprop(early_dropout_model.parameters()),\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    epochs=20,\n",
    "    batch_size=512,\n",
    "    early_stopping=True,\n",
    "    patience=2\n",
    ")\n",
    "\n",
    "compare_losses_plot(\n",
    "    early_dropout_model_metrics['val_loss'],\n",
    "    dropout_model_metrics['val_loss'],\n",
    "    'early stopping',\n",
    "    'dropout'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ebcd32",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As you can see, the early stopping callback worked\n",
    "and the model was trained for only a few epochs.\n",
    "Now, evaluate this model on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3431206",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:39.307391Z",
     "iopub.status.busy": "2021-11-23T09:18:39.297927Z",
     "iopub.status.idle": "2021-11-23T09:18:39.322774Z",
     "shell.execute_reply": "2021-11-23T09:18:39.323133Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: Evaluate this model on the test data and print the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075f405b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As you can see, the loss is close to the lowest loss in the graph.\n",
    "\n",
    "The take-home message for this exercise is: large neural networks can easily overfit,\n",
    "especially with small training sets. This means that the network learns spurious patterns\n",
    " that are present in the training data and, therefore, fails to generalize to unseen examples.\n",
    " In such a scenario, your options are:\n",
    "\n",
    " 1. Get more training data\n",
    " 2. Reduce the size of the network\n",
    " 3. Regularize the network\n",
    "\n",
    "## Exercise 3\n",
    "\n",
    "Consider an error function of the form:\n",
    "\n",
    "\\begin{equation}\n",
    "E=\\frac 1 2 \\lambda_1 x_1^2+\\frac 1 2 \\lambda_2 x_2^2\n",
    "\\end{equation}\n",
    "\n",
    "With $\\lambda_1\\geq 0$ and $\\lambda_2\\geq 0$. First, show that the global minimum of\n",
    "$E$ is at $x_1=x_2=0$, then find the matrix $\\textbf{H}$ such that $E=1/2 \\cdot \\textbf{x}^T\\textbf{H}\\textbf{x}$.\n",
    "Show that the two eigenvectors $\\textbf{u}_1$ and $\\textbf{u}_2$ of this matrix are\n",
    "axis-aligned, and have $\\lambda_1$ and $\\lambda_2$ as eigenvalues.\n",
    "\n",
    "Note that any vector $\\textbf{x}$ can be expressed as\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{x}=\\sum_i \\alpha_i\\textbf{u}_i\n",
    "\\end{equation}\n",
    "where $\\alpha_i$ is the distance from the origin to $\\textbf{x}$ along the $i$-th axis\n",
    "(assuming the eigenvectors have unit length). Now find the gradient of $E$ with respect\n",
    "to $\\textbf{x}$, and express it in terms of $\\alpha_i$, $\\lambda_i$ and $\\textbf{u}_i$.\n",
    "\n",
    "Then, use this gradient to perform one step of gradient descent, i.e. compute\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{x}^\\prime=\\textbf{x}-\\eta\\nabla_{\\textbf{x}}E\n",
    "\\end{equation}\n",
    "\n",
    "And show that\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha^\\prime_i=(1-\\eta\\lambda_i)\\alpha_i\n",
    "\\end{equation}\n",
    "\n",
    "Which means that the distances from the origin to the current location evolve\n",
    "independently for each axis, and at every step the distance along the direction\n",
    "$\\textbf{u}_i$ is multiplied by $(1-\\eta\\lambda_i)$. After $T$ steps, we have\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha^{(T)}_i=(1-\\eta\\lambda_i)^T\\alpha^{(0)}_i\n",
    "\\end{equation}\n",
    "So that, as long as $|1-\\eta\\lambda_i|<1$ for every $i$, $\\textbf{x}^{(T)}$ converges\n",
    "to the origin as $T$ goes to infinity.\n",
    "\n",
    "Now, find the largest learning rate that guarantees convergence along all directions\n",
    "and show that, when using this learning rate, the slowest direction of convergence is\n",
    "along the eigenvector with the smallest eigenvalue. Also show that the rate of\n",
    "convegence along this direction is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\left(1-2\\frac{\\lambda_{\\text{min}}}{\\lambda_{\\text{max}}}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Where $\\lambda_{\\text{min}}$ and $\\lambda_{\\text{max}}$ are the smallest and largest\n",
    "eigenvalues of $\\textbf{H}$.\n",
    "\n",
    "This exercise shows that the largest eigenvalue determines the maximum learning rate,\n",
    "and that the relationship between smallest and largest eigenvalues determines the speed\n",
    "of convergence. Note that the ratio $\\lambda_{\\text{max}}/\\lambda_{\\text{min}}$ is known\n",
    "as the _condition number_ of $\\textbf{H}$, and plays an important role in numerical\n",
    "analysis. Matrices with large condition number make optimization algorithms slower and/or\n",
    "more imprecise.\n",
    "\n",
    "\n",
    "## Exercise 4\n",
    "In this exercise we play a bit with the quadratic error surfaces that we analyzed in the\n",
    "previous exercise. We will apply the insights we got, and test different forms of\n",
    "gradient descent. The purpose is to get an intuitive feeling for how these things work,\n",
    "and for this some playful interaction is required from your side.\n",
    "\n",
    "Remember that the error function was:\n",
    "\n",
    "\\begin{equation}\n",
    "E=\\frac 1 2 \\lambda_1 x_1^2+\\frac 1 2 \\lambda_2 x_2^2\n",
    "\\end{equation}\n",
    "\n",
    "We first create an object to compute $E$ and its gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7436c1b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:39.327526Z",
     "iopub.status.busy": "2021-11-23T09:18:39.327072Z",
     "iopub.status.idle": "2021-11-23T09:18:39.355014Z",
     "shell.execute_reply": "2021-11-23T09:18:39.354610Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Function:\n",
    "    def __init__(self, lambda_1: float, lambda_2: float):\n",
    "        self.lambda_1 = lambda_1\n",
    "        self.lambda_2 = lambda_2\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        return self.lambda_1 * x[:, 0]**2 / 2 + self.lambda_2 * x[:, 1]**2 / 2\n",
    "\n",
    "    def grad(self, x:Tensor) -> Tensor:\n",
    "        return torch.stack([\n",
    "# TODO: Compute the two components of the gradient of E at x\n",
    "        ]).T\n",
    "\n",
    "    def plot(self, show: bool = False):\n",
    "        grid_range = torch.linspace(-5, 5, 50)\n",
    "        grid_x, grid_y = torch.meshgrid(grid_range, grid_range)\n",
    "        grid_data = torch.stack([grid_x.flatten(), grid_y.flatten()]).T\n",
    "        plt.contour(grid_x, grid_y, self(grid_data).view(grid_x.shape))\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "quad_function = Function(lambda_1=1, lambda_2=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327e1a0d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And we visualize a contour plot of the surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87806c55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:39.366235Z",
     "iopub.status.busy": "2021-11-23T09:18:39.358074Z",
     "iopub.status.idle": "2021-11-23T09:18:39.381856Z",
     "shell.execute_reply": "2021-11-23T09:18:39.382147Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "quad_function.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2709f427",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now create a vanilla gradient descent optimizer that returns all points\n",
    "visited during the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f45ee076",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:39.393176Z",
     "iopub.status.busy": "2021-11-23T09:18:39.392651Z",
     "iopub.status.idle": "2021-11-23T09:18:39.408157Z",
     "shell.execute_reply": "2021-11-23T09:18:39.407778Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def do_gradient_descent(x: Tensor, func_obj: Function, max_steps: int, lr: float) -> Tensor:\n",
    "    path: List = [x.clone()]\n",
    "    for _ in range(max_steps):\n",
    "# TODO: Modify `x` performing one step of gradient descent\n",
    "        path.append(x.clone())\n",
    "    return torch.cat(path)\n",
    "\n",
    "hist_slow = do_gradient_descent(torch.tensor([[4., 4.]]), quad_function, 10, 0.05)\n",
    "hist_fast = do_gradient_descent(torch.tensor([[4., 4.]]), quad_function, 10, 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d1f993",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And a function that plots several traces together, so that we can compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf6f9dfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:39.419710Z",
     "iopub.status.busy": "2021-11-23T09:18:39.419300Z",
     "iopub.status.idle": "2021-11-23T09:18:39.434117Z",
     "shell.execute_reply": "2021-11-23T09:18:39.433658Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def plot_histories(histories: List[Tensor], labels: List[str]) -> None:\n",
    "    for history, label in zip(histories, labels):\n",
    "        plt.plot(history[:, 0], history[:, 1], label=label)\n",
    "    plt.legend()\n",
    "\n",
    "quad_function.plot()\n",
    "plot_histories([hist_slow, hist_fast], ['fast', 'slow'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e65e30",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, recall from the previous exercise that the learning rate cannot be larger than\n",
    "$2\\lambda_{\\text{min}}/\\lambda_{\\text{max}}$.\n",
    "Compute this upper bound for the example here, use it to optimize the error starting\n",
    "from $\\textbf{x}=|4,4|^T$, and plot the resulting trajectory.\n",
    "What can you notice?\n",
    "Try to slightly reduce it and increase it, and verify that when it is larger than the\n",
    "upper bound, the procedure diverges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be28d0ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:39.445925Z",
     "iopub.status.busy": "2021-11-23T09:18:39.437802Z",
     "iopub.status.idle": "2021-11-23T09:18:39.461399Z",
     "shell.execute_reply": "2021-11-23T09:18:39.460962Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_learning_rate =(\n",
    "# TODO: Compute the maximum learning rate possible in this case.\n",
    ")\n",
    "\n",
    "hist_max = do_gradient_descent(torch.tensor([[4., 4.]]), quad_function, 10, max_learning_rate)\n",
    "quad_function.plot()\n",
    "plot_histories([hist_max], ['boundary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcb887e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now try to change the eigenvalues so as to increase the condition number\n",
    "and verify that convergence becomes slower as the condition number increases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3f417a42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:39.473744Z",
     "iopub.status.busy": "2021-11-23T09:18:39.473339Z",
     "iopub.status.idle": "2021-11-23T09:18:39.488404Z",
     "shell.execute_reply": "2021-11-23T09:18:39.488052Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "quad_function = Function(lambda_1=1, lambda_2=30)\n",
    "\n",
    "max_learning_rate =(\n",
    "# TODO: Compute the maximum learning rate possible in this case.\n",
    ")\n",
    "hist_max = do_gradient_descent(torch.tensor([[4., 4.]]), quad_function, 10, max_learning_rate)\n",
    "quad_function.plot()\n",
    "plot_histories([hist_max], ['boundary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4e2b4d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, modify the optimizer to use momentum, and verify that convergence becomes faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2df3ec4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T09:18:39.500790Z",
     "iopub.status.busy": "2021-11-23T09:18:39.500308Z",
     "iopub.status.idle": "2021-11-23T09:18:39.515935Z",
     "shell.execute_reply": "2021-11-23T09:18:39.515510Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def do_momentum_gd(\n",
    "        x: Tensor, func_obj: Function, max_steps: int, lr: float, momentum: float) -> Tensor:\n",
    "    path: List = [x.clone()]\n",
    "    velocity = torch.zeros(x.shape)\n",
    "    for _ in range(max_steps):\n",
    "# TODO: Modify `x` and the velocity performing one step of gd with momentum.\n",
    "        path.append(x.clone())\n",
    "    return torch.cat(path)\n",
    "\n",
    "momentum = (\n",
    "# #   try several values for the momentum\n",
    ")\n",
    "\n",
    "hist_momentum = do_momentum_gd(\n",
    "    x=torch.tensor([[4., 4.]]),\n",
    "    func_obj=quad_function,\n",
    "    max_steps=10,\n",
    "    lr=max_learning_rate,\n",
    "    momentum=momentum\n",
    ")\n",
    "quad_function.plot()\n",
    "plot_histories([hist_max, hist_momentum], ['vanilla', 'momentum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116d3386",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now explore the convergence behavior as momentum and learning rate change.\n",
    "Does momentum bring any improvement when the condition number is one\n",
    "(i.e. the eigenvalues are identical)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
