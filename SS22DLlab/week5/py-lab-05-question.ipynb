{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bd40ac8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab 5\n",
    "\n",
    "Hüseyin Anil Gündüz\n",
    "\n",
    "Welcome to the fifth lab. We will first implement a simple scalar automatic\n",
    "differentiation engine to compute partial derivatives for us,\n",
    "then do a theoretical exercise about L2 regularization.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f36c0d9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:31.002415Z",
     "iopub.status.busy": "2021-11-16T10:05:30.992274Z",
     "iopub.status.idle": "2021-11-16T10:05:31.357810Z",
     "shell.execute_reply": "2021-11-16T10:05:31.357411Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from abc import ABC, abstractmethod\n",
    "from random import uniform\n",
    "from typing import Optional, List, Union, Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats('png', 'pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f70aae",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "Modern deep learning frameworks compute gradients automatically,\n",
    "so that you only need to define how to perform the forward pass in your code.\n",
    "Under the hood, the framework constructs a computational graph based on the operations\n",
    "you used. For example, consider the node:\n",
    "\n",
    "\\begin{equation}\n",
    "4xy+e^{-y}\n",
    "\\label{eq:ex}\n",
    "\\end{equation}\n",
    "\n",
    "It can be translated into a graph that looks like this:\n",
    "\n",
    "![](labweek5.png)\n",
    "\n",
    "Where we have 'leaf' nodes at the top for variables and constants, and 'internal' nodes\n",
    "for operations. To make things simpler, in this exercise we will only work with\n",
    "scalar operations and scalar variables, but what we are going to create could, in\n",
    "principle, be extended to work with vectors and matrices. Section 6 of chapter 5 of the\n",
    "_Mathematics for Machine Learning_ book (https://mml-book.github.io/) is a good\n",
    "supplementary read.\n",
    "\n",
    "The naming of the classes responds to:\n",
    "\n",
    " - `Sum` for Addition $x+y$\n",
    " - `Sub'` for Subtraction $x-y$\n",
    " - `Mul` for Product $x\\cdot y$\n",
    " - `Div` for Division $x / y$\n",
    " - `Exp` for Exponentiation $e^x$\n",
    " - `TanH` for Hyperbolic tangent $\\tanh(x)$\n",
    " - `Log` for Logarithm $\\log(x)$\n",
    "\n",
    "\n",
    "We first define some utilities to easily create nodes.\n",
    "An abstract class gives us a common interface across all the respective nodes that\n",
    "we will derive from it. This doesn't make sense now, as the class is basically empty,\n",
    "but we will extend the definition over the course of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df9feb62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:31.362861Z",
     "iopub.status.busy": "2021-11-16T10:05:31.362519Z",
     "iopub.status.idle": "2021-11-16T10:05:31.364040Z",
     "shell.execute_reply": "2021-11-16T10:05:31.363712Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BaseNode(ABC):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Const(BaseNode):\n",
    "    def __init__(self, value: Union[float, int]):\n",
    "        self.value = value\n",
    "\n",
    "\n",
    "class Var(BaseNode):\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "\n",
    "\n",
    "class BinaryOperation(BaseNode, ABC):\n",
    "    def __init__(self, x: BaseNode, y: BaseNode):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "class Sum(BinaryOperation):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Sub(BinaryOperation):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Mul(BinaryOperation):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Div(BinaryOperation):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Function(BaseNode, ABC):\n",
    "    def __init__(self, x: BaseNode):\n",
    "        self.x = x\n",
    "\n",
    "\n",
    "class Exp(Function):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Log(Function):\n",
    "    pass\n",
    "\n",
    "\n",
    "class TanH(Function):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e1b31ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:31.367084Z",
     "iopub.status.busy": "2021-11-16T10:05:31.366682Z",
     "iopub.status.idle": "2021-11-16T10:05:31.369254Z",
     "shell.execute_reply": "2021-11-16T10:05:31.368980Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Sum object at 0x7f0262be0cd0>\n",
      "<__main__.Mul object at 0x7f0262be0b90>\n",
      "<__main__.Exp object at 0x7f0262be0c90>\n"
     ]
    }
   ],
   "source": [
    "# We then define the graph from the equation before.\n",
    "x = Var('x')\n",
    "y = Var('y')\n",
    "\n",
    "z = Sum(\n",
    "    x=Mul(\n",
    "        x=Mul(\n",
    "            x=Const(4),\n",
    "            y=x\n",
    "        ),\n",
    "        y=y,\n",
    "    ),\n",
    "    y=Exp(\n",
    "        x=Mul(\n",
    "            x=Const(-1),\n",
    "            y=y\n",
    "    ))\n",
    ")\n",
    "\n",
    "print(z)\n",
    "print(z.x.x)\n",
    "print(z.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d390b5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This structure of nested objects contains the computational graph for the node above.\n",
    "Now, we can write code to manipulate this expression as we please.\n",
    "In the course of this exercise, we will see how:\n",
    "\n",
    " 1. Print an expression,\n",
    " 2. Compute its value, given the values of the variables involved,\n",
    " 3. Differentiate it to automatically find partial derivatives with respect to any given variable,\n",
    " 4. Transform it into simpler expressions that are cheaper to handle, and\n",
    " 5. Write code to train a neural network without getting our hands dirty with derivatives ever again.\n",
    "\n",
    "\n",
    "### Printing an expression\n",
    "First, since it is quite hard to understand the node from the representation above,\n",
    "let us extend the classes to convert a computational graph into a string representation\n",
    "that is easier to understand. For example, the expression $x+2y$ should be converted to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3306458a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:31.373201Z",
     "iopub.status.busy": "2021-11-16T10:05:31.372874Z",
     "iopub.status.idle": "2021-11-16T10:05:31.375215Z",
     "shell.execute_reply": "2021-11-16T10:05:31.375484Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(', 'x', '+', '(', '2', '*', 'y', ')', ')']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['(', 'x', '+', '(', '2', '*', 'y', ')', ')']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95193d49",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Which can be printed easily using `join`, resulting in `( x + ( 2 * y ) )`.\n",
    "\n",
    "Such a function should be _recursive_. This means that when simplifying a complicated\n",
    "expression it will call itself on each constituting piece of that expression, and\n",
    "\"assemble\" the results together.\n",
    "Conceptually, the procedure is similar to the factorial operation,\n",
    "which is recursively defined in terms of the factorial of a smaller number:\n",
    "\n",
    "\\begin{equation}\n",
    "n!=\\begin{cases}\n",
    "1 & \\text{if }n < 1 \\\\\n",
    "n\\cdot(n-1)! & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "This definition can be converted into Python as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62553505",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:31.378448Z",
     "iopub.status.busy": "2021-11-16T10:05:31.378106Z",
     "iopub.status.idle": "2021-11-16T10:05:31.380037Z",
     "shell.execute_reply": "2021-11-16T10:05:31.379760Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "def factorial(n: int) -> int:\n",
    "    if n < 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * factorial(n - 1)\n",
    "\n",
    "print(factorial(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222bccac",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In a similar way, we extend the classes with `__str__`, which is a Python utility to\n",
    "obtain a custom string representation of an object. If we define it correctly, we\n",
    "are able to traverse through the tree and print the expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72c99f43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:31.383426Z",
     "iopub.status.busy": "2021-11-16T10:05:31.383075Z",
     "iopub.status.idle": "2021-11-16T10:05:31.389070Z",
     "shell.execute_reply": "2021-11-16T10:05:31.389339Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class BaseNode(ABC):\n",
    "    @abstractmethod\n",
    "    def __str__(self) -> str:\n",
    "        pass\n",
    "\n",
    "\n",
    "class Const(BaseNode):\n",
    "    def __init__(self, value: Union[float, int]):\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str(self.value)\n",
    "\n",
    "\n",
    "class Var(BaseNode):\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.name\n",
    "\n",
    "\n",
    "class BinaryOperation(BaseNode, ABC):\n",
    "    def __init__(self, x: BaseNode, y: BaseNode):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "class Sum(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "# Formulate an expression to print the operation recursively.\n",
    "\n",
    "\n",
    "class Sub(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "# Formulate an expression to print the operation recursively.\n",
    "\n",
    "\n",
    "class Mul(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "# Formulate an expression to print the operation recursively.\n",
    "\n",
    "\n",
    "class Div(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "# Formulate an expression to print the operation recursively.\n",
    "\n",
    "\n",
    "class Function(BaseNode, ABC):\n",
    "    def __init__(self, x: BaseNode):\n",
    "        self.x = x\n",
    "\n",
    "\n",
    "class Exp(Function):\n",
    "    def __str__(self) -> str:\n",
    "# Formulate an expression to print the operation recursively.\n",
    "\n",
    "\n",
    "class Log(Function):\n",
    "    def __str__(self) -> str:\n",
    "# Formulate an expression to print the operation recursively.\n",
    "\n",
    "\n",
    "class TanH(Function):\n",
    "    def __str__(self) -> str:\n",
    "# Formulate an expression to print the operation recursively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "026ba128",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:31.393818Z",
     "iopub.status.busy": "2021-11-16T10:05:31.393461Z",
     "iopub.status.idle": "2021-11-16T10:05:31.397521Z",
     "shell.execute_reply": "2021-11-16T10:05:31.397772Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x = Var('x')\n",
    "y = Var('y')\n",
    "\n",
    "z = Sum(\n",
    "    x=Mul(\n",
    "        x=Mul(\n",
    "            x=Const(4),\n",
    "            y=x\n",
    "        ),\n",
    "        y=y,\n",
    "    ),\n",
    "    y=Exp(\n",
    "        x=Mul(\n",
    "            x=Const(-1),\n",
    "            y=y\n",
    "    ))\n",
    ")\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5063438",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is much simpler to read!\n",
    "\n",
    "### Computing the value of an expression\n",
    "We can now extend the classes to compute the value of an expression given values for\n",
    "the variables. The methods should be recursive too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd7c9bcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:31.401383Z",
     "iopub.status.busy": "2021-11-16T10:05:31.401028Z",
     "iopub.status.idle": "2021-11-16T10:05:31.407183Z",
     "shell.execute_reply": "2021-11-16T10:05:31.406849Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class BaseNode(ABC):\n",
    "    @abstractmethod\n",
    "    def __str__(self) -> str:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        pass\n",
    "\n",
    "\n",
    "class Const(BaseNode):\n",
    "    def __init__(self, value: Union[float, int]):\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str(self.value)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return self.value\n",
    "\n",
    "\n",
    "class Var(BaseNode):\n",
    "    def __init__(self, name: str, value: Optional[Union[float, int]] = None):\n",
    "        self.name = name\n",
    "        self.value = value\n",
    "        \n",
    "    def set_value(self, value: Union[float, int]) -> None:\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.name\n",
    "    \n",
    "    def eval(self) -> Union[float, int]:\n",
    "        if self.value is None:\n",
    "            raise ValueError('Value is not set. Evaluation is not possible.')\n",
    "        return self.value\n",
    "\n",
    "\n",
    "class BinaryOperation(BaseNode, ABC):\n",
    "    def __init__(self, x: BaseNode, y: BaseNode):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "class Sum(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} + {})'.format(self.x, self.y)\n",
    "    \n",
    "    def eval(self) -> Union[float, int]:\n",
    "# Formulate an expression to compute the operation recursively.\n",
    "\n",
    "\n",
    "class Sub(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} - {})'.format(self.x, self.y)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "# Formulate an expression to compute the operation recursively.\n",
    "\n",
    "\n",
    "class Mul(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} * {})'.format(self.x, self.y)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "# Formulate an expression to compute the operation recursively.\n",
    "\n",
    "\n",
    "class Div(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} / {})'.format(self.x, self.y)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "# Formulate an expression to compute the operation recursively.\n",
    "\n",
    "\n",
    "class Function(BaseNode, ABC):\n",
    "    def __init__(self, x: BaseNode):\n",
    "        self.x = x\n",
    "\n",
    "\n",
    "class Exp(Function):\n",
    "    def __str__(self) -> str:\n",
    "        return 'exp({})'.format(self.x)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "# Formulate an expression to compute the operation recursively.\n",
    "\n",
    "class Log(Function):\n",
    "    def __str__(self) -> str:\n",
    "        return 'log({})'.format(self.x)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "# Formulate an expression to compute the operation recursively.\n",
    "\n",
    "class TanH(Function):\n",
    "    def __str__(self) -> str:\n",
    "        return 'tanh({})'.format(self.x)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "# Formulate an expression to compute the operation recursively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b184b111",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:31.411643Z",
     "iopub.status.busy": "2021-11-16T10:05:31.408762Z",
     "iopub.status.idle": "2021-11-16T10:05:31.416100Z",
     "shell.execute_reply": "2021-11-16T10:05:31.416394Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x = Var('x', value=2)\n",
    "y = Var('y', value=3)\n",
    "\n",
    "z = Sum(\n",
    "    x=Mul(\n",
    "        x=Mul(\n",
    "            x=Const(4),\n",
    "            y=x\n",
    "        ),\n",
    "        y=y,\n",
    "    ),\n",
    "    y=Exp(\n",
    "        x=Mul(\n",
    "            x=Const(-1),\n",
    "            y=y\n",
    "    ))\n",
    ")\n",
    "\n",
    "print(z.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5c3415",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The result that we expect is, of course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e27bbe9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:31.419496Z",
     "iopub.status.busy": "2021-11-16T10:05:31.419122Z",
     "iopub.status.idle": "2021-11-16T10:05:31.421164Z",
     "shell.execute_reply": "2021-11-16T10:05:31.420816Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.049787068367863\n"
     ]
    }
   ],
   "source": [
    "print(4 * 2 * 3 + math.exp(-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aa695f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Differentiating an expression\n",
    "\n",
    "We can finally see how to differentiate an expression with respect to a variable.\n",
    "We do this again through extending the classes to differentiates each argument\n",
    "and merge the result. Note that this should return a new computational\n",
    "graph that contains the operations necessary to compute the partial derivative\n",
    "we are interested in.\n",
    "Each `differentiate` method gets the argument `var` that specifies the name of the\n",
    "variable of which we are computing the gradient.\n",
    "\n",
    "Remember to use the chain rule where appropriate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ce69807",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:31.425227Z",
     "iopub.status.busy": "2021-11-16T10:05:31.424683Z",
     "iopub.status.idle": "2021-11-16T10:05:31.430802Z",
     "shell.execute_reply": "2021-11-16T10:05:31.431111Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class BaseNode(ABC):\n",
    "    @abstractmethod\n",
    "    def __str__(self) -> str:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        pass\n",
    "\n",
    "\n",
    "class Const(BaseNode):\n",
    "    def __init__(self, value: Union[float, int]):\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str(self.value)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return self.value\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Const(0)\n",
    "\n",
    "\n",
    "class Var(BaseNode):\n",
    "    def __init__(self, name: str, value: Optional[Union[float, int]] = None):\n",
    "        self.name = name\n",
    "        self.value = value\n",
    "\n",
    "    def set_value(self, value: Union[float, int]) -> None:\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.name\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        if self.value is None:\n",
    "            raise ValueError('Value is not set. Evaluation is not possible.')\n",
    "        return self.value\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Const(1) if self.name == var else Const(0)\n",
    "\n",
    "\n",
    "class BinaryOperation(BaseNode, ABC):\n",
    "    def __init__(self, x: BaseNode, y: BaseNode):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "class Sum(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} + {})'.format(self.x, self.y)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return self.x.eval() + self.y.eval()\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "# TODO Return a new node that sums the derivatives of the left and right part.\n",
    "# Note: We are returning a new graph node that connects the two\n",
    "# graphs representing the derivatives of the left and right parts\n",
    "\n",
    "\n",
    "class Sub(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} - {})'.format(self.x, self.y)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return self.x.eval() - self.y.eval()\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "# TODO Return a new node that subtracts the derivatives of the left and right part.\n",
    "\n",
    "\n",
    "class Mul(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} * {})'.format(self.x, self.y)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return self.x.eval() * self.y.eval()\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "# TODO differentiate the product x * y. (Hint: Product rule)\n",
    "\n",
    "\n",
    "class Div(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} / {})'.format(self.x, self.y)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return self.x.eval() / self.y.eval()\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "# TODO differentiate the quotient x / y. (Hint: Quotient rule)\n",
    "\n",
    "\n",
    "class Function(BaseNode, ABC):\n",
    "    def __init__(self, x: BaseNode):\n",
    "        self.x = x\n",
    "\n",
    "\n",
    "class Exp(Function):\n",
    "    def __str__(self) -> str:\n",
    "        return 'exp({})'.format(self.x)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return math.exp(self.x.eval())\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "# TODO differentiate exp(x). (Hint: Chain rule)\n",
    "\n",
    "\n",
    "class Log(Function):\n",
    "    def __str__(self) -> str:\n",
    "        return 'log({})'.format(self.x)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return math.log(self.x.eval())\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "# TODO differentiate log(x). (Hint: Chain rule)\n",
    "\n",
    "\n",
    "class TanH(Function):\n",
    "    def __str__(self) -> str:\n",
    "        return 'tanh({})'.format(self.x)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return math.tanh(self.x.eval())\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "# TODO differentiate log(x). (Hint: Chain rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "735099d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:31.435860Z",
     "iopub.status.busy": "2021-11-16T10:05:31.435481Z",
     "iopub.status.idle": "2021-11-16T10:05:31.440616Z",
     "shell.execute_reply": "2021-11-16T10:05:31.440295Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x = Var('x', value=2)\n",
    "y = Var('y', value=3)\n",
    "\n",
    "z = Sum(\n",
    "    x=Mul(\n",
    "        x=Mul(\n",
    "            x=Const(4),\n",
    "            y=x\n",
    "        ),\n",
    "        y=y,\n",
    "    ),\n",
    "    y=Exp(\n",
    "        x=Mul(\n",
    "            x=Const(-1),\n",
    "            y=y\n",
    "    ))\n",
    ")\n",
    "\n",
    "dz = z.differentiate('x')\n",
    "print(dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830dec2b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This looks a bit complicated, but by applying some trivial simplifications we see it is correct:\n",
    "\n",
    "\\begin{align*}\n",
    "& ( ( ( ( ( 0 \\cdot x ) + ( 4 \\cdot 1 ) ) \\cdot y ) + ( ( 4 \\cdot x ) \\cdot 0 ) ) + ( exp ( ( -1 \\cdot y ) ) \\cdot ( ( 0 \\cdot y ) + ( -1 \\cdot 0 ) ) ) )  \\\\\n",
    "&\\qquad= ( ( ( 0 + 4 ) \\cdot y ) + 0 ) + ( exp ( ( -1 \\cdot y ) ) \\cdot ( 0 + 0 ) ) ) \\\\\n",
    "&\\qquad= ( 4 \\cdot y ) + ( exp ( ( -1 \\cdot y ) ) \\cdot 0 ) ) \\\\\n",
    "&\\qquad= ( 4 \\cdot y ) + 0 \\\\\n",
    "&\\qquad= 4 \\cdot y \\\\\n",
    "&\\qquad= \\frac{\\text{d}}{\\text{d}x} \\left(4xy+e^{-y}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "These simplification rules are trivial arithmetic identities:\n",
    "\n",
    " - $0+x=x$\n",
    " - $0\\cdot x=0$\n",
    " - $1\\cdot x=x$\n",
    " - $0/x=0$\n",
    "\n",
    "Let us extend the classes that use these identities to automatically simplify `dz` in\n",
    "the same way we just did. As with differentiation, this should return a\n",
    "new computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2cddc46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:31.446845Z",
     "iopub.status.busy": "2021-11-16T10:05:31.442198Z",
     "iopub.status.idle": "2021-11-16T10:05:31.451293Z",
     "shell.execute_reply": "2021-11-16T10:05:31.451606Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class BaseNode(ABC):\n",
    "    @abstractmethod\n",
    "    def __str__(self) -> str:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def simplify(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Const(BaseNode):\n",
    "    def __init__(self, value: Union[float, int]):\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str(self.value)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return self.value\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Const(0)\n",
    "\n",
    "    def simplify(self):\n",
    "        return self\n",
    "\n",
    "\n",
    "class Var(BaseNode):\n",
    "    def __init__(self, name: str, value: Optional[Union[float, int]] = None):\n",
    "        self.name = name\n",
    "        self.value = value\n",
    "\n",
    "    def set_value(self, value: Union[float, int]) -> None:\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.name\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        if self.value is None:\n",
    "            raise ValueError('Value is not set. Evaluation is not possible.')\n",
    "        return self.value\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Const(1) if self.name == var else Const(0)\n",
    "\n",
    "    def simplify(self):\n",
    "        return self\n",
    "\n",
    "\n",
    "class BinaryOperation(BaseNode, ABC):\n",
    "    def __init__(self, x: BaseNode, y: BaseNode):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "class Sum(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} + {})'.format(self.x, self.y)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return self.x.eval() + self.y.eval()\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Sum(self.x.differentiate(var), self.y.differentiate(var))\n",
    "\n",
    "    def simplify(self):\n",
    "        simple_x = (\n",
    "# Simplify the left part.\n",
    "        )\n",
    "\n",
    "        simple_y = (\n",
    "# Simplify the right part.\n",
    "        )\n",
    "\n",
    "        if isinstance(simple_x, Const):\n",
    "            if simple_x.value == 0:\n",
    "                # Rule: 0 + y = y\n",
    "                return simple_y\n",
    "\n",
    "        if isinstance(simple_y, Const):\n",
    "            if simple_y.value == 0:\n",
    "                # Rule: x + 0 = x\n",
    "                return simple_x\n",
    "\n",
    "        if isinstance(simple_x, Const) and isinstance(simple_y, Const):\n",
    "            # If both arguments are constants we can perform the sum immediately\n",
    "            return Const(simple_x.value + simple_y.value)\n",
    "\n",
    "        # Cannot simplify further. Return a new sum node with the simplified operands.\n",
    "        return Sum(simple_x, simple_y)\n",
    "\n",
    "\n",
    "class Sub(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} - {})'.format(self.x, self.y)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return self.x.eval() - self.y.eval()\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Sub(self.x.differentiate(var), self.y.differentiate(var))\n",
    "\n",
    "    def simplify(self):\n",
    "# TODO simplify x - y\n",
    "\n",
    "\n",
    "class Mul(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} * {})'.format(self.x, self.y)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return self.x.eval() * self.y.eval()\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Sum(\n",
    "            x=Mul(self.x.differentiate(var), self.y),\n",
    "            y=Mul(self.x, self.y.differentiate(var))\n",
    "        )\n",
    "\n",
    "    def simplify(self):\n",
    "# TODO simplify x * y\n",
    "\n",
    "\n",
    "class Div(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} / {})'.format(self.x, self.y)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return self.x.eval() / self.y.eval()\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Div(\n",
    "            x=Sub(\n",
    "                x=Mul(self.x.differentiate(var), self.y),\n",
    "                y=Mul(self.y.differentiate(var), self.x)\n",
    "            ),\n",
    "            y=Mul(self.y, self.y)\n",
    "        )\n",
    "\n",
    "    def simplify(self):\n",
    "# TODO simplify x / y\n",
    "\n",
    "\n",
    "class Function(BaseNode, ABC):\n",
    "    def __init__(self, x: BaseNode):\n",
    "        self.x = x\n",
    "\n",
    "\n",
    "class Exp(Function):\n",
    "    def __str__(self) -> str:\n",
    "        return 'exp({})'.format(self.x)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return math.exp(self.x.eval())\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Mul(\n",
    "            x=Exp(self.x),\n",
    "            y=self.x.differentiate(var)\n",
    "        )\n",
    "\n",
    "    def simplify(self):\n",
    "# TODO simplify exp(x)\n",
    "\n",
    "\n",
    "class Log(Function):\n",
    "    def __str__(self) -> str:\n",
    "        return 'log({})'.format(self.x)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return math.log(self.x.eval())\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Div(\n",
    "            x=self.x.differentiate(var),\n",
    "            y=self.x\n",
    "        )\n",
    "\n",
    "    def simplify(self):\n",
    "# TODO simplify log(x)\n",
    "\n",
    "\n",
    "class TanH(Function):\n",
    "    def __str__(self) -> str:\n",
    "        return 'tanh({})'.format(self.x)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return math.tanh(self.x.eval())\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Mul(\n",
    "            x=Sub(\n",
    "                x=Const(1),\n",
    "                y=Mul(TanH(self.x), TanH(self.x))\n",
    "            ),\n",
    "            y=self.x.differentiate(var)\n",
    "        )\n",
    "\n",
    "    def simplify(self):\n",
    "# TODO simplify log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea998ca8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:31.456372Z",
     "iopub.status.busy": "2021-11-16T10:05:31.455978Z",
     "iopub.status.idle": "2021-11-16T10:05:31.461014Z",
     "shell.execute_reply": "2021-11-16T10:05:31.460635Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x = Var('x', value=2)\n",
    "y = Var('y', value=3)\n",
    "\n",
    "z = Sum(\n",
    "    x=Mul(\n",
    "        x=Mul(\n",
    "            x=Const(4),\n",
    "            y=x\n",
    "        ),\n",
    "        y=y,\n",
    "    ),\n",
    "    y=Exp(\n",
    "        x=Mul(\n",
    "            x=Const(-1),\n",
    "            y=y\n",
    "    ))\n",
    ")\n",
    "\n",
    "dz = z.differentiate('x')\n",
    "print(dz.simplify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849be214",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The result matches what we showed above, $4y$. Simplifying the graph with these and\n",
    "other, more advanced tricks, can greatly speed up code.\n",
    "\n",
    "Now we are also equipped to perform differentiation of any order,\n",
    "for example $\\partial z / \\partial x\\partial y$ is simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a92cf9f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:31.465975Z",
     "iopub.status.busy": "2021-11-16T10:05:31.465583Z",
     "iopub.status.idle": "2021-11-16T10:05:31.470022Z",
     "shell.execute_reply": "2021-11-16T10:05:31.469689Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(z.differentiate('x').differentiate('y'))\n",
    "print(z.differentiate('x').differentiate('y').simplify())\n",
    "print(z.differentiate('x').differentiate('y').eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1276850",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Training a network\n",
    "\n",
    "Let us now define a computational graph that performs the forward pass of a\n",
    "simple network, and use the functions above to compute the gradients of the parameters.\n",
    "We will use the same network we used in the third lab, reproduced below, and, as usual,\n",
    "we will test the code on the five points dataset. Since the functions we have written\n",
    "so far only work with scalar values, we will perform stochastic gradient descent\n",
    "using one sample at a time.\n",
    "\n",
    "![](../utils/03-lab-nn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2f699c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:31.474838Z",
     "iopub.status.busy": "2021-11-16T10:05:31.473434Z",
     "iopub.status.idle": "2021-11-16T10:05:31.479842Z",
     "shell.execute_reply": "2021-11-16T10:05:31.480160Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The two input nodes\n",
    "x1 = Var('x1')\n",
    "x2 = Var('x2')\n",
    "\n",
    "# Parameters for the first hidden neuron\n",
    "b1 = Var('b1')\n",
    "w11 = Var('w11')\n",
    "w21 = Var('w21')\n",
    "\n",
    "# Compute the output of the first hidden neuron\n",
    "z1in = Sum(\n",
    "    x=b1,\n",
    "    y=Sum(\n",
    "       x=Mul(x1, w11),\n",
    "       y=Mul(x2, w21)\n",
    "    )\n",
    ")\n",
    "\n",
    "z1out = TanH(z1in)\n",
    "\n",
    "print(z1out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9496d1a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, complete the remaining part of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5b21a00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:31.485046Z",
     "iopub.status.busy": "2021-11-16T10:05:31.484553Z",
     "iopub.status.idle": "2021-11-16T10:05:31.489264Z",
     "shell.execute_reply": "2021-11-16T10:05:31.488935Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO define the parameters of the second hidden neuron.\n",
    "\n",
    "z2out = (\n",
    "# TODO compute the output of the second hidden neuron\n",
    ")\n",
    "\n",
    "# TODO define the parameters of the output neuron\n",
    "\n",
    "fin = (\n",
    "# TODO compute the input to the sigmoid (called logits)\n",
    ")\n",
    "\n",
    "fout = (\n",
    "# TODO compute the output of the network: sigmoid(fin)\n",
    ")\n",
    "\n",
    "print(fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88e2cdb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And this defines the forward pass.\n",
    "\n",
    "We can now compute the predictions of the network by evaluating `fout`,\n",
    "providing values for the inputs and weights. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09291e6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:31.494070Z",
     "iopub.status.busy": "2021-11-16T10:05:31.492816Z",
     "iopub.status.idle": "2021-11-16T10:05:31.499139Z",
     "shell.execute_reply": "2021-11-16T10:05:31.499460Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Values for weights and biases\n",
    "b1.set_value(1.543385)\n",
    "w11.set_value(3.111573)\n",
    "w12.set_value(-2.808800)\n",
    "\n",
    "b2.set_value(1.373085)\n",
    "w21.set_value(3.130452)\n",
    "w22.set_value(-2.813466)\n",
    "\n",
    "c.set_value(-4.241453)\n",
    "u1.set_value(4.036489)\n",
    "u2.set_value(4.074885)\n",
    "\n",
    "# Values for the input\n",
    "x1.set_value(1)\n",
    "x2.set_value(-1)\n",
    "\n",
    "fout.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ffdb28",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Which should be about 0.9.\n",
    "We now have to compute the cross-entropy loss.\n",
    "For numerical stability, we will compute the loss using $f_{in}$ instead of $f_{out}$.\n",
    "Therefore, first, show that:\n",
    "\n",
    "\\begin{equation}\n",
    "-y\\cdot\\log(f_{out})-(1-y)\\cdot\\log(1-f_{out})=\n",
    "f_{in}-f_{in}\\cdot y+\\log(1+e^{-f_{in}})\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3036437",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:31.504299Z",
     "iopub.status.busy": "2021-11-16T10:05:31.503908Z",
     "iopub.status.idle": "2021-11-16T10:05:31.508587Z",
     "shell.execute_reply": "2021-11-16T10:05:31.508900Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This variable contains the label for the sample the network is predicting\n",
    "y = Var('y')\n",
    "\n",
    "loss = (\n",
    "# TODO compute the binary cross entropy loss with the logits (Eq. 3, right)\n",
    ")\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2331b77e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is starting to look complicated!\n",
    "Luckily, this time, we do not have to get our hands dirty with derivatives.\n",
    "Let us find the graphs for the derivatives of each parameter of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53d7cae2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:31.513700Z",
     "iopub.status.busy": "2021-11-16T10:05:31.512406Z",
     "iopub.status.idle": "2021-11-16T10:05:31.518293Z",
     "shell.execute_reply": "2021-11-16T10:05:31.518606Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_gradient_graphs(graph: BaseNode, param_nodes: List[Var]) -> Dict:\n",
    "    graph_dict: Dict = {}\n",
    "    for param_node in param_nodes:\n",
    "        graph_dict.update({\n",
    "            param_node.name : graph.differentiate(param_node.name)\n",
    "        })\n",
    "    return graph_dict\n",
    "\n",
    "parameter_nodes = [b1, w11, w12, b2, w21, w22, c, u1, u2]\n",
    "gradient_graphs = get_gradient_graphs(loss, parameter_nodes)\n",
    "print(gradient_graphs.get('w11'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61a7c5f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As you can see, there is a great deal of repetition in this expression.\n",
    "The repetitions could be removed by storing, in each node, its current value and\n",
    "gradient, so that we would not need to re-compute them every time. Modern deep\n",
    "learning frameworks indeed do this, and are able to compute the gradient of the\n",
    "loss with respect to all parameters in a single pass, but here we accept these\n",
    "inefficiencies for the sake of simplicity.\n",
    "\n",
    "We are now ready to train this network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a301c8b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:31.522676Z",
     "iopub.status.busy": "2021-11-16T10:05:31.522119Z",
     "iopub.status.idle": "2021-11-16T10:05:31.528595Z",
     "shell.execute_reply": "2021-11-16T10:05:31.528913Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create dataset.\n",
    "x = [\n",
    "    [0, 0],\n",
    "    [1, 0],\n",
    "    [0, -1],\n",
    "    [-1, 0],\n",
    "    [0, 1]\n",
    "]\n",
    "labels = [1, 0, 0, 0, 0]\n",
    "\n",
    "# Glorot initialization for the parameters.\n",
    "def init_params(param_nodes: List[Var], bias_names: List[str]) -> None:\n",
    "    b = math.sqrt(6 / 4)\n",
    "    for param_node in param_nodes:\n",
    "        if param_node.name in bias_names:\n",
    "            param_node.set_value(0.0)\n",
    "        else:\n",
    "            param_node.set_value(uniform(-b, b))\n",
    "\n",
    "init_params(parameter_nodes, ['b1', 'b2', 'c'])\n",
    "\n",
    "# Training Loop.\n",
    "# Losses dict follows (index of sample : list of losses during training).\n",
    "loss_dict = {i: [] for i in range(len(x))}\n",
    "for ep in range(250):\n",
    "    ep_loss = 0.\n",
    "    for i in range(len(x)):\n",
    "        # Set the correct values for the inputs and label.\n",
    "        x1.set_value(x[i][0])\n",
    "        x2.set_value(x[i][1])\n",
    "        y.set_value(labels[i])\n",
    "\n",
    "        loss_dict[i].append(\n",
    "# TODO compute the loss for sample i.\n",
    "        )\n",
    "        ep_loss += loss_dict[i][-1]\n",
    "\n",
    "# TODO Compute and evaluate gradient graphs.\n",
    "# TODO Perform one step of gradient descent.\n",
    "# Hint: Use the `get_gradient_graphs` function we defined above.\n",
    "# Hint: When having convergence issues, it might help to start with a large\n",
    "# learning rate (e.g. 2.5) and reduce it in the higher epochs.\n",
    "\n",
    "    print('EPOCH: \\t {:5} \\t LOSS: \\t {:.5f}'.format(ep + 1, float(ep_loss)), end='\\r')\n",
    "\n",
    "\n",
    "# Plot loss development.\n",
    "for sample_loss in loss_dict.values():\n",
    "    plt.plot(sample_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6eaa76",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can clearly see how the loss of each individual training sample evolves over time.\n",
    "This also explains the \"saddle\" you might have noticed in the loss curve from the\n",
    "previous lab.\n",
    "\n",
    "And these are the predictions for the five points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec545bdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:31.534414Z",
     "iopub.status.busy": "2021-11-16T10:05:31.534015Z",
     "iopub.status.idle": "2021-11-16T10:05:31.538369Z",
     "shell.execute_reply": "2021-11-16T10:05:31.537968Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(x)):\n",
    "    x1.set_value(x[i][0])\n",
    "    x2.set_value(x[i][1])\n",
    "    y.set_value(labels[i])\n",
    "\n",
    "    prediction = fout.eval()\n",
    "    print('SAMPLE:\\t{}\\tTRUE:\\t{}\\tPRED:\\t{:.3f}'.format(i, labels[i], prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfef0ef8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Conclusion\n",
    "What we did in this exercise is (a simplification of) how deep learning frameworks\n",
    "evaluate the code you write. You only need to define how to compute the output of\n",
    "the network, and the framework figures out the necessary gradients on its own.\n",
    "They provide a much better user interface, allowing you to use `+`, `-`, `/`, `*` etc.\n",
    "as you normally would instead of the clumsy node constructors we defined here,\n",
    "but there is always a computational graph hidden behind the curtains.\n",
    "\n",
    "\n",
    "\n",
    "## Exercise 2\n",
    "\n",
    "This exercise should improve your understanding of weight decay (or L2 regularization).\n",
    "\n",
    "  1. Consider a quadratic error function $E(\\textbf{w})=E_0+\\textbf{b}^T\\textbf{w}+1/2\\cdot\\textbf{w}^T\\textbf{H}\\textbf{w}$ and its regularized counterpart $E'(\\textbf{w})=E(\\textbf{w})+\\tau/2 \\cdot\\textbf{w}^T\\textbf{w}$, and let $\\textbf{w}^*$ and $\\tilde{\\textbf{w}}$ be the minimizers of $E$ and $E'$ respectively. We want to find a node to express $\\tilde{\\textbf{w}}$ as a function of $\\textbf{w}^*$, i.e. find the displacement introduced by weight decay.\n",
    "\n",
    "      - Find the gradients of $E$ and $E'$. Note that, at the global minimum,\n",
    "     we have $\\nabla E(\\textbf{w}^*)=\\nabla E'(\\hat{\\textbf{w}})=0$.\n",
    "      - In the equality above, express $\\textbf{w}^*$ and $\\tilde{\\textbf{w}}$ as a\n",
    "     linear combination of the eigenvectors of $\\textbf{H}$.\n",
    "      - Through algebraic manipulation, obtain $\\tilde{\\textbf{w}}_i$ as a function\n",
    "     of $\\textbf{w}^*_i$.\n",
    "      - Interpret this result geometrically.\n",
    "      - Note: $\\textbf{H}$ is square, symmetric, and positive definite,\n",
    "     which means that its eigenvectors are pairwise orthogonal and its eigenvalues\n",
    "     are positive (spectral theorem).\n",
    "\n",
    "  2. Consider a linear network of the form $y=\\textbf{w}^T\\textbf{x}$ and the mean\n",
    "squared error as a loss function. Assume that every observation is corrupted\n",
    "with Gaussian noise $\\epsilon\\sim\\mathcal{N}(\\textbf{0}, \\sigma^2\\textbf{I})$.\n",
    "Compute the expectation of the gradient under $\\epsilon$ and,\n",
    "show that adding gaussian noise to the inputs has the same effect of weight decay.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
