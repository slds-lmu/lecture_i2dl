{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b5bab54",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab 5\n",
    "\n",
    "Hüseyin Anil Gündüz\n",
    "\n",
    "Welcome to the fifth lab. We will first implement a simple scalar automatic\n",
    "differentiation engine to compute partial derivatives for us,\n",
    "then do a theoretical exercise about L2 regularization.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07021f95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:33.405368Z",
     "iopub.status.busy": "2021-11-16T10:05:33.404592Z",
     "iopub.status.idle": "2021-11-16T10:05:33.646352Z",
     "shell.execute_reply": "2021-11-16T10:05:33.646635Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from abc import ABC, abstractmethod\n",
    "from random import uniform\n",
    "from typing import Optional, List, Union, Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats('png', 'pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6b7cee",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "Modern deep learning frameworks compute gradients automatically,\n",
    "so that you only need to define how to perform the forward pass in your code.\n",
    "Under the hood, the framework constructs a computational graph based on the operations\n",
    "you used. For example, consider the node:\n",
    "\n",
    "\\begin{equation}\n",
    "4xy+e^{-y}\n",
    "\\label{eq:ex}\n",
    "\\end{equation}\n",
    "\n",
    "It can be translated into a graph that looks like this:\n",
    "\n",
    "![](labweek5.png)\n",
    "\n",
    "Where we have 'leaf' nodes at the top for variables and constants, and 'internal' nodes\n",
    "for operations. To make things simpler, in this exercise we will only work with\n",
    "scalar operations and scalar variables, but what we are going to create could, in\n",
    "principle, be extended to work with vectors and matrices. Section 6 of chapter 5 of the\n",
    "_Mathematics for Machine Learning_ book (https://mml-book.github.io/) is a good\n",
    "supplementary read.\n",
    "\n",
    "The naming of the classes responds to:\n",
    "\n",
    " - `Sum` for Addition $x+y$\n",
    " - `Sub` for Subtraction $x-y$\n",
    " - `Mul` for Product $x\\cdot y$\n",
    " - `Div` for Division $x / y$\n",
    " - `Exp` for Exponentiation $e^x$\n",
    " - `TanH` for Hyperbolic tangent $\\tanh(x)$\n",
    " - `Log` for Logarithm $\\log(x)$\n",
    "\n",
    "\n",
    "We first define some utilities to easily create nodes.\n",
    "An abstract class gives us a common interface across all the respective nodes that\n",
    "we will derive from it. This doesn't make sense now, as the class is basically empty,\n",
    "but we will extend the definition over the course of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dbc75aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:33.651750Z",
     "iopub.status.busy": "2021-11-16T10:05:33.651419Z",
     "iopub.status.idle": "2021-11-16T10:05:33.653271Z",
     "shell.execute_reply": "2021-11-16T10:05:33.652940Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BaseNode(ABC):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Const(BaseNode):\n",
    "    def __init__(self, value: Union[float, int]):\n",
    "        self.value = value\n",
    "\n",
    "\n",
    "class Var(BaseNode):\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "\n",
    "\n",
    "class BinaryOperation(BaseNode, ABC):\n",
    "    def __init__(self, x: BaseNode, y: BaseNode):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "class Sum(BinaryOperation):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Sub(BinaryOperation):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Mul(BinaryOperation):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Div(BinaryOperation):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Function(BaseNode, ABC):\n",
    "    def __init__(self, x: BaseNode):\n",
    "        self.x = x\n",
    "\n",
    "\n",
    "class Exp(Function):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Log(Function):\n",
    "    pass\n",
    "\n",
    "\n",
    "class TanH(Function):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5d6c2cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:33.656652Z",
     "iopub.status.busy": "2021-11-16T10:05:33.656142Z",
     "iopub.status.idle": "2021-11-16T10:05:33.658158Z",
     "shell.execute_reply": "2021-11-16T10:05:33.658422Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Sum object at 0x7f7da09fb090>\n",
      "<__main__.Mul object at 0x7f7da09f8f10>\n",
      "<__main__.Exp object at 0x7f7da09fb050>\n"
     ]
    }
   ],
   "source": [
    "# We then define the graph from the equation before.\n",
    "x = Var('x')\n",
    "y = Var('y')\n",
    "\n",
    "z = Sum(\n",
    "    x=Mul(\n",
    "        x=Mul(\n",
    "            x=Const(4),\n",
    "            y=x\n",
    "        ),\n",
    "        y=y,\n",
    "    ),\n",
    "    y=Exp(\n",
    "        x=Mul(\n",
    "            x=Const(-1),\n",
    "            y=y\n",
    "    ))\n",
    ")\n",
    "\n",
    "print(z)\n",
    "print(z.x.x)\n",
    "print(z.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67282bb3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This structure of nested objects contains the computational graph for the node above.\n",
    "Now, we can write code to manipulate this expression as we please.\n",
    "In the course of this exercise, we will see how:\n",
    "\n",
    " 1. Print an expression,\n",
    " 2. Compute its value, given the values of the variables involved,\n",
    " 3. Differentiate it to automatically find partial derivatives with respect to any given variable,\n",
    " 4. Transform it into simpler expressions that are cheaper to handle, and\n",
    " 5. Write code to train a neural network without getting our hands dirty with derivatives ever again.\n",
    "\n",
    "\n",
    "### Printing an expression\n",
    "First, since it is quite hard to understand the node from the representation above,\n",
    "let us extend the classes to convert a computational graph into a string representation\n",
    "that is easier to understand. For example, the expression $x+2y$ should be converted to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "086f6bda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:33.662364Z",
     "iopub.status.busy": "2021-11-16T10:05:33.662020Z",
     "iopub.status.idle": "2021-11-16T10:05:33.664653Z",
     "shell.execute_reply": "2021-11-16T10:05:33.664918Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(', 'x', '+', '(', '2', '*', 'y', ')', ')']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['(', 'x', '+', '(', '2', '*', 'y', ')', ')']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cd6950",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Which can be printed easily using `join`, resulting in `( x + ( 2 * y ) )`.\n",
    "\n",
    "Such a function should be _recursive_. This means that when simplifying a complicated\n",
    "expression it will call itself on each constituting piece of that expression, and\n",
    "\"assemble\" the results together.\n",
    "Conceptually, the procedure is similar to the factorial operation,\n",
    "which is recursively defined in terms of the factorial of a smaller number:\n",
    "\n",
    "\\begin{equation}\n",
    "n!=\\begin{cases}\n",
    "1 & \\text{if }n < 1 \\\\\n",
    "n\\cdot(n-1)! & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "This definition can be converted into Python as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7413678",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:33.667611Z",
     "iopub.status.busy": "2021-11-16T10:05:33.667287Z",
     "iopub.status.idle": "2021-11-16T10:05:33.669103Z",
     "shell.execute_reply": "2021-11-16T10:05:33.669368Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "def factorial(n: int) -> int:\n",
    "    if n < 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * factorial(n - 1)\n",
    "\n",
    "print(factorial(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558c55f5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In a similar way, we extend the classes with `__str__`, which is a Python utility to\n",
    "obtain a custom string representation of an object. If we define it correctly, we\n",
    "are able to traverse through the tree and print the expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88c5711c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:33.675885Z",
     "iopub.status.busy": "2021-11-16T10:05:33.675548Z",
     "iopub.status.idle": "2021-11-16T10:05:33.678012Z",
     "shell.execute_reply": "2021-11-16T10:05:33.677674Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class BaseNode(ABC):\n",
    "    @abstractmethod\n",
    "    def __str__(self) -> str:\n",
    "        pass\n",
    "\n",
    "\n",
    "class Const(BaseNode):\n",
    "    def __init__(self, value: Union[float, int]):\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str(self.value)\n",
    "\n",
    "\n",
    "class Var(BaseNode):\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.name\n",
    "\n",
    "\n",
    "class BinaryOperation(BaseNode, ABC):\n",
    "    def __init__(self, x: BaseNode, y: BaseNode):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "class Sum(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} + {})'.format(self.x, self.y)\n",
    "\n",
    "\n",
    "class Sub(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} - {})'.format(self.x, self.y)\n",
    "\n",
    "\n",
    "class Mul(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} * {})'.format(self.x, self.y)\n",
    "\n",
    "\n",
    "class Div(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} / {})'.format(self.x, self.y)\n",
    "\n",
    "\n",
    "class Function(BaseNode, ABC):\n",
    "    def __init__(self, x: BaseNode):\n",
    "        self.x = x\n",
    "\n",
    "\n",
    "class Exp(Function):\n",
    "    def __str__(self) -> str:\n",
    "        return 'exp({})'.format(self.x)\n",
    "\n",
    "\n",
    "class Log(Function):\n",
    "    def __str__(self) -> str:\n",
    "        return 'log({})'.format(self.x)\n",
    "\n",
    "\n",
    "class TanH(Function):\n",
    "    def __str__(self) -> str:\n",
    "        return 'tanh({})'.format(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e534a59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:33.680980Z",
     "iopub.status.busy": "2021-11-16T10:05:33.680657Z",
     "iopub.status.idle": "2021-11-16T10:05:33.682715Z",
     "shell.execute_reply": "2021-11-16T10:05:33.682451Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(((4 * x) * y) + exp((-1 * y)))\n"
     ]
    }
   ],
   "source": [
    "x = Var('x')\n",
    "y = Var('y')\n",
    "\n",
    "z = Sum(\n",
    "    x=Mul(\n",
    "        x=Mul(\n",
    "            x=Const(4),\n",
    "            y=x\n",
    "        ),\n",
    "        y=y,\n",
    "    ),\n",
    "    y=Exp(\n",
    "        x=Mul(\n",
    "            x=Const(-1),\n",
    "            y=y\n",
    "    ))\n",
    ")\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db97ffb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is much simpler to read!\n",
    "\n",
    "### Computing the value of an expression\n",
    "We can now extend the classes to compute the value of an expression given values for\n",
    "the variables. The methods should be recursive too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e33e549",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:33.692796Z",
     "iopub.status.busy": "2021-11-16T10:05:33.692450Z",
     "iopub.status.idle": "2021-11-16T10:05:33.694237Z",
     "shell.execute_reply": "2021-11-16T10:05:33.693814Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class BaseNode(ABC):\n",
    "    @abstractmethod\n",
    "    def __str__(self) -> str:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        pass\n",
    "\n",
    "\n",
    "class Const(BaseNode):\n",
    "    def __init__(self, value: Union[float, int]):\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str(self.value)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return self.value\n",
    "\n",
    "\n",
    "class Var(BaseNode):\n",
    "    def __init__(self, name: str, value: Optional[Union[float, int]] = None):\n",
    "        self.name = name\n",
    "        self.value = value\n",
    "        \n",
    "    def set_value(self, value: Union[float, int]) -> None:\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.name\n",
    "    \n",
    "    def eval(self) -> Union[float, int]:\n",
    "        if self.value is None:\n",
    "            raise ValueError('Value is not set. Evaluation is not possible.')\n",
    "        return self.value\n",
    "\n",
    "\n",
    "class BinaryOperation(BaseNode, ABC):\n",
    "    def __init__(self, x: BaseNode, y: BaseNode):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "class Sum(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} + {})'.format(self.x, self.y)\n",
    "    \n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return self.x.eval() + self.y.eval()\n",
    "\n",
    "\n",
    "class Sub(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} - {})'.format(self.x, self.y)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return self.x.eval() - self.y.eval()\n",
    "\n",
    "\n",
    "class Mul(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} * {})'.format(self.x, self.y)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return self.x.eval() * self.y.eval()\n",
    "\n",
    "\n",
    "class Div(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} / {})'.format(self.x, self.y)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return self.x.eval() / self.y.eval()\n",
    "\n",
    "\n",
    "class Function(BaseNode, ABC):\n",
    "    def __init__(self, x: BaseNode):\n",
    "        self.x = x\n",
    "\n",
    "\n",
    "class Exp(Function):\n",
    "    def __str__(self) -> str:\n",
    "        return 'exp({})'.format(self.x)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return math.exp(self.x.eval())\n",
    "\n",
    "class Log(Function):\n",
    "    def __str__(self) -> str:\n",
    "        return 'log({})'.format(self.x)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return math.log(self.x.eval())\n",
    "\n",
    "class TanH(Function):\n",
    "    def __str__(self) -> str:\n",
    "        return 'tanh({})'.format(self.x)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return math.tanh(self.x.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71cd39cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:33.697113Z",
     "iopub.status.busy": "2021-11-16T10:05:33.696789Z",
     "iopub.status.idle": "2021-11-16T10:05:33.698742Z",
     "shell.execute_reply": "2021-11-16T10:05:33.698479Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.049787068367863\n"
     ]
    }
   ],
   "source": [
    "x = Var('x', value=2)\n",
    "y = Var('y', value=3)\n",
    "\n",
    "z = Sum(\n",
    "    x=Mul(\n",
    "        x=Mul(\n",
    "            x=Const(4),\n",
    "            y=x\n",
    "        ),\n",
    "        y=y,\n",
    "    ),\n",
    "    y=Exp(\n",
    "        x=Mul(\n",
    "            x=Const(-1),\n",
    "            y=y\n",
    "    ))\n",
    ")\n",
    "\n",
    "print(z.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ca032d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The result that we expect is, of course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0d95eca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:33.701178Z",
     "iopub.status.busy": "2021-11-16T10:05:33.700859Z",
     "iopub.status.idle": "2021-11-16T10:05:33.702681Z",
     "shell.execute_reply": "2021-11-16T10:05:33.702358Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.049787068367863\n"
     ]
    }
   ],
   "source": [
    "print(4 * 2 * 3 + math.exp(-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0c9718",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Differentiating an expression\n",
    "\n",
    "We can finally see how to differentiate an expression with respect to a variable.\n",
    "We do this again through extending the classes to differentiates each argument\n",
    "and merge the result. Note that this should return a new computational\n",
    "graph that contains the operations necessary to compute the partial derivative\n",
    "we are interested in.\n",
    "Each `differentiate` method gets the argument `var` that specifies the name of the\n",
    "variable of which we are computing the gradient.\n",
    "\n",
    "Remember to use the chain rule where appropriate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf7813fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:33.717825Z",
     "iopub.status.busy": "2021-11-16T10:05:33.717457Z",
     "iopub.status.idle": "2021-11-16T10:05:33.719314Z",
     "shell.execute_reply": "2021-11-16T10:05:33.718984Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class BaseNode(ABC):\n",
    "    @abstractmethod\n",
    "    def __str__(self) -> str:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        pass\n",
    "\n",
    "\n",
    "class Const(BaseNode):\n",
    "    def __init__(self, value: Union[float, int]):\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str(self.value)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return self.value\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Const(0)\n",
    "\n",
    "\n",
    "class Var(BaseNode):\n",
    "    def __init__(self, name: str, value: Optional[Union[float, int]] = None):\n",
    "        self.name = name\n",
    "        self.value = value\n",
    "\n",
    "    def set_value(self, value: Union[float, int]) -> None:\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.name\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        if self.value is None:\n",
    "            raise ValueError('Value is not set. Evaluation is not possible.')\n",
    "        return self.value\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Const(1) if self.name == var else Const(0)\n",
    "\n",
    "\n",
    "class BinaryOperation(BaseNode, ABC):\n",
    "    def __init__(self, x: BaseNode, y: BaseNode):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "class Sum(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} + {})'.format(self.x, self.y)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return self.x.eval() + self.y.eval()\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Sum(self.x.differentiate(var), self.y.differentiate(var))\n",
    "\n",
    "\n",
    "class Sub(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} - {})'.format(self.x, self.y)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return self.x.eval() - self.y.eval()\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Sub(self.x.differentiate(var), self.y.differentiate(var))\n",
    "\n",
    "\n",
    "class Mul(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} * {})'.format(self.x, self.y)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return self.x.eval() * self.y.eval()\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Sum(\n",
    "            x=Mul(self.x.differentiate(var), self.y),\n",
    "            y=Mul(self.x, self.y.differentiate(var))\n",
    "        )\n",
    "\n",
    "\n",
    "class Div(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} / {})'.format(self.x, self.y)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return self.x.eval() / self.y.eval()\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Div(\n",
    "            x=Sub(\n",
    "                x=Mul(self.x.differentiate(var), self.y),\n",
    "                y=Mul(self.y.differentiate(var), self.x)\n",
    "            ),\n",
    "            y=Mul(self.y, self.y)\n",
    "        )\n",
    "\n",
    "\n",
    "class Function(BaseNode, ABC):\n",
    "    def __init__(self, x: BaseNode):\n",
    "        self.x = x\n",
    "\n",
    "\n",
    "class Exp(Function):\n",
    "    def __str__(self) -> str:\n",
    "        return 'exp({})'.format(self.x)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return math.exp(self.x.eval())\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Mul(\n",
    "            x=Exp(self.x),\n",
    "            y=self.x.differentiate(var)\n",
    "        )\n",
    "\n",
    "\n",
    "class Log(Function):\n",
    "    def __str__(self) -> str:\n",
    "        return 'log({})'.format(self.x)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return math.log(self.x.eval())\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Div(\n",
    "            x=self.x.differentiate(var),\n",
    "            y=self.x\n",
    "        )\n",
    "\n",
    "\n",
    "class TanH(Function):\n",
    "    def __str__(self) -> str:\n",
    "        return 'tanh({})'.format(self.x)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return math.tanh(self.x.eval())\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Mul(\n",
    "            x=Sub(\n",
    "                x=Const(1),\n",
    "                y=Mul(TanH(self.x), TanH(self.x))\n",
    "            ),\n",
    "            y=self.x.differentiate(var)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b03635ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:33.722524Z",
     "iopub.status.busy": "2021-11-16T10:05:33.722199Z",
     "iopub.status.idle": "2021-11-16T10:05:33.724120Z",
     "shell.execute_reply": "2021-11-16T10:05:33.723859Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(((((0 * x) + (4 * 1)) * y) + ((4 * x) * 0)) + (exp((-1 * y)) * ((0 * y) + (-1 * 0))))\n"
     ]
    }
   ],
   "source": [
    "x = Var('x', value=2)\n",
    "y = Var('y', value=3)\n",
    "\n",
    "z = Sum(\n",
    "    x=Mul(\n",
    "        x=Mul(\n",
    "            x=Const(4),\n",
    "            y=x\n",
    "        ),\n",
    "        y=y,\n",
    "    ),\n",
    "    y=Exp(\n",
    "        x=Mul(\n",
    "            x=Const(-1),\n",
    "            y=y\n",
    "    ))\n",
    ")\n",
    "\n",
    "dz = z.differentiate('x')\n",
    "print(dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6902e6c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This looks a bit complicated, but by applying some trivial simplifications we see it is correct:\n",
    "\n",
    "\\begin{align*}\n",
    "& ( ( ( ( ( 0 \\cdot x ) + ( 4 \\cdot 1 ) ) \\cdot y ) + ( ( 4 \\cdot x ) \\cdot 0 ) ) + ( exp ( ( -1 \\cdot y ) ) \\cdot ( ( 0 \\cdot y ) + ( -1 \\cdot 0 ) ) ) )  \\\\\n",
    "&\\qquad= ( ( ( 0 + 4 ) \\cdot y ) + 0 ) + ( exp ( ( -1 \\cdot y ) ) \\cdot ( 0 + 0 ) ) ) \\\\\n",
    "&\\qquad= ( 4 \\cdot y ) + ( exp ( ( -1 \\cdot y ) ) \\cdot 0 ) ) \\\\\n",
    "&\\qquad= ( 4 \\cdot y ) + 0 \\\\\n",
    "&\\qquad= 4 \\cdot y \\\\\n",
    "&\\qquad= \\frac{\\text{d}}{\\text{d}x} \\left(4xy+e^{-y}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "These simplification rules are trivial arithmetic identities:\n",
    "\n",
    " - $0+x=x$\n",
    " - $0\\cdot x=0$\n",
    " - $1\\cdot x=x$\n",
    " - $0/x=0$\n",
    "\n",
    "Let us extend the classes that use these identities to automatically simplify `dz` in\n",
    "the same way we just did. As with differentiation, this should return a\n",
    "new computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aa8f158",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:33.747491Z",
     "iopub.status.busy": "2021-11-16T10:05:33.747091Z",
     "iopub.status.idle": "2021-11-16T10:05:33.748958Z",
     "shell.execute_reply": "2021-11-16T10:05:33.748530Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class BaseNode(ABC):\n",
    "    @abstractmethod\n",
    "    def __str__(self) -> str:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def simplify(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Const(BaseNode):\n",
    "    def __init__(self, value: Union[float, int]):\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str(self.value)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return self.value\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Const(0)\n",
    "\n",
    "    def simplify(self):\n",
    "        return self\n",
    "\n",
    "\n",
    "class Var(BaseNode):\n",
    "    def __init__(self, name: str, value: Optional[Union[float, int]] = None):\n",
    "        self.name = name\n",
    "        self.value = value\n",
    "\n",
    "    def set_value(self, value: Union[float, int]) -> None:\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.name\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        if self.value is None:\n",
    "            raise ValueError('Value is not set. Evaluation is not possible.')\n",
    "        return self.value\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Const(1) if self.name == var else Const(0)\n",
    "\n",
    "    def simplify(self):\n",
    "        return self\n",
    "\n",
    "\n",
    "class BinaryOperation(BaseNode, ABC):\n",
    "    def __init__(self, x: BaseNode, y: BaseNode):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "class Sum(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} + {})'.format(self.x, self.y)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return self.x.eval() + self.y.eval()\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Sum(self.x.differentiate(var), self.y.differentiate(var))\n",
    "\n",
    "    def simplify(self):\n",
    "        simple_x = (\n",
    "            self.x.simplify()\n",
    "        )\n",
    "\n",
    "        simple_y = (\n",
    "            self.y.simplify()\n",
    "        )\n",
    "\n",
    "        if isinstance(simple_x, Const):\n",
    "            if simple_x.value == 0:\n",
    "                # Rule: 0 + y = y\n",
    "                return simple_y\n",
    "\n",
    "        if isinstance(simple_y, Const):\n",
    "            if simple_y.value == 0:\n",
    "                # Rule: x + 0 = x\n",
    "                return simple_x\n",
    "\n",
    "        if isinstance(simple_x, Const) and isinstance(simple_y, Const):\n",
    "            # If both arguments are constants we can perform the sum immediately\n",
    "            return Const(simple_x.value + simple_y.value)\n",
    "\n",
    "        # Cannot simplify further. Return a new sum node with the simplified operands.\n",
    "        return Sum(simple_x, simple_y)\n",
    "\n",
    "\n",
    "class Sub(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} - {})'.format(self.x, self.y)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return self.x.eval() - self.y.eval()\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Sub(self.x.differentiate(var), self.y.differentiate(var))\n",
    "\n",
    "    def simplify(self):\n",
    "        simple_x =  self.x.simplify()\n",
    "        simple_y =  self.y.simplify()\n",
    "\n",
    "        if isinstance(simple_x, Const):\n",
    "            if simple_x.value == 0:\n",
    "                # Rule: 0 - y = -1 * y\n",
    "                return Mul(Const(-1), simple_y)\n",
    "\n",
    "        if isinstance(simple_y, Const):\n",
    "            if simple_y.value == 0:\n",
    "                # Rule: x - 0 = x\n",
    "                return simple_y\n",
    "\n",
    "        if isinstance(simple_x, Const) and isinstance(simple_y, Const):\n",
    "            # If both arguments are constants we can perform the subtraction immediately\n",
    "            return Const(simple_x.value - simple_y.value)\n",
    "\n",
    "        # Cannot simplify further.\n",
    "        return Sub(simple_x, simple_y)\n",
    "\n",
    "\n",
    "class Mul(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} * {})'.format(self.x, self.y)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return self.x.eval() * self.y.eval()\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Sum(\n",
    "            x=Mul(self.x.differentiate(var), self.y),\n",
    "            y=Mul(self.x, self.y.differentiate(var))\n",
    "        )\n",
    "\n",
    "    def simplify(self):\n",
    "        simple_x =  self.x.simplify()\n",
    "        simple_y =  self.y.simplify()\n",
    "\n",
    "        if isinstance(simple_x, Const):\n",
    "            if simple_x.value == 0:\n",
    "                # Rule: 0 * y = 0\n",
    "                return Const(0)\n",
    "            elif simple_x.value == 1:\n",
    "                # Rule: 1 * y = y\n",
    "                return simple_y\n",
    "\n",
    "        if isinstance(simple_y, Const):\n",
    "            if simple_y.value == 0:\n",
    "                # Rule: x * 0 = 0\n",
    "                return Const(0)\n",
    "            elif simple_y.value == 1:\n",
    "                # Rule: x * 1 = x\n",
    "                return simple_x\n",
    "\n",
    "        if isinstance(simple_x, Const) and isinstance(simple_y, Const):\n",
    "            # Perform the operation if possible\n",
    "            return Const(simple_x.value * simple_y.value)\n",
    "\n",
    "        # Cannot simplify further.\n",
    "        return Mul(simple_x, simple_y)\n",
    "\n",
    "\n",
    "class Div(BinaryOperation):\n",
    "    def __str__(self) -> str:\n",
    "        return '({} / {})'.format(self.x, self.y)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return self.x.eval() / self.y.eval()\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Div(\n",
    "            x=Sub(\n",
    "                x=Mul(self.x.differentiate(var), self.y),\n",
    "                y=Mul(self.y.differentiate(var), self.x)\n",
    "            ),\n",
    "            y=Mul(self.y, self.y)\n",
    "        )\n",
    "\n",
    "    def simplify(self):\n",
    "        simple_x =  self.x.simplify()\n",
    "        simple_y =  self.y.simplify()\n",
    "\n",
    "        if isinstance(simple_x, Const):\n",
    "            if simple_x.value == 0:\n",
    "                # Rule: 0 / y = 0\n",
    "                return Const(0)\n",
    "\n",
    "        if isinstance(simple_y, Const):\n",
    "            if simple_y.value == 0:\n",
    "                # Rule: x / 0 = ERROR\n",
    "                raise ZeroDivisionError()\n",
    "            elif simple_y.value == 1:\n",
    "                # Rule: x / 1 = x\n",
    "                return simple_x\n",
    "\n",
    "        if isinstance(simple_x, Const) and isinstance(simple_y, Const):\n",
    "            # Perform the operation if possible\n",
    "            return Const(simple_x.value / simple_y.value)\n",
    "\n",
    "        # Cannot simplify further.\n",
    "        return Div(simple_x, simple_y)\n",
    "\n",
    "\n",
    "class Function(BaseNode, ABC):\n",
    "    def __init__(self, x: BaseNode):\n",
    "        self.x = x\n",
    "\n",
    "\n",
    "class Exp(Function):\n",
    "    def __str__(self) -> str:\n",
    "        return 'exp({})'.format(self.x)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return math.exp(self.x.eval())\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Mul(\n",
    "            x=Exp(self.x),\n",
    "            y=self.x.differentiate(var)\n",
    "        )\n",
    "\n",
    "    def simplify(self):\n",
    "        simple_x =  self.x.simplify()\n",
    "\n",
    "        if isinstance(simple_x, Const):\n",
    "            if simple_x.value == 0:\n",
    "                # Rule: exp(0) = 1\n",
    "                return Const(1)\n",
    "            # Perform the operation if possible\n",
    "            return Const(math.exp(simple_x.value))\n",
    "\n",
    "        # Cannot simplify further.\n",
    "        return Exp(simple_x)\n",
    "\n",
    "\n",
    "class Log(Function):\n",
    "    def __str__(self) -> str:\n",
    "        return 'log({})'.format(self.x)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return math.log(self.x.eval())\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Div(\n",
    "            x=self.x.differentiate(var),\n",
    "            y=self.x\n",
    "        )\n",
    "\n",
    "    def simplify(self):\n",
    "        simple_x =  self.x.simplify()\n",
    "\n",
    "        if isinstance(simple_x, Const):\n",
    "            if simple_x.value <= 0:\n",
    "                # Rule: No log of non-positiv number!\n",
    "                raise ValueError('Logarithm of non-positiv number.')\n",
    "            # Perform the operation if possible\n",
    "            return Const(math.log(simple_x.value))\n",
    "\n",
    "        # Cannot simplify further.\n",
    "        return Log(simple_x)\n",
    "\n",
    "\n",
    "class TanH(Function):\n",
    "    def __str__(self) -> str:\n",
    "        return 'tanh({})'.format(self.x)\n",
    "\n",
    "    def eval(self) -> Union[float, int]:\n",
    "        return math.tanh(self.x.eval())\n",
    "\n",
    "    def differentiate(self, var: str) -> BaseNode:\n",
    "        return Mul(\n",
    "            x=Sub(\n",
    "                x=Const(1),\n",
    "                y=Mul(TanH(self.x), TanH(self.x))\n",
    "            ),\n",
    "            y=self.x.differentiate(var)\n",
    "        )\n",
    "\n",
    "    def simplify(self):\n",
    "        simple_x =  self.x.simplify()\n",
    "\n",
    "        if isinstance(simple_x, Const):\n",
    "            if simple_x.value == 0:\n",
    "                # Rule: tanh(0) = 0\n",
    "                return Const(0)\n",
    "            # Perform the operation if possible\n",
    "            return Const(math.tanh(simple_x.value))\n",
    "\n",
    "        # Cannot simplify further.\n",
    "        return TanH(simple_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b7f8a39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:33.752272Z",
     "iopub.status.busy": "2021-11-16T10:05:33.751948Z",
     "iopub.status.idle": "2021-11-16T10:05:33.753547Z",
     "shell.execute_reply": "2021-11-16T10:05:33.753811Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4 * y)\n"
     ]
    }
   ],
   "source": [
    "x = Var('x', value=2)\n",
    "y = Var('y', value=3)\n",
    "\n",
    "z = Sum(\n",
    "    x=Mul(\n",
    "        x=Mul(\n",
    "            x=Const(4),\n",
    "            y=x\n",
    "        ),\n",
    "        y=y,\n",
    "    ),\n",
    "    y=Exp(\n",
    "        x=Mul(\n",
    "            x=Const(-1),\n",
    "            y=y\n",
    "    ))\n",
    ")\n",
    "\n",
    "dz = z.differentiate('x')\n",
    "print(dz.simplify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553acf0c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The result matches what we showed above, $4y$. Simplifying the graph with these and\n",
    "other, more advanced tricks, can greatly speed up code.\n",
    "\n",
    "Now we are also equipped to perform differentiation of any order,\n",
    "for example $\\partial z / \\partial x\\partial y$ is simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b47825a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:33.756569Z",
     "iopub.status.busy": "2021-11-16T10:05:33.756239Z",
     "iopub.status.idle": "2021-11-16T10:05:33.758208Z",
     "shell.execute_reply": "2021-11-16T10:05:33.757928Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(((((((0 * x) + (0 * 0)) + ((0 * 1) + (4 * 0))) * y) + (((0 * x) + (4 * 1)) * 1)) + ((((0 * x) + (4 * 0)) * 0) + ((4 * x) * 0))) + (((exp((-1 * y)) * ((0 * y) + (-1 * 1))) * ((0 * y) + (-1 * 0))) + (exp((-1 * y)) * (((0 * y) + (0 * 1)) + ((0 * 0) + (-1 * 0))))))\n",
      "4\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(z.differentiate('x').differentiate('y'))\n",
    "print(z.differentiate('x').differentiate('y').simplify())\n",
    "print(z.differentiate('x').differentiate('y').eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e0a2c7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Training a network\n",
    "\n",
    "Let us now define a computational graph that performs the forward pass of a\n",
    "simple network, and use the functions above to compute the gradients of the parameters.\n",
    "We will use the same network we used in the third lab, reproduced below, and, as usual,\n",
    "we will test the code on the five points dataset. Since the functions we have written\n",
    "so far only work with scalar values, we will perform stochastic gradient descent\n",
    "using one sample at a time.\n",
    "\n",
    "![](../utils/03-lab-nn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91d15712",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:33.761331Z",
     "iopub.status.busy": "2021-11-16T10:05:33.761007Z",
     "iopub.status.idle": "2021-11-16T10:05:33.762881Z",
     "shell.execute_reply": "2021-11-16T10:05:33.762556Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh((b1 + ((x1 * w11) + (x2 * w21))))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The two input nodes\n",
    "x1 = Var('x1')\n",
    "x2 = Var('x2')\n",
    "\n",
    "# Parameters for the first hidden neuron\n",
    "b1 = Var('b1')\n",
    "w11 = Var('w11')\n",
    "w21 = Var('w21')\n",
    "\n",
    "# Compute the output of the first hidden neuron\n",
    "z1in = Sum(\n",
    "    x=b1,\n",
    "    y=Sum(\n",
    "       x=Mul(x1, w11),\n",
    "       y=Mul(x2, w21)\n",
    "    )\n",
    ")\n",
    "\n",
    "z1out = TanH(z1in)\n",
    "\n",
    "print(z1out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4755b632",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, complete the remaining part of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ac218e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:33.766831Z",
     "iopub.status.busy": "2021-11-16T10:05:33.766495Z",
     "iopub.status.idle": "2021-11-16T10:05:33.768411Z",
     "shell.execute_reply": "2021-11-16T10:05:33.768662Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1 / (1 + exp((-1 * (c + ((tanh((b1 + ((x1 * w11) + (x2 * w21)))) * u1) + (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * u2)))))))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "b2 = Var('b2')\n",
    "w12 = Var('w12')\n",
    "w22 = Var('w22')\n",
    "\n",
    "z2out = (\n",
    "    TanH(\n",
    "        Sum(\n",
    "            x=b2,\n",
    "            y=Sum(\n",
    "                x=Mul(x1, w12),\n",
    "                y=Mul(x2, w22)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "c = Var('c')\n",
    "u1 = Var('u1')\n",
    "u2 = Var('u2')\n",
    "\n",
    "fin = (\n",
    "    Sum(\n",
    "        x=c,\n",
    "        y=Sum(\n",
    "            x=Mul(z1out, u1),\n",
    "            y=Mul(z2out, u2)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "fout = (\n",
    "    Div(\n",
    "        x=Const(1),\n",
    "        y=Sum(\n",
    "            x=Const(1),\n",
    "            y=Exp(Mul(Const(-1), fin))\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "print(fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e436c19",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And this defines the forward pass.\n",
    "\n",
    "We can now compute the predictions of the network by evaluating `fout`,\n",
    "providing values for the inputs and weights. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e56c4aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:33.772682Z",
     "iopub.status.busy": "2021-11-16T10:05:33.772291Z",
     "iopub.status.idle": "2021-11-16T10:05:33.774737Z",
     "shell.execute_reply": "2021-11-16T10:05:33.774411Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9533527373765618"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Values for weights and biases\n",
    "b1.set_value(1.543385)\n",
    "w11.set_value(3.111573)\n",
    "w12.set_value(-2.808800)\n",
    "\n",
    "b2.set_value(1.373085)\n",
    "w21.set_value(3.130452)\n",
    "w22.set_value(-2.813466)\n",
    "\n",
    "c.set_value(-4.241453)\n",
    "u1.set_value(4.036489)\n",
    "u2.set_value(4.074885)\n",
    "\n",
    "# Values for the input\n",
    "x1.set_value(1)\n",
    "x2.set_value(-1)\n",
    "\n",
    "fout.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb9d27b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Which should be about 0.9.\n",
    "We now have to compute the cross-entropy loss.\n",
    "For numerical stability, we will compute the loss using $f_{in}$ instead of $f_{out}$.\n",
    "Therefore, first, show that:\n",
    "\n",
    "\\begin{equation}\n",
    "-y\\cdot\\log(f_{out})-(1-y)\\cdot\\log(1-f_{out})=\n",
    "f_{in}-f_{in}\\cdot y+\\log(1+e^{-f_{in}})\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Solution:\n",
    "\n",
    "\\begin{align}\n",
    "&-y\\cdot\\log (f_{out})-(1-y)\\cdot\\log(1-f_{out}) \\\\\n",
    "&\\qquad=-y\\cdot\\log\\frac{1}{1+e^{-f_{in}}}-(1-y)\\cdot\\log\\left(1-\\frac{1}{1+e^{-f_{in}}}\\right) \\\\\n",
    "&\\qquad=\n",
    "-y\\cdot-\\log\\left(1+e^{-f_{in}}\\right)\n",
    "-(1-y)\\cdot\\left(-f_{in}-\\log\\left(1+e^{-f_{in}}\\right)\\right) \\\\\n",
    "&\\qquad=\n",
    "y\\cdot\\log\\left(1+e^{-f_{in}}\\right)\n",
    "+f_{in}+\\log\\left(1+e^{-f_{in}}\\right)\n",
    "-y\\cdot f_{in}-y\\cdot\\log\\left(1+e^{-f_{in}}\\right) \\\\\n",
    "&\\qquad=f_{in}-f_{in}\\cdot y+\\log(1+e^{-f_{in}})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54885f9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:33.778072Z",
     "iopub.status.busy": "2021-11-16T10:05:33.777712Z",
     "iopub.status.idle": "2021-11-16T10:05:33.779413Z",
     "shell.execute_reply": "2021-11-16T10:05:33.779778Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(((c + ((tanh((b1 + ((x1 * w11) + (x2 * w21)))) * u1) + (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * u2))) - ((c + ((tanh((b1 + ((x1 * w11) + (x2 * w21)))) * u1) + (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * u2))) * y)) + log((1 + exp((-1 * (c + ((tanh((b1 + ((x1 * w11) + (x2 * w21)))) * u1) + (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * u2))))))))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This variable contains the label for the sample the network is predicting\n",
    "y = Var('y')\n",
    "\n",
    "loss = (\n",
    "    Sum(\n",
    "        x=Sub(\n",
    "            x=fin,\n",
    "            y=Mul(fin, y)\n",
    "        ),\n",
    "        y=Log(\n",
    "            Sum(\n",
    "                x=Const(1),\n",
    "                y=Exp(Mul(Const(-1), fin)))\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d40f3de",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is starting to look complicated!\n",
    "Luckily, this time, we do not have to get our hands dirty with derivatives.\n",
    "Let us find the graphs for the derivatives of each parameter of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "def374be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:33.784743Z",
     "iopub.status.busy": "2021-11-16T10:05:33.784408Z",
     "iopub.status.idle": "2021-11-16T10:05:33.786394Z",
     "shell.execute_reply": "2021-11-16T10:05:33.786057Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(((0 + (((((1 - (tanh((b1 + ((x1 * w11) + (x2 * w21)))) * tanh((b1 + ((x1 * w11) + (x2 * w21)))))) * (0 + (((0 * w11) + (x1 * 1)) + ((0 * w21) + (x2 * 0))))) * u1) + (tanh((b1 + ((x1 * w11) + (x2 * w21)))) * 0)) + ((((1 - (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * tanh((b2 + ((x1 * w12) + (x2 * w22)))))) * (0 + (((0 * w12) + (x1 * 0)) + ((0 * w22) + (x2 * 0))))) * u2) + (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * 0)))) - (((0 + (((((1 - (tanh((b1 + ((x1 * w11) + (x2 * w21)))) * tanh((b1 + ((x1 * w11) + (x2 * w21)))))) * (0 + (((0 * w11) + (x1 * 1)) + ((0 * w21) + (x2 * 0))))) * u1) + (tanh((b1 + ((x1 * w11) + (x2 * w21)))) * 0)) + ((((1 - (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * tanh((b2 + ((x1 * w12) + (x2 * w22)))))) * (0 + (((0 * w12) + (x1 * 0)) + ((0 * w22) + (x2 * 0))))) * u2) + (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * 0)))) * y) + ((c + ((tanh((b1 + ((x1 * w11) + (x2 * w21)))) * u1) + (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * u2))) * 0))) + ((0 + (exp((-1 * (c + ((tanh((b1 + ((x1 * w11) + (x2 * w21)))) * u1) + (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * u2))))) * ((0 * (c + ((tanh((b1 + ((x1 * w11) + (x2 * w21)))) * u1) + (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * u2)))) + (-1 * (0 + (((((1 - (tanh((b1 + ((x1 * w11) + (x2 * w21)))) * tanh((b1 + ((x1 * w11) + (x2 * w21)))))) * (0 + (((0 * w11) + (x1 * 1)) + ((0 * w21) + (x2 * 0))))) * u1) + (tanh((b1 + ((x1 * w11) + (x2 * w21)))) * 0)) + ((((1 - (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * tanh((b2 + ((x1 * w12) + (x2 * w22)))))) * (0 + (((0 * w12) + (x1 * 0)) + ((0 * w22) + (x2 * 0))))) * u2) + (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * 0)))))))) / (1 + exp((-1 * (c + ((tanh((b1 + ((x1 * w11) + (x2 * w21)))) * u1) + (tanh((b2 + ((x1 * w12) + (x2 * w22)))) * u2))))))))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_gradient_graphs(graph: BaseNode, param_nodes: List[Var]) -> Dict:\n",
    "    graph_dict: Dict = {}\n",
    "    for param_node in param_nodes:\n",
    "        graph_dict.update({\n",
    "            param_node.name : graph.differentiate(param_node.name)\n",
    "        })\n",
    "    return graph_dict\n",
    "\n",
    "parameter_nodes = [b1, w11, w12, b2, w21, w22, c, u1, u2]\n",
    "gradient_graphs = get_gradient_graphs(loss, parameter_nodes)\n",
    "print(gradient_graphs.get('w11'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeec69c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As you can see, there is a great deal of repetition in this expression.\n",
    "The repetitions could be removed by storing, in each node, its current value and\n",
    "gradient, so that we would not need to re-compute them every time. Modern deep\n",
    "learning frameworks indeed do this, and are able to compute the gradient of the\n",
    "loss with respect to all parameters in a single pass, but here we accept these\n",
    "inefficiencies for the sake of simplicity.\n",
    "\n",
    "We are now ready to train this network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cffe50c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:33.851863Z",
     "iopub.status.busy": "2021-11-16T10:05:33.834305Z",
     "iopub.status.idle": "2021-11-16T10:05:39.656430Z",
     "shell.execute_reply": "2021-11-16T10:05:39.656704Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: \t   250 \t LOSS: \t 1.90523\r"
     ]
    },
    {
     "data": {
      "application/pdf": "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\n",
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Create dataset.\n",
    "x = [\n",
    "    [0, 0],\n",
    "    [1, 0],\n",
    "    [0, -1],\n",
    "    [-1, 0],\n",
    "    [0, 1]\n",
    "]\n",
    "labels = [1, 0, 0, 0, 0]\n",
    "\n",
    "# Glorot initialization for the parameters.\n",
    "def init_params(param_nodes: List[Var], bias_names: List[str]) -> None:\n",
    "    b = math.sqrt(6 / 4)\n",
    "    for param_node in param_nodes:\n",
    "        if param_node.name in bias_names:\n",
    "            param_node.set_value(0.0)\n",
    "        else:\n",
    "            param_node.set_value(uniform(-b, b))\n",
    "\n",
    "init_params(parameter_nodes, ['b1', 'b2', 'c'])\n",
    "\n",
    "# Training Loop.\n",
    "# Losses dict follows (index of sample : list of losses during training).\n",
    "loss_dict = {i: [] for i in range(len(x))}\n",
    "for ep in range(250):\n",
    "    ep_loss = 0.\n",
    "    for i in range(len(x)):\n",
    "        # Set the correct values for the inputs and label.\n",
    "        x1.set_value(x[i][0])\n",
    "        x2.set_value(x[i][1])\n",
    "        y.set_value(labels[i])\n",
    "\n",
    "        loss_dict[i].append(\n",
    "            loss.eval()\n",
    "        )\n",
    "        ep_loss += loss_dict[i][-1]\n",
    "\n",
    "        gradient_graphs = get_gradient_graphs(loss, parameter_nodes)\n",
    "        gradients_eval = {key: graph.simplify().eval() for key, graph in gradient_graphs.items()}\n",
    "\n",
    "        for parameter_node in parameter_nodes:\n",
    "            lr = 2.5 if ep < 100 else 0.5\n",
    "            new_val = parameter_node.value - lr * gradients_eval[parameter_node.name]\n",
    "            parameter_node.set_value(new_val)\n",
    "\n",
    "    print('EPOCH: \\t {:5} \\t LOSS: \\t {:.5f}'.format(ep + 1, float(ep_loss)), end='\\r')\n",
    "\n",
    "\n",
    "# Plot loss development.\n",
    "for sample_loss in loss_dict.values():\n",
    "    plt.plot(sample_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b6d53",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can clearly see how the loss of each individual training sample evolves over time.\n",
    "This also explains the \"saddle\" you might have noticed in the loss curve from the\n",
    "previous lab.\n",
    "\n",
    "And these are the predictions for the five points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca4b059f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T10:05:39.660196Z",
     "iopub.status.busy": "2021-11-16T10:05:39.659828Z",
     "iopub.status.idle": "2021-11-16T10:05:39.661984Z",
     "shell.execute_reply": "2021-11-16T10:05:39.661681Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE:\t0\tTRUE:\t1\tPRED:\t0.386\n",
      "SAMPLE:\t1\tTRUE:\t0\tPRED:\t0.001\n",
      "SAMPLE:\t2\tTRUE:\t0\tPRED:\t0.000\n",
      "SAMPLE:\t3\tTRUE:\t0\tPRED:\t0.001\n",
      "SAMPLE:\t4\tTRUE:\t0\tPRED:\t0.387\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(x)):\n",
    "    x1.set_value(x[i][0])\n",
    "    x2.set_value(x[i][1])\n",
    "    y.set_value(labels[i])\n",
    "\n",
    "    prediction = fout.eval()\n",
    "    print('SAMPLE:\\t{}\\tTRUE:\\t{}\\tPRED:\\t{:.3f}'.format(i, labels[i], prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c2e0c1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Conclusion\n",
    "What we did in this exercise is (a simplification of) how deep learning frameworks\n",
    "evaluate the code you write. You only need to define how to compute the output of\n",
    "the network, and the framework figures out the necessary gradients on its own.\n",
    "They provide a much better user interface, allowing you to use `+`, `-`, `/`, `*` etc.\n",
    "as you normally would instead of the clumsy node constructors we defined here,\n",
    "but there is always a computational graph hidden behind the curtains.\n",
    "\n",
    "\n",
    "\n",
    "## Exercise 2\n",
    "\n",
    "This exercise should improve your understanding of weight decay (or L2 regularization).\n",
    "\n",
    "  1. Consider a quadratic error function $E(\\textbf{w})=E_0+\\textbf{b}^T\\textbf{w}+1/2\\cdot\\textbf{w}^T\\textbf{H}\\textbf{w}$ and its regularized counterpart $E'(\\textbf{w})=E(\\textbf{w})+\\tau/2 \\cdot\\textbf{w}^T\\textbf{w}$, and let $\\textbf{w}^*$ and $\\tilde{\\textbf{w}}$ be the minimizers of $E$ and $E'$ respectively. We want to find a node to express $\\tilde{\\textbf{w}}$ as a function of $\\textbf{w}^*$, i.e. find the displacement introduced by weight decay.\n",
    "\n",
    "      - Find the gradients of $E$ and $E'$. Note that, at the global minimum,\n",
    "     we have $\\nabla E(\\textbf{w}^*)=\\nabla E'(\\hat{\\textbf{w}})=0$.\n",
    "      - In the equality above, express $\\textbf{w}^*$ and $\\tilde{\\textbf{w}}$ as a\n",
    "     linear combination of the eigenvectors of $\\textbf{H}$.\n",
    "      - Through algebraic manipulation, obtain $\\tilde{\\textbf{w}}_i$ as a function\n",
    "     of $\\textbf{w}^*_i$.\n",
    "      - Interpret this result geometrically.\n",
    "      - Note: $\\textbf{H}$ is square, symmetric, and positive definite,\n",
    "     which means that its eigenvectors are pairwise orthogonal and its eigenvalues\n",
    "     are positive (spectral theorem).\n",
    "\n",
    "  2. Consider a linear network of the form $y=\\textbf{w}^T\\textbf{x}$ and the mean\n",
    "squared error as a loss function. Assume that every observation is corrupted\n",
    "with Gaussian noise $\\epsilon\\sim\\mathcal{N}(\\textbf{0}, \\sigma^2\\textbf{I})$.\n",
    "Compute the expectation of the gradient under $\\epsilon$ and,\n",
    "show that adding gaussian noise to the inputs has the same effect of weight decay.\n",
    "\n",
    "\n",
    "### Solution\n",
    "\n",
    "#### Question 1\n",
    "\n",
    "The error is computed as:\n",
    "\n",
    "\\begin{equation}\n",
    "    E(\\textbf{w})=E_0+\\sum_i w_ib_i+\\frac 1 2 \\sum_i\\sum_j w_iw_jh_{ij}\n",
    "\\end{equation}\n",
    "\n",
    "The derivative with respect to $w_i$ is, then:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial E}{\\partial w_i}=b_i+\\sum_j w_jh_{ij}\n",
    "\\end{equation}\n",
    "\n",
    "Where the factor $1/2$ was removed since the pair $w_i$ and $w_j$ is multiplied\n",
    "together twice, and $h_{ij}=h_{ji}$. In vector form:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\nabla_{\\textbf{w}} E(\\textbf{w})=\\textbf{b}+\\textbf{H}\\textbf{w}\n",
    "\\end{equation}\n",
    "\n",
    "The same reasoning applied to $E'$ yields:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\textbf{w}} E'(\\textbf{w})=\\textbf{b}+\\textbf{H}\\textbf{w}+\\tau\\textbf{w}\n",
    "\\end{equation}\n",
    "\n",
    "Now let $\\textbf{u}_i$ and $\\lambda_i$ be the eigenvectors and eigenvalues of\n",
    "$\\textbf{H}$, so that $\\textbf{H}\\textbf{u}_i=\\lambda_i\\textbf{u}_i$.\n",
    " Any vector $\\textbf{v}$ can then be expressed as\n",
    " $\\textbf{v}=\\sum_i\\gamma_i\\textbf{u}_i$. Now, note that\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{H}\\textbf{v}=\\sum_i\\gamma_i\\textbf{H}\\textbf{u}_i=\\sum_i\\gamma_i\\lambda_i\\textbf{u}_i\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Moreover, at the global minimum, both gradients equal zero, hence:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{b}+\\underbrace{\n",
    "  \\sum_i\\alpha_i\\lambda_i\\textbf{u}_i\n",
    "}_{\n",
    "  \\textbf{H}\\textbf{w}^*\n",
    "}\n",
    "=\n",
    "\\textbf{b}+\\underbrace{\n",
    "  \\sum_i\\beta_i\\lambda_i\\textbf{u}_i\n",
    "}_{\n",
    "  \\textbf{H}\\tilde{\\textbf{w}}\n",
    "}+\\tau\\underbrace{\n",
    "  \\sum_i\\beta_i\\textbf{u}_i\n",
    "  }_{\n",
    "    \\tilde{\\textbf{w}}\n",
    "  }\n",
    "\\Longleftrightarrow\n",
    "\\sum_i\\left( \\alpha_i\\lambda_i-\\beta_i\\lambda_i-\\tau\\beta_i \\right)\\textbf{u}_i=\\textbf{0}\n",
    "\\end{equation}\n",
    "\n",
    "Since the eigenvectors are linearly independent, the above expression is zero only\n",
    "when each term inside the sum is zero, i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha_i\\lambda_i-\\beta_i\\lambda_i-\\tau\\beta_i=0\n",
    "\\Longleftrightarrow \\beta_i=\\frac{\\lambda_i}{\\lambda_i+\\tau}\\alpha_i\n",
    "\\end{equation}\n",
    "\n",
    "Now, by replacing this into the expression for $\\hat{\\textbf{w}}$, we get:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{\\textbf{w}}=\\beta^T\\textbf{u}=\\sum_i\\beta_i\\textbf{u}_i=\\sum_i\\frac{\\lambda_i}{\\lambda_i+\\tau}\\alpha_i\\textbf{u}_i\n",
    "\\end{equation}\n",
    "\n",
    "The eigenvalues of $\\textbf{H}$ indicate how much the error changes by moving in\n",
    "the direction of the corresponding eigenvector, with larger changes associated\n",
    "to smaller eigenvalues. In light of this, the node above is saying that the largest\n",
    "changes are applied to the weights that have little influence on the error,\n",
    "while \"important\" weights are not perturbed much.\n",
    "\n",
    "#### Question 2\n",
    "\n",
    "The prediction for $\\tilde{\\textbf{x}}=\\textbf{x}+\\epsilon$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{y}=\\textbf{w}^T\\left(\\textbf{x}+\\epsilon\\right)=\\textbf{w}^T\\textbf{x}+\\textbf{w}^T\\epsilon\n",
    "\\end{equation}\n",
    "\n",
    "The error of this sample is\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{E}=\\frac 1 2 \\left(\\hat{y}-\\textbf{w}^T\\textbf{x}-\\textbf{w}^T\\epsilon\\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "And its gradient with respect to a single weight is\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial\\tilde{E}}{\\partial w_i}\n",
    "&=\\left(\\hat{y}-\\textbf{w}^T\\textbf{x}-\\textbf{w}^T\\epsilon\\right)(-x_i-\\epsilon_i) \\\\\n",
    "&=-x_i\\left(\\hat{y}-\\textbf{w}^T\\textbf{x}-\\textbf{w}^T\\epsilon\\right)-\\epsilon_i\\left(\\hat{y}-\\textbf{w}^T\\textbf{x}-\\textbf{w}^T\\epsilon\\right)\n",
    "\\end{align*}\n",
    "\n",
    "The expectation with respect to $\\epsilon$ is\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}\\left[\\frac{\\partial\\tilde{E}}{\\partial w_i}\\right]\n",
    "&=\\mathbb{E}\\left[\n",
    "-x_i\\left(\\hat{y}-\\textbf{w}^T\\textbf{x}-\\textbf{w}^T\\epsilon\\right)\n",
    "\\right]+\\mathbb{E}\\left[\n",
    "-\\epsilon_i\\left(\\hat{y}-\\textbf{w}^T\\textbf{x}-\\textbf{w}^T\\epsilon\\right)\n",
    "\\right] \\\\\n",
    "&= -x_i\\left(\\hat{y}-\\textbf{w}^T\\textbf{x}\\right)+\\mathbb{E}\\left[\n",
    "-\\epsilon_i\\hat{y}+\\epsilon_i\\textbf{w}^T\\textbf{x}+\\epsilon_i\\textbf{w}^T\\epsilon\n",
    "\\right] \\\\\n",
    "&\\stackrel{*}{=} \\frac{\\partial E}{\\partial w_i}+\\sum_j w_j\\mathbb{E}\\left[\\epsilon_i\\epsilon_j\\right] \\\\\n",
    "&= \\frac{\\partial E}{\\partial w_i}+w_i\\sigma^2    \n",
    "\\end{align*}\n",
    "\n",
    "Where we used $\\partial E/\\partial w_i$ to denote the gradient of the error of the de-noised sample, and the step marked with $*$ follows because $\\mathbb{E}[\\epsilon_i\\epsilon_j]=\\text{Cov}\\left[\\epsilon_i, \\epsilon_j\\right]=\\delta_{ij}\\sigma^2$.\n",
    "\n",
    "Clearly, the gradient is the same that results from weight decay.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
