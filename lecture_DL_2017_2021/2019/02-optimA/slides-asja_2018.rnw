%Blank Lecture
%This file is a child of preamble.Rnw in the style folder
%if you want to add stuff to the preamble go there to make
%your changes available to all childs


<<setup-child, include = FALSE>>=
library(knitr)
set_parent("../style/preamble.Rnw")
@

<<size = "scriptsize", include=FALSE>>=
source("code/functions.R")
@

\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-nn}

% \usepackage{wrapfig}

\lecturechapter{2}{Optimization I}
\lecture{Deeplearning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{Revision : How learning differs from Pure optimization(Remove/Shorten)}
%   \begin{itemize}
%     \item In machine learning we usually act \textbf{indirectly}.
%     \item Technically, we would like to minimize the expected generalization error (or risk):
%       $$\riskt = \E_{(x,y)\sim p_{data}} [\Lxyt]$$
%     with $p_{data}$ being the true underlying distribution.
%     \lz
%       \begin{itemize}
%         \item If we knew $p_{data}$, the minimization of the risk would be an optimization task!
%         \item However, when we only have a set of training samples, we deal with a machine learning problem.
%       \end{itemize}
%   \end{itemize}
% \framebreak
%   \begin{itemize}
%     \item An alternative without directly assuming something about $p_{data}$ is to approximate $\riskt$ based on the training data, by means of the empirical risk:
%       $$\risket = \frac{1}{n} \sumin \Lxyit$$
%     \item So rather than optimizing the risk directly, we optimize the empirical risk, and hope that the risk decreases as well.
%     \item The empirical risk minimization is prone to overfitting as models with high capacity can simply memorize the training set.
%     \item Thus, we have to tweak our optimization such that the quantity that we actually optimize is even more different from the quantity that we truly want to optimize (in reality we obviously optimize $\Oreg$, but to keep things easy we spare that).
%   \end{itemize}
% \end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{Recall: Gradient descent (Chapter 1)}
%   \begin{center}
%     $f(x_1, x_2) = -\sin(x_1) \cdot \frac{1}{2\pi} \exp\left( (x_2 - \pi / 2)^2 \right)$
% <<echo=FALSE, fig.width=8, fig.height=4>>=
% require("colorspace")
% require("ggplot2")
% foo = function(x, y) {
%   -1 * sin(x) * dnorm(y, mean = pi / 2, sd = 0.8)
% }
% 
% x = y = seq(0, pi, length = 50)
% z = outer(x, y, foo)
% p = c(list(list(.1, 3)), optim0(.1, 3, FUN = foo, maximum = FALSE))
% 
% sd_plot(phi = 25, theta = 20, xlab = "x1", ylab = "x2")
% @
%   \end{center}
%   \hspace{2cm} "Walking down the hill, towards the valley."
% \end{vbframe}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{Gradient based learning}
%   \begin{itemize}
%     \item Nonlinearity of neural nets causes loss functions to become non-convex. Thus, convex optimization algorithms do not work anymore.
%     \item We use iterative, gradient-based optimization instead!
%     \begin{itemize}
%       \item But: does not guarantee convergence and results may depend heavily on initial parameters.
%     \end{itemize}
%   \end{itemize}
% \end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{Revision of gradient descent}
%   \begin{itemize}
%     \item First we need to recall the method of gradient descent in numerical optimization.
%     \item Let $\fx$ be an arbitrary, differentiable, unrestricted target function, which we want to minimize.
%       \begin{itemize}
%         \item We can calculate the gradient $\nabla \fx$, which always points in the direction of the steepest ascent.
%         \item Thus $-\nabla \fx$ points in the direction of the steepest descent!
%       \end{itemize}
%   \end{itemize}
% \framebreak
%   \begin{itemize}
%     \item Standing at a point $x_k$ during minimization, we can improve this point by doing the following step:
% $$f(x_{k+1}) = f(x_k) - \nu \nabla f(x_k)$$
%  \enquote{Walking down the hill, towards the valley.}
%     \item $\nu$ determined the length of the step and is called step size.
% To find the optimal $\nu$ we need to look at:
% $$g(\nu) = f(x_k) - \nu \nabla f(x_k) = min!$$
%     \item This minimization problem only has one real parameter, and is therefore \enquote{easy} to solve.
% These kind of methods are known as line search methods.
%   \end{itemize}
% \framebreak
%     \begin{figure}
%       \centering
%         \includegraphics[width=10.2cm]{plots/ascent.png}
%     \end{figure}
% \end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%\frametitle{Lecture outline}
%\tableofcontents
%\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Gradient Descent}

\begin{vbframe}{Gradient descent (GD)}
  \begin{itemize}
    \item Let $f: \R^m \to \R$ be an arbitrary, differentiable function  (of $\mathbf{x} \in \R^m$), 
    which we want to minimize.
      \item The gradient $\mathbf{g} = \nabla f(\mathbf{x}) = (\frac{d f}{d x_1}, \ldots, \frac{d f}{d x_m})^T$ points in the direction of the \textbf{steepest ascent}, and $-\mathbf{g} = -\nabla f(\mathbf{x})$ towards \textbf{steepest descent}.
    \item GD is an iterative, local minimization technique, which improves a point $\mathbf{x}^{[t]}$ by
    $$\mathbf{x}^{[t+1]} = \mathbf{x}^{[t]} - \alpha \nabla f(\mathbf{x}^{[t]})$$ which implies (for sufficiently small $\alpha$),
$$f(\mathbf{x}^{[t+1]}) \leq f(\mathbf{x}^{[t]})$$
    \item $\alpha$ is called \textbf{step size} or \textbf{learning rate} in risk minimization.
  \end{itemize}
\framebreak
\begin{minipage}{0.45\textwidth}
    \begin{itemize}
      \item Gradient descent is a \textit{greedy} algorithm. That is, in every iteration, it makes locally optimal moves.
      \lz
      \item If $f(\mathbf{x})$ is a \textit{convex} function, gradient descent can find the global minimum.
      \lz
    \item On the other hand, if $f(\mathbf{x})$ is a \textit{non-convex} function, gradient descent might only find a local minimum, depending on the initial point.
    \end{itemize}
  \end{minipage}\hfill
  \begin{minipage}{0.5\textwidth}
    \begin{figure}
      \centering
        \scalebox{1}{\includegraphics{plots/gdes2.png}}
    \end{figure}
  \end{minipage}  
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Example: Gradient descent(Convex func.)}
  \begin{center}
    $f(x_1, x_2) = -\sin(x_1) \cdot \frac{1}{2\pi} \exp\left( (x_2 - \pi / 2)^2 \right)$
<<echo=FALSE, fig.width=8, fig.height=4>>=
require("colorspace")
require("ggplot2")
foo = function(x, y) {
  -1 * sin(x) * dnorm(y, mean = pi / 2, sd = 0.8)
}

x = y = seq(0, pi, length = 50)
z = outer(x, y, foo)
p = c(list(list(.1, 3)), optim0(.1, 3, FUN = foo, maximum = FALSE))

sd_plot(phi = 25, theta = 20, xlab = "x1", ylab = "x2")
@
  \end{center}
\hspace{2cm} "Walking down the hill, towards the valley."\\
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{vbframe}{Step-size/Learning Rate}

The step-size $\alpha$ plays a key role in the convergence of the algorithm.
\lz

If the step size is too small, the training process may converge \textit{very} slowly (see left image). If the step size is too large, the process may not converge, because it \textit{jumps} around the optimal point (see right image).

\begin{center}
\includegraphics[width = 0.3\textwidth]{plots/stepsize_small.png}~~
\includegraphics[width = 0.3\textwidth]{plots/stepsize_large.png}
\end{center}

\end{vbframe}

\begin{vbframe}{Step-size/Learning Rate}


So far we have assumed a fixed value of $\alpha$ in every iteration :

\vspace*{-0.2cm}
$$\alpha^{[t]} = \alpha \quad \forall t = {\{1, \ldots, T\}}$$

% \textbf{Konvergenz:} Es sei $f:\R^n \to \R$ konvex, differenzierbar und Liptschitz-stetig, d.h. es gibt ein $L > 0$
%
% $$
% \|\nabla f(\bm{x}) - \nabla f(\bm{y})\| \le L\|\bm{x} - \bm{y}\| \quad \text{für alle} x, y
%

However, it makes sense to adapt $\alpha$ in every iteration:

% We'll look at more sophisticated ways to 

\vspace*{-0.1cm}
\begin{center}
\includegraphics[width = 0.3\textwidth]{plots/stepsize_small.png} ~~~ \includegraphics[width = 0.3\textwidth]{plots/stepsize_adaptive.png} \\
\begin{footnotesize}
Steps of gradient descent for $f(\mathbf{x}) = 10 x_1^2 + 0.5 x_2^2$. Left :  100 steps with a fixed learning rate. Right : 40 steps with an adaptive learning rate.
\end{footnotesize}
\end{center}

\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}{Local Minima/Convexity}
% \end{frame}

\begin{frame}{GD for empirical risk minimization}
  \begin{itemize}
      \item In DL we often aim to minimize the empirical risk %by gradient descent 
      $$\risket = \frac{1}{n} \sumin \Lxyit$$
      where $\theta$ consists of the weights (and biases) of the network. 
%\item  We can set $L_i(\thata):= \Lxyit$ to indicate that it is function of $\theta$, and $\risket=\frac{1}{n} \sumin L_i(\thata)$.
     \item We can now perform GD to minimize $\risket$, i.e.
     \begin{align*}
      \theta^{[t+1]} &=\theta^{[t]} - \alpha  \nabla_\theta \risket \\
      &= \theta^{[t]} - \alpha \frac{1}{n} \sum_{i=1}^{n} \nabla_\theta L(\yi, f(x^{(i)} | \theta^{[t]}))
\end{align*}     
     \end{itemize}
\end{frame}

\section{Stochastic Gradient Descent}

\begin{vbframe}{Stochastic gradient descent (SGD)}
$$  \theta^{[t+1]} =\theta^{[t]} - \alpha \frac{1}{n} \sum_{i=1}^{n} \nabla_\theta L(\yi, f(x^{(i)} | \theta^{[t]}))$$
  \begin{itemize}
 \item  Instead of letting the sum run over the whole data set (\textbf{batch mode}) one can
also let it run only over small subsets (\textbf{minibatches}), or only over a single
example (\textbf{online mode}).
\item  The online gradient $\nabla_\theta L(\yi, f(x^{(i)} | \theta^{[t]}))$  is different for each training example.
\item If the index $i$ of the training example in the online mode is a random variable with uniform distribution, then its expectation is the batch gradient $ \nabla_\theta \risket$.
\item This interprets the online gradient as a \textbf{stochastic}, noisy version of the batch gradient.
\item The online gradient is fast to compute but not reliable. It can be used simply as a computational trick to deal with large data or to operate on real streams of online data in online learning.
\item In contrast, the full
batch error is costly (or even impossible, e.g., when data does not even fit into memory) to compute, particularly in DL, but it averages out all the noise from sub-sampling.
\item Minibatches are in between. The batch size decides upon the compromise
between speed and averaging (smoothing).
  \item In summary: SGD computes an unbiased estimate of the gradient by taking the average gradient over a minibatch (or one sample) to update the parameter $\theta$ in this direction.
%    \item Optimization algorithms that use the entire training set to calculate an update are called \textbf{batch} or \textbf{deterministic}. This is computationally very costly, particularly in DL, or often impossible,
  %    e.g., when data does not even fit into memory anymore.
%    \item Optimization algorithms that use only a single example at a time are called \textbf{stochastic} or \textbf{online}. This can be used simply as a computational trick to deal with large data or to operate on real streams of online data in online learning.
  %  \item Most optimization algorithms applied in DL are somewhere in between! 
  %    Usually more than one, but less than all training samples are used at each iteration. 
   %   Traditionally called \textbf{minibatch}, nowadays \textbf{stochastic} methods.
 
  \end{itemize}
 

  \framebreak
 
 An illustration of the SGD alforithm (to minimize the function $1.25(x_1 + 6)^2 + (x_28)^2)$.
 \begin{figure}
 \includegraphics[width=8cm]{SGD.png}
 \caption{from Shalev-Shwartz and  Ben-David.
Understanding machine learning: From theory to algorithms. Cambridge University Press, 2014.}
 \end{figure}
 
 
  \framebreak
  
  \vspace{5cm}
  
  \begin{algorithm}[H]
  \footnotesize
    \caption{Basic SGD pseudo code}
    \begin{algorithmic}[1]
    \State Initialize parameter vector $\theta^{[0]}$ 
    \State Randomly shuffle data and partition into minibatches of size $m$
    \State $t \leftarrow 0$
    \While{stopping criterion not met}
    \State Take a minibatch $J$ of $m$ examples from training set, $J \subset \nset$
        \State Compute gradient estimate: $\hat{g}^{[t]} \leftarrow \frac{1}{m} \sum_{i \in J} \nabla_\theta L(\yi, f(x^{(i)} | \theta^{[t]}) $
        \State Apply update: $\theta^{[t]} \leftarrow \theta^{[t-1]} - \alpha \hat{g}^{[t]}$
        \State $t \leftarrow t + 1$
      \EndWhile
    \end{algorithmic}
  \end{algorithm}
 % \begin{itemize}
 
 % \end{itemize}
  
\framebreak

  \begin{itemize}
   % \item One \textbf{epoch} means one pass of the full training set.
    \item With minibatches of size $m$ %(typically between 50 and 1000), %one per iteration and gradient update, an 
    a full pass over the training set (called an \textbf{epoch}) consists of $\frac{n}{m}$ gradient updates/\textbf{iterations}.
    \item SGD and its modifications are the most used optimization algorithms for ML in general and for deep learning in particular.
    \item SGD (with one or a few samples per minibatch) updates have a high variance, even though they are unbiased. 
      Because of this variance, the learning rate $\alpha$ is typically much smaller than in the full-batch scenario.
    \item When the learning rate is slowly decreased, SGD converges to a local minimum.
    \item SGD with minibatches reduces the variance of the parameter updates and utilizes highly optimized matrix operations to efficiently compute gradients.
    \item Minibatch sizes are typically between 50 and 1000.
    \item Recent results indicate, that SGD often leads to better generalizing models then GD, and thus may perform some kind of indirect regularization.
    \item Now we know how we can optimize/train a neural network based on the gradient, but how to compute the gradient? 
  \end{itemize}
\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame} {Weight Initialization}
  \begin{itemize}
    \item The weights (and biases) of a neural network must be assigned some initial values before training can begin.
    \vspace{2mm}
    \item It's important to initialize the weights randomly in order to "break symmetry". If two neurons (with the same activation function in a fully connected network) are connected to the same inputs and have the same initial weights, then both neurons will have the same gradient update in a given iteration and they'll end up learning the same features.
    \vspace{2mm}
    \item Weights are typically drawn from a uniform distribution or a Gaussian (both centered at 0 with a small variance).
    \vspace{2mm}
    \item Two common initialization strategies are 'Glorot initialization' and 'He initialization' which tune the variance of these distributions based on the topology of the network.
  \end{itemize}
\end{frame}
% \begin{frame} {Weight Initialization}
% \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\section{Chain rule and Computational graphs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Chain rule of calculus}
  % \begin{itemize}
  %   \item The chain rule can be used to compute derivatives of the composition of two or more functions.
  %   \item Let $x \in \R^m$, $y \in \R^n$, \\
  %         $g: \R^m \to \R^n$ and $f: \R^n \to \R$. \\
  %   \item If $y = g(x)$ and $z = f(y)$, the chain rule yields $$\frac{d z}{d x_i} = \sum_j \frac{d z}{d y_j} \frac{d y_j}{d x_i}$$
  %         or in vector notation $$\nabla_x z = \Big(\frac{d y}{d x}\Big)^T \nabla_y z,$$
  %         where $\frac{d y}{d x}$ is the $n \times m$ jacobian matrix of $g$.
  % \end{itemize}
  \begin{itemize}
    \item The chain rule can be used to compute derivatives of the composition of two or more functions.
    \item Let $\mathbf{x} \in \R^m$, $\mathbf{y} \in \R^n$, \\
          $g: \R^m \to \R^n$ and $f: \R^n \to \R$. \\
    \item If $\mathbf{y} = g(\mathbf{x})$ and $z = f(\mathbf{y})=f(g(\mathbf{x}))$, the chain rule yields : $$\frac{d z}{d x_i} = \sum_j \frac{d z}{d y_j} \frac{d y_j}{d x_i}$$
          or, in vector notation : $$\nabla_{\mathbf{x}} z = \Big(\frac{d \mathbf{y}}{d \mathbf{x}}\Big)^T \nabla_{\mathbf{y}} z,$$
          where, $\frac{d \mathbf{y}}{d \mathbf{x}}$ is the ($n \times m$) Jacobian matrix of $g$.
  \end{itemize}
\end{vbframe}  

\begin{vbframe}{Computational graphs}
  \begin{minipage}{0.45\textwidth}
    \begin{itemize}
      \item Computational graphs are a very helpful language to understand and visualize the chain rule.
      \item Each node describes a variable.
      \item Operations are functions applied to one or more variables.
    \end{itemize}
  \end{minipage}\hfill
  \begin{minipage}{0.5\textwidth}
    \begin{figure}
      \centering
        \includegraphics[width=6cm]{plots/compgraph1.png}
        \caption{The computational graph for the expression $H = \sigma(XW + b)$.}
    \end{figure}
  \end{minipage}  
\end{vbframe}

\begin{vbframe}{Chain rule of calculus : Example 1}
  \begin{minipage}{0.45\textwidth}
    \begin{itemize}
      \item Suppose we have the following computational graph.
      \item To compute the derivative of $\frac{d z}{d w}$ %$\frac{d z}{d w}$ 
      we need to recursively apply the chain rule. That is:
    \end{itemize}
      \begin{eqnarray*}
        \frac{d z}{d w} &=& \frac{d z}{d y} \frac{d y}{d x} \frac{d x}{d w} \\
                                  &=& f'_3(y) f'_2(x) f'_1(w) \\
                                  &=& f'_3(f_2(f_1(w)))f'_2(f_1(w))f'_1(w)
      \end{eqnarray*}
  \end{minipage}\hfill
  \begin{minipage}{0.40\textwidth}
    \begin{figure}
      \centering
        \includegraphics[width=1cm]{plots/compgraph2.png}
        \caption{A computational graph, such that $x = f_1(w),$ $y = f_2(x)$ and $z = f_3(y)$.}
    \end{figure}
  \end{minipage}
% \framebreak
%   \begin{figure}
%     \centering
%       \includegraphics[width=4.5cm]{plots/compgraph3.png}
%       \caption{Applying the chain rule to the example yields us a computational graph with a symbolic description of the
% derivatives.}
%   \end{figure}  
\end{vbframe}

\begin{frame}{Chain rule of calculus : Example 2}

   \begin{figure}
    \centering
      \scalebox{0.27}{\includegraphics{plots/chain_tree.png}}
  \end{figure}
  
To compute $\nabla_\mathbf{x} z$, we apply the chain rule to computational graph above,
  \begin{itemize}
    \item $\frac {d z}{d x_1} = \sum_j \frac{d z}{d y_j} \frac{d y_j}{d x_1} = \frac {d z}{d y_1} \frac {d y_1}{d x_1} + \frac {d z}{d y_2} \frac {d y_2}{d x_1}$
    \item $\frac {d z}{d x_2} = \sum_j \frac{d z}{d y_j} \frac{d y_j}{d x_2} = \frac {d z}{d y_1} \frac {d y_1}{d x_2} + \frac {d z}{d y_2} \frac {d y_2}{d x_2}$
  \end{itemize}
  \vspace{2mm}
    Therefore, the gradient of $z$ w.r.t $\mathbf{x}$ is
    \begin{itemize}
      \item  $\nabla_\mathbf{x} z = \begin{bmatrix}
               \frac {d z}{d x_1} \\
               \frac {d z}{d x_2} \\
             \end{bmatrix} = \underbrace{\begin{bmatrix} \frac{d y_1}{d x_1}&\frac {d y_2}{d x_1}\\
                                             \frac {d y_1}{d x_2}&\frac {d y_2}{d x_2}\\
             \end{bmatrix}}_{\textcolor{red}{(\frac{d \mathbf{y}}{d \mathbf{x}})^T}} \underbrace{\begin{bmatrix} \frac {d z}{d y_1} \\
                                            \frac {d z}{d y_2} \\ \end{bmatrix}}_{\textcolor{red}{\nabla_{\mathbf{y}} z}} = \Big(\frac{d \mathbf{y}}{d \mathbf{x}}\Big)^T \nabla_{\mathbf{y}} z $
  \end{itemize}
\end{frame}

\begin{frame} {Computational Graph : Neural Net}
  \begin{figure}
      \centering
        \scalebox{0.75}{\includegraphics{plots/neo_comp.png}}
        \caption{A neural network can be seen as a computational graph. $\phi$ is the weighted sum and $\sigma$ and $\tau$ are the activations. %Note : In contrast to the top figure, the arrows in the computational graph below merely indicate \textit{dependence}, not weights.
        }
    \end{figure}
\end{frame}
% \begin{frame}{Chain rule of calculus}
% \begin{figure}
%     \centering
%       \scalebox{0.4}{\includegraphics{plots/chain_tree.png}}
%   \end{figure}
%   
%     $\nabla_x z = \begin{bmatrix}
%            \frac {\partial z}{\partial x_1} \\
%            \frac {\partial z}{\partial x_2} \\
%          \end{bmatrix} = \begin{bmatrix}
%            \frac {\partial z}{\partial x_1} \\
%            \frac {\partial z}{\partial x_2} \\
%          \end{bmatrix}$
% \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame} {Simple Example?}
%   \begin{figure}
%     \centering
%       \scalebox{1}{\includegraphics{plots/simex.png}}
%   \end{figure}
% \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Backpropagation}
\begin{frame}{Training Neural Networks}
%   \begin{itemize}
%     \item Backpropagation is the method used to compute the gradient of a neural network. That is, recursively applying the chain rule to obtain the error of each node. 
%       \begin{itemize}
%         \item Keep in mind that another algorithm uses this gradient to perform the actual learning (for example stochastic gradient descent).
%       \end{itemize}
%   \end{itemize}
% \framebreak
  % \begin{itemize}
     Recall, that we want to apply SGD to minimize
      $$\risket = \frac{1}{n} \sumin \Lxyit$$
      where $\theta$ consists of the weights (and biases) of the network. 
     % Later, we will see that we do something more complex, but that is the basic idea.
     
    (Online) training of NNs happens in 3 consecutive steps: \\
    For observation $x^{(i)}$
      \begin{enumerate}
        \item Forward pass: the input values are forward propagated through the model to calculate the prediction $f(x^{(i)}|\theta)$. 
        Based on that, we compute the value of the loss function $\Lxyit$. %We covered that.
        \item Backward pass/Backpropagation: the information of the error that happened in the calculation of the network prediction for $x$ is propagated backwards through the model by calculating the gradient $\nabla_{\theta}\Lxyit$ of the loss. 
        % Thereby we use the error values to calculate the gradient of the loss with respect to each weight. \\
        % In a final step we update the weights (i.e. \enquote{move} them in the direction of the steepest descent of the loss).
        \item Then the weights are updated by taking a little step in the direction of the negative gradient.
      \end{enumerate}
      %This is simply gradient descent in disguise (for one observation).
  % \end{itemize}
   
  
      % \item This is then used by an enveloping optimization algorithm to adjust the weight of each neuron (e.g. SGD, Adam. More on this later.).
    % \item To apply backpropagation, we first choose a loss function $\Lxy$ , which we would like to minimize.
    % \vspace{8mm}
    % \item \footnotesize{(Note : Generally, x and y are vectors. This should hopefully be clear from the context even though we suppress the vector notation.)}
  % \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Weight update rule}
  \begin{itemize}
    \item Backpropagation is an algorithm  to compute the gradient of $\Lxy$ in an \textit{extremely} efficient way.
    \item The weight update per iteration, for one $x$, with learning rate $\alpha$, is 
      $$w^{[t+1]} = w^{[t]} - \alpha \cdot \nabla_{w}\Lxy$$ 
        \item We'll see that, at its core, backpropagation is just a clever implementation of the chain rule. Nothing more!
      % \item We can think of $\alpha$ as how fast the network will abandon its old beliefs. Thus, as we've seen earlier, we would like to apply a learning rate which is low enough to converge to something useful, but also high enough that we do not have to spend years of training.
    % \item We will inspect the details of the weight upate in the optimization chapter.
  \item We could now sum up these gradients for all $\xi$ from $\Dset$ to compute the gradient over the complete training set, to perform a full GD step. But as we want to arrive at stochastic gradient descent, 
    we stick for now with update for single $x$.
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Backpropagation example}
  \begin{itemize}
    \item Let us recall the XOR example, but this time with randomly initialized weights.
    \begin{figure}
    \centering
      \scalebox{0.7}{\includegraphics{plots/forwardprop1.png}}
     % \caption{A neural network with two neurons in the hidden layer. $W$ and $\biasb$ are the weights and biases of the hidden layer. $\wtu$ and $c$ are the weights and bias of the output layer. Based on the values for the weights (and biases), we will perform one forward and one backward pass.}
  \end{figure}
    \item For activations in both the hidden and output layer, we apply the \textbf{sigmoidal logistic function}.
    \item To perform one forward and one backward pass we feed our neural network with example $X = [1,0]^T$ (positive sample).
    \end{itemize}
    \framebreak
    \begin{itemize}
    \item We will divide the forward pass into four steps:
      \begin{itemize}
        \item the inputs of $z_m$: $z_{i,in}$
        \item the activations of $z_m$: $z_{i,out}$
        \item the input of $f$: $f_{in}$
        \item and finally the activation of $f$: $f_{out}$
      \end{itemize}
    \item We optimize the model using the squared error between the binary 0-1 labels and the
      predicted probabilities rather than the cross-entropy - 
      a bit unusual but computations become simpler for this instructive example.
    \item Then we compute the backward pass and apply backpropagation to update two weights.
    \item Finally we evaluate the model with our updated weights.
    \item NB: We only show rounded decimals. 
  \end{itemize}
%\framebreak
%  \begin{figure}
%    \centering
%      \scalebox{0.8}{\includegraphics{plots/forwardprop1.png}}
%      \caption{A neural network with two neurons in the hidden layer. $W$ and $\biasb$ are the weights and biases of the hidden layer. $\wtu$ and $c$ are the weights and bias of the output layer. Based on the values for the weights (and biases), we will perform one forward and one backward pass.}
%  \end{figure}
\end{vbframe}
\begin{vbframe}{Backprop Example: Forward Pass}
  \begin{figure}
    \centering
      \includegraphics[width=6cm]{plots/forwardprop2b.png}
  \end{figure}
  \begin{itemize}
    \item $z_{1,in} = 1 \cdot (-0.07) + 0 \cdot 0.22 + 1 \cdot (-0.46) = -0.53$
    \item $z_{1,out} = \frac{1}{(1+exp(-(-0.53)))} = \num[round-mode=places,round-precision=4]{0.3705169}$
  \end{itemize}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% OLD STYLE: first slide plot of whole network, second slide computations %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   \begin{figure}
%     \centering
%       \includegraphics[width=10cm]{plots/forwardprop2.png}
%       \caption{A neural network with two neurons in the hidden layer and two bias terms c and b. Based on the values for the weights, we will compute the value of $z_1$.}
%   \end{figure}
% \framebreak
%   \begin{itemize}
%     \item The input of $z_{1,in}$ is given by: $$1 \cdot (-0.07) + 0 \cdot 0.22 + 1 \cdot (-0.46) = -0.53$$
%     \item Following up we apply the activation function to obtain $z_{1,out}$: $$\frac{1}{(1+exp(-(-0.53)))} = \num[round-mode=places,round-precision=4]{0.3705169}$$
%   \end{itemize}
% \framebreak
  \begin{figure}
    \centering
      \includegraphics[width=6cm]{plots/forwardprop3b.png}
  \end{figure}
  \begin{itemize}
    \item $z_{2,in} = 1 \cdot 0.94 + 0 \cdot 0.46 + 1 \cdot 0.1 = 1.04$
    \item $z_{2,out} = \frac{1}{(1+exp(-1.04))} = \num[round-mode=places,round-precision=4]{0.73885}$
  \end{itemize}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% OLD STYLE: first slide plot of whole network, second slide computations %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   \begin{figure}
%     \centering
%       \includegraphics[width=10cm]{plots/forwardprop3.png}
%       \caption{A neural network with two neurons in the hidden layer and two bias terms c and b. Based on the values for the weights, we will compute the value of $z_2$.}
%   \end{figure}
% \framebreak
%   \begin{itemize}
%     \item The input of $z_{2,in}$ is given by: $$1 \cdot 0.94 + 0 \cdot 0.46 + 1 \cdot 0.1 = 1.04$$
%     \item Following up we apply the activation function to obtain $z_{2,out}$: $$\frac{1}{(1+exp(-1.04))} = \num[round-mode=places,round-precision=4]{0.73885}$$
%   \end{itemize}
% \framebreak
  \begin{figure}
    \centering
      \includegraphics[width=5cm]{plots/forwardprop4b.png}
  \end{figure}
  \begin{itemize}
    \item $f_{in} = \num[round-mode=places,round-precision=4]{0.3705169} \cdot (-0.22) + \num[round-mode=places,round-precision=4]{0.73885} \cdot 0.58 + 1 \cdot 0.78 = \num[round-mode=places,round-precision=4]{1.112242}$
    \item $f_{out} = \frac{1}{(1+exp(\num[round-mode=places,round-precision=4]{-1.112242}))} = \num[round-mode=places,round-precision=4]{0.7525469}$
  \end{itemize}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% OLD STYLE: first slide plot of whole network %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   \begin{figure}
%     \centering
%       \includegraphics[width=10cm]{plots/forwardprop4.png}
%       \caption{A neural network with two neurons in the hidden layer and two bias terms c and b. Based on the values for the weights, we will compute the value of $\hat{y}$.}
%   \end{figure}
% \framebreak
  \begin{itemize}
    \item The forward pass of our neural network predicted a value of $$f_{out} = 0.7525$$
    \item Now we evaluate the result with L2 loss:
  \end{itemize}
      \begin{eqnarray*}
        \Lxy &=& \frac{1}{2}(y - f(x| \theta))^2 = \frac{1}{2}(y - f_{out})^{2} \\
                  &=& \frac{1}{2}(1 - \num[round-mode=places,round-precision=4]{0.7525469})^2 = \num[round-mode=places,round-precision=4]{0.03061652}
      \end{eqnarray*}
%\framebreak
\end{vbframe}
\begin{vbframe}{Backprob Example: Backward pass}
  \begin{itemize}
    \item Assume we would like to know how much and in which direction a change in $u_1$ affects the total error. To this we have to recursively apply the chain rule and compute: $$\frac{d \Lxy}{d u_1} = \frac{d \Lxy}{d f_{out}} \cdot \frac{d f_{out}}{d f_{in}} \cdot \frac{d f_{in}}{d u_1}$$
  \end{itemize}
  \begin{figure}
    \centering
      \includegraphics[width=6cm]{plots/backprop1.png}
      \caption{Snippet from our neural network showing the backward path to compute the gradient with respect to weight $u_1$.}
  \end{figure}
\framebreak
  \begin{itemize}
    \item 1st step (backwards)
  \end{itemize}
    \begin{eqnarray*}
      \frac{d \Lxy}{d f_{out}} &=& \frac{d}{d f_{out}} \frac{1}{2}(y - f_{out})^2 = -(y - f_{out}) \\
                                                    &=& -(1 - \num[round-mode=places,round-precision=4]{0.7525469}) = \num[round-mode=places,round-precision=4]{-0.2474531}
    \end{eqnarray*}
    \begin{figure}
      \centering
        \includegraphics[width=6cm]{plots/backprop1_b.png}
        \caption{The first term of our chain rule $\frac{d \Lxy}{d f_{out}}$}
    \end{figure}
\framebreak
  \begin{itemize}
    \item 2nd step (backwards). $f_{out} = \sigma(f_{in})$ and we apply the rule for $\sigma'$
  \end{itemize}
    \begin{eqnarray*}
      \frac{d f_{out}}{d f_{in}}  &=& \sigma(f_{in})(1-\sigma(f_{in})) \\
                                                        &=& \num[round-mode=places,round-precision=4]{0.7525469} (1 - \num[round-mode=places,round-precision=4]{0.7525469}) = \num[round-mode=places,round-precision=4]{0.1862201}
    \end{eqnarray*}
    \begin{figure}
      \centering
        \includegraphics[width=6cm]{plots/backprop1_c.png}
        \caption{The second term of our chain rule $\frac{d f_{out}}{d f_{in}}$}
    \end{figure}
\framebreak
  \begin{itemize}
    \item 3rd step (backwards). $f_{in} = u_1 \cdot z_{1,out} + u_2 \cdot z_{2,out} + c \cdot 1$
  \end{itemize}
    \begin{eqnarray*}
      \frac{d f_{in}}{d u_1} = z_{1,out} = \num[round-mode=places,round-precision=4]{0.3705169}
    \end{eqnarray*}
    \begin{figure}
      \centering
        \includegraphics[width=6cm]{plots/backprop1_d.png}
        \caption{The third term of our chain rule $\frac{d f_{in}}{d u_1}$}
    \end{figure}
\framebreak
  \begin{itemize}
    \item Finally we are able to plug all three parts together and obtain:
  \end{itemize}
  \begin{eqnarray*}
    \frac{d \Lxy}{d u_1} &=& \frac{d \Lxy}{d f_{out}} \cdot \frac{d f_{out}}{d f_{in}} \cdot \frac{d f_{in}}{d u_1} \\
                                        &=& \num[round-mode=places,round-precision=4]{-0.2474531} \cdot \num[round-mode=places,round-precision=4]{0.1862201} \cdot \num[round-mode=places,round-precision=4]{0.3705169} \\
                                        &=& \num[round-mode=places,round-precision=4]{-0.01707369}
  \end{eqnarray*}
    \begin{figure}
      \centering
        \includegraphics[width=6cm]{plots/backprop1_e.png}
        \caption{All three terms of our chain rule $\frac{d \Lxy}{d u_1} = \frac{d \Lxy}{d f_{out}} \cdot \frac{d f_{out}}{d f_{in}} \cdot \frac{d f_{in}}{d u_1}$}
    \end{figure}
\framebreak
  \begin{itemize}
    \item Consider a learning rate of $\alpha = 0.5$. Then we obtain:
  \end{itemize}
    \begin{eqnarray*}
      u_1^{[new]} &=& u_1^{[old]} - \alpha \cdot \frac{d \Lxy}{d u_1} \\
                &=& -0.22 - 0.5 \cdot (\num[round-mode=places,round-precision=4]{-0.01707369}) \\
                &=& \num[round-mode=places,round-precision=4]{-0.2114632}
    \end{eqnarray*}
\framebreak
  \begin{itemize}
    \item Now assume we would also like to do the same for weight $W_{11}$. This time we have to compute: $$\frac{d \Lxy}{d W_{11}} = \frac{d \Lxy}{d f_{out}} \cdot \frac{d f_{out}}{d f_{in}} \cdot \frac{d f_{in}}{d z_{1,out}} \cdot \frac{d z_{1,out}}{d z_{1,in}} \cdot \frac{d z_{1,in}}{d W_{11}}$$
  \end{itemize}
  \begin{figure}
    \centering
      \includegraphics[width=9cm]{plots/backprop2.png}
      \caption{Snippet from our neural network showing the backward path to compute the gradient with respect to weight $W_{11}$.}
  \end{figure}
\framebreak
  \begin{itemize}
    \item We already know $\frac{d \Lxy}{d f_{out}}$ and $\frac{d f_{out}}{d f_{in}}$ from the computations before.
  \begin{figure}
    \centering
      \includegraphics[width=10cm]{plots/backprop2_bc.png}
      \caption{The first and second term of our chain rule $\frac{d \Lxy}{d f_{out}}$ and $\frac{d f_{out}}{d f_{in}}$}
  \end{figure}
\framebreak
    \item With $f_{in} = u_1 \cdot z_{1,out} + u_2 \cdot z_{2,out} + c \cdot 1$ we can compute:
  \end{itemize}
  \begin{eqnarray*}
    \frac{d f_{in}}{d z_{1,out}} = u_1 = -0.22
  \end{eqnarray*}
  \begin{figure}
    \centering
      \includegraphics[width=10cm]{plots/backprop2_d.png}
      \caption{The third term of our chain rule $\frac{d f_{in}}{d z_{1,out}}$}
  \end{figure}
\framebreak
  \begin{itemize}
    \item Next, we need
  \end{itemize}
  \begin{eqnarray*}
    \frac{d z_{1,out}}{d z_{1,in}}  &=& \sigma(z_{1,in})(1-\sigma(z_{1,in})) \\
                                              &=&  \num[round-mode=places,round-precision=4]{0.3705169} (1 - \num[round-mode=places,round-precision=4]{0.3705169}) = \num[round-mode=places,round-precision=4]{0.2332341}
  \end{eqnarray*}
  \begin{figure}
    \centering
      \includegraphics[width=9cm]{plots/backprop2_e.png}
      \caption{The fourth term of our chain rule $\frac{d z_{1,out}}{d z_{1,in}}$}
  \end{figure}
\framebreak
  \begin{itemize}
    \item With $z_{1,in} = x_1 \cdot W_{11} + x_2 \cdot W_{21} + b_1 \cdot 1$ we can compute the last component:
  \end{itemize}
  \begin{eqnarray*}
    \frac{d z_{1,in}}{d W_{11}} = x_1 = 1
  \end{eqnarray*}
  \begin{figure}
    \centering
      \includegraphics[width=9cm]{plots/backprop2_f.png}
      \caption{The fifth term of our chain rule $\frac{d z_{1,in}}{d W_{11}}$}
  \end{figure}
\framebreak
  \begin{itemize}
    \item Plugging all five components together yields us: 
      \begin{eqnarray*}
        \frac{d \Lxy}{d W_{11}} &=& 
        \frac{d \Lxy}{d f_{out}} \cdot \frac{d f_{out}}{d f_{in}} \cdot \frac{d f_{in}}{d z_{1,out}} \cdot \frac{d z_{1,out}}{d z_{1,in}} \cdot \frac{d z_{1,in}}{d W_{11}} 
        \\ &=& (\num[round-mode=places,round-precision=4]{-0.2474531}) \cdot \num[round-mode=places,round-precision=4]{0.1862201} \cdot (-0.22) \cdot \num[round-mode=places,round-precision=4]{0.2332341} \cdot 1 
        \\ &=& \num[round-mode=places,round-precision=4]{0.0023645}
      \end{eqnarray*}
  \begin{figure}
    \centering
      \includegraphics[width=9cm]{plots/backprop2_g.png}
      \caption{All five terms of our chain rule}
  \end{figure}
\framebreak
    \item Consider the same learning rate of $\alpha = 0.5$. Then we obtain:
  \end{itemize}
    \begin{eqnarray*}
      W_{11}^{[new]}  &=& W_{11}^{[old]} - \alpha \cdot \frac{d \Lxy}{d W_{11}} \\
                  &=& -0.07 - 0.5 \cdot \num[round-mode=places,round-precision=4]{0.0023645} = \num[round-mode=places,round-precision=4]{-0.0711823}
    \end{eqnarray*}
  \begin{itemize}
    \item We would now like to check how the performance has improved. Our updated weights are:
  \end{itemize}
  \begin{eqnarray*}
    W = \begin{pmatrix}
    \num[round-mode=places,round-precision=4]{-0.0711823} & \num[round-mode=places,round-precision=4]{0.9425785} \\
    0.22 & 0.46
    \end{pmatrix},
    b = \begin{pmatrix}
    \num[round-mode=places,round-precision=4]{-0.454562} \\
    \num[round-mode=places,round-precision=4]{0.100258}
    \end{pmatrix},
  \end{eqnarray*}
  \begin{eqnarray*}
    u = \begin{pmatrix}
    \num[round-mode=places,round-precision=4]{-0.2114632} \\
    \num[round-mode=places,round-precision=4]{0.5970234}
    \end{pmatrix}
    \text{and} \ c = \num[round-mode=places,round-precision=4]{0.79797}
  \end{eqnarray*}
\framebreak  
  \begin{itemize}
    \item Plugging all values into our model yields $$f(x| W, b, u, c) = \num[round-mode=places,round-precision=4]{0.7614865}$$ and a squared error of $$\Lxy = \frac{1}{2}(1 - \num[round-mode=places,round-precision=4]{0.7614865})^2 = \num[round-mode=places,round-precision=4]{0.02844434}.$$
    \item The initial weights predicted $f = \num[round-mode=places,round-precision=4]{0.7525469}$ and a slightly higher error value of $\Lxy = \num[round-mode=places,round-precision=4]{0.03061652}$.
    \lz
    \item Keep in mind that this is the result of only one training iteration. When applying a neural network, one usually conducts thousands of those.
  \end{itemize}
\end{vbframe}


\begin{frame} {Backpropagation}
  \begin{itemize}
    \item Recall, in the previous example, we computed :
    $$\frac{d \Lxy}{d W_{11}} = 
        \frac{d z_{1,in}}{d W_{11}} \cdot \frac{d z_{1,out}}{d z_{1,in}} \cdot \frac{d f_{in}}{d z_{1,out}} \cdot \frac{d f_{out}}{d f_{in}} \cdot \frac{d \Lxy}{d f_{out}}$$
  \end{itemize}
  \begin{figure}
    \centering
      \includegraphics[width=9cm]{plots/backprop_gg0.png}
      \caption{All five terms of our chain rule}
  \end{figure}
\end{frame}
\begin{frame} {Backpropagation}
  \begin{itemize}
    % \item Recall, in the XOR example, we computed
    % $$\frac{d \Lxy}{d W_{11}} = 
    %     \Bigg( \frac{d z_{1,in}}{d W_{11}} \Bigg( \frac{d z_{1,out}}{d z_{1,in}} \Bigg( \frac{d f_{in}}{d z_{1,out}} \Bigg( \frac{d f_{out}}{d f_{in}} \cdot \frac{d \Lxy}{d f_{out}}\Bigg)\Bigg)\Bigg)\Bigg)$$
      \item Next, let's compute :
      $$\frac{d \Lxy}{d W_{21}} = 
        \Bigg( \frac{d z_{1,in}}{d W_{21}} \Bigg( \frac{d z_{1,out}}{d z_{1,in}} \Bigg( \frac{d f_{in}}{d z_{1,out}} \Bigg( \frac{d f_{out}}{d f_{in}} \cdot \frac{d \Lxy}{d f_{out}}\Bigg)\Bigg)\Bigg)\Bigg)$$
        \begin{figure}
    \centering
      \includegraphics[width=9cm]{plots/backprop_gg.png}
      \caption{All five terms of our chain rule}
  \end{figure}
  \end{itemize}
\end{frame}
\begin{frame} {Backpropagation}
  \begin{itemize}
    \item Examining the two expressions :
    $$\frac{d \Lxy}{d W_{11}} = 
        \Bigg( \frac{d z_{1,in}}{d W_{11}} \textcolor{red}{\Bigg( \frac{d z_{1,out}}{d z_{1,in}} \Bigg( \frac{d f_{in}}{d z_{1,out}} \Bigg( \frac{d f_{out}}{d f_{in}} \cdot \frac{d \Lxy}{d f_{out}}\Bigg)\Bigg)\Bigg)}\Bigg)$$
      $$\frac{d \Lxy}{d W_{21}} = 
        \Bigg( \frac{d z_{1,in}}{d W_{21}} \textcolor{red}{\Bigg( \frac{d z_{1,out}}{d z_{1,in}} \Bigg( \frac{d f_{in}}{d z_{1,out}} \Bigg( \frac{d f_{out}}{d f_{in}} \cdot \frac{d \Lxy}{d f_{out}}\Bigg)\Bigg)\Bigg)}\Bigg)$$
    \item We see that there is massive overlap / redundancy in the two expressions. A huge chunk of the second expression has already been computed while computing the first one.
    \item \textbf{Main idea} : Simply cache these subexpressions instead of recomputing them each time.
    
  \end{itemize}
\end{frame}
\begin{frame} {Backpropagation}
  \begin{itemize}
    \item Let
      $$\delta = 
        \textcolor{red}{\Bigg( \frac{d z_{1,out}}{d z_{1,in}} \Bigg( \frac{d f_{in}}{d z_{1,out}} \Bigg( \frac{d f_{out}}{d f_{in}} \cdot \frac{d \Lxy}{d f_{out}}\Bigg)\Bigg)\Bigg)}$$
        be cached.
    \item The two expressions now become,
    $$\frac{d \Lxy}{d W_{11}} = 
         \frac{d z_{1,in}}{d W_{11}} \cdot \delta$$ 
      $$\frac{d \Lxy}{d W_{21}} = 
        \frac{d z_{1,in}}{d W_{21}} \cdot \delta$$
        where $\delta$ is simply "plugged in".
    \item %As you can imagine
    Caching subexpressions in this way and plugging in where needed can result in \emph{massive} gains in efficiency. %for deep and "wide" neural networks. 
    \item In fact, this simple algorithm, which was first applied to neural networks way back in 1985, may \textit{still} be considered the biggest breakthrough in deep learning.
  \end{itemize}
\end{frame}
\begin{frame} {Backpropagation}
  \begin{itemize}
    \item On the other hand, if we had done a \textit{forward} computation of the derivatives:
    $$\frac{d \Lxy}{d W_{11}} = \Bigg(\Bigg(\Bigg(\Bigg( \frac{d z_{1,in}}{d W_{11}} \frac{d z_{1,out}}{d z_{1,in}} \Bigg)  \frac{d f_{in}}{d z_{1,out}} \Bigg) \frac{d f_{out}}{d f_{in}} \Bigg) \frac{d \Lxy}{d f_{out}}\Bigg)$$
      $$\frac{d \Lxy}{d W_{21}} = 
        \Bigg(\Bigg(\Bigg(\Bigg( \frac{d z_{1,in}}{d W_{21}} \frac{d z_{1,out}}{d z_{1,in}} \Bigg)  \frac{d f_{in}}{d z_{1,out}} \Bigg) \frac{d f_{out}}{d f_{in}} \Bigg) \frac{d \Lxy}{d f_{out}}\Bigg)$$
      there would \textit{be} no common subexpressions to cache. We would have to compute the \text{entire} expression for each and every weight!
    \item This would make the computations too inefficient to make gradient descent tractable for large neural networks.
  \end{itemize}
\end{frame}

\begin{frame} {Backpropagation}
  \begin{itemize}
    \item \small{To arrive at a general formulation of backprop, let's adopt some new notation.}
      \begin{itemize}
        \item \small{The neurons in layers $i-1$, $i$ and $i+1$ are indexed by $j$, $k$ and $m$, respectively.
      %  \item For neuron $k$ in layer $i$, $z_{k,out}^{(i)} = z_k^{(i)}$ and $z_{k,in}^{(i)} = \tilde{z_k}^{(i)}$.
        \item The output layer will be referred to as layer O.}
      \end{itemize}
   \begin{figure}
    \centering
      \scalebox{0.87}{\includegraphics{plots/backnet.png}}
      \tiny{\\Credit:Erik Hallstrom}
    \end{figure}
  \end{itemize}
\end{frame}

\begin{frame} {Backpropagation}
  \begin{itemize}
    \item The derivative of the loss $L$ w.r.t weight  $W_{j,k}^{(i)}$ between neuron $j$ in layer $i-1$ and neuron $k$ in layer $i$ is,
    \begin{eqnarray*}
    \frac{d L}{d W_{j,k}^{(i)}} &=& \frac{d L}{d {z}_{k,in}^{(i)}} \frac{d  {z}_{k,in}^{(i)}}{d W_{j,k}^{(i)}} = \frac{d L}{d z_{k,out}^{(i)}} \frac{d z_{k,out}^{(i)}}{d {z}_{k,in}^{(i)}} \frac{d {z}_{k,in}^{(i)}}{d W_{j,k}^{(i)}} \\
    &=& \sum_m \Bigg( \frac{d L}{d {z}_{m, in}^{(i+1)}} \frac{d {z}_{m, in}^{(i+1)}}{d z_{k,out}^{(i)}} \Bigg) \frac{d z_{k,out}^{(i)}}{d {z}_{k, in}^{(i)}} \frac{d {z}_{k,in}^{(i)}}{d W_{j,k}^{(i)}}
    \end{eqnarray*}
    \item Note : The sum in the expression above is over all the neurons in layer $i+1$. This is simply an application of the chain rule.
  \end{itemize}
\end{frame}
\begin{frame} {Backpropagation}
  \begin{itemize}
    \item We have, 
      \begin{eqnarray*}
       \frac{d L}{d W_{j,k}^{(i)}} &=& %\sum_m \Bigg( \frac{d L}{d \tilde{z}_m^{(i+1)}} \frac{d \tilde{z}_m^{(i+1)}}{d z_k^{(i)}} \Bigg) \frac{d z_k^{(i)}}{d \tilde{z}_k^{(i)}} \frac{d \tilde{z}_k^{(i)}}{d W_{j,k}^{(i)}}
       \sum_m \Bigg( \frac{d L}{d {z}_{m, in}^{(i+1)}} \frac{d {z}_{m, in}^{(i+1)}}{d z_{k,out}^{(i)}} \Bigg) \frac{d z_{k,out}^{(i)}}{d {z}_{k, in}^{(i)}} \frac{d {z}_{k,in}^{(i)}}{d W_{j,k}^{(i)}}
      \end{eqnarray*}
      \item Here,
        \begin{itemize}
          \item $z_{k, out}^{(i)} = \sigma({z}_{k,in}^{(i)})$ 
          \item ${z}_{k,in}^{(i)} = \sum_j W_{j,k}^{(i)}z_{j,out}^{(i-1)}  + b_k^{(i)}$
          \item ${z}_{m, in}^{(i+1)} = \sum_k W_{k,m}^{(i+1)}z_{k,out}^{(i)} + b_m^{(i+1)}$
        \end{itemize}
      \item Therefore,
      $$\frac{d L}{d W_{j,k}^{(i)}} = \underbrace{\sum_m \Bigg( \frac{d L}{d {z}_{m,in}^{(i+1)}} W_{k,m}^{(i+1)} \Bigg) \sigma'({z}_{k,in}^{(i)})}_{= \frac{dL}{d z_{k,in}^{(i)}}} z_{j,out}^{(i-1)}$$
    \end{itemize}
\end{frame}
\begin{frame} {Backpropagation}
  \begin{itemize}
    \item The "$\delta$", or \textit{error signal},  for a neuron $k$ in layer $i$ will now represent how much the loss $L$ changes when the input to the neuron (that is, the weighted sum ${z}_{k,in}^{(i)}$) changes:
    $$ \delta_k^{(i)} = \frac{d L}{d {z}_{k,in}^{(i)}}$$
    \item Using the results from the previous slide, this can be rewritten as,
        $$ \delta_k^{(i)} = \sum_m \Bigg( \frac{d L}{d {z}_{m,in}^{(i+1)}} W_{k,m}^{(i+1)} \Bigg) \sigma'({z}_{k,in}^{(i)})$$ which, in turn, can be rewritten as,
        $$ \delta_k^{(i)} = \sum_m \Bigg( \delta_m^{(i+1)} W_{k,m}^{(i+1)} \Bigg) \sigma'({z}_{k,in}^{(i)})$$
    \item Therefore, we now have a \textbf{recursive definition} for the error signal of a neuron in layer $i$ in terms of the error signals of the neurons in layer $i+1$ and, by induction, layers \{i+2, i+3 \ldots , O\}!
  \end{itemize}
\end{frame}
\begin{frame} {Backpropagation}
  \begin{itemize}
    \item Given the error signal $\delta_k^{(i)}$ of neuron $k$ in layer $i$, the derivative of loss $L$ w.r.t to the weight $W_{j,k}$ is simply :
        $$
           \frac{d L}{d W_{j,k}^{(i)}} = \frac{d L}{d {z}_{k,in}^{(i)}} \frac{d {z}_{k,in}^{(i)}}{d W_{j,k}^{(i)}} 
           = \delta_k^{(i)} z_{j,out}^{(i-1)} $$
        because ${z}_{k,in}^{(i)} = \sum_j W_{j,k}^{(i)}z_{j,out}^{(i-1)}  + b_k^{(i)}$
    \item Similarly, the derivative of loss $L$ w.r.t bias $b_k^{(i)}$ is :
      $$ \frac{d L}{d b_k^{(i)}} = \frac{d L}{d {z}_{k,in}^{(i)}} \frac{d 
      {z}_{k,in}^{(i)}}{d b_k^{(i)}} = \delta_k^{(i)}$$
  \end{itemize}
\end{frame}
\begin{frame} {Backpropagation}
  \begin{itemize}
    \item We've seen how to compute the error signals for individual neurons. It can be shown that the error signal $\delta^{i}$ for an entire layer $i$ can be computed as follows ($\odot$ = element-wise product):
      \begin{itemize}
        \item $\delta^{(O)} = \nabla_{f_{out}}L \odot \tau'(f_{in})$
        \item $\delta^{(i)} = W^{(i+1)}\delta^{(i+1)} \odot \sigma'(z_{in}^{(i)})$
      \end{itemize}
    \item As we've seen earlier, the error signal for a given layer $i$ depends recursively on the error signals of \textit{later} layers \{i+1, i+2, \ldots , O\}.
    \item Therefore, backpropagation works by computing and storing the error signals \textbf{backwards}. That is, starting at the output layer and ending at the first hidden layer. This way, the error signals of later layers \textit{propagate backwards} to the earlier layers.
    \item The derivative of the loss $L$ w.r.t a given weight is computed efficiently by plugging in the cached error signals and values calculated in the forward pass thereby avoiding expensive and redundant computations. 
  \end{itemize}
\end{frame}

\begin{frame} {Backpropagation}
\begin{figure}
\includegraphics[width=10cm]{plots/backprop.png}
\caption{was kindly provided by Tobias Glasmachers. \\  (Here $u^{(i)}=z^{(i)}_{in}$ $v^{(i)}=z^{(i)}_{out}$)}
\end{figure}
\end{frame}


\begin{frame} {Summary}
  \begin{itemize}
    \item Our goal was to minimize the true/expected risk $$\riskt = \E_{(x,y)\sim p_{data}} [\Lxyt]$$
    with respect to the true underlying distribution $p_{data}$.
    \item Because we don't know $p_{data}$, we decided to minimize the empirical risk $$\risket = \frac{1}{n} \sumin \Lxyit$$ w.r.t the training set and hope for the best.
    \item However, even this is not possible because there is no way to analytically find a global minimum for deep neural networks.
    \item Therefore, we decided to use stochastic gradient descent to iteratively find a local minimum of the (typically) non-convex loss function.
  \end{itemize}
\end{frame}

\begin{frame} {Summary}
  \begin{itemize}
      \item To perform stochastic gradient descent, we want to approximate the gradient of the empirical risk $\risket$ with respect to \textit{all} the weights and biases in the network based on a mini-batch (or a single sample).%(where the number of components in the gradient is the total number of weights and biases).
      \item For each input $x$, to compute each component of the gradient, we perform backpropagation (by applying the chain rule in an efficient way to the relevant portion of the computational graph).
      \item In software implementations, a vectorized version of the chain rule is used to compute the derivatives w.r.t multiple weights simultaneously.
      \item Loosely speaking, each term in the gradient represents the extent to which the corresponding weight is responsible for the loss. In other words, it is a way to assign "blame" to each weight in the network.
  \end{itemize}
\end{frame}

\begin{frame} {Summary}
  \begin{itemize}
    \item All the activations for the inputs that we feed to the network (forward pass) are stored in memory and used to compute the derivatives at each node in the computation graph. For example, to compute the derivative of the sigmoid activation at $z_{1,out}$ in the XOR example, where $\frac{d z_{1,out}}{d z_{1,in}} = \sigma(z_{1,in})(1-\sigma(z_{1,in}))$, we plugged in $\sigma(z_{1,in}) = \num[round-mode=places,round-precision=4]{0.3705169}$.
 % \item In our example, we updated w.r.t. a single training example but typically, we feed subsets of the training set (more on this later).
    \item The term "backpropagation" refers to the fact that the computation of the gradient using the chain rule is performed backwards (that is, starting at the output layer and finishing at the first hidden layer). 
      A \textit{forward} computation using the chain rule also results in the same gradient but can be computationally expensive. (see http://colah.github.io/posts/2015-08-Backprop/)
 \end{itemize}
\end{frame} 

\begin{frame} {Summary}
  \begin{itemize}
    \item After computing the gradient (using backpropagation) of the loss for a minibatch of samples from the training set, we subtract the average gradient (scaled by the learning-rate $\alpha$) from the current set of weights (and biases) which results in a new set of weights (and biases).
    \item The empirical loss for this new set of weights is now lower ("walking down the hill").
    \item Next, we once again compute the forward pass for this new set of weights and store the activations.
    \item Then, we compute the gradient of the empirical loss using backpropagation and take another step down the hill.
    \item Rinse and repeat (until the loss stops decreasing substantially). 
      However, we'll see later that this naive approach often results in overfitting!
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%
%\section{Hardware and Software for Deep Learning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame} {Hardware for Deep Learning}
%
%  \begin{itemize}
%    \item Deep neural networks require special hardware to be trained efficiently.
%    \item The training is done using \textbf{G}raphics \textbf{P}rocessing \textbf{U}nits (GPUs) and a special programming language called CUDA.
%    \item Training on standard CPUs takes a very long time and is infeasible for anything but toy examples.
%  \end{itemize}
%\begin{figure}
%    \centering
%      \scalebox{1}{\includegraphics{plots/cpu_gpu.png}}
%      \caption{\textit{Left:} Each CPU can do 2-8 parallel computations. \textit{Right:} A single GPU can do thousands of simple parallel computations.}
%  \end{figure}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame} {Software for Deep Learning}
%  \begin{itemize}
%    \vspace{15mm}
%    \item CUDA is a very \textit{low level} programming language and thus writing code for deep learning requires a lot of work.
%    \vspace{5mm}
%    \item Deep learning (software) frameworks, like Tensorflow and Pytorch, abstract CUDA and provide additional functionality.
%  \end{itemize}
%\end{frame}
%\begin{frame} {Software for Deep Learning}
%  \begin{wrapfigure}{R}{0.3\textwidth}
%    \centering
%      \scalebox{0.3}{\includegraphics{plots/tflow.png}}
%  \end{wrapfigure}
%\textbf{Tensorflow}
%\begin{itemize}
%  \item Currently the most popular framework
%  \item Supported by Google
%  \item Python, R, C++ and Javascript APIs
%  \item Distributed training on GPUs and TPUs
%  \item Tools for visualizing neural nets, running them efficiently on phones and embedded devices.
%\end{itemize}
%
%\begin{wrapfigure}{R}{0.3\textwidth}
%    \centering
%      \scalebox{0.3}{\includegraphics{plots/keras.png}}
%  \end{wrapfigure}
%\textbf{Keras}
%  \begin{itemize}
%    \item Intuitive, high-level wrapper for rapid prototyping
%    \item Capable of running on Tensorflow, CNTK or Theano backends
%    \item Python and R APIs
%  \end{itemize}
%\end{frame}
%
%\begin{frame} {Software for Deep Learning}
%  \begin{wrapfigure}{R}{0.3\textwidth}
%    \centering
%      \scalebox{0.3}{\includegraphics{plots/pytorch.png}}
%  \end{wrapfigure}
%
%\textbf{Pytorch}
%  \begin{itemize}
%    \item Extremely popular framework that is suitable for both research and production
%    \item Supported by Facebook
%    \item Python and C++ APIs
%    \item Distributed training on GPUs
%  \end{itemize}
%
%  \begin{wrapfigure}{R}{0.3\textwidth}
%    \centering
%      \scalebox{0.3}{\includegraphics{plots/mxnet.png}}
%  \end{wrapfigure}
%  \vspace{3mm}
%  \textbf{MXNet}
%    \begin{itemize}
%      \item Open-source deep learning framework written in C++ and cuda (used by Amazon for their Amazon Web Services) 
%      \item Scalable, allowing fast model training
%      \item Supports flexible model programming and multiple languages (C++, Python, Julia, Matlab, JavaScript, Go, \textbf{R}, Scala, Perl)
%    \end{itemize}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\section{MXNet Example : MNIST digit recognizer}
%
%\begin{vbframe}{MNIST digit recognizer}
%    \begin{itemize}
%      \item The MNIST dataset is a large dataset of handwritten digits (black and white) that is commonly used for benchmarking various image processing algorithms.
%      \item It is a good dataset for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal effort on preprocessing and formatting.
%      \item  There have been a number of scientific papers on attempts to achieve the lowest error rate. One paper, using a hierarchical system of convolutional neural networks (chapter 5), manages to get an error rate of only 0.23 percent.
%    \end{itemize}
%\framebreak
%  \begin{figure}
%    \centering
%      \includegraphics[width=10cm]{plots/mnist.png}
%      \caption{Snipped from the mnist data set (LeCun and Cortes (2010)).}
%  \end{figure}
%  \begin{itemize}
%    \item 70k image data of handwritten digits with $28 \times 28$ pixels.
%    \item Classification task with 10 classes (e.g. 0, 1, 2, ..., 9).
%  \end{itemize}
%\framebreak
%  \begin{itemize}
%    \item We download the data sets (train.csv and test.csv) from \url{https://www.kaggle.com/c/digit-recognizer/data}.
%    \item We obtain $42.000$ images for training and $28.000$ for testing.
%  \end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% READ ME!!! %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% The following slides include lots of code chunks which have been     %
%% temporarily disabled (eval = FALSE, echo = FALSE) and replaced by    %
%% screenshots of the corresponding outputs (to maintain colorization). %
%% Else, one would need a working version of mxnet (and a fast CPU/GPU) %
%% to compile the code in a finite amount of time.                      %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  % \begin{figure}
%  %   \centering
%  %     \includegraphics[width=12cm]{plots/mxnet_codechunk_1.png}
%  % \end{figure}
%<<mxnet1, size = "small", cache = TRUE, eval = FALSE, echo = TRUE>>=
%# assign the location of the data as your wd()
%
%train = read.csv("train.csv", header = TRUE)
%test = read.csv("test.csv", header = TRUE)
%
%train = data.matrix(train)
%test = data.matrix(test)
%@
%\framebreak
%  \begin{figure}
%    \centering
%      \includegraphics[width=11cm]{plots/mxnet_codechunk_2.png}
%  \end{figure}
%<<mxnet2, size = "normalsize", cache = TRUE, eval = FALSE, echo = FALSE>>=
%# Split data into matrix containing features and
%# vector with labels
%train.x = train[, -1]
%train.y = train[, 1]
%
%# normalize to (0,1) and transpose data
%train.x = t(train.x/255)
%dim(train.x)
%
%test = t(test/255)
%
%table(train.y)
%@
%\framebreak
%  \begin{itemize}
%    \item Now we define the architecture of our model.
%  \end{itemize}
%  % \begin{figure}
%  %   \centering
%  %     \includegraphics[width=11cm]{plots/mxnet_codechunk_3.png}
%  % \end{figure}
%<<mxnet3, size = "scriptsize", cache = TRUE, eval = FALSE, echo = TRUE>>=
%require("mxnet")
%
%data = mx.symbol.Variable(name = "data")
%
%layer1 = mx.symbol.FullyConnected(data = data, name = "layer1",
%  num_hidden = 10L)
%activation1 = mx.symbol.Activation(data = layer1, name = "activation1",
%  act_type = "relu")
%layer2 = mx.symbol.FullyConnected(data = activation1, name = "layer2",
%  num_hidden = 10L)
%activation2 = mx.symbol.Activation(data = layer2, name = "activation2",
%  act_type = "relu")
%layer3 = mx.symbol.FullyConnected(data = activation2, name = "layer3",
%  num_hidden = 10L)
%softmax = mx.symbol.SoftmaxOutput(data = layer3, name = "softmax")
%@
%\framebreak
%  \begin{minipage}{0.45\textwidth}
%    \begin{itemize}
%      \item Mxnet enables us to easily visualize the models architecture
%    \end{itemize}
%  % \begin{figure}
%  %   \centering
%  %     \includegraphics[width=6cm]{plots/mxnet_codechunk_4a.png}
%  % \end{figure}
%<<mxnet4, size = "footnotesize", cache = TRUE, eval = FALSE, echo = TRUE>>=
%graph.viz(model$symbol)
%@
%  \end{minipage}
%  \begin{minipage}{0.45\textwidth}
%    \begin{figure}
%      \centering
%        \includegraphics[width=1.5cm]{plots/mxnet_codechunk_4b.png}
%    \end{figure}
%  \end{minipage}
% \framebreak
%  % \begin{algorithm}[H]
%  % \footnotesize
%  %   \caption{Basic SGD parameter update at training iteration k}
%  %   \begin{algorithmic}[1]
%  %   \State \textbf{require} learning rate $\alpha$ \strut
%  %   \State \textbf{require} initial parameter $\theta$ \strut
%  %     \While{stopping criterion not met}
%  %       \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Sample a minibatch of $m$ examples from the training set $\{\tilde{x}^{(1)},\dots,\tilde{x}^{(m)}\}$ \strut}
%  %       \State Compute gradient estimate: $\hat{g} \leftarrow \frac{1}{m} \nabla_\theta \sum_{i} \Lxym$
%  %       \State Apply update: $\theta \leftarrow \theta - \alpha \hat{g}$
%  %     \EndWhile
%  %   \end{algorithmic}
%  % \end{algorithm}
%  % \begin{itemize}
%  %   \item Thus, what SGD basically does is computing an unbiased estimate of the gradient by taking the average gradients of a minibatch to update the parameter $\theta$.
%  % \end{itemize}
%  % \begin{itemize}
%  %   \item In a final step, we have to assign some parameters.
%  % \end{itemize}
%
%  % \begin{figure}
%  %   \centering
%  %     \includegraphics[width=11cm]{plots/mxnet_codechunk_5.png}
%  % \end{figure}
%<<mxnet5, size = "footnotesize", cache = TRUE, eval = FALSE, echo = TRUE>>=
%devices = mx.cpu()
%
%mx.set.seed(1337)
%
%model = mx.model.FeedForward.create(
%  symbol = softmax,
%  X = train.x, y = train.y,
%  ctx = devices,
%  num.round = 10L, array.batch.size = 100L,
%  learning.rate = 0.05,
%  eval.metric = mx.metric.accuracy,
%  initializer = mx.init.uniform(0.07),
%  epoch.end.callback = mx.callback.log.train.metric(100L))
%@
%
%  \begin{itemize}
%    \item We used SGD with a minibatch of size 100 and trained for 10 epochs.
%    \item Consequently we feed our algorithm successively with 100 training samples before updating the weights.
%  \end{itemize}
%% \framebreak
%% # <<mxnet99, size = "footnotesize", cache = TRUE, eval = FALSE, echo = TRUE>>=
%% # devices = mx.cpu()
%% # 
%% # mx.set.seed(1337)
%% # 
%% # model = mx.model.FeedForward.create(
%% #   symbol = softmax,
%% #   X = train.x, y = train.y,
%% #   ctx = devices,
%% #   num.round = 10L, array.batch.size = 100L,
%% #   learning.rate = 0.05,
%% #   eval.metric = mx.metric.accuracy,
%% #   initializer = mx.init.uniform(0.07),
%% #   epoch.end.callback = mx.callback.log.train.metric(100L))
%% # @
%% #   \begin{itemize}
%% #     \item An epoch means that the model gets to see the whole training set one time.
%% #     \item Thus, one epoch involves $\frac{\text{training samples}}{\text{batch size}}$ gradient updates.
%% #     \item We repeat this procedure 10 times.
%% #   \end{itemize}
%\framebreak
%  \begin{figure}
%    \centering
%      \includegraphics[width=10.5cm]{plots/mxnet_codechunk_6.png}
%  \end{figure}
%<<mxnet6, size = "scriptsize", warning = FALSE, cache = TRUE, eval = FALSE, echo = FALSE>>=
%require("mxnet")
%
%train = read.csv("train.csv", header = TRUE)
%test = read.csv("test.csv", header = TRUE)
%train = data.matrix(train)
%test = data.matrix(test)
%train.x = train[,-1]
%train.y = train[,1]
%train.x = t(train.x/255)
%test = t(test/255)
%data = mx.symbol.Variable("data")
%layer1 = mx.symbol.FullyConnected(data, name = "layer1",num_hidden = 10)
%activation1 = mx.symbol.Activation(layer1, name = "activation1", act_type = "relu")
%layer2 = mx.symbol.FullyConnected(activation1, name = "layer2", num_hidden = 10)
%activation2 = mx.symbol.Activation(layer2, name = "activation2", act_type = "relu")
%layer3 = mx.symbol.FullyConnected(activation2, name = "layer3", num_hidden = 10)
%softmax = mx.symbol.SoftmaxOutput(layer3, name = "softmax")
%devices = mx.cpu()
%mx.set.seed(1337)
%model = mx.model.FeedForward.create(softmax, X = train.x, y = train.y,
%  ctx = devices, num.round = 10, array.batch.size = 100,
%  learning.rate = 0.05, momentum = 0.9,
%  eval.metric = mx.metric.accuracy,
%  initializer = mx.init.uniform(0.07),
%  epoch.end.callback = mx.callback.log.train.metric(100))
%@
%  \begin{itemize}
%    \item After 10 epochs, our neural network begins to stagnate at a training accuracy of roughly $93.5\%$
%  \end{itemize}
%\framebreak
%  \begin{itemize}
%    \item Next, we use the model to predict the test data.
%  \end{itemize}
%  \begin{figure}
%    \centering
%      \includegraphics[width=11cm]{plots/mxnet_codechunk_7.png}
%  \end{figure}
%<<mxnet7, size = "scriptsize", warning = FALSE, cache = TRUE, eval = FALSE, echo = FALSE>>=
%preds = predict(model, test)
%# this yields us predicted probabilities for all 10 classes
%dim(preds)
%
%# we choose the maximum to obtain quantities for each class
%pred.label = max.col(t(preds)) - 1
%table(pred.label)
%@
%
%\begin{itemize}
%  \item We find that the accuracy (not shown) of the model on the test data is only $89.843\%$ which is unsatisfactory. 
%\end{itemize}
%\framebreak
%%
%% \framebreak
%%   \begin{itemize}
%%     \item Finally we want to submit our predictions on kaggle to see how good we performed.
%%     \item Thus, we save our results in a csv file and upload it on \url{https://www.kaggle.com/c/digit-recognizer/submit}
%%   \end{itemize}
%%   % \begin{figure}
%%   %   \centering
%%   %     \includegraphics[width=11cm]{plots/mxnet_codechunk_8.png}
%%   % \end{figure}
%% <<mxnet8, size = "footnotesize", cache = TRUE, eval = FALSE, echo = TRUE>>=
%% submission = data.frame(ImageId = 1:ncol(test), Label = pred.label)
%%
%% write.csv(submission, file = 'submission.csv', row.names = FALSE,
%%   quote = FALSE)
%% @
%% \framebreak
%%   \begin{itemize}
%%     \item After making your submission, you should see something like this:
%%   \end{itemize}
%%   \begin{figure}
%%     \centering
%%       \includegraphics[width=10.5cm]{plots/mxnet_codechunk_9.png}
%%   \end{figure}
%%   \begin{itemize}
%%     \item For this competition Kaggle uses accuracy (score) to measure each participant's performance.
%%     \item While the ratio of the train to test data makes the problem really difficult, $89.843\%$ is still a very bad result and we would like to improve our performance.
%%   \end{itemize}
%\framebreak
%  \begin{minipage}{0.45\textwidth}
%    \begin{itemize}
%      \item Because the performance of the previous model was somewhat poor, we try the following, much larger, network (all other parameters remain the same):
%      \item Rerunning the training with the new architecture, this model yields us a training accuracy of $99.39\%$ and a test accuracy of $96.514\%$.
%    \end{itemize}
%  \end{minipage}
%  \begin{minipage}{0.45\textwidth}
%    \begin{figure}
%      \centering
%        % \scalebox{0.25}{\includegraphics{plots/mxnet_codechunk_10.png}}
%        \includegraphics[width=1.5cm]{plots/mxnet_codechunk_10.png}
%    \end{figure}
%
%  \end{minipage}
%% \framebreak
%%   \begin{figure}
%%     \centering
%%       \includegraphics[width=11cm]{plots/mxnet_codechunk_11.png}
%%   \end{figure}
%  % \begin{itemize}
%  %   \item Rerunning the training with the new architecture, this model yields us a training accuracy of $99.39\%$ and a test accuracy of $96.514\%$.
%  % \end{itemize}
%\end{vbframe}
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{frame} {Weight Initialization}
%% \end{frame}
%% \begin{frame} {Weight Initialization}
%% \end{frame}
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \begin{vbframe}{Stochastic gradient descent}
%%   \begin{itemize}
%%     \item Optimization algorithms that use the entire training set are called \textbf{batch} or \textbf{deterministic}.
%%       \begin{itemize}
%%         \item All training samples are processed in one huge step.
%%         \item Computationally very costly, particularly in deeplearning!
%%       \end{itemize}
%%     \item Optimization algorithms that use only a single example at a time are called \textbf{stochastic} or \textbf{online}.
%%     \item Most optimization algorithms applied in deeplearning are somewhere in between!
%%       \begin{itemize}
%%         \item Usually more than one, but less than all training samples are used at each iteration. Traditionally called \textbf{minibatch}, nowadays \textbf{stochastic} methods.
%%       \end{itemize}
%%     \item One \textbf{epoch} means one pass of the full training set.
%%     \item Since we divide the training set into minibatches, each epoch goes through the whole training set. Each iteration goes through one minibatch.
%%     \item SGD and its modifications are the most used optimization algorithms for machine learning in general and for deep learning in particular.
%%   \end{itemize}
%%   \begin{algorithm}[H]
%%   \footnotesize
%%     \caption{Basic SGD parameter update at training iteration k}
%%     \begin{algorithmic}[1]
%%     \State \textbf{require} learning rate $\alpha$ \strut
%%     \State \textbf{require} initial parameter $\theta$ \strut
%%       \While{stopping criterion not met}
%%         \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Sample a minibatch of $m$ examples from the training set $\{\tilde{x}^{(1)},\dots,\tilde{x}^{(m)}\}$ \strut}
%%         \State Compute gradient estimate: $\hat{g} \leftarrow \frac{1}{m} \nabla_\theta \sum_{i} \Lxym$
%%         \State Apply update: $\theta \leftarrow \theta - \alpha \hat{g}$
%%       \EndWhile
%%     \end{algorithmic}
%%   \end{algorithm}
%%   \begin{itemize}
%%     \item Thus, what SGD basically does is computing an unbiased estimate of the gradient by taking the average gradients of a minibatch to update the parameter $\theta$.
%%   \end{itemize}
%% \framebreak
%% \begin{itemize}
%%   \item For the mnist example , we used SGD
%%   \begin{itemize} 
%%     \item with a minibatch of size 100
%%     \item and trained for 10 epochs.
%%   \end{itemize}
%%   <<batchsize, size = "footnotesize", cache = TRUE, eval = FALSE, echo = TRUE>>=
%% 
%% model = mx.model.FeedForward.create(
%%     symbol = softmax, 
%%     X = train.x, y = train.y,
%%     optimizer = "sgd",
%%     array.batch.size = 100L,
%%     num.round = 10L
%% )
%% @
%%   \item Consequently we feed our algorithm successively with 100 training samples before updating the weights.
%%   \item An epoch means that the model gets to see the whole training set one time.
%%   \item Thus, one epoch involves $\frac{\text{training samples}}{\text{batch size}}$ gradient updates.
%%   \item We repeat this procedure for 10 times.
%% \end{itemize}
%% \end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%\begin{frame} {Key hyperparameters}
%  \begin{itemize}
%    \item In addition to the structure/topology of the neural network, the performance of a model is also strongly affected by some key hyperparameters such as:
%      \begin{itemize}
%        \item $\alpha$, the learning rate
%        \item $\lambda$, the regularization coefficient
%        \item $T$, the number of training iterations
%        \item $m$, the minibatch size
%        \item and others...
%      \end{itemize}
%    \vspace{2mm}
%    \item These hyperparameters typically control the complexity of the model and the convergence of the training algorithm.
%    \vspace{2mm}
%    \item In the next couple of lectures, we'll examine methods and techniques to set these hyperparameters and the theoretical motivations behind many of them.
%    \vspace{2mm}
%    \item For now, let's briefly examine (empirically) the effect of different batch sizes on the training error and training time.
%  \end{itemize}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{vbframe}{Batch size as a hyperparameter}
%
%<<batchSizeBenchmark, size = "small", cache = TRUE, eval = FALSE, echo = TRUE>>=
%data = mx.symbol.Variable("data")
%fc1 = mx.symbol.FullyConnected(data, num_hidden = 128)
%act1 = mx.symbol.Activation(fc1, act_type = "relu")
%fc2 = mx.symbol.FullyConnected(act1, num_hidden = 64)
%act2 = mx.symbol.Activation(fc2, act_type = "relu")
%fc3 = mx.symbol.FullyConnected(act2, num_hidden = 32)
%act3 = mx.symbol.Activation(fc3, ,act_type = "relu")
%fc4 = mx.symbol.FullyConnected(act3, num_hidden = 10)
%softmax = mx.symbol.SoftmaxOutput(fc4)
%@
%
%%\begin{itemize}
%  %\item 
%  Let us train this architecture with three different values for the
%    $$\text{batch size} \in (20, 100, 1000)$$
%  on mnist.
%%\end{itemize}
%
%\framebreak
%
%\begin{figure}
%  \centering
%  \scalebox{0.8}{\includegraphics{plots/batchsize_train.png}}
%\end{figure}
%
%\vspace{-0.5cm}
%
%\begin{minipage}{\linewidth}
%\centering
%  \begin{center}
%    \captionof{table}{Different batch sizes and total training time (in minutes).}
%      \begin{tabular}{l | c c c}
%      Batch size &  20 & 100 & 1000 \\
%      \hline 
%      Training time & 6.36 & 1.47 & 0.48
%      \end{tabular}
%  \end{center}
%\end{minipage}
%
%%  \framebreak
%%  
%% \begin{figure}
%%   \centering
%%   \includegraphics[width=11cm]{plots/batchSizeBenchmarkTest.png}
%% \end{figure}
%% 
%% \vspace{-0.5cm}
%% 
%% \begin{minipage}{\linewidth}
%% \centering
%%   \begin{center}
%%     \captionof{table}{Different batch sizes and training time in seconds}
%%       \begin{tabular}{l | c c c c c c}
%%       Batch size & 16 & 17 & 20 & 100 & 128 & 1000 \\
%%       \hline 
%%       Training time & 44.42 & 44.84 & 38.84 &  8.91 & 7.67 & 2.59
%%       \end{tabular}
%%   \end{center}
%% \end{minipage}
%\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{vbframe}
%\frametitle{References}
%\footnotesize{
%\begin{thebibliography}{99}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\bibitem[Mont\'{u}far et al., 2014]{1} Guido Mont\'{u}far, Razvan Pascanu, Kyunghyun Cho and Yoshua Bengio (2014)
%\newblock On the Number of Linear Regions of Deep Neural Networks
%\newblock \emph{\url{https://arxiv.org/pdf/1402.1869.pdf}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\bibitem[Yann LeCun and Corinna Cortes, 2010]{2} Yann LeCun and Corinna Cortes (2010)
%\newblock MNIST handwritten digit database 
%\newblock \emph{\url{http://yann.lecun.com/exdb/mnist/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\bibitem[Yann LeCun et al., 1998]{3} Yann Lecun, Leon Bottou, Genevieve B. Orr and Klaus-Robert Müller (1998)
%\newblock Efficient BackProp
%\newblock \emph{\url{http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\bibitem[Geoffrey Hinton and Ruslan Salakhutdinov, 2006]{4} Geoffrey Hinton and Ruslan Salakhutdinov (2006)
%\newblock Reducing the Dimensionality of Data with Neural Networks
%\newblock \emph{\url{https://www.cs.toronto.edu/\%7Ehinton/science.pdf}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\bibitem[Ian Goodfellow et al., 2016]{5} Ian Goodfellow, Yoshua Bengio and Aaron Courville (2016)
%\newblock Deep Learning
%\newblock \emph{\url{http://www.deeplearningbook.org/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\end{thebibliography}
%}
%\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlecture
