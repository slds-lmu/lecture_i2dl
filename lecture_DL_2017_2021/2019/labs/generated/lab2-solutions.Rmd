---
title: "Lab 2"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, echo = TRUE)
```


Welcome to the second lab. The first exercise is a simple extension of the last lab, where we adapt the network to multiclass classification and we write generic facilities for the forward pass. The second exercise will put your geometric understanding of neural networks to the test. The third exercise contains some theoretical results about symmetries in the weight space to prompt some thinking on your part.


## Exercise 1
This is a simple continuation of the second exercise of the previous lab, and you are allowed to re-use parts the solution you found. As a reminder, suppose you have five input points, $\textbf{x}_1=|0,0|^T$, $\textbf{x}_2=|1,0|^T$, $\textbf{x}_3=|0,-1|^T$, $\textbf{x}_4=|-1,0|^T$ and $\textbf{x}_5=|0,1|^T$, and the corresponding classes are $y_1=1$ and $y_2=y_3=y_4=y_5=0$.

 1. Add a second neuron to the output layer, and make it compute $1-y$
     - How do the weights/biases change?
     - What should you use as activation function for the last layer?
     - And what loss function?
     - Pen and paper!
 2. Write the code to perform the forward pass for a neural network with a generic number of layers, and test it with the weights and biases you found previously.


```{r}
xs = matrix(c(
  0, 0,
  1, 0,
  0, -1,
  -1, 0,
  0, 1
), nrow = 5, ncol = 2, byrow = TRUE)

ys = matrix(c(
  1, 0,
  0, 1,
  0, 1,
  0, 1,
  0, 1
), nrow = 5, ncol = 2, byrow = TRUE)

ws1 = matrix(
  c(
    -20, 20,
    -20, 20
  ), nrow = 2, ncol = 2, byrow = TRUE
)

bs1 = c(
  0, 0
)

ws2 = matrix(
  c(
    -1, 1,
    -1, 1
  ), nrow = 2, ncol = 2, byrow = TRUE
)

bs2 = c(
  10, -10
)

relu = function(x) {
  ifelse(x > 0, x, 0)
}

softmax = function(predictions) {
  exp(predictions) / rowSums(exp(predictions))
}

predictions = (
  softmax(relu(xs %*% ws1 + bs1) %*% ws2 + bs2)
)

predictions
```

Compare with the desired outputs to make sure it is correct.

```{r}
categorical_crossentropy = function(y_true, y_predicted) {
  -sum(log(y_predicted) * y_true) / nrow(y_true)
}

loss = categorical_crossentropy(ys, predictions)

loss
```

As the network is giving the correct outputs, the loss should be low.

We can now create a generic function to perform the forward pass:

```{r}

# This function represents a dense layer with given weights and biases.
dense = function(weights, bias) function(x) {
  x %*% weights + matrix(rep(bias, nrow(x)), byrow = TRUE, nrow = nrow(x))
}

forward_pass = function(layers, x) {
  for(ll in layers) {
    x = ll(x)
  }
  x
}


forward_pass(c(
  dense(ws1, bs1),
  relu,
  dense(ws2, bs2),
  softmax
), xs)
```

Make sure this matches the output you had earlier.

## Exercise 2

Create a feed-forward neural network that can correctly classify the blue region in the plot below as $y=1$, and the points outside it as $y=0$.

 - Explain why it cannot be done with a single hidden layer, but it is possible with two.
 - Use four units for the first hidden layer, and two for the second hidden layer.
 - Use the sigmoid activation function.
 - Use the code you wrote previously to perform the forward pass.

```{r eval=TRUE}
library(ggplot2)

df = data.frame(x=c(-1, -1, 0, 1, 1, 0, -1), y=c(1, -1, 0, -1, 1, 0, 1))
ggplot(df, aes(x=x, y=y)) +
  geom_polygon(fill='#b3cde3') +
  xlim(-2, 2) +
  ylim(-2, 2)
```

```{r}
sigmoid = function(x) {
  1 / (1 + exp(-x))
}


network_params = c(
  dense(
    weights = matrix(
      c(100, 100, 100, -100, 100, 0, -100, 0), ncol = 4
    ), bias = c(
      0, 0, 100, 100
    )
  ),
  sigmoid,
  dense(
    weights = matrix(
      c(-100, -100, 100, 100, 100, 100, 100, 100), ncol = 2
    ), bias = c(
      c(-150, -350)
    )
  ),
  sigmoid,
  dense(
    weights = matrix(
      c(100, 100), ncol = 1
    ), bias = c(
      -50
    )
  ), 
  sigmoid
)

# We evaluate the predictions of the network on a grid of points
halfres = 30
data = as.matrix(expand.grid(-halfres:halfres, -halfres:halfres)) / halfres * 2
predictions = forward_pass(network_params, data)

df = data.frame(x=data[,1], y=data[,2], c=predictions[,1])
ggplot(df, aes(x=x, y=y, col=c)) +
  geom_point(size=3) +
  scale_colour_gradient2(midpoint = 0.5)
```

## Exercise 3

Consider a neuron with incoming weights $\textbf{w}=w_1,\ldots,w_n$ and bias $b$. This neuron is connected to the $i$-th neuron of the next layer with the weight $v_i$, and the bias of the latter neuron is $c_i$. We want to replace $\textbf{w}$, $v_i$, $b$ and $c_i$ with new parameters $\textbf{w}'$, $v'_i$, $b'$ and $c'_i$ so that the output of the network is unchanged for all inputs. At least one of the new parameters must be different, but some are allowed to equal the old ones.

 1. Suppose that $\tau$ is the hyperbolic tangent. Show that the network computes the same function if we let $\textbf{w}'=-\textbf{w}$, $v'_i=-v_i$, $b'=-b$ and $c'_i=c_i$.
 2. Now suppose that $\tau$ is the logistic sigmoid function. How should you set $\textbf{w}'$, $v'_i$, $b'$ and $c'_i$?
 3. Can you find other ways of modifying the parameters of a neural network without altering its output? Equivalently, given a neural network computing a certain function, how can you find a different network that computes the same function?
    - You do not have to provide a formal answer, but you can do so if you wish.

### Solution
 1. The output of the neuron is $z'=\tanh(\textbf{w}'^T\textbf{x}+b')=\tanh(-\textbf{w}^T\textbf{x}-b)$. Since $\tanh(x)=-\tanh(-x)$, we have $z'=-\tanh(\textbf{w}^T\textbf{x}+b)=-z$. Since $v'_i=-v_i$, we have that $v'_iz'=(-v_i)\cdot(-z)=v_iz$.
 2. We first prove that $\sigma(x)=1-\sigma(-x)$:
 
    \begin{align*}
    \sigma(x)-1+\sigma(-x)
    &=\frac{1}{1+e^{-x}}-1+\frac{1}{1+e^x} \\
    &=\frac{(1+e^x)-(1+e^{-x})(1+e^{x})+(1+e^{-x})}{(1+e^{-x})(1+e^{x})} \\
    &=\frac{1+e^x-1-e^x-e^{-x}-e^0+1+e^{-x}}{(1+e^{-x})(1+e^{x})} \\
    &=0
    \end{align*}

    We can follow the same idea as the previous question, and set $\textbf{w}'=-\textbf{w}$, $v_i'=-v_i$ and $b'=-b$. Then, we have $z'=\sigma(\textbf{w}'^T\textbf{x}+b')=\sigma(-\textbf{w}^T\textbf{x}-b')=1-\sigma(\textbf{w}^T\textbf{x}+b)=1-z$. Since $v'_i=-v_i$, the contribution of this neuron to the neurons of the next layer is $v'_iz'=-v_i(1-z)=-v_i+v_iz$, therefore we can set $c_i'=c_i+v_i$.
