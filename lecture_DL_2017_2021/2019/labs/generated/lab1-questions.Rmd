---
title: "Lab 1"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE, echo = TRUE)
```

Welcome to the first lab. The goal of this lab is to perform the forward pass on shallow neural networks with a single hidden layer. Remember that a layer uses a weight matrix $\textbf{W}$, a bias vector $\textbf{b}$ and an activation function $\sigma(\cdot)$ to transform its input $\textbf{x}$ to the output $\textbf{y}$ with an affine transformation followed by a point-wise non-linearity $\textbf{y}=\sigma(\textbf{W}\textbf{x}+\textbf{b})$.

## Exercise 1
Recall that a neural network with a single output neuron and no hidden layer is equivalent to linear or logistic regression, and that the optimal parameters for linear regression are given by $\textbf{w}^*=(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{y}$. Imagine you have four points $x_1=2$, $x_2=-1$, $x_3=4$ and $x_4=-3$ and the corresponding regression targets $y_1=3$, $y_2=-3$, $y_3=7$ and $y_4=-7$. Write code to:

 1. Compute the optimal regression parameters with vectorized operations.
 2. Use these parameters as weights for a neural network with a single output neuron with linear activation and no hidden layer. Compute the output of the network, again with vectorized operations, and verify that it is very close to the regression targets.

```{r}
xs = matrix(c(
  2, 1,
  -1, 1,
  4, 1,
  -3, 1
), nrow = 4, ncol = 2, byrow=TRUE)
ys = matrix(
  c(3, -3, 7, -7),
  nrow = 4, ncol = 1
)

ws = (
  # TODO compute the optimal regression parameters
)

ws
```

Check the weights. Do they make sense?

```{r}
neuron_output = (
  # TODO compute the output of the neuron
)

neuron_output
```

Check that the neuron output matches the targets.

## Exercise 2
Suppose you have five input points, $\textbf{x}_1=|0,0|^T$, $\textbf{x}_2=|1,0|^T$, $\textbf{x}_3=|0,-1|^T$, $\textbf{x}_4=|-1,0|^T$ and $\textbf{x}_5=|0,1|^T$, and the corresponding classes are $y_1=1$ and $y_2=y_3=y_4=y_5=0$ (see the figure below). The "ReLU" (Rectified Linear Unit) activation function is defined as $\sigma(x)=\max(0, x)$. Design a neural network with one hidden layer that performs this classification:

 1. Find the values of $\textbf{W}_1,\textbf{b}_1,\textbf{W}_2,\textbf{b}_2$ to perform the classification correctly.
     - The hidden layer uses the ReLU activation function, the output layer uses the sigmoid.
     - Write down the equation for the forward pass, and find the shapes for the weight matrices and biases $\textbf{W}_1,\textbf{W}_2,\textbf{b}_1,\textbf{b}_2$
     - How many neurons do the input and output layers have? How many neurons do you think the hidden layer should have?
     - Compute the loss between the labels and the classification of the neural network.
     - Pen and paper!
 2. Write the code to perform point (1).
 3. Explain graphically what your neural network is doing.
     - Does your network generalize well?
     - Would it classify correctly the four points $x_6,\ldots,x_9=|\pm 1, \pm 1|^T$ with class $0$?
     - If not, how would you change it?


```{r eval=TRUE}
# Plot the dataset
xs = matrix(c(
  0, 0,
  1, 0,
  0, -1,
  -1, 0,
  0, 1
), ncol = 2, byrow = TRUE)

ys = matrix(c(1, 0, 0, 0, 0), nrow = 5, ncol = 1)

plot(xs, col=c("blue", "red", "red", "red", "red"), pch=19)
legend("topleft", c("y=0", "y=1"), fill=c("red", "blue"))
```

```{r}
ws1 = matrix(
  # TODO fill in the weights for the first layer
)

bs1 = (
  # TODO fill in the biases for the first layer
)

ws2 = matrix(
  # TODO fill in  the weights for the second layer
)

bs2 = (
  # TODO fill in the biases for the second layer
)
```

```{r}
relu = function(x) {
  # TODO compute the relu activation on x
}

sigmoid = function(x) {
  # TODO compute the sigmoid activation on x
}

predictions = (
  # TODO use the weights and biases to compute the output of the network
)

predictions
```

Compare the predicted classes with `ys`, they should be very similar.

```{r}
binary_crossentropy = function(y_true, y_predicted) {
  # TODO compute the loss between the predictions and ys
}

loss = binary_crossentropy(ys, predictions)

loss
```

The loss should be very low, as the predictions are all correct.


