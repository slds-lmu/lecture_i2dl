%Blank Lecture
%This file is a child of preamble.Rnw in the style folder
%if you want to add stuff to the preamble go there to make
%your changes available to all childs


<<setup-child, include = FALSE>>=
library(knitr)
set_parent("../style/preamble.Rnw")
@

\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-nn}

\newcommand{\Dsubtrain}{\mathcal{D}_{\text{subtrain}}}
\newcommand{\Dval}{\mathcal{D}_{\text{val}}}


\lecturechapter{8}{Adversarial Examples}
\lecture{Deeplearning}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Adversarial Robustness}
    \begin{itemize}
        \item It is critical to examine if a trained neural net is robust and reliable. 
        \item \textbf{Adversarial robustness} of a model means that a model is robust to (test time) perturbations of its inputs. 
        \item \textbf{Adversarial machine learning} studies technique which attempt  to fool machine learning models through malicious input.
      %  \item To make a model more robust, we can train our model on adversarially perturbed examples, called \textbf{adversarial examples}, derived from the training set.  
      %  \item This chapter summarizes  high-level ideas in adversarial robustness. 
        %\item We start with a few remarks on adversarial examples.  
    \end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Adversarial Examples}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 \begin{frame} {Adversarial Examples}
   \begin{itemize}
     \item An adversarial example is an input to a model that is deliberately designed to "fool" the network into misclassifying it.
     \item The test error of a model is only an indicator of how well the model performs with respect to samples from the data-generating distribution.
     \item The performance of the same model can be drastically different on samples from a completely different distribution (on the same input space).
      \item It is possible to make changes to an image that makes a pretrained CNN output a completely different predicted class even though the change is imperceptible to the human eye.
     \item These examples suggest that even models that have very good test set performance do not have a deep understanding of the underlying concepts that determine the correct output label.
     \end{itemize}
    \end{frame}
    
  \begin{frame} {Adversarial Examples}
    \begin{itemize}
      \item Adversarial examples are \textit{not} unique to deep neural nets. Many other models (such as logistic regression) are also susceptible to them.
      \item They pose serious security concerns in many areas.
      \item Example: Fooling autonomous cars into thinking that a stop sign is a 45 km/h sign.
      \item Example: Evading law enforcement by fooling facial recognition systems into misidentifying individuals.
   \end{itemize}
 \end{frame}
 
 \begin{frame} {Adversarial Examples}
  \begin{figure}
    \centering
       \captionsetup{font=footnotesize,labelfont=footnotesize, labelfont = bf}
      \scalebox{1}{\includegraphics{plots/dog.png}}
      \tiny{\\Credit: Maxence Prevost}
      \caption{\footnotesize The difference between the left and the right golden retriever is unperceptible to humans.
      The last image is considered as a plane by a ResNet50 with higher than 99\% confidence. 
      The only difference between the left and right image are small pixel perturbations, showed in the second picture.}
  \end{figure}
\end{frame}

\begin{frame} {Adversarial Examples}
  \begin{figure}
    \centering
       \captionsetup{font=footnotesize,labelfont=footnotesize, labelfont = bf}
      \scalebox{1}{\includegraphics{plots/glasses.png}}
      \tiny{\\Credit: Sharif et al.}
      \caption{\footnotesize A CNN misidentified each person in the top row (with the funky looking "adversarial" glasses) as the one in the corresponding position in the bottom row. The generated images do often contain some features of the target class.}
  \end{figure}
\end{frame}

\begin{frame} {Adversarial Examples}
  \begin{figure}
    \centering
       \captionsetup{font=footnotesize,labelfont=footnotesize, labelfont = bf}
      \scalebox{0.8}{\includegraphics{plots/easyfooled.png}}
      \tiny{\\Credit: Nguyen et al.}
      \caption{\footnotesize All 8 images above are unrecognizable to humans but are misclassified by a CNN trained on the ImageNet 2012 dataset with higher than 99\% confidence.  The CNN was trained by Krizhevsky et al. and consists of five convolutional layers, some of which are followed by max-pooling layers,and three fully-connected layers with a final softmax.}
  \end{figure}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Creation of Adversarial Examples}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe} {Creation of Adversarial Examples}
  \begin{itemize}
  \item Let us see how we can create adversarial examples for one datapoint $\xv$. 
    %\item The Fast Gradient Sign Method (FGSM) is a very simple way to generate adversarial examples.
    \item Our goal is to find an input $\tilde{\xv}$ near a datapoint $\xv$ such that a pretrained neural net (which accurately classifies $\xv$), ends up misclassifying $\tilde{\xv}$ even though $\tilde{\xv}$ is visually indistinguishable from $\xv$ to human beings.
    \item When we train a neural network, we typically want to optimize the parameter $\thetab$, so that we minimize the loss. 
    \item By contrast, to find an adversarial example $\tilde{\xv}$ that is in the vicinity of $\xv$, we know want to optimize $\xv$ to maximize the loss.    \framebreak
    \item To ensure that $\tilde{\xv}$ is close to $\xv$, we optimize over the perturbation of $\xv$, denoted as $\deltab$, and define an allowable set of perturbations $\Delta$. 
    \begin{equation*}
        \argmax_{\deltab \in \Delta} L(y, \hat{f}(\xv + \deltab| \thetab))
    \end{equation*}
    \item A common perturbation set is $\mathcal{B}^{\infty}_{\epsilon}$ which is the $\epsilon$-ball measured by $\ell_{\infty} = \|\cdot\|_{\infty}$.
    \begin{equation*}
        \Delta = \mathcal{B}^{\infty}_{\epsilon}(\deltab) = \{\deltab: ||\deltab||_{\infty} \le \epsilon\} \text{ with } ||\deltab||_{\infty} = \max_i|\delta_i|
    \end{equation*}
    It allows each component of the perturbation $\delta$ to lie between  $-\epsilon$ and $+ \epsilon$.
    \item In practice, $\Delta$ can also depend on the input data point $\xv$, denoted as $\Delta(\xv)$.
  \end{itemize}
\end{vbframe}

\begin{frame} {Example: ResNet50}
\begin{itemize}
     \item  Using gradient descent, we compute the gradient of the loss with respect to $\deltab$ and we move with a step size of $\alpha$ \textbf{in the direction} of this gradient. 
     \item In each step of gradient descent we clip the corresponding values of $\delta$ outside $[-\epsilon, \epsilon]$ back to $\pm \epsilon$. This procedure is also called \textbf{projected gradient descent}. 
     \end{itemize}
  \begin{figure}
   \captionsetup{font=footnotesize,labelfont=footnotesize, labelfont = bf}
    \centering
      \scalebox{0.9}{\includegraphics{plots/pig.png}}
     \tiny{\\Credit: Kolter \& Madry (2019)}
     \vspace{-0.2cm}
      \caption{Adversarial example for one datapoint of the ImageNet dataset and pre-trained ResNet50. By adding an imperceptibly small perturbation to the original image,  an image was created that looks identical to our original image, but is missclassified.}
  \end{figure}
\end{frame}

 \begin{frame} {Fast Gradient Sign Method}
    \begin{itemize}
   
    \item Let $\hat{\theta}$ be the (fixed) parameters of a pretrained model, $x$ the input to the model, $y$ the target and $L\left(y, f(x | \hat{\theta}) \right)$ the loss function used to train the network.
    \item The FGSM algorithm:
      \begin{itemize}
        \item Computes:
      \begin{equation*}
        \eta = \epsilon sign(\nabla_x L\left(y, f(x | \hat{\theta}) \right))
      \end{equation*}
        \item Adds $\eta$ to $x$: $\tilde{x} = x + \eta = x + \epsilon sign(\nabla_x L\left(y, f(x | \hat{\theta}) \right))$
        \end{itemize}
    \item Implicitly, we want to constrain the size of the "step" that we take in the direction of the gradient (because we don't want the adversarial image $\tilde{x}$ to look too different from $x$).
    \item The (element-wise) $sign$ function is simply a way to enforce this constraint. It basically ensures that no single pixel can change by more than $\epsilon$. 
  \end{itemize}
\end{frame}

\begin{frame} {Adversarial Examples: FGSM example}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/fsgm.png}}
%      \tiny{\\Credit: Goodfellow}
      \caption{\footnotesize By adding an imperceptibly small vector whose elements are equal to the sign of the elements of the gradient of the cost function with respect to the input, GoogLeNet's classification of the image was changed from 'panda' to 'gibbon'. In this example, the $\epsilon$ is 0.007.}
  \end{figure}
\end{frame}


\begin{frame}{Targeted Attacks}
    \begin{itemize}
        \item It is also possible to generate adversarial examples classified virtually as any class desire. This is known as a \textbf{targeted attack}.
        \item The only difference is that instead of trying to just maximize the loss of the correct class, we maximize the loss of the correct class while also minimizing the loss of a target class $y_{target}$.
        \begin{equation*}
             \argmax_{\deltab \in \Delta} L(y, \hat{f}(\xv + \deltab| \thetab)) - L(y_{target}, \hat{f}(\xv + \deltab| \thetab))
        \end{equation*}
    \end{itemize}
\end{frame}
% \begin{frame} {Other forms}
% In practice, adversarial examples of images can be generated not only by making small modifications to the input pixels, but also using spatial transformations.
%         \begin{figure}
%    \captionsetup{font=footnotesize,labelfont=footnotesize, labelfont = bf}
%     \centering
%       \scalebox{0.6}{\includegraphics{plots/adversarials_forms.png}}
%      \tiny{\\Credit: Brown (2018)}
%   \end{figure}
% \end{frame}
  
  
\begin{frame} {Adversarial Examples-in Neural Networks}
  \begin{itemize}
    \item Goodfellow et al. (2014) showed that linear behaviour of models in high dimensional spaces is the reason for the presence of such adversarial examples.
    \item Neural networks are built from "mostly" linear building blocks. In the case of ReLU activations, the mapping from the input image to the output logits (the inputs to the softmax) is a piece-wise linear function.
    \item The value of a linear function changes rapidly if it has many inputs. Specifically, if each input is modified by $\epsilon$, a linear function with weights $\mathbf{w}$ can change by as much as $\epsilon \lVert \mathbf{w} \rVert_1$, which can be very large if $\mathbf{w}$ is high-dimensional.
    \item There is a tradeoff between using models that are easy to train due to their linearity and models that can use non-linear effects to become robust to adversarial perturbations. 
  \end{itemize}
\end{frame}  
\begin{frame} {Adversarial Examples: Audio}
  \begin{figure}
    \centering
      \scalebox{0.8}{\includegraphics{plots/adv_speech.png}}
      \tiny{\\Credit: Carlini et al}
      \caption{\footnotesize  It is possible to add a small perturbation to any waveform in order to fool a speech-to-text neural network into transcribing it as any desired target phrase. (This was not generated using FGSM.)}
  \end{figure}
\end{frame}

\begin{frame} {Adversarial Examples}
  \begin{itemize}
    \item The FGSM method that we've looked at is only one of \textit{many} different algorithms for generating adversarial examples.
    \item Athalye et al. (2017) 3-D printed a turtle that fooled the network into classifying it as a rifle from most angles.
    \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/turtle.png}}
      \tiny{\\Credit: Athalye}
  \end{figure}
\end{itemize}
\end{frame}

\begin{frame} {Adversarial Examples}
  \begin{itemize}
    \item Papernot (2016) discusses ways to fool a classifier even if the model (that is, the network structure and the weights) is unknown. Such methods are called \textbf{black-box methods}.
    \item The library CleverHans can be used to both generate robust adversarial examples and build effective defences against adversarial attacks.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Adversarial  Training}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Adversarial Training}
    \begin{itemize}
        \item In order to modify a trained model in a manner that it is more resistant to such attacks adversarial training can be performed. 
        \lz
        \item Adversarial training can be seen as a form of regularization.
          \lz
        \item The goal is to minimize the \textbf{empirical adversarial risk} which measures the worst-case empirical loss of a model, if we are able to manipulate every input $\xv$ in the training data set within the allowable set $\Delta(\xv)$
        \begin{equation*}
            \min_{\thetab} \risk_{adv}(\thetab) = \min_{\thetab} \frac{1}{N} \sum_{i = 1}^{N} \max_{\deltab \in \Delta(\xv)} L(\yi, f(\xi + \deltab| \thetab))
        \end{equation*}
          
          \framebreak
      
        \item To solve the optimization problem, we use SGD over $\thetab$. In each SGD step $t \in \{1, 2,...\}$ we repeatedly choose a minibatch of size $m$ and repeat the following until a stopping criterion is met: 
        \begin{enumerate}
            \item For each $(\xi, \yi), i = 1, ..., m$, compute an adversarial example 
            \begin{equation*}
                \deltab^{*}(\xi) = \argmax_{\deltab \in \Delta(\xi)} L(\yi, f(\xi + \deltab| \thetab^{[t]}))
            \end{equation*}
            \item Compute the gradient of the empirical adversarial risk given $\deltab^* = (\deltab^*(\xv^{(1)}), ..., \deltab^*(\xv^{(m)})$ and update $\thetab$ 
            \begin{equation*}
                \thetab^{[t+1]} := \thetab^{[t]} - \alpha \frac{1}{m} \sum_{i = 1}^{m} \nabla_{\thetab} L(\yi, f(\xi + \deltab^*(\xi)| \thetab^{[t]}))
            \end{equation*}
        \end{enumerate}
        \item The first step is derived from Danskin's theorem, which states that the gradient of the inner function (maximization term) is simply given by the gradient of the function evaluated at its maximum.
    \end{itemize}
\end{vbframe}

\begin{frame}{Summary}
\begin{itemize}
\item \textbf{Adversarial machine learning} studies technique which attempt  to fool machine learning models through malicious input.
\item  An adversarial example is an input to a model that is deliberately designed to "fool" the network into misclassifying it.
\item There are different techniqies like projected gradient decent or the fast gradient sign method to create adversarial examples for neural networks. 
\item To make a model more robust, we can train our model adversarial examples derived from the training set.  
      %  \item This chapter summarizes  high-level ideas in adversarial robustness. 
        %\item We start with a few remarks on adversarial examples.  
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Linear Models}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{vbframe} {Linear Models}
%  \begin{itemize}
%    \item Let us first consider the case of binary classification using linear models.
%    \item Recall, the hypothesis space for logistic regression consists of models of the form:
%          $$\Hspace = \left\{f: \R^p \to [0, 1] ~\bigg|~ \fx = \tau\left(\sum_{j = 1}^p \theta_j x_j + b\right), \thetab \in \R^p, b \in \R \right\},$$
%          where $\tau(z) = (1 + \exp(-z))^{-1}$ is the logistic sigmoid function. 
%    \item For class labels $y \in \{+1,-1\}$ , the logistic loss is (wrong.fix) :
%          $$\Lxyt = \log[1 + \exp(- {y \fxt}] \equiv \Psi(y \fxt)$$
%          where we define $\Psi(\hidz) = \log(1+\exp(-\hidz))$.
%  \end{itemize}
%
%\framebreak
%
%  \begin{itemize}
%    \item The inner maximization in the adversarial risk which we saw earlier can be written as:
%    $$\max_{\deltab \in \Delta(\xv)} \Lossdelta = \max_{\deltab \in \Delta(\xv)} \Psi(y (\thetab^T(\xv + \deltab) + b)) $$
%    \item In this particular case, it is possible to solve the inner maximization exactly.
%    \item First, note that $\Psi$ is a monotonically decreasing function.
%    \begin{figure}
%    \centering
%      \scalebox{0.5}{\includegraphics{plots/logistic_loss.png}}
%      \tiny{\\Credit: Kolter and Madry}
%    \end{figure}
%  \end{itemize}
%
%\framebreak
%
%  \begin{itemize}
%    \item Maximizing such a monotonically decreasing function is equivalent to minimizing the argument.
%    \lz
%    \item Therefore
%          \begin{align*}
%            \max_{\deltab \in \Delta(\xv)} \Psi(y (\thetab^T(\xv + \deltab) + b)) &= \Psi(\min_{\deltab \in \Delta(\xv)}y (\thetab^T(\xv + \deltab) + b)) \\
%                                                                                    &= \Psi (y (\thetab^T\xv + b) + \min_{\deltab \in \Delta(\xv)} y (\thetab^T \deltab))
%          \end{align*}
%    \lz
%    \item We have to solve the problem
%    $$ \min_{\deltab \in \Delta} y (\thetab^T \deltab) $$
%  \end{itemize}
%
%\framebreak
%
%  \begin{itemize}
%    \item To get a feel for the problem, let us consider the case where $y = +1$ and use $\Delta = \mathcal{B}^{\infty}_{\epsilon}$. The latter constraints each element of $\delta$ to lie between $-\epsilon$ and $+\epsilon$.
%    \lz
%    \item The quantity $y (\thetab^T \deltab)$ is then minimized when $\delta_i = - \epsilon$ for $\theta_{i} \geq 0$ and $\delta_i = \epsilon$ for $\theta_{i}<0$.
%    \lz
%    \item For $y = -1$, the signs would be flipped.
%    \lz
%    \item The optimal solution then, is
%    $$\deltab^* = -y\epsilon \cdot \sign(\thetab) $$
%    \item Notice, that the optimal solution does not depend on $x$. 
%
%\framebreak
%
%  \item The function value acheived by the solution is 
%  $$y \cdot \thetab^T \deltab^* = y \cdot \sum_i -y \epsilon \cdot \sign(\theta_i)\theta_i = - y^2 \epsilon \sum_i \| \theta_i \| = -\epsilon\|\thetab\|_{1} $$
%  \item Therefore, we have analytically computed the solution to the inner maximization problem! The solution is
%  $$ \max_{\deltab \in \Delta(\xv)} \Psi(y (\thetab^T(\xv + \deltab) + b)) =  \Psi (y (\thetab^T(\xv + \deltab)) -\epsilon\|\thetab\|_{1} $$
%  \item As a result, the adversarial risk, which was a min-max problem, has now been converted to a pure minimization problem
%  $$\min_{\thetab} \frac{1}{N} \sum_{i = 1}^{N} \Psi \left(\yi \cdot\left(\thetab^{T} \xi +b\right)-\epsilon\|\thetab\|_{1}\right)$$
%  \item This problem is convex in $\{\thetab, b\}$ and can be solved exactly. An iterative optimizer such as SGD will also approach the global minimum.
%  \end{itemize}
%  
%\end{vbframe}
%
%\begin{vbframe} {MNIST example}
%
%  \begin{itemize}
%    \item As an example we have a look on the MNIST dataset, but this time we only conduct binary classification and focus just on classifying 0s vs. 1s. 
%    \item A logistic regression classifier was trained for 10 epochs with SGD on the training set. The model obtained an error of 0.0009 on the test set.  
%
%  \framebreak
%    \end{itemize}
%    \begin{figure}
%    \centering
%      \scalebox{0.5}{\includegraphics{plots/perturbation.png}}
%      \tiny{\\source: https://adversarial-ml-tutorial.org/}
%    \end{figure}
%
%\end{vbframe}
%
%  \begin{frame} {MNIST example}
%    
%    \begin{figure}
%    \centering
%      \scalebox{0.8}{\includegraphics{plots/perturbed.png}}
%      \tiny{\\source: https://adversarial-ml-tutorial.org/}
%    \end{figure}
%    
%  \end{frame}
%
%  \begin{vbframe} {MNIST example}
%  % \begin{itemize}
%  %   \item Robust training
%  % \end{itemize}
%  % 
%  % \framebreak
%  
%    \begin{figure}
%    \centering
%      \scalebox{0.8}{\includegraphics{plots/rob_perturbation.png}}
%      \tiny{\\source: https://adversarial-ml-tutorial.org/}
%    \end{figure}
%  
%\end{vbframe}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Neural Networks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{frame} {Adversarial Examples-in Neural Networks}
%  \begin{itemize}
%    \item Goodfellow et al. (2014) showed that linear behaviour of models in high dimensional spaces is the reason for the presence of such adversarial examples.
%    \item Neural networks are built from "mostly" linear building blocks. In the case of ReLU activations, the mapping from the input image to the output logits (the inputs to the softmax) is a piece-wise linear function.
%    \item The value of a linear function changes rapidly if it has many inputs. Specifically, if each input is modified by $\epsilon$, a linear function with weights $\mathbf{w}$ can change by as much as $\epsilon \lVert \mathbf{w} \rVert_1$, which can be very large if $\mathbf{w}$ is high-dimensional.
%    \item There is a tradeoff between using models that are easy to train due to their linearity and models that can use non-linear effects to become robust to adversarial perturbations. 
%  \end{itemize}
%\end{frame}

%\begin{frame} {Adversarial Examples}
%  \begin{figure}
%    \centering
%      \scalebox{0.9}{\includegraphics{plots/linear_logit.png}}
%      \tiny{\\Credit: Goodfellow}
%  \end{figure}
%  \footnotesize{The figure shows the result of moving along a single direction (not necessarily axis-aligned) in the input space of a CNN. We begin with an image of an automobile (somewhere in the end of the fifth row) and move an $\epsilon$ amount in this direction (negative $\epsilon$ = opposite direction.). The images in the top half are the result of moving in the "negative $\epsilon$" direction and those in the bottom half are the result of moving in the "positive $\epsilon$" direction.}
%\end{frame}

%\begin{frame} {Adversarial Examples}
%  \begin{figure}
%    \centering
%      \scalebox{0.9}{\includegraphics{plots/linear_logit.png}}
%      \tiny{\\Credit: Goodfellow}
%  \end{figure}
%  \footnotesize{The figure on the right shows the logits (the inputs to the softmax) for each value of $\epsilon$. Each curve is a logit for a specific class. As we move away from the image of the automobile in either direction, the logits for the 'frog' class become extremely high and the images are misclassified by the CNN. The logits for the 'automobile' class are (relatively) high only in the middle of the plot and the CNN correctly classifies these images (yellow boxes on the left).}
%\end{frame}


%  \begin{frame} {Fast Gradient Sign Method}
%    \begin{itemize}
%   
%    \item Let $\hat{\theta}$ be the (fixed) parameters of a pretrained model, $x$ the input to the model, $y$ the target and $L\left(y, f(x | \hat{\theta}) \right)$ the loss function used to train the network.
%    \item The FGSM algorithm:
%      \begin{itemize}
%        \item Computes:
%      \begin{equation*}
%        \eta = \epsilon sign(\nabla_x L\left(y, f(x | \hat{\theta}) \right))
%      \end{equation*}
%        \item Adds $\eta$ to $x$: $\tilde{x} = x + \eta = x + \epsilon sign(\nabla_x L\left(y, f(x | \hat{\theta}) \right))$
%        \end{itemize}
%    \item Implicitly, we want to constrain the size of the "step" that we take in the direction of the gradient (because we don't want the adversarial image $\tilde{x}$ to look too different from $x$).
%    \item The (element-wise) $sign$ function is simply a way to enforce this constraint. It basically ensures that no single pixel can change by more than $\epsilon$. 
%  \end{itemize}
%\end{frame}
%
%\begin{frame} {Adversarial Examples: FGSM example}
%  \begin{figure}
%    \centering
%      \scalebox{1}{\includegraphics{plots/fsgm.png}}
%%      \tiny{\\Credit: Goodfellow}
%      \caption{\footnotesize By adding an imperceptibly small vector whose elements are equal to the sign of the elements of the gradient of the cost function with respect to the input, GoogLeNet's classification of the image was changed from 'panda' to 'gibbon'. In this example, the $\epsilon$ is 0.007.}
%  \end{figure}
%\end{frame}


%\begin{frame} {Adversarial Subspaces}
%  \begin{figure}
%    \center
%      \includegraphics[width = 6.5cm]{plots/adv_cross.png}
%      \tiny{\\Credit: Goodfellow\vspace{0.5cm}}
%  \end{figure}
%  \only<1>{\footnotesize Each square above represents a 2-dimensional cross section of the input space where the center corresponds to a test example (different squares = different test examples). Moving up or down in a given square indicates moving in a random direction that is orthogonal to the direction of the FGSM. White pixels indicate that the classifier outputs the correct label for the corresponding points and the colored pixels indicate that the classifier misclassifies the corresponding points (different colours = different incorrect classes).}
%  \only<2>{The FGSM method identifies a direction such that moving along \textit{any} direction whose unit vector has a large (positive) dot product with the "FGSM vector" results in an adversarial example. Therefore, adversarial examples live in \textbf{linear subspaces} of the input space.}
%\end{frame}
%%
%%\begin{frame} {Adversarial Subspaces}
%%  \begin{figure}
%%    \centering
%%      \scalebox{1}{\includegraphics{plots/rand_cross.png}}
%      \tiny{\\Credit:Goodfellow}
%  \end{figure}
%    For a given input, moving in \textit{completely} random directions is unlikely to result in adversarial examples.
%
%\end{frame}
%
%\begin{frame} {Adversarial Examples}
%  \begin{figure}
%  \captionsetup{font=footnotesize,labelfont=footnotesize, labelfont = bf}
%    \centering
%      \includegraphics[width = 4cm]{plots/wrong_everywhere.png}
%      \caption{\footnotesize Each square above corresponds to Gaussian noise that was run through a Cifar-10 classifier.}
%  \end{figure}
%  \small
%      \only<1>{\small{Instead of measuring the performance of a classifier with respect to the data-generating distribution, if we measure it with respect to a uniform distribution over the whole input space, these classifiers are \textbf{wrong almost everywhere.}}}
%      \only<2>{For all the inputs in the pink boxes, the classifier was reasonably confident (in terms of the softmax values) that the image contained something rather than nothing.}
%      \only<3>{ For the inputs in the yellow boxes, just one step of FGSM was sufficient to convince the model that it was looking at an airplane, specifically.}
%\end{frame}

%\begin{frame} {Adversarial Examples: Audio}
%  \begin{figure}
%    \centering
%      \scalebox{0.8}{\includegraphics{plots/adv_speech.png}}
%      \tiny{\\Credit: Carlini et al}
%      \caption{\footnotesize  It is possible to add a small perturbation to any waveform in order to fool a speech-to-text neural network into transcribing it as any desired target phrase. (This was not generated using FGSM.)}
%  \end{figure}
%\end{frame}
%
%\begin{frame} {Adversarial Examples}
%  \begin{itemize}
%    \item The FGSM method that we've looked at is only one of \textit{many} different algorithms for generating adversarial examples.
%    \item Athalye et al. (2017) 3-D printed a turtle that fooled the network into classifying it as a rifle from most angles.
%    \begin{figure}
%    \centering
%      \scalebox{1}{\includegraphics{plots/turtle.png}}
%      \tiny{\\Credit: Athalye}
%  \end{figure}
%\end{itemize}
%\end{frame}
%
%\begin{frame} {Adversarial Examples}
%  \begin{itemize}
%    \item Papernot (2016) discusses ways to fool a classifier even if the model (that is, the network structure and the weights) is unknown. Such methods are called \textbf{black-box methods}.
%    \item The library CleverHans can be used to both generate robust adversarial examples and build effective defences against adversarial attacks.
%  \end{itemize}
%\end{frame}
%
%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{References}
\begin{vbframe}
\frametitle{References}
\footnotesize{
\begin{thebibliography}{99}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Kolter and Madry (2019)]{1} Zico Kolter and Aleksander Madry (2019)
\newblock Adversarial Robustness - Theory and Practice
\newblock \emph{\url{https://adversarial-ml-tutorial.org/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Goodfellow et al., 2016]{1} Ian Goodfellow, Yoshua Bengio and Aaron Courville (2016)
\newblock Deep Learning
\newblock \emph{\url{http://www.deeplearningbook.org/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Goodfellow et al., 2017]{1}  Ian GoodfellowNicolas PapernotSandy HuangRocky DuanPieter AbbeelJack Clark (2017)
\newblock Attacking Machine Learning with Adversarial Examples
\newblock \emph{\url{https://openai.com/blog/adversarial-example-research/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Hastie et al., 2009]{2} Trevoe Hastie, Robert Tibshirani and Jerome Friedman (2009)
\newblock The Elements of Statistical Learning
\newblock \emph{\url{https://statweb.stanford.edu/\%7Etibs/ElemStatLearn/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Nguyen et al., 2015]{1} Anh Nguyen, Jason Yosinski and Jeff Clune (2015)
\newblock Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images
\newblock \emph{\url{https://arxiv.org/abs/1412.1897}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Krizhevsky et al., 2012]{1} Alex Krizhevsky, Ilya Sutskever and Geoffrey E. Hinto (2012)
\newblock ImageNet Classification with Deep Convolutional Neural Networks. NIPS. 
\newblock \emph{\url{https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Hinton et al., 2012]{3} Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky Ilya Sutskever and Ruslan Salakhutdinov (2012)
\newblock Improving neural networks by preventing co-adaptation of feature detectors
\newblock \emph{\url{http://arxiv.org/abs/1207.0580}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Maxence Prevost (2018)]{1} Maxence Prevost (2018)
\newblock Adversarial ResNet50 
\newblock \emph{\url{http://arxiv.org/abs/1207.0580}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Sharif et al. (2016)]{1} Mahmood Sharif, Sruti  Bhagavatula and Lujo Bauer (2016)
\newblock Accessorize to a Crime: Real and Stealthy Attacks onState-of-the-Art Face Recognition. Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security.
\newblock \emph{\url{https://dl.acm.org/doi/10.1145/2976749.2978392}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Goodfellow et al., 2014]{5} Goodfellow, Shlens (2014)
\newblock Explaining and Harnessing Adversarial Examples
\newblock \emph{\url{https://github.com/maxpv/maxpv.github.io/blob/master/notebooks/Adversarial_ResNet50.ipynb}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Papernot et al., 2016]{5} Papernot , McDaniel, Goodfellow, Jha, Celik, Swamy (2016)
\newblock Practical Black-Box Attacks against Machine Learning
\newblock \emph{\url{https://arxiv.org/abs/1602.02697}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Athalye et al., 2017]{5} Athalye , Engstrom, Ilyas, Kwok (2017)
\newblock Synthesizing Robust Adversarial Examples
\newblock \emph{\url{https://arxiv.org/abs/1707.07397}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Brown et al., 2018]{1} Tom B. Brown and Catherine Olsson, Research Engineers, Google Brain Team (2018)
\newblock 
Introducing the Unrestricted Adversarial Examples Challenge 
\newblock \emph{\url{https://ai.googleblog.com/2018/09/introducing-unrestricted-adversarial.html}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{thebibliography}
}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlecture
