<<setup-child, include = FALSE>>=
    library(knitr)
set_parent("../style/preamble_temp.Rnw")
knitr::opts_chunk$set(cache=TRUE)

@
    

\newcommand{\xv}{\mathbf{x}}

\lecturechapter{10}{Generative Adversarial Networks (GANs)}
\lecture{Deeplearning}

\begin{frame} {Generative Adversarial Networks (GANs)}

Generative adversarial networks
\begin{itemize}
\item define the generative model as in VAEs,
\item but approach problem of learning a directed generative model $p(\mathbf{x}| \mathbf{z})$ from a totally different perspective.

\end{itemize}
\begin{figure}
\includegraphics[width=0.37\framewidth]{plots/VAE.png}
\includegraphics[width=5cm]{plots/VAE-generativeModel2.png}

\end{figure}

\end{frame}

\begin{frame} {What Is a GAN?}
    \begin{figure}
    \centering
    \scalebox{0.75}{\includegraphics{plots/gan.png}}
    \tiny{\\Source: dl4j}
    \end{figure}
    \begin{itemize}
    \item In its simplest form, a GAN consists of two deep neural networks:
    \begin{itemize}
    \item the generator
    \item the discriminator
    \end{itemize}
    \item The generator is fed a random noise vector which it transforms into a fake sample in a given domain.
    \item The discriminator is fed both real and fake samples and outputs a number between 0 and 1 indicating the probability of the input being real.
    \end{itemize}
    \end{frame}
    
    \begin{frame} {What Is a GAN?}
\begin{figure}
\centering
\scalebox{0.5}{\includegraphics{plots/gan.png}}
\tiny{\\Source: dl4j}
\end{figure}
\begin{itemize}
\item The goal of the generator is to fool the discriminator into thinking that the synthesized samples are real.
\vspace{1mm}
\item The goal of the discriminator is to accurately recognize real samples and not be fooled by the generator.
\vspace{1mm}
\item This sets off an arms race. As the generator gets better at producing realistic samples, the discriminator is forced to get better at detecting the fake samples which in turn forces the generator to get even better at producing realistic samples and so on.
\end{itemize}
\end{frame}

\begin{frame} {Fake currency illustration}
\vspace{15mm}
The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistinguishable from the genuine articles. \\
\hspace{45mm} -Ian Goodfellow
\end{frame}


\begin{frame} {Minimax Loss for GANs}
\begin{tcolorbox}
$\min \limits_G \max \limits_D V(D,G) = \E_{\mathbf{x}\sim p_{\text{data}}(\mathbf{x})}[\log D(\mathbf{x})] + \E_{\mathbf{z}\sim p(\mathbf{z})}[\log (1 - D(G(\mathbf{z})))]$
    \end{tcolorbox}
\begin{itemize}
\vspace{6mm}
\item $G(\mathbf{z})$ is the output of the generator for a given state $\mathbf{z}$ of the latent variables.
\vspace{6mm}
\item $D(\mathbf{x})$ is the output of the discriminator for a real sample $\mathbf{x}$.
\vspace{6mm}
\item $D(G(\mathbf{z}))$ is the output of the discriminator for a fake sample $G(\mathbf{z})$ synthesized by the generator.
\end{itemize}
\end{frame}


\begin{frame} {Minimax Loss for GANs}
\begin{tcolorbox}
$\min \limits_G \max \limits_D V(D,G) = \E_{\mathbf{x}\sim p_{\text{data}}(\mathbf{x})}[\log D(\mathbf{x})] + \E_{\mathbf{z}\sim p(\mathbf{z})}[\log (1 - D(G(\mathbf{z})))]$
    \end{tcolorbox}
\begin{itemize}
\vspace{6mm}
\item $p_{\text{data}}(\mathbf{x})$ is our target, the data distribution.
\vspace{6mm}
\item Recall that we defined the generator to be a neural network mapping a latent random vector $\mathbf{z}$ to generated samples $G(\mathbf{z})$. So even if the generator is a determinisic function, we have random outputs, i.e. variability. 
%\item Because a neural network (the generator) is a deterministic function, we feed a latent random vector $\mathbf{z}$ to the generator to induce variability in its outputs.
\vspace{6mm}
\item $p(\mathbf{z})$ is usually a uniform distribution or an isotropic Gaussian. It is typically fixed and not adapted during training.

\end{itemize}
\end{frame}


\begin{frame} {Minimax Loss for GANs}
\begin{tcolorbox}
$\min \limits_G \max \limits_D V(D,G) = \E_{\mathbf{x}\sim p_{\text{data}}(\mathbf{x})}[\log D(\mathbf{x})] + \E_{\mathbf{z}\sim p(\mathbf{z})}[\log (1 - D(G(\mathbf{z})))]$
    \end{tcolorbox}
\begin{itemize}
\item Roughly speaking, $\E_{\mathbf{x}\sim p_{\text{data}}(\mathbf{x})}[\log D(\mathbf{x})]$ is the log-probability of correctly classifying real data points as real. 
\vspace{2mm}
\item $\E_{\mathbf{z}\sim p(\mathbf{z})}[\log (1 - D(G(\mathbf{z})))]$ is the log-probability of correctly classifying fake samples as fake.
\vspace{2mm}
\item Therefore, with each gradient update, the discriminator tries to push $D(\mathbf{x})$ toward 1 and $D(G(\mathbf{z})))$ toward 0. This is the same as maximizing V(D,G).
\vspace{2mm}
\item The generator, on the other hand, only has control over $D(G(\mathbf{z}))$ and tries to push that toward 1 with each gradient update. This is the same as minimizing V(D,G).
\end{itemize}
\end{frame}

\begin{frame} {GAN training: Pseudocode}
\begin{algorithm}[H]
\footnotesize
\caption{Minibatch stochastic gradient descent training of GANs}
The number of steps $k$ to apply to the discriminator is a hyperparameter.
\begin{algorithmic}[1]
\For{number of training iterations}
\For{k steps}
\State Sample minibatch of $m$ noise samples $\{\mathbf{z}^{(1)} \ldots \mathbf{z}^{(m)}$\} from the noise prior $p(\mathbf{z})$
    \State Sample minibatch of $m$ examples $\{\mathbf{x}^{(1)} \ldots \mathbf{x}^{(m)}$\} from the data \item[] \hspace{0.8 cm}
generating distribution $p_{\text{data}}(\mathbf{x})$.
\State Update the discriminator by ascending its stochastic gradient: \item[]
\hspace{2.5 cm}          $\nabla_{{\theta}_d} \frac {1}{m} \sum \limits_{i=1} \limits^{m} \left [ \log D(\mathbf{x}^{(i)}) + \log (1 - D(G(\mathbf{z}^{(i)}))) \right]$
    % + \log (1 - D(G(\mathbf{z}^{(i)})))}\right]$
    \EndFor
\State Sample minibatch of $m$ noise samples $\{\mathbf{z}^{(1)} \ldots \mathbf{z}^{(m)}$\} from the noise prior $p(\mathbf{z})$
    \State Update the generator by descending its stochastic gradient: \item[]
\hspace{2.5 cm}       $\nabla_{{\theta}_g} \frac {1}{m} \sum \limits_{i=1} \limits^{m} \log (1 - D(G(\mathbf{z}^{(i)})))$
    \EndFor
\end{algorithmic}
\end{algorithm}
\end{frame}

\begin{frame} {GAN training: Illustration}
\begin{figure}
\centering
\scalebox{0.75}{\includegraphics{plots/gan_training.png}}
\tiny{\\Source: Pascual et al (2017)}
\end{figure}
\begin{itemize}
\item For $k$ steps, G's parameters are frozen and one performs \textbf{gradient ascent} on D to increase its accuracy.
\item Finally, D's parameters are frozen and one performs \textbf{gradient descent} on G to increase its generation performance. %/ to make D misclassify.
\item Note, that G gets to peek at D's internals (from the back-propagated errors) but D  does not get to peek at G.
\end{itemize}
\end{frame}

\begin{frame} {Adversarial Training}
\begin{itemize}
\vspace{12mm}
\item So, GANs have intuitive appeal and produce state-of-the-art results but the reason they are such an important breakthrough is that they introduce a whole new paradigm to training deep neural networks: Adversarial Training$^1$.
\vspace{4mm}
\item Compared to plain-vanilla gradient descent, %we'll see throughout the lecture that
adversarial training is a whole different beast; one that we have not figured out how to tame (yet).
\vspace{4mm}
\item To illustrate this, let us look at a simple example.
\vspace{6mm}
\end{itemize}
\tiny{1. The term 'adversarial training' is used in slightly different contexts in the community. We have already encountered a variant of adversarial training in a previous lecture.}

\end{frame}

\begin{frame} {Adversarial Training: A Toy Example}
\begin{figure}
\centering
\scalebox{0.3}{\includegraphics{plots/adv_xy.png}}
\end{figure}
\begin{itemize}
\item Consider the function $f(x,y) = xy$, where $x$ and $y$ are both scalars.
\item Player A can control $x$ and Player B can control $y$.
    \item The loss:
    \begin{itemize}
\item Player A: $L_{A}(x,y) = xy$
    \item Player B: $L_{B}(x,y) = -xy$
    \end{itemize}
\item This can be rewritten as $L(x,y) = \min \limits_x \max \limits_y xy$
    \item What we have here is a simple zero-sum game with its characteristic minimax loss.
\end{itemize}
\end{frame}

\begin{frame} {Possible behaviour \#1: Convergence}
    \begin{figure}
    \centering
    \scalebox{0.3}{\includegraphics{plots/adv_xy.png}}
    \end{figure}
    \begin{itemize}
    \item The partial derivatives of the losses are:
        \begin{equation*}
    \frac {\partial{L_{A}}}{\partial x} = y \text{ , }
    \frac {\partial{L_{B}}}{\partial y} = -x
    \end{equation*}
    \item In adversarial training, both players perform gradient descent on their respective losses.
    \item To perform simultaneous gradient descent, we update $x$ with $x - \alpha \cdot y$ and $y$ with $y + \alpha \cdot x$ simultaneously in one iteration, where $\alpha$ is the learning rate.
    \end{itemize}
    \end{frame}
    
\begin{frame} {Possible behaviour \#1: Convergence}
  \begin{figure}
    \centering
      \scalebox{0.3}{\includegraphics{plots/adv_xy.png}}
  \end{figure}
  \begin{itemize}
    \item The partial derivatives of the losses are:
     \begin{equation*}
       \frac {\partial{L_{A}}}{\partial x} = y \text{ , }
       \frac {\partial{L_{B}}}{\partial y} = -x
     \end{equation*}
    \item In adversarial training, both players perform gradient descent on their respective losses.
    \item To perform simultaneous gradient descent, we update $x$ with $x - \alpha \cdot y$ and $y$ with $y + \alpha \cdot x$ simultaneously in one iteration, where $\alpha$ is the learning rate.
  \end{itemize}
\end{frame}

\begin{frame} {Possible behaviour \#1: Convergence}
  \begin{figure}
    \centering
      \scalebox{0.3}{\includegraphics{plots/adv_xy.png}}
  \end{figure}
  \begin{itemize}
    \item In order for simultaneous gradient descent to converge to a fixed point, both gradients have to be simultaneously 0.
    \item They are both (simultaneously) zero only for the point (0,0).
    \item This is a saddle point of the function $f(x,y) = xy$.
    \begin{itemize}
       \item The fixed point for a minimax game is typically a saddle point.
      \item Such a fixed point is an example of a Nash equilibrium.
    \end{itemize}
    \item In adversarial training, convergence to a fixed point is \textbf{not} guaranteed.
  \end{itemize}
\end{frame}

\begin{frame} {Possible behaviour \#2: Chaotic behaviour}
  \begin{figure}
    \centering
      \scalebox{0.6}{\includegraphics{plots/sim_grad.png}}
      \tiny{\\Credit: Lilian Weng}
      \caption{\footnotesize A simulation of our example for updating x to minimize xy and updating y to minimize -xy. The learning rate $\alpha$ = 0.1. With more iterations, the oscillation grows more and more unstable.}
  \end{figure}
  \begin{itemize}
    \item Once $x$ and $y$ have different signs, every following gradient update causes huge oscillation and the instability gets worse in time, as shown in the figure.
  \end{itemize}
\end{frame}


\begin{frame} {Possible behaviour \#3: Cycles}
  \begin{figure}
    \centering
      \scalebox{0.3}{\includegraphics{plots/adv_cycle.png}}
      \tiny{\\Credit: Goodfellow}
      \caption{\footnotesize Simultaneous gradient descent with an infinitesimal step size can result in a circular orbit in the parameter space.}
  \end{figure}
  \begin{itemize}
    \item A Discrete Example: A never-ending game of Rock-Paper-Scissors where player A chooses 'Rock' $\rightarrow$ player B chooses 'Paper' $\rightarrow$ A chooses 'Scissors' $\rightarrow$ B chooses 'Rock' $\rightarrow$ ...
   % \item In GAN training, this happens when the generator keeps switching the category of the samples generated without a noticeable improvement in image quality for any category.
   \item  \textbf{Takeaway:} Adversarial training is highly unpredictable. It can get stuck in cycles or become chaotic.
  \end{itemize}
\end{frame}


\begin{frame} {Non-stationary loss surface}
   \begin{itemize}
     \item Once again, it is extremely important to note that from the perspective of one of the players, the loss surface changes every time the other player makes a move.
  \vspace{2mm}
     \item This is in stark contrast to the (full batch) gradient descent case where the loss surface is stationary no matter how many iterations of gradient descent are performed.
%  \vspace{2mm}
   %  \item It's \textit{somewhat} analogous to a game of football played by two players where the goalposts for a given player changes position every time the opponent kicks the ball and vice versa!
%   \vspace{2mm}
 %   \item The only way this game ends(\textit{if} it ends, that is) is if the ball goes through both players' goalposts simultaneously.
%   \vspace{2mm}
%    \item This is what a saddle point represents. It is simultaneously a minimum for the first player and a maximum for the second player.

   \end{itemize}
 \end{frame}

\begin{frame} {Illustration of Convergence}
  \begin{figure}
    \centering
      \scalebox{0.8}{\includegraphics{plots/illus_conv_one.png}}
      \tiny{\\Credit: Mark Chang}
  \end{figure}
\end{frame}

\begin{frame} {Illustration of Convergence: Final Step}
  \begin{figure}
    \centering
      \scalebox{0.8}{\includegraphics{plots/illus_conv_two.png}}
      \tiny{\\Credit: Mark Chang}
  \end{figure}
  Such convergence is not guaranteed, however.
\end{frame}

%\begin{frame} {Toy Example: Estimating a 1D Gaussian}
%  \vspace{10mm}
%  \begin{itemize}
%    \item Our target distribution is a simple Gaussian with mean 4 and standard deviation of 0.5.
%    \begin{figure}
%    \centering
%      \scalebox{0.66}{\includegraphics{plots/toy_gaussian.png}}
%      \tiny{\\Source: Aylien}
%  \end{figure}
%  \end{itemize}
%\end{frame}
%
%\begin{frame} {Toy Example: Estimating a 1D Gaussian}
%  \begin{itemize}
%    \item Our generator
%      \begin{itemize}
%        \item Takes a scalar noise input
%        \item Has only a single hidden layer with a softplus activation
%        \item The output layer has only a single neuron with a linear activation
%      \end{itemize}
%  \vspace{4mm}
%    \item Our discriminator
%      \begin{itemize}
%        \item Has 3 hidden layers with 'tanh' activations
%        \item The output layer once again has a single neuron with a sigmoid activation
%      \end{itemize}
%  \vspace{4mm}
%    \item Loss: Non-saturating Loss
%  \vspace{4mm}
%    \item Optimizer: Gradient Descent with exponential learning rate decay
%  \end{itemize}
%\end{frame}
%
%\begin{frame} {Toy Example: Estimating a 1D Gaussian}
%
%  \begin{itemize}
%  \item After training, the the two distributions look like this:
%  \begin{figure}
%    \centering
%      \scalebox{0.75}{\includegraphics{plots/gan_gaussian_post.png}}
%      \tiny{\\Source: Aylien}
%  \end{figure}
%  \item This makes sense intuitively.If the generator just produces the mean value of the real data in this simple example, then it is going to be quite likely to fool the discriminator.
%  \end{itemize}
%\end{frame}



\begin{frame} {Divergence measures}
  \begin{itemize}
    \item Recall that the goal of generative modeling is to learn $p_{\text{data}}(\mathbf{x})$.
    \vspace{2mm}
    \item In order to understand the differences between different generative models, it is sometimes helpful to consider them from the angle of \textbf{divergence measures}.
    \vspace{2mm}
    \item A divergence measure quantifies the distance between two distributions. It is a measure of how different one distribution is from another.
    \vspace{2mm}
    \item There are many different divergence measures that one can use here (such as the Kullback-Leibler divergence).
    \vspace{2mm}
    \item One thing that all such measures have in common is that they are 0 if and only if the two distributions are equal to each other (otherwise, they are all positive).
  \end{itemize}
\end{frame}

\begin{frame} {Divergence measures}
  \begin{itemize}
    \small{\item One approach to training generative models, then, is to explicitly minimize the distance between $p_{\text{data}}(\xv)$ and the model distribution $p_{\theta}(\xv)$ according to some divergence measure.
    \vspace{2mm}
    \item If our generator has the capacity to model $p_{\text{data}}(\xv)$ perfectly, the choice of divergence does not matter much because they all achieve their minimum (that is 0) when $p_g(\xv) = p_{\text{data}}(\xv)$.
    \vspace{2mm}
    \item However, it is not likely that that the generator, which is  parametrized by the weights of a neural network, is capable of perfectly modelling an arbitrary $ p_{\text{data}}(\xv)$.
    \vspace{2mm}
    \item In such a scenario, the choice of divergence measure matters, because the parameters that miniminize the various divergence measures differ.}
  \end{itemize}
\end{frame}

\begin{frame} {Implicit Divergence measure of GANs}
  \begin{itemize}
   \item GANs do not explicitly minimize any divergence measure.
   \item However, (under some assumptions!) optimizing the minimax loss is equivalent to implicitly minimizing a divergence measure.
    \item That is, if the optimal discriminator is found in every iteration,  the generator minimizes %a divergence measure between distributions known as
    the  \textbf{Jensen-Shannon divergence  (JSD)} (theorem and proof are given by the original GAN paper (Goodfellow et al, 2014)):
    \begin{align*}
      JS(p_{\text{data}}||p_g) & =  \frac{1}{2} KL(p_{\text{data}}||\frac{p_{\text{data}}+p_g}{2}) + \frac{1}{2}  KL(p_g || \frac {p_{\text{data}} + p_g}{2}) \\
        KL(p_{\text{data}}||p_g) & = E_{\xv \sim p_{\text{data}}(\xv)}[log \frac {p_{\text{data}}(\xv)}{p_g(\xv)}]
    \end{align*}
 % \vspace{2mm}
%    \item A generator with sufficient capacity successfully learns the target distribution
 % \vspace{2mm}
 %   \item Otherwise, it exhibits "mode-seeking" behaviour.
%  \vspace{2mm}
%    \item Researchers originally speculated that this behaviour is the reason GANs produce stunningly realistic samples.
  \end{itemize}
\end{frame}


\begin{frame} {Optimal Discriminator}

  For G fixed, the optimal discriminator $D^*_G$ is:
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics[width=7cm]{plots/opt_discriminator.png}}
      \tiny{\\Credit: Mark Chang}
  \end{figure}
   \begin{itemize}
  \item  The optimal discriminator returns a value greater than 0.5 if the probability to come from the data ($p_{data}(x)$)  is larger than the probability to come from the generator ($p_g(x)$).
  \end{itemize}
\end{frame}

\begin{frame} {Optimal Discriminator}

  For G fixed, the optimal discriminator $D^*_G$ is:
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics[width=7cm]{plots/opt_discriminator.png}}
      \tiny{\\Credit: Mark Chang}
  \end{figure}
  \begin{itemize}
  \item  Note: The optimal solution is almost never found in practice, since the discriminator has a finite capacity and is trained on a finite amount of data.
  \item Therefore, the assumption needed to guarantee that the generator minimizes the JSD does usually not hold in practice.
  \end{itemize}
\end{frame}

%
\begin{frame} {Trade-offs made by some common divergences}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/div_tradeoffs.png}}
      \tiny{\\Source: Theis et al. 2016}
      \caption{\small: An isotropic Gaussian distribution was fit to data drawn from a mixture of Gaussians by either minimizing Kullback-Leibler divergence (KLD), maximum mean discrepancy (MMD), or
Jensen-Shannon divergence (JSD). The different fits demonstrate different tradeoffs made by the
three measures of distance between distributions.}
  \end{figure}
\end{frame}


\begin{frame} {Trade-offs made by some common divergences}

  \begin{figure}
    \centering
      \scalebox{0.8}{\includegraphics{plots/implicit_div.png}}
      \tiny{\\Credit: Aiden NIbali}
  \end{figure}
  \begin{itemize}
    \item In this simplified 1-dimensional example, $p_{\text{data}}(x)$ is a bimodal distribution, but $p_{\theta}(x)$ only has the modelling capacity of a single Gaussian.
    \vspace{2mm}
    \item Therefore, based on the divergence measure, $p_{\theta}(x)$ can either fit a single mode really well, i.e.~be `mode-seeking' (e.g.~JSD), or attempt to cover both modes, i.e. be `mode-covering' (e.g.~KLD).
  \end{itemize}
\end{frame}

\begin{frame} {Non-Saturating Loss}
  \begin{figure}
    \centering
      \scalebox{0.82}{\includegraphics{plots/ns_loss.png}}
      \tiny{\\Credit: Daniel Seita}
      \caption{\footnotesize Various generator loss functions ($J^{(G)}$).}
  \end{figure}
  \begin{itemize}
    \item It was discovered that a relatively strong discriminator could completely dominate the generator.
    \item %The reason for this is that
   When optimizing the minimax loss, as the discriminator gets good at identifying fake images, i.e.~as $D(G(\mathbf{z}))$ approaches 0, the gradient with respect to the generator parameters vanishes.

  \end{itemize}
\end{frame}

\begin{frame} {Non-Saturating Loss}
  \begin{figure}
    \centering
      \scalebox{0.82}{\includegraphics{plots/ns_loss.png}}
      \tiny{\\Credit: Daniel Seita}
      \caption{\footnotesize Various generator loss functions ($J^{(G)}$).}
  \end{figure}
  \begin{itemize}
    \item Solution: Use a non-saturating generator loss instead:  $J^{(G)} = - \frac{1}{2} \E_{\vec z \sim p(\vec z)} [\log D(G(\mathbf{x}))]$
    \item In contrast to the minimax loss, when the discriminator gets good at identifying fake images, the magnitude of the gradient of $J^{(G)}$ increases and the generator is able to learn to produce better images in successive iterations.
  \end{itemize}
\end{frame}
\begin{frame} {Other loss functions}

Various losses for GAN training with different properties have been proposed:

  \vspace{10mm}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/other_losses.png}}
      \tiny{\\Source: Lucic et al. 2016}
  \end{figure}
\end{frame}


\begin{frame} {Explosive growth in the number of (named) GAN papers}

Understanding and improving GAN training is a very active area of research.

  \begin{figure}
    \centering
      \scalebox{0.8}{\includegraphics{plots/named_gans.png}}
      \tiny{\\Credit: hindupuravinash}
  \end{figure}
\end{frame}



\begin{frame} {Conditional GANs: Motivation}
  \begin{itemize}
    \item In an ordinary GAN, the only thing that is fed to the generator are the latent variables $\mathbf{z}$.
    \item A conditional GAN allows you to condition the generative model on additional variables. %have more control over the samples produced by the generator. This makes it very easy to work with multiple modalities.
    \item E.g. a generator conditioned on text input (in addition to $\mathbf{z}$) can be trained to generate the image described by the text.
  \end{itemize}
\end{frame}

\begin{frame} {Conditional GANs: Architecture}
  \begin{figure}
    \centering
      \scalebox{0.75}{\includegraphics{plots/cgan_arch.png}}
      \tiny{\\Credit: Guim Perarnau}
  \end{figure}
  \begin{itemize}
    \item In a conditional GAN, additional information in the form of vector $\vec y$  is fed to both the generator and the discriminator.
    \item $\vec z$  can then encode all  variations in $\vec z$ that are not encoded by $\vec y$.
    \item E.g.~ $\vec y$ could encode the class of a hand-written number (from 0 to 9). Then,  $\vec z$ could encode  the style of the number (size, weight, rotation, etc).
  \end{itemize}
\end{frame}

%\begin{frame} {Conditional GANs: Loss}
%\end{frame}

\begin{frame} {Conditional GANs: Example}
  \vspace{10mm}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/cgan_mnist.png}}
      \tiny{\\Source: Mirza et al. 2014}
      \caption{\footnotesize When the model is conditioned on a one-hot coded class label, it generates random images that belong (mostly) to that particular class. The randomness here comes from the randomly sampled $\mathbf{z}$. (Note : $\mathbf{z}$ is implicit. It is not shown above.)}
  \end{figure}
\end{frame}

\begin{frame} {Conditional GANs: More Examples}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/congan.png}}
       \tiny{\\Source: Isola et al. 2016}
       \caption{\footnotesize Conditional GANs can translate images of one type to another. In each of the 4 examples above, the image on the left is fed to the network and the image on the right is generated by the network.}
 \end{figure}
\end{frame}


\begin{frame} {More Generative Models}
  \begin{itemize}
   \vspace{8mm}
   \item Today, we learned about two kinds of (directed) generative models:
      \begin{itemize}
      \vspace{-0.4cm}
        \item Variational Autoencoders (VAEs)
        \item Generative Adversarial Networks (GANs).
      \end{itemize}
   \item There are other interesting generative models, e.g.:
      \begin{itemize}
        \item autoregressive models
        \item restricted Boltzmann machines.
      \end{itemize}
    \item Note:
      \begin{itemize}
        \item It is important to bear in mind that generative models are not a solved problem.
        \item There are many interesting hybrid models that combine two or more of these approaches.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Acknowledgements}

Big thanks to
\begin{itemize}
\item \textbf{Christian Igel} (University of Copenhagen) for providing his material on graphical models
\item \textbf{Aaron Courville} (Univerite de Montreal) for providing his material on VAEs
\end{itemize}
which helped creating this lecture.

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}
\frametitle{References}
\footnotesize{
\begin{thebibliography}{99}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \bibitem[Zhang et al., 2017]{1} Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris Metaxas (2017)
% \newblock StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks
% \newblock \emph{\url{https://arxiv.org/abs/1612.03242}}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \bibitem[Wang et al., 2017]{2} Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, Bryan Catanzaro (2017)
% \newblock High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs
% \newblock \emph{\url{https://arxiv.org/abs/1711.11585}}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Goodfellow et al., 2014]{5} Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio (2014)
\newblock Generative Adversarial Networks
\newblock \emph{\url{https://arxiv.org/abs/1406.2661}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Pascual et al., 2017]{6} Santiago Pascual, Antonio Bonafonte, Joan Serra (2017)
\newblock SEGAN: Speech Enhancement Generative Adversarial Network
\newblock \emph{\url{https://arxiv.org/abs/1703.09452}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Goodfellow, 2016]{7} Ian Goodfellow (2016)
\newblock NIPS 2016 Tutorial: Generative Adversarial Networks
\newblock \emph{\url{https://arxiv.org/abs/1701.00160}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Weng, 2017]{8} Lilian Weng (2017)
\newblock From GAN to WGAN
\newblock \emph{\url{https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Chang, 2016]{9} Mark Chang (2016)
\newblock Generative Adversarial Networks
\newblock \emph{\url{https://www.slideshare.net/ckmarkohchang/generative-adversarial-networks}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Theis et al., 2016]{10} Lucas Theis, Aaron van den Oord, Matthias Bethge (2016)
\newblock A note on the evaluation of generative models
\newblock \emph{\url{https://arxiv.org/abs/1511.01844}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Nibali, 2016]{11} Aiden Nibali (2016)
\newblock The GAN objective, from practice to theory and back again
\newblock \emph{\url{https://aiden.nibali.org/blog/2016-12-21-gan-objective/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Mirza, 2014]{12} Mehdi Mirza, Simon Osindero (2014)
\newblock Conditional Generative Adversarial Nets
\newblock \emph{\url{https://arxiv.org/abs/1411.1784}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Isola et al., 2016]{13} Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros (2016)
\newblock Image-to-Image Translation with Conditional Adversarial Networks
\newblock \emph{\url{https://arxiv.org/abs/1611.07004}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Perarnau, 2017]{14} Guim Perarnau (2017)
\newblock Fantastic GANs and where to find them
\newblock \emph{\url{https://guimperarnau.com/blog/2017/03/Fantastic-GANs-and-where-to-find-them}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{thebibliography}
}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlecture
