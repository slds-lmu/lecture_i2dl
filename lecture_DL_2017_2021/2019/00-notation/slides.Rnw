%Blank Lecture
%This file is a child of preamble.Rnw in the style folder
%if you want to add stuff to the preamble go there to make
%your changes available to all childs


<<setup-child, include = FALSE>>=
  library(knitr)
set_parent("../style/preamble.Rnw")
@

\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-nn}
  
\lecturechapter{0}{Notation}
\lecture{Deeplearning}
  
\begin{vbframe}{Fundamental definitions and notation}

\begin{itemize}
\item $\Xspace$, $p$-dim. input space, usually we assume $\Xspace = \R^p$, but
categorical features can occur, too
\item $\Yspace$, target space,  \\
e. g. $\Yspace = \R$, $\Yspace = \lbrace 0, 1 \rbrace$, $\Yspace = \lbrace -1, 1 \rbrace$, $\Yspace = \gset$, $\Yspace = \lbrace \textrm{label}_1, \ldots, \textrm{label}_g \rbrace$
  \item $\xb = \xvec \in \Xspace$, observation
\item $y \in \Yspace$, dependent variable (target, label, output)
\item $\P_{xy}$, joint probability distribution on $\Xspace \times \Yspace$
  \item $\pdfxy$ or $\pdfxyt$, joint probability density function (pdf) for $\xb$ and $y$
  \end{itemize}

% \remark{
%   This lecture is mainly developed from a frequentist perspective. If parameters appear behind the |, this is
%   for better reading, and does not imply that we condition on them in a Bayesian sense (but this notation
%                                                                                         would actually make a Bayesian treatment simple).
%   So formally, $p(x | \theta)$ should be read as $p_\theta(x)$ or $p(x, \theta)$ or $p(x; \theta)$.
% }

\remark{
  Formally, $p(\xb ~|~ \thetab)$ should be read as $p_\theta(\xb)$ or $p(\xb; \thetab)$ (In other words, we adopt the frequentist perspective instead of a Bayesian one).
}


\framebreak

\begin{itemize}
\item $\D = \Dset$, data set with $n$ observations
\item $\xyi$, $i$-th observation
\item $\Dtrain$, $\Dtest$, data for training and testing, often $\D = \Dtrain \dot{\cup} ~ \Dtest$
  \item $\fx$ or $\fxt \in \R$, prediction function learnt on data, we might suppress $\thetab$ in notation
\item $\hx$ or $h(\xb ~|~\thetab) \in \Yspace$, discrete prediction function for classification
\item $\thetab \in \Theta$, model parameters\\
(some models may traditionally use different symbols)
\item $\Hspace$, hypothesis space, $f$ lives here, restricts the functional form of $f$
  \item $\eps = y - \fx$ or $\epsi = \yi - \fxi$, residual in regression
\item $\yf$ or $\yfi$, margin for binary classification with  $\Yspace = \{-1, 1\}$
  \end{itemize}

\framebreak

\begin{itemize}
\item $\pikx = \postk$, posterior probability for class $k$, given $x$, in case of binary labels we might abbreviate
$\pix = \post$
  \item $\pi_k = \P(y = k)$, prior probability for class $k$, in case of binary labels we might abbreviate
$\pi = \P(y = 1)$
  \item $\LLt$ and $\llt$, Likelihood and log-Likelihood for a parameter $\thetab$, based on a statistical model
\item $\fh$, $\hh$, $\pikxh$, $\pixh$ and $\thetah$, learned functions and parameters
  \item $\text{diag}(a)$, diagonal matrix ($a$ on the diagonal)
  \item $\Omega(\thetab)$, parameter norm penalty
  \item $\lambda$, regularization coefficient
  \item $\nabla \fx$, gradient of $\fx$
  \item $\alpha$, learning rate/step-size

% \item $\phi(x | \mu, \sigma^2)$, $\Phi(x | \mu, \sigma^2)$, pdf and cdf of the univariate normal distribution,
% with $\phi(x)$ and $\Phi(x)$ for the N(0,1) standard normal case,
% and $\phi(x | \mu, \Sigma)$ (or $\phi_p(x)$) for the (standard) multivariate case in $p$ dimensions

\end{itemize}

\remark{
  With a slight abuse of notation we write random variables, e.g. $\xb$ and $y$, in lowercase, as normal
  variables or function arguments. The context will make clear what is meant.
}

\framebreak

% 
% 
% In the simplest case we have i.i.d. data $\D$, where the input and output space are both real-valued and one-dimensional.
% 
% \vspace{0.5cm}
% 
% <<fig.height=4>>=
%   qplot(wt, mpg, data = mtcars) + xlab("x") + ylab("y")
% @
%   
%   \framebreak

Design matrix and intercept term:
  
  \begin{minipage}{0.45\textwidth}
$$
  \Xmat  = \mat{x_1^{(1)} & \cdots & x_p^{(1)} \\
    \vdots    & \vdots & \vdots \\
    x_1^{(n)} & \cdots & x_p^{(n)}}
$$
  \end{minipage}
\begin{minipage}{0.45\textwidth}
$$
  \Xmat  = \mat{1      & x_1^{(1)} & \cdots & x_p^{(1)} \\
    \vdots & \vdots    & \vdots & \vdots \\
    1      & x_1^{(n)} & \cdots & x_p^{(n)}}
$$
  \end{minipage}

\begin{itemize}
\item The right design matrix demonstrates the trick to encode the intercept via an additional
constant-1 feature, so the feature space will be $(p+1)$-dimensional.
This allows to simplify notation, e.g., to write $\fx = \thetab^\top \xb$, instead of $\fx = \thetab^\top \xb + \theta_0$.
\item $\xjb = \xjvec$ j-th observed feature vector.
\item $\ydat = \yvec$ vector of target values.
\end{itemize}

\end{vbframe}

\begin{frame}   {Fundamental definitions and notation}
    \textbf{Neural networks - General}:\\
    \vspace{3mm}
    \makebox[4cm]{$\wtw$}  \hspace{1cm}weights of a (hidden) neuron\par
    \makebox[4cm]{$\Wmat$}   \hspace{1cm}weight matrix with dimensions $p \times m$\par
    \makebox[4cm]{$\wtu$}  \hspace{1cm}weights of an output neuron\par
    \makebox[4cm]{$\bm{U}$}   \hspace{1cm}weight matrix with dimensions $m \times g$\par
    \makebox[4cm]{$b$ and $\biasb$}  \hspace{1cm}bias (hidden layer) \par
    \makebox[4cm]{$c$ and $\mathbf{c}$}  \hspace{1cm}bias (output layer) \par
    \vspace{3mm}
    \makebox[4cm]{$\hidz_{in}$}  \hspace{1cm}vector of "weighted sums" (hidden)\par
    \makebox[4cm]{$\hidz = \hidz_{out} = (z_1, \dots, z_m)^T$}  \hspace{1cm}vector of hidden units\par
    \vspace{3mm}
    \makebox[4cm]{$\sigma$}  \hspace{1cm}activation function (hidden layer)\par
    \makebox[4cm]{$\tau$}  \hspace{1cm}activation function (output layer)\par
    \vspace{3mm}
    \makebox[4cm]{$\mathbf{f}_{in}$}  \hspace{1cm}vector of \enquote{weighted sums} (output)\par
    \makebox[4cm]{$\mathbf{f} = \mathbf{f}_{out} = (f_1, \dots, f_g)^T$}  \hspace{1cm}vector of output units\par
    \vspace{3mm}
    
% z_{in,j}
% z_{out,j}
% \hidz_{in} = (z_{in,1}, \dots, z_{in,m})^T


\end{frame}

\begin{frame}   {Fundamental definitions and notation}
    
    \textbf{RNNs}:\\
    \makebox[4cm]{$\bm{U}$}  \hspace{1cm}matrix of input-to-hidden weights\par
    \makebox[4cm]{$\Wmat$}  \hspace{1cm}matrix of hidden-to-hidden weights\par
    \makebox[4cm]{$\bm{V}$}  \hspace{1cm}matrix of hidden-to-output weights\par
    \makebox[4cm]{$t$}  \hspace{1cm}time-step\par
    \makebox[4cm]{$s^{[t]}$}  \hspace{1cm}LSTM cell state (at time t)\par
    
    \vspace{3mm}
    
    \textbf{Autoencoders}:\\
    \makebox[4cm]{$\bm{r}, \hat{\xb}$}  \hspace{1cm}reconstruction of the input $\xb$\par
    \makebox[4cm]{$\tilde{\xb}$}  \hspace{1cm}noise-corrupted copy for $\xb$\par
    \makebox[4cm]{$C(\tilde{\xb}~|~\xb)$}  \hspace{1cm}conditional distribution of $\tilde{\xb}$ given $\xb$\par
    
    
\end{frame}


\endlecture
