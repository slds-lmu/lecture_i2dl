<<setup-child, include = FALSE>>=
library(knitr)
set_parent("../style/preamble.Rnw")
knitr::opts_chunk$set(cache=TRUE)

@

\lecturechapter{6}{Autoencoders}
\lecture{Deeplearning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Introduction to Autoencoders}
  \begin{itemize}
    \item So far we were dealing with different types of neural networks to infer classification and regression tasks.
    \item In the world of \textbf{supervised learning}, we exploit information of class memberships (or numeric values) to train our algorithm. That means in particular, that we have access to labeled data.
    \item Think of mnist:
    \begin{itemize}
      \item We know the ground truth for each image of our dataset.
      \item Our goal was to train a network that predicts the class labels.
    \end{itemize}
  \end{itemize}
\framebreak
  \begin{itemize}
    \item[] \textbf{Clustering}
  \end{itemize}
  \begin{figure}
    \centering
    \includegraphics[width=6cm]{plots/unsupervised_2.png}
  \end{figure}
\framebreak
  \begin{itemize}
    \item[] \textbf{Dimensionality reduction}
    \item[]
  \end{itemize}
  \begin{minipage}{0.43\textwidth}
    \begin{itemize}
      \item Principle Component Analysis (PCA)
      \item Linear Discriminant Analysis (LDA)
      \item Filter Methods
      \end{itemize}
  \end{minipage}\hfill
    \begin{minipage}{0.53\textwidth}
    \begin{figure}
        \only<1-2>{\includegraphics[width=5.5cm]{plots/unsupervised_3.png}}
    \end{figure}
  \end{minipage}
\framebreak
  \begin{itemize}
    \item The basic idea of an autoencoder is to obtain a neural network, that is able to \textbf{reconstruct its input}.
    \item[] $\Rightarrow$ Traditionally used for dimensionality reduction!
    \item (Very) simple example: given the input vector (1, 0, 1, 0, 0), an autoencoder will try to output (1, 0, 1, 0, 0).
    \item Therefore, we do not need class labels and pass over into the world of \textbf{unsupervised learning}.
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Revision of PCA}
  \begin{itemize}
    \item The purpose of PCA is to project the data $\mathbf{x}^{(1)}, \dots, \mathbf{x}^{(n)}$ onto a lower-dimensional subspace (e.g. to save memory). \\
    \item For each point $\mathbf{x}^{(i)} \in \mathbb{R}^p$ we need to find a corresponding code vector $\mathbf{c}^{(i)} \in \mathbb{R}^l$ with $l < p$. That step is accomplished by the encoding function which produces the code for an input: $$f(\mathbf{x}) = \mathbf{c}$$
    \item  Additionally, we need a decoding function to produce the reconstruction of the input given its code: $$\mathbf{x} \approx g(f(\mathbf{x}))$$
    %\item PCA is then defined by the choice of the encoding and decoding function.
    \item We can choose matrix multiplication to map the data back into $\mathbb{R}^{p}$: $g(\mathbf{c}) = \mathbf{Dc}$, with $\mathbf{D} \in \mathbb{R}^{p \times l}$, defining the decoding. 
  \framebreak
    \item To keep the encoding problem easy, PCA constrains the columns of $\mathbf{D}$ to be orthogonal.
    %\item We begin with the optimal code $\mathbf{c}^{*}$ for each input. We could achieve this by minimizing the distance between the input $\mathbf{x}$ and its reconstruction $g(\mathbf{c})$ (PCA is a linear transformation with minimum reconstruction error)
    \item One way to obtain the optimal code $\mathbf{c}^{*}$ is to minimize the distance between the input $\mathbf{x}$ and its reconstruction $g(\mathbf{c})$ (that means, linear transformation with minimum reconstruction error): 
    $$\mathbf{c}^{*} = \displaystyle\argmin_{\mathbf{c}} ||\mathbf{x} - g(\mathbf{c})||^2_2$$
    \item Solving this optimization problem leads to
    $$\mathbf{c} = \mathbf{D}^T\mathbf{x}$$
    \item Thus, to encode a vector, we apply the encoder function
    $$f(\mathbf{x}) = \mathbf{D}^T \mathbf{x}$$
  \end{itemize}
\framebreak
  \begin{itemize}
    \item We can also define the PCA as the reconstruction operation:
    $$r(\mathbf{x}) = g(f(\mathbf{x})) = \mathbf{DD}^T \mathbf{x}$$  
    \item To find the encoding matrix $\mathbf{D^*}$, we minimize the Frobenius norm of the matrix of errors computed over all dimensions and points:
    $$\mathbf{D^*} = \displaystyle\argmin_{\mathbf{D}} \sqrt{\displaystyle\sum_{i,j} \Big(x^{(i)}_j - r(x^{(i)})_j\Big)^2}, \text{ subject to } \mathbf{D^T}\mathbf{D} = \mathbf{I}_l$$
    \item for $l = 1$, $\mathbf{D^*}$ collapses to a single vector and we can rewrite the equation as
    $$\mathbf{d^*} = \displaystyle\argmin_{\mathbf{d}} ||\mathbf{X} - \mathbf{X}\mathbf{d}\mathbf{d}^T||^2_F, \text{ subjected to } \mathbf{d}^T\mathbf{d} = 1$$
    \item The optimal $\mathbf{d^*}$ is given by the eigenvector of $\mathbf{X}^T\mathbf{X}$ corresponding to the largest eigenvalue.
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{General structure}
  \begin{itemize}
    \item An autoencoder maps an input $x$ to an output $r$ (reconstruction)
    \item We can distinguish its general architecture in the following manner:
    \begin{itemize}
      \item an encoder $z = enc(x)$ and
      \item a decoder $r = dec(z)$ 
    \end{itemize}
    \item[] to generate the reconstruction.
    \item We call $z$ the \textbf{internal representation} or \textbf{code}
  \end{itemize}
\framebreak
  The general structure of an autoencoder:
  \begin{figure}
    \centering
    \includegraphics[width=5.5cm]{plots/autoencoder_basic_structure.png}
  \end{figure}
  \begin{itemize}
    \item An autoencoder has two computational steps:
    \begin{itemize}
      \item the encoder $enc$, mapping $x$ to $z$
      \item the decoder $dec$, mapping $z$ to $r$
    \end{itemize}
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Use case}
  \begin{itemize}
    \item Practitioners utilize autoencoders, although they have access to labeled data.
    \item[] $\Rightarrow$ Why?
    \item Autoencoders may be used for:
    \begin{itemize}
      \item Learning a representation of the input data\\ $\Rightarrow$ dimensionality reduction \& pretraining
      \item Advanced: some variants of the autoencoder can be regarded as generative models \\ $\Rightarrow$ may be used to draw synthetic data
    \end{itemize}
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \frame{
% 
% \frametitle{Autoencoders - use case}
% 
%   \begin{itemize}
% 
%     \only<1-2>{\item Practitioners utilize autoencoders, although they have access to labeled data.}
%     \only<1-2>{\item[] $\Rightarrow$ Why?}
%     \only<2-2>{\item Autoencoders may be used for:}
%     \only<2>{\begin{itemize}
%       \only<2>{\item Learning a representation of the input data\\ $\Rightarrow$ dimensionality reduction \& pretraining}
%       \only<2>{\item Advanced: some variants of the autoencoder can be regarded as generative models \\ $\Rightarrow$ may be used to draw synthetic data}
%     \end{itemize}}
% 
%   \end{itemize}
% }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Undercomplete Autoencoders}
  \begin{itemize}
    \item If an autoencoder simply learns the identity $dec(enc(x)) = x$, it would be of no use. In fact, we want the autoencoder to learn \enquote{useful} or \enquote{significant} properties of the data.
    \item To extract such \enquote{useful} or \enquote{significant} properties, we introduce the \textbf{undercomplete autoencoder}.
    \item Therefore, we restrict the architecture, such that $$dim(z) < dim(x)$$
    \item[] Or in other words: the hidden layer must have fewer neurons than the input layer.
    \item[] $\Rightarrow$ That will force the autoencoder to capture only the most salient features of the training data!
    \item Consequently, the undercomplete autoencoder tries to learn a \enquote{compressed} representation of the input.
  \end{itemize}
\framebreak
  \begin{itemize}
    \item Learning such a net is done by minimizing a loss function $$\uauto$$ which penalizes the autoencoder $dec(enc(x))$ for being distinct of $x$.
    \vspace{0.5cm}
    \begin{itemize}
      \item Typical choice: MSE
    \end{itemize}
    \item[]
    \item To carry out training, the very same optimization techniques of the previous chapters are applied (stochastic gradient descent, ..)
    \item[]
    \item How could a potential architecture of an undercomplete autoencoder look like for our (very) simple example?
    \item[] Reminder: $x = (1, 0, 1, 0, 0)$
  \end{itemize}
\framebreak
  \begin{figure}
    \centering
    \includegraphics[width=6.5cm]{plots/autoencoder_simple_example.png}
    \caption{Potential architecture. On a first glimpse, the architecture looks just like a standard feed forward neural network.}
  \end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example}
  \begin{itemize}
    \item Let us try to compress the mnist data as good as possible.
    \item Therefore, we will fit an undercomplete autoencoder to learn the best possible representation,
    \item[] $\Rightarrow$ as few as possible dimensions in the internal representation $h$.
  \end{itemize}
  \begin{figure}
    \centering
    \includegraphics[width=11cm]{plots/autoencoder_mnist_problem.png}
    \caption{Flow chart of our our autoencoder: reconstruct the input with fixed dimensions $dim(z) << dim(x)$}
  \end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example}
  \begin{figure}
    \centering
    \includegraphics[width=6.5cm]{plots/autoencoder_mnist_example.png}
    \caption{Architecture of the autoencoder.}
  \end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Example}

<<mxnet1, size = "footnotesize", cache = TRUE, eval = FALSE, echo = TRUE>>=
z = c(784, 256, 64, 32, 16, 8, 4, 2, 1)

input = mx.symbol.Variable("data") # mnist with 28x28 = 784
encoder = mx.symbol.FullyConnected(input, num_hidden = z[i])
decoder = mx.symbol.FullyConnected(encoder, num_hidden = 784)
activation = mx.symbol.Activation(decoder, "sigmoid")
output = mx.symbol.LinearRegressionOutput(activation)

model = mx.model.FeedForward.create(output,
  X = train.x, y = train.x,
  num.round = 50, 
  array.batch.size = 32,
  optimizer = "adam",
  initializer = mx.init.uniform(0.01), 
  eval.metric = mx.metric.mse
)
@

\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{

\frametitle{Example}
  
  \center
  \begin{figure}
  
    \only<1>{\includegraphics[width=11.2cm]{plots/784.png}}%
    \only<2>{\includegraphics[width=11.2cm]{plots/256.png}}%
    \only<3>{\includegraphics[width=11.2cm]{plots/64.png}}%
    \only<4>{\includegraphics[width=11.2cm]{plots/32.png}}%
    \only<5>{\includegraphics[width=11.2cm]{plots/16.png}}%
    \only<6>{\includegraphics[width=11.2cm]{plots/8.png}}%
    \only<7>{\includegraphics[width=11.2cm]{plots/4.png}}%
    \only<8>{\includegraphics[width=11.2cm]{plots/2.png}}%
    \only<9>{\includegraphics[width=11.2cm]{plots/1.png}}%
    \caption{The top row shows the original digits, the bottom row the reconstructed ones.}
    
  \end{figure}
  
  \vspace{-0.3cm}
  
  \begin{itemize}
  
    \only<1>{\item $dim(z) = 784$ (identity)}
    \only<2>{\item $dim(z) = 256$}
    \only<3>{\item $dim(z) = 64$}
    \only<4>{\item $dim(z) = 32$}
    \only<5>{\item $dim(z) = 16$}
    \only<6>{\item $dim(z) = 8$}
    \only<7>{\item $dim(z) = 4$}
    \only<8>{\item $dim(z) = 2$}
    \only<9>{\item $dim(z) = 1$}
    
  \end{itemize}
  
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Undercomplete Autoencoders}
  \begin{itemize}
    \item If our decoder is linear and the loss function the mean squared error, the undercomplete autoencoder learns to span the same subspace as principal component analysis.
    \item Autoencoders with nonlinear encoder and nonlinear decoder functions can learn more powerful gerneralizations of PCA.
    \item Overcomplete autoencoders without \textbf{regularization} make no sense. Even a linear encoder and linear decoder will just learn to copy the complete input layer and thus, extract no meaningful features.
    \item[] $\Rightarrow$ we will introduce variants of \textbf{regularized} autoencoders, including \textbf{sparse} and \textbf{denoising} autoencoders.
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{Sparse autoencoder}
%   \begin{itemize}
%     \item The goal of a sparse autoencoder is to have as many inactive neurons in the hidden layer as possible.
%     \item That means in particular, if we use the sigmoidal activation function, we want each neuron to be
%     \begin{itemize}
%       \item active, if its output is close to $1$ and
%       \item inactive, if its output is close to $0$
%     \end{itemize}
%     \item The average activation of hidden unit $j$ (over the training set) is given by $$\hat\sigma = \frac{1}{m} \displaystyle\sum_{i=1}^N (\sigma_j(x^{(i)}))$$ 
%     \item We would like to impose a \textbf{sparsity parameter} $\rho$ on the average activation of each hidden unit, such that $$\hat\sigma \leq \rho$$ 
%     \item To obtain alot of inactive units in the hidden layer, the value for $\rho$ must be set close to $0$ (e.g. $0.05$).
%     \item Furthermore, we need a penalty term $\Omega(h)$ for our internal representation, that measures how distinct $\hat\sigma$ and $\rho$ are.
%     \item A common variant is the Kullback Leibler Divergence: $$KL(\rho||\hat\sigma) = \displaystyle\sum_{j=1}^{M} \rho \ log \ \frac{\rho}{\hat\sigma} + (1 - \rho) \ log \ \frac{(1 - \rho)}{(1 - \hat\sigma)}$$
%     \item Thus, our new objective evolved to $$\uauto + \Omega(h)$$
%     \item That is equivalent to a feedforward network with regularization (chapter 2) whose task is to copy the input to the output.
%     \item An autoencoder that has been regularized to be sparse must respond to unique statistical features of the dataset, rather than simply acting as an identity function.
% \framebreak
%     \item One nice way to view the sparse autoencoder model is to think of a \enquote{Jack of all trades}-person.
%     \begin{itemize}
%       \item If a person is able to exert many jobs, then in general, he or she is not a master of those. On the other hand, if someone does only one or two jobs, he or she is alot more likely to be a specialist.
%       \item Similarly, if a neuron is forced to activate for any input, even if those are enormously different, then that unit will most likely not work well for all samples.
%     \end{itemize}
%     \item A typical use case for sparse autoencoders is to learn features for another objective (like classification tasks).
%   \end{itemize}
% \end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Denoising autoencoder (DAE)}
  \begin{itemize}
    \item A denoising autoencoder minimizes $$\dauto$$ where $\tilde{x}$ is a copy of x, which has been corrupted by noise.
    \item Denoising autoencoders must therefore undo this corruption rather than simply copying their input. Training forces f and g to implicitly learn the structure of $p_{data}(x)$. Thus, the denoising autoencoder is a stochastic version of the autoencoder!
    \item We introduce a corruption process $C(\tilde{x}|x)$ to represent the conditional distribution of corrupted samples $\tilde{x}$, given a data sample $x$.
    \item The autoencoder then learns the reconstruction distribution $p_{reconstruct}(x|\tilde{x})$ estimated from training pairs $(x, \tilde{x})$.
  \end{itemize}
\framebreak
  The general structure of a denoising autoencoder:
  \begin{figure}
    \centering
    \includegraphics[width=4.5cm]{plots/denoising_autoencoder_basic_structure.png}
    \caption{Denoising autoencoder: \enquote{idea of making the learned representation
robust to partial corruption of the input pattern}}
  \end{figure}
\framebreak
  \begin{algorithm}[H]
    \caption{Training denoising autoencoders}
    \begin{algorithmic}[1]
    \State Sample a training example $x$ from the training data.
    \State Sample a corrupted version $\tilde{x}$ from $C(\tilde{x}|x)$
    \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Use $(x, \tilde{x})$ as a training example for estimating the autoencoder reconstruction $p_{reconstruct}(x|\tilde{x}) = p_{decoder}(x|z)$, where \\ 
    - $h$ is the output of the encoder $enc(\tilde{x})$ and \\
    - $p_{decoder}$ defined by a decoder $dec(z)$}
    \end{algorithmic}
  \end{algorithm}
  \begin{itemize}
    \item So the denoising autoencoder still tries to preserve the information about the input (encode it), but also to undo the effect of a corruption process!
    \item All we have to do to transform a autoencoder to a denoising autoencoder is to add a stochastic corruption process on the input.
  \end{itemize} 
\framebreak
  \begin{figure}
    \centering
    \includegraphics[width=6.5cm]{plots/denoising_autoencoder.png}
    \caption{Denoising autoencoders \enquote{manifold perspective} (Ian Goodfellow et al. (2016))}
  \end{figure}
  \begin{itemize}
    \item A denoising autoencoder is trained to map a corrupted data point $\tilde{x}$ back to
the original data point $x$.
    \item The corruption process $C(\tilde{x}|x)$ is displayed by the gray circle of equiprobable corruptions
  \end{itemize}
\framebreak
  \begin{itemize}
    \item When the denoising autoencoder is trained to minimize the MSE $||dec(enc(\tilde{x}) - x||^2$, the reconstruction $dec(enc(\tilde{x}))$ estimates $$\mathbb{E}_{x,\tilde{x}\sim p_{data}(x)C(\tilde{x}|x)}[x|\tilde{x}]$$
    \item The vector $dec(enc(\tilde{x})) - \tilde{x}$ points approximately towards the nearest point in the manifold, since $dec(enc(\tilde{x}))$ estimates the center of mass of clean points $x$ which could have given rise to $\tilde{x}$.
    \item Thus, the autoencoder learns a vector field $dec(enc(x)) - x$ indicated by the green arrows.
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example}
  \begin{itemize}
    \item We will now corrupt the mnist data with gaussian noise and then try to denoise it as good as possible.
  \end{itemize}
  \begin{figure}
    \centering
    \includegraphics[width=11cm]{plots/denoised_autoencoder_mnist_problem.png}
    \caption{Flow chart of our our autoencoder: denoise the corrupted input}
  \end{figure}  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Example}
  \begin{itemize}
    \item To corrupt the input, we randomly add or subtract values from a uniform distribution to each of the image entries.
  \end{itemize}
  \begin{figure}
    \centering
    \includegraphics[width=9cm]{plots/mnist_noise.png}
    \caption{Top row: original data, bottom row: corrupted mnist data}
  \end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Example}

<<mxnet2, size = "footnotesize", cache = TRUE, eval = FALSE, echo = TRUE>>=
z = c(1568, 784, 256, 64, 32, 16, 8)

input = mx.symbol.Variable("data") # mnist with 28x28 = 784
encoder = mx.symbol.FullyConnected(input, num_hidden = z[i])
decoder = mx.symbol.FullyConnected(encoder, num_hidden = 784)
activation = mx.symbol.Activation(decoder, "sigmoid")
output = mx.symbol.LinearRegressionOutput(activation)

model = mx.model.FeedForward.create(output,
  X = train.x, y = train.x,
  num.round = 50, 
  array.batch.size = 32,
  optimizer = "adam",
  initializer = mx.init.uniform(0.01), 
  eval.metric = mx.metric.mse
)
@

\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{

\frametitle{Example}
  
  \center
  \begin{figure}

    \only<1>{\includegraphics[width=10.7cm]{plots/denoised_1568_2.png}}%  
    \only<2>{\includegraphics[width=10.7cm]{plots/denoised_784_2.png}}%
    \only<3>{\includegraphics[width=10.7cm]{plots/denoised_256_2.png}}%
    \only<4>{\includegraphics[width=10.7cm]{plots/denoised_64_2.png}}%
    \only<5>{\includegraphics[width=10.7cm]{plots/denoised_32_2.png}}%
    \only<6>{\includegraphics[width=10.7cm]{plots/denoised_16_2.png}}%
    \only<7>{\includegraphics[width=10.7cm]{plots/denoised_8_2.png}}%
    \caption{The top row shows the original digits, the intermediate one the corrupted and the bottom row the denoised/recontructed digits (prediction).}
    
  \end{figure}
  
  \vspace{-0.3cm}
  
  \begin{itemize}
  
    \only<1>{\item $dim(z) = 1568$ (overcomplete)}
    \only<2>{\item $dim(z) = 784$ (identity)}
    \only<3>{\item $dim(z) = 256$}
    \only<4>{\item $dim(z) = 64$}
    \only<5>{\item $dim(z) = 32$}
    \only<6>{\item $dim(z) = 16$}
    \only<7>{\item $dim(z) = 8$}
    
  \end{itemize}
  
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{

\frametitle{Example}

  \begin{itemize}
  
    \only<1>{\item Let us increase the amount of noise and see how the autoencoder with $dim(z) = 64$ deals with it (for science!)}
    \only<2>{\item Alot of noise}
    \only<3>{\item \textbf{Alot} of noise}
    
  \end{itemize}
  
  \center
  \begin{figure}

    \only<2>{\includegraphics[width=10.7cm]{plots/denoised_alot_64.png}}%  
    \only<3>{\includegraphics[width=10.7cm]{plots/denoised_alot2_64.png}}%
    
  \end{figure}
  
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Convolutional autoencoder (ConvAE)}
  \begin{itemize}
    \item In the last example, we have seen that autoencoder inputs are images. So, it makes sense to ask whether a convolutional architecture can work better than the autoencoder architectures discussed previously.
    \item The encoder consists of convolutional layers, the decoder on the other hand of transpose convolution layers (sometimes mistakenly called deconvolution layers) or simple upsampling operations.
    \begin{itemize}
      \item Instead of convolving a filter mask with an image, to get a set of activations as in a convolutional neural net, we are trying to infer the activations that when convolved with the filter mask, would yield the image.
    \end{itemize}
    \item The original aim was to learn a hierarchy of features in an unsupervised manner. However, now its more commonly being used to invert the downsampling that takes place in a convolutional network and \enquote{expand} the image back to its original size.
  \end{itemize}
\framebreak
  \begin{figure}
    \centering
    \includegraphics[width=11cm]{plots/convolutional_autoencoder3.png}
    \caption{Potential architecture of an convolutional autoencoder}
  \end{figure}
  We now apply this architecture to denoise mnist.
\framebreak
  \begin{figure}
    \centering
    \includegraphics[width=6.3cm]{plots/convolutional_autoencoder2.png}
    \caption{Top row: noised data, second row: auto encoder with $dim(z) = 32$ (roughly 50k params), third row: convolutional autoencoder(roughly 25k params), fourth row: ground truth}
  \end{figure}
\framebreak
  Convolutional autoencoders may also be used for image segmentation:
  \begin{figure}
    \centering
    \includegraphics[width=11cm]{plots/convolutional_autoencoder.png}
    \caption{SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation (Vijay Badrinarayanan et al. (2016))}
  \end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Realworld Application}
  Medical image denoising using convolutional denoising autoencoders
  \begin{figure}
    \centering
    \includegraphics[width=6.5cm]{plots/denoising_autoencoder_application.png}
    \caption{Top row shows real image, second row noisy version, third row results of a (convolutional) denoising autoencoder and fourth row results of a median filter (Lovedeep Gondara (2016))}
  \end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{Contractive autoencoder (CAE)}
%   \begin{itemize}
%     \item Explicit regularizer on the code $h$: $$\Omega(h) = \lambda \Big|\Big|\frac{\delta enc(x)}{\delta x}\Big|\Big|_F^2$$
%     \item $\Omega(h)$ is called squared Frobenius norm (sum of squared elements) of the Jacobian matrix of partial derivatives associated with the encoder function.
%   \end{itemize}
% \end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Stacking autoencoders}
  \begin{itemize}
    \item Stacked autoencoder is used to pretrain each layer unsupervised to have better weight initialization for the actual supervised training.
    \item The better weight initialization usually eliminates the risk of vanishing gradients in feed forward nets.
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Variational autoencoder (VAE)}
  \begin{itemize}
    \item VAEs stand for Variational Autoencoders. By its name, you can probably tell that a VAE is pretty similar to an autoencoder, and it technically is, but with one major twist.
    \item While an autoencoder just has to reproduce its input, a variational autoencoder has to reproduce its output, \textbf{while keeping its hidden neurons to a specific distribution}.
    \begin{itemize}
      \item That means, the output of the network will have to get used to the hidden neurons output based on a distribution, and so we can generate new images, just by sampling from that distribution, and inputting it into the network's hidden layer.
    \end{itemize}
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}
\frametitle{References}
\footnotesize{
\begin{thebibliography}{99}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Ian Goodfellow et al., 2016]{1} Ian Goodfellow, Yoshua Bengio and Aaron Courville (2016)
\newblock Deep Learning
\newblock \emph{\url{http://www.deeplearningbook.org/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Lovedeep Gondara., 2016]{2} Lovedeep Gondara (2016)
\newblock Medical image denoising using convolutional denoising autoencoders
\newblock \emph{\url{https://arxiv.org/abs/1608.04667}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Vijay Badrinarayanan et al., 2015]{3} Vijay Badrinarayanan, Alex Kendall and Roberto Cipolla (2016)
\newblock SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation
\newblock \emph{\url{https://arxiv.org/abs/1511.00561}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{thebibliography}
}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\endlecture