%Blank Lecture
%This file is a child of preamble.Rnw in the style folder
%if you want to add stuff to the preamble go there to make
%your changes available to all childs


<<setup-child, include = FALSE>>=
library(knitr)
set_parent("../style/preamble.Rnw")
@

\lecturechapter{2}{Regularization}
\lecture{Deeplearning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Revision of overfitting}
  \begin{itemize}
    \item A model finds a pattern in the data that is actually not true in the real world. That means the model \textbf{overfits} the data.
      \begin{itemize}
        \item Humans also overfit when they overgeneralize from an incomplete picture of the world.
        \item Every powerful model can \enquote{hallucinate} patterns.
      \end{itemize}
    \item Happens when you have too many hypotheses and not enough data to tell them apart.
    \begin{itemize}
      \item The more data, the more \enquote{bad} hypotheses are eliminated.
      \item If the hypothesis space is not constrained, there may never be enough data.
      \item There is often a parameter that allows you to constrain (\textbf{regularize}) the model.
    \end{itemize}
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Avoiding overfitting}
  \begin{itemize}
    \item You should never believe your model until you've \textit{verified it on data that it didn't see}.
    \item Scientific method applied to machine learning: model must make new predictions that can be experimentally verified.
    \item Randomly divide the data into:
      \begin{itemize}
        \item \textit{Training set} $\Dtrain$, which we will feed the model with.
        \item \textit{Test set} $\Dtest$, which we will hide to verify its predictive performance.
      \end{itemize}
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Overfitting and noise}
  \begin{itemize}
    \item Overfitting is seriously exacerbated by \textit{noise} (errors in the training data).
    \item An unconstrained learner will model that noise.
    \item A popular misconception is that overfitting is always caused by noise.
    \item It can also arise when relevant features are missing in the data.
    \item In general, it's better to make some mistakes on training data (\enquote{ignore some observations}) than trying to get all correct.
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Triple trade-off}

In all learning algorithms that are trained from example data, there is a trade-off between three factors:

  \begin{itemize}
    \item the complexity of the hypothesis we fit to data
    \item the amount of training data
    \item the generalization error on new examples
  \end{itemize}

The generalization error decreases with the amount of training data.

As the complexity of the hypothesis space $H$ increases, the generalization error decreases first and then starts to increase (overfitting).

\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}
  \begin{itemize}
    \item Training error and test error evolve in the opposite direction with increasing complexity:
  \end{itemize}
  \begin{figure}
    \centering
      \includegraphics[width=10cm]{plots/overfitting_2.png}
      \caption{Underfitting vs. overfitting (Goodfellow et al. (2016))}
  \end{figure}
  
\vspace{-0.2cm}
$\Rightarrow$ Optimization regarding the model complexity is desirable!
  
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Generalization Error}

The \textit{generalization error} is defined as the error that occurs when a model $\fh_{\D}$ that was trained on observed data $\D$ is applied to (unseen) data:

  $$\GE{\D} = \E( L(y, \fh_{\D}(x)) | \D),$$

  where

\begin{itemize}
  \item $\fh_{\D}$ is the prediction model that was estimated using the data $\D$,
  \item the expectation is conditional on $\D$ that was used to build the prediction model and
  \item $L$ is an \textit{outer} loss function that tries to measure the model performance
  (which can be different from the \textit{inner} loss function that was used for the empirical risk minimization).
\end{itemize}

\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe} {Bias-Variance decomposition}

% Let's take a closer look at the generalized prediction error, given by
%   $$\GE{\D} = \E( L(y, \fh_{\D}(x)) | \D).$$
% Assuming the normal linear regression model
%   $$y = f(x) + \epsilon$$
% with $\epsilon \sim N(0, \sigma^2)$, we get
% \begin{eqnarray*}
%   \GE{\D} &=& \E( L(y, \fh_{\D}(x)) | \D) = \E((y - \fh_{\D}(x))^2) \\
%   &=& \E(y^2) + \E(\fh_{\D}(x)^2) - \E(2y\fh_{\D}(x)) \\
%   &=& \var(y) + \E(y)^2 + \var(\fh_{\D}(x)) + \E(\fh_{\D}(x))^2 - 2f(x)\E(\fh_{\D}(x)) \\
%   &=& \var(y) + \var(\fh_{\D}(x)) + \E(f(x)-\fh_{\D}(x))^2 \\
%   &=& \sigma^2 + \var(\fh_{\D}(x)) + \text{Bias}(\fh_{\D}(x))^2.
% \end{eqnarray*}
% 
% \framebreak

% So for the squared error loss, the generalized prediction error can be decomposed into:
% \begin{itemize}
%   \item \textbf{Noise}: Intrinsic error, independent from the learner, cannot be avoided.
%   \item \textbf{Variance}: Models tendency to learn random things irrespective of the real signal (\emph{overfitting}).
%   \item \textbf{Bias}: Models tendency to \emph{consistently} misclassify certain instances (\emph{underfitting}).
% \end{itemize}
% 
% \framebreak

\begin{columns}[T,onlytextwidth]
  \column{0.6\textwidth}
    \includegraphics{plots/biasvariance.png}
  \vspace{0.05cm}
  \\
  Reduce variance $\Rightarrow$ Reduce overfitting \\ $\Rightarrow$ make model less flexible \\ $\Rightarrow$ \textbf{regularization}, or add more data.
  
  \column{0.38\textwidth}
  \vspace{1cm}
  Reduce bias \\ $\Rightarrow$ reduce underfitting \\$\Rightarrow$ make model more flexible.
\end{columns}

\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Overfitting in regression}
<<echo=FALSE, warning=FALSE, message=FALSE, results="hide">>=

library("mlr")
set.seed(1337L)
n = 50L
x = sort(10 * runif(n))
y = sin(x) + 0.2 * rnorm(x)
df = data.frame(x = x, y = y)
tsk = makeRegrTask("sine function example", data = df, target = "y")
plotLearnerPrediction("regr.nnet", tsk, size = 1L, maxit = 1000)
plotLearnerPrediction("regr.nnet", tsk, size = 5L, maxit = 1000)
plotLearnerPrediction("regr.nnet", tsk, size = 11L, maxit = 1000)

@
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Overfitting in classification}
  \begin{columns}[T,onlytextwidth]
    \column{0.5\textwidth}
      \begin{itemize}
        \item Non-overfitting model
      \end{itemize}
      \vspace{0.5cm}
<<echo=FALSE, results='hide', fig.height=6.5>>=
library("mlr")
library("mlbench")
library("BBmisc")
library("MASS")
set.seed(21L)
Sigma = matrix(c(1,0,0,1), nrow = 2)
 d1 = mvrnorm(n = 50, c(3,3), Sigma, tol = 1e-6, empirical = FALSE, EISPACK = FALSE)
 d2 = mvrnorm(n = 50, c(1,1), Sigma, tol = 1e-6, empirical = FALSE, EISPACK = FALSE)
d1 = as.data.frame(d1)
d1$classes = "3"
d2 = as.data.frame(d2)
d2$classes = "1"
data = rbind(d1,d2)
#data = as.data.frame(mlbench.2dnormals(n = 75, cl = 3))
#data$classes = mapValues(data$classes, "3", "1")
colnames(data) = c("x1","x2","class")
data$class
data$class = as.factor(data$class)
task = makeClassifTask(data = data, target = "class")
lrn = makeLearner("classif.nnet")

plotLearnerPrediction("classif.nnet", task, size = 5L, maxit = 1000, pointsize = 4)

@
      \begin{itemize}
        \item[] Better test accuracy
      \end{itemize}
    \column{0.5\textwidth}
      \begin{itemize}
        \item Overfitting model
      \end{itemize}
      \vspace{0.5cm}
<<echo=FALSE, results='hide', fig.height=6.5>>=

plotLearnerPrediction("classif.nnet", task, size = 50L, maxit = 1000, pointsize = 4)

@
      \begin{itemize}
        \item[] Better training accuracy
      \end{itemize}
  \end{columns}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Parameter Norm Penalties}
  \begin{itemize}
    \item Norm penalization aims to limit the complexity of the model.
    \lz
    \item Suppose we would like to regularize an objective function $\Ounreg$, which generally is a loss function \Lxy. 
    \item By adding a parameter norm penalty term $\Pen$, we obtain a regularized version of the objective function:
    
      $$\Oreg = \Ounreg + \lambda \Pen$$
      
      with hyperparamater $\lambda \in [0, \infty)$, that weights the penalty term,
      relative to the unconstrained objective function $\Ounreg$.

    \lz
    \item Declaring $\lambda = 0$ obviously results in no penalization. 
    \item We can choose between different parameter norm penalties $\Pen$.
\framebreak
    \item[]
    \item Keep in mind, when penalizing a linear model, the parameters $\theta$ are the coefficients $\beta$.
    \item In neural networks, the parameters $\theta$ are the weights (e.g. $w$).
    \item In general, we do not penalize the bias.
      \begin{itemize}
        \item Less data required for fitting the bias than for $w$.
        \item A possible consequence of regularizing the bias is underfitting!
      \end{itemize}
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{L2 Regularization (Weight decay)}
  \begin{itemize}
    \item Analogue to \textbf{ridge regression}: $\Omega(w) = \frac{1}{2}||w||_2^2$ such that 
    $$ \Oregweight = \Oweight + \frac{\lambda}{2} w^Tw $$
    with corresponding gradient: 
    $$ \triangledown_w \Oregweight = \triangledown_w \Oweight + \lambda w $$
    \item One weight update using gradient descent is
      \begin{eqnarray*}
        w_{i+1} &=& w_i - \alpha (\lambda w_i + \triangledown_w \Oweighti) \\
              &=& \underbrace{(1 - \alpha \lambda)}_{<1}w - \alpha \triangledown_{w_i} \Oweighti)
      \end{eqnarray*}
    \item Therefore termed \textbf{weight decay} in neural net applications
  \end{itemize}
%\framebreak
% \begin{enumerate}
%   \item Quadratic Taylor-approximation of unregularized $\Oweight$ at minimum $w^* = argmin_w \Oweight$:
%     \begin{eqnarray*}
%       \Oopt &=& \Oweightopt \\
%              &+& \frac{1}{1!}\frac{\delta \Oweightopt}{\delta w} (w - w^*) \\
%              &+& \frac{1}{2!}\frac{\delta^2 \Oweightopt}{\delta w \delta w} (w - w^*)^2 \\
%              &=& \Oweightopt + \frac{1}{2} (w - w^*)^T H (w - w^*)
%     \end{eqnarray*}
%     with $H$ being the positive-semidefinite Hessian $\Oweight$ at $w^*$.
%     \item The minimum of $\Oopt$ where its first derivative equals 0:
%     $$ \triangledown_w \Oopt = H(w - w^*) = 0 $$
% \framebreak
%     \item Transform into regularized version:
%       \begin{eqnarray*}
%         H(\tilde{w} - w^*) + \alpha \tilde{w} &=& 0 \\ 
%         \Leftrightarrow (H + \alpha I) \tilde{w} &=& Hw^* \\
%         \tilde{w} &=& (H + \alpha I)^{-1} H w^*
%       \end{eqnarray*}
%     \item Now apply eigendecomposition, such that $H=Q\Lambda Q^T$ (H is real and symmetric);
%       \begin{eqnarray*}
%         \tilde{w} &=& Q(\Lambda + \alpha I)^{-1} \Lambda Q^T w^* \\
%               &\Rightarrow& \text{ collapses to } w^* \text{ for } \alpha = 0
%       \end{eqnarray*}
%       With
%       \begin{itemize}
%         \item $\Lambda = diag(\lambda_1, \dots, \lambda_d)$ and $\lambda_i$ the eigenvalue $i$ of $H$.  
%         \item $Q$ the orthonormal basis of eigenvectors $v_i, i = 1, \dots, d$
%       \end{itemize}
% \framebreak
%     \item Effect on weight i:
%       \begin{eqnarray*}
%         \tilde{w_i} &=& v_i \frac{\lambda_i}{\lambda_i + \alpha} v_i^T w_i^* \\
%         &\overset{v_i v_i^T = 1}{=}& \frac{\lambda_i}{\lambda_i + \alpha} w_i^* \\
%         &\Rightarrow& \text{ \textbf{rescaling} of } w^* \text{ with eigenvalues of the Hessian }
%       \end{eqnarray*}
%       \begin{itemize}
%         \item For $\lambda_i$ we can think of a parameter describing the \enquote{importance} of each weight $w_i$.
%         \item If $\lambda_i > \alpha$: high curvature $\rightarrow$ important weight $\rightarrow$ low shrinkage of update
%         \item If $\lambda_i < \alpha$: low curvature $\rightarrow$ less important weight $\rightarrow$ heavy shrinkage
%       \end{itemize}
% \end{enumerate}
\framebreak
  \begin{figure}
    \centering
      \includegraphics[width=12cm]{plots/weight_decay_2.png}
  \end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Weight decay example}
  \begin{itemize}
    \item Let's fit the following huge neural network on a smaller fraction of the mnist data:
  \end{itemize}
  \begin{figure}
    \centering
      \includegraphics[width=10cm]{plots/mxnet_regulariation_1.png}
  \end{figure}
  \begin{itemize}
    \item That is to say 5000 training and 1000 testing samples with evenly distributed class labels.
    \item We try out different values of weight decay: $$\alpha \in (0.01, 0.001, 0.0001, 0.00001, 0)$$
  \end{itemize}
  
\framebreak

<<fig.height=5.5>>=
require("ggplot2")

wdTrain = read.csv("code/mnist_weight_decay_wdTrain", header = TRUE)
options(scipen=999)
wdTrain$variable = factor(wdTrain$variable)

ggplot(data = wdTrain, aes(x = epoch, y = value, colour = variable)) +
  geom_line() +
  scale_y_continuous(name = "train error", limits = c(0, 0.1)) + 
  scale_x_continuous(labels = function (x) floor(x), 
    name = "epochs") + 
  labs(colour = "weight decay")
@

\framebreak

<<fig.height=5.5>>=
wdTest = read.csv("code/mnist_weight_decay_wdTest", header = TRUE)
options(scipen=999)
wdTest$variable = factor(wdTest$variable)

ggplot(data = wdTest, aes(x = epoch, y = value, colour = variable)) +
  geom_line() +
  scale_y_continuous(name = "test error", limits = c(0, 0.1)) + 
  scale_x_continuous(labels = function (x) floor(x), 
    name = "epochs") + 
  labs(colour = "weight decay")
@

\framebreak

  \begin{itemize}
    \item The misclassification accuracy of a neural network can still improve even if we reach 100\% training accuracy.
    \item Consider a binary classification problem with labels 0 and 1.
    \begin{itemize}
      \item Assume a training sample with label 0 is assigned output probabilities $[0.75, 0.25]$ by the neural net.
      \item Thus, the training error with respect to this point it 0, because the predicted label is 0.
      \item Applying the negative log-likelihood (cross entropy) as loss function may still lead to improvements in the neural networks \enquote{confidence}, even after training error reaches 0\% (i.e.$[0.9, 0.1]$).
    \end{itemize}
    \item Note that this is a theoretical concept and in practice we will almost always overfit by the time we reach 100\% training accuracy.
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{L1 Regularization}
%   \begin{itemize}
%     \item Analogue to \textbf{lasso regression}: $\Pen = ||w||_1$ such that
%       $$\Oregweight = \Oweight + \alpha ||w||_1$$
%     leading to gradient:
%       $$\triangledown_w \Oregweight = \triangledown_w \Oweight + \alpha sign(w)$$
%     \item Harder penalization: shrinks weights \textbf{and} sets some to 0
%     \item Creates \textbf{sparse} models and is therefore used for \textbf{feature selection}
%   \end{itemize}
% \framebreak
%   \begin{itemize}
%     \item Quadratic Taylor-approximation of $\hat J(\theta)$
%       \begin{itemize}
%         \item Assumption: uncorrelated inputs $\rightarrow H = diag(H_{1,1},...,H_{n,n}), H_{i,i} > 0$
%         \item L1 regularized objective function:
%         $$\Oopt = \Oweightopt + \sum_i  \bigg[ \frac{1}{2}H_{i,i}(w_i - w_i^*)^2 + \alpha|w_i| \bigg] $$
%       \end{itemize}
%     \item Solution of the minimization problem:
%   $$\tilde w_i = sign(w_i^*)max \bigg[ |w_i^*|-\frac{\alpha}{H_{i,i}},0 \bigg]$$
%   with two possible situations given $w_i^* > 0$:
%     \begin{itemize}
%       \item High $H_{i,i} \rightarrow$ high curvature $\rightarrow$ low penalization of  \enquote{necessary} weight $\rightarrow$ weight is just shrinked $\rightarrow 0 < \tilde w_i < w_i^*$
%       \item Low $H_{i,i}\rightarrow$ low curvature $\rightarrow$ hard penalization of \enquote{unnecessary} weight $\rightarrow w_i^* \leq \frac{\alpha}{H_{i,i}} \rightarrow \tilde w_i = 0$
%     \end{itemize}
%   \end{itemize}
% \framebreak
%   \begin{itemize}
%     \item Compare both methods with $\lambda_i=H_{i,i}$ and $ w_i^*>0$:
%     $$L2: \tilde w_{i} = \frac{H_{i,i}}{H_{i,i}+\alpha} w_i^* \text{ }\text{ }\text{ } \text{ vs. } \text{ }\text{ }\text{ } L1: \tilde w_i= max \bigg[ |w_i^*|-\frac{\alpha}{H_{i,i}},0 \bigg]$$
%   \end{itemize}
%   L2-penalization solely shrinks weights towards 0 and L1-penalization furthermore eradicates weights.
%   \begin{figure}
%     \centering
%       \includegraphics[width=5cm]{plots/lasso_ridge.png}
%       \caption{(Hastie et al. (2009))}
%   \end{figure}
% \end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Early Stopping}
  \begin{itemize}
    \item Goal: find optimal number of epochs.
    \item Stop algorithm early, before generalization error increases.
  \end{itemize}
  \begin{figure}
    \centering
      \includegraphics[width=10cm]{plots/overfitting.png}
      \caption{Underfitting vs. overfitting (Goodfellow et al. (2016))}
  \end{figure}
\framebreak
  How early stopping works:
  \vspace{0.3cm}
  \begin{enumerate}
    \item Split training data $(X^{(train)}, y^{(train)})$ into $(X^{(subtrain)}, y^{(subtrain)})$ and $(X^{(validation)}, y^{(validation)})$ (e.g. with a ratio of 2:1).
    \item Use $(X^{(subtrain)}, y^{(subtrain)})$ and evaluate model using the $(X^{(validation)}, y^{(validation)})$.
    \item Stop training when validation error stops decreasing (after a range of \enquote{patience} steps).
    \item Use parameters of the previous step for the actual model.
  \end{enumerate}
  \vspace{0.3cm}
  More sophisticated forms also apply cross-validation.
\framebreak
  \begin{table}
    \begin{tabular}{p{4cm}|p{6cm}}
    Strengths & Weaknesses \\
    \hline
    \hline
    Effective and simple & Periodical evaluation of validation error\\
    \hline
    Applicable to almost any model without adjustment \note{of objective function, parameter space, training procedure} & Temporary copy of $\theta^{*}$ (we have to save the whole model at each iteration). \\
    \hline
    Combinable with other regularization methods & Less data for training $\rightarrow$ include $(X^{(validation)}, y^{(validation)})$ afterwards
    \end{tabular}
  \end{table}
  \begin{itemize}
    \item Relation between optimal early stopping iteration m and weight decay penalization parameter $\lambda$ (see Goodfellow et al. (2016) page 251-252 for proof):
  \end{itemize}
    \begin{eqnarray*}
      m &\approx& \frac{1}{\alpha \lambda} \\
        &\Leftrightarrow& \lambda \approx \frac{1}{m \alpha}
    \end{eqnarray*}
  \vspace{-0.3cm}
  \begin{itemize}
    \item Small $\lambda$ (low penalization) $\Rightarrow$ high m (deep model/lots of updates)
  \end{itemize}
\framebreak
  \begin{itemize}
    \item[]
  \end{itemize}
  \begin{figure}
    \centering
      \includegraphics[width=11cm]{plots/early_stopping}
      \caption{Optimization path of early stopping (left) and weight decay (right) (Goodfellow et al. (2016))}
  \end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Dropout}
  \begin{itemize}
    \item Idea: constrain the networks adaptation to the training data to avoid becoming \enquote{too smart} in learning the input data.
      \begin{itemize}
        \item Dropout can be thought of as making bagging practical for ensembles of many large neural networks!
        \item In ensemble learning we take a number of weaker classifiers, train them separately and finally average them.
        \item Since each classifier has been trained independently, it has learned different \enquote{aspects} of the data.
        \item Combining them helps to produce an stronger classifier, which is less prone to overfitting (e.g. random forests)
      \end{itemize}
    \item So how does dropout actually work?
  \end{itemize}
\framebreak
  \begin{figure}
    \centering
      \includegraphics[width=10cm]{plots/dropout.png}
  \end{figure}
\framebreak
  \begin{figure}
    \centering
      \includegraphics[width=10cm]{plots/subnet1.png}
  \end{figure}
\framebreak
  \begin{figure}
    \centering
      \includegraphics[width=10cm]{plots/subnet2.png}
  \end{figure}
\framebreak
  \begin{figure}
    \centering
      \includegraphics[width=10cm]{plots/subnet3.png}
  \end{figure}
\framebreak
  \begin{itemize}
    \item Bagging: all models are independent.
    \item Dropout: models not independent as they share parameters!
      \begin{itemize}
        \item Each subnets archtitecture is defined by a \enquote{mask} $\mu$. The mask $\mu$ randomly determines the in-or exclusion of units (neurons) and is trained on one randomly sampled training data point $(x, y)$ (or mini batch).
        \item The mask $\mu$ is a vector of length $d$ (total number of units (neurons) in the network) with $\mu = (\mu_1, \mu_2, \dots, \mu_d), \ \mu_i = \{0,1\}$ and $P(\mu_i = 1) = p$.
        \item Thus, each subnet inherits a different subset of parameters from the parent neural network.
        \item Parameter sharing makes it possible to represent huge number of of models with particular amount of memory (hardware limitation!).
      \end{itemize}
   \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{

\frametitle{Parameter sharing}

  \begin{itemize}
    \item Parameter sharing:
    \begin{itemize}
      \item In the case of bagging, the models are all independent.
      \item In dropout on the other hand, all models share parameters. That means each model inherits a different subset of parameters from the parent neural network.
    \end{itemize}
  \end{itemize}

  \center
  \only<1>{\includegraphics[width=7cm]{plots/dropout_param_sharing.png}}%
  \only<2>{\includegraphics[width=7cm]{plots/subnet1_param_sharing.png}}%
  \only<3>{\includegraphics[width=7cm]{plots/subnet2_param_sharing.png}}%
  \only<4>{\includegraphics[width=7cm]{plots/subnet3_param_sharing.png}}%
  \only<5>{\includegraphics[width=7cm]{plots/subnet1_param_sharing2.png}}%
  \only<6>{\includegraphics[width=7cm]{plots/subnet2_param_sharing2.png}}%
  \only<7>{\includegraphics[width=7cm]{plots/subnet3_param_sharing2.png}}%

}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \framebreak
%   \begin{itemize}
%     \item Consider we would like to update some parameters with SGD. 
%       \begin{itemize}
%         \item We have a total number of $\eta$ weights, with $\theta_i$, $i = 1, \dots, \eta$
%         \item m training samples $(X^{(j)}, y^{(j)})$
%         \item A learning rate $\alpha$
%         \item An objectice funtion $\Ounreg$, which is a loss function
%       \end{itemize}
%   \end{itemize}
%   \begin{algorithm}[H]
%   \caption{Simplified version of SGD}
%     \begin{algorithmic}[1]
%       \State randomly shuffle training samples
%       \Repeat
%       \For{$m=1,...,j$}
%       \For{$i=1,..,\eta$}
%       \State $\theta_{i,(i+1)} = \theta_{i,(i)} - \alpha \frac{\delta \Ounreg} {\delta \theta_i}$
%       \EndFor
%       \EndFor
%       \Until{stop criterion is reached}
%     \end{algorithmic}
%   \end{algorithm}
% \framebreak
%   \begin{itemize}
%     \item Now we add the mask $\mu$ to determine the architecture of our subnet $i$.
%   \end{itemize}
%   \begin{algorithm}[H]
%   \caption{Simplified version of SGD with dropout}
%     \begin{algorithmic}[1]
%       \State randomly shuffle training samples
%       \Repeat 
%       \For{ $i=1,...,m$}
%       \State randomly draw mask $\mu$ for sub-net $i$
%       \For{$j=1,..,\eta$}
%       \State $\theta_{j,(i+1)} = \theta_{j,(i)} - \alpha \frac{\delta \Odropout_i} {\delta \theta_j}$
%       \EndFor
%       \EndFor
%       \Until{stop criterion is reached}
%     \end{algorithmic}
%   \end{algorithm}
\begin{vbframe}
  \begin{itemize}
    \item Models output \textbf{probability distributions}:  $p(y=y_j|x,\mu)$
    \item Bagging: arithmetic mean: $$\tilde p_{ensemble} (y = y_k|x)  =\frac{1}{B}\sum_{i=1}^B p^{(i)}(y = y_k| x)$$
    \item Dropout: more robust weighting via geometric mean:
      $$\tilde{p}_{ensemble} (y = y_k|x) = \sqrt[2^B]{\prod_{\mu}p(y = y_k|x, \mu)}$$
      and normalized for prediction:
      $$p_{ensemble} (y = y_k|x) = \frac{\tilde p_{ensemble} (y = y_k |x)}{\sum_{j}\tilde p_{ensemble} (y = y_k |x)}$$
  \end{itemize}
\end{vbframe}
\begin{frame}
\frametitle{Illustration geometric mean in dropout}
  \begin{table}[ht]
  \centering
    \begin{tabular}{c|c|c|c|c}
    \scriptsize
    & $P(y = y_1|x)$ & $P(y = y_2|x)$ & $P(y = y_3|x)$ & $\sum$\\
      \hline
    Model 1 & 0.20 & 0.70 & 0.10 & 1.00 \\ 
      Model 2 & 0.10 & 0.80 & 0.10 & 1.00 \\ 
      Model 3 & 0.05 & 0.90 & 0.05 & 1.00 \\ 
      Model 4 & 0.05 & 0.90 & 0.05 & 1.00 \\ 
      \textbf{Model 5} & \textbf{0.80} & \textbf{0.10} & \textbf{0.10} & \textbf{1.00} \\ 
      \hline
      Arithmetic mean & 0.24 & 0.68 & 0.08 & 1.00 \\ 
      Geometric mean & 0.13 & 0.54 & 0.08 & 0.75 \\ 
      Re-normalized & 0.18 & 0.72 & 0.10 & 1.00 \\ 
    \end{tabular}
  \end{table}
  \begin{equation*}
    \begin{split}
    &mean_{arithmetic}(x_1,...,x_n) = \frac{1}{n}\sum_{i=1}^n x_i\\
    &mean_{geometric}(x_1,...,x_n) = \sqrt[n]{\prod_{i=1}^n x_i} \text{ with } x_i > 0 \forall i = 1,..,n
    \end{split}
  \end{equation*}
\end{frame}
\begin{vbframe}{Dropout}
  \begin{table}
  \centering
    \begin{tabular}{p{1.8cm} || c | p{4cm}}
      & Bagging & Dropout \\
      \hline
      \hline
      Basic Idea & \multicolumn{2}{|c}{model averaging} \\
      \hline
      \hline
      \# models & $B$ & up to $2^{B}$\\
      \hline
      Prediction& atrithmetic mean & geometric mean \\
      \hline
      Parameters & B independent models & parameter sharing\\
      \hline
      Train method& each model to convergence & each sub-net trained on mini batch restricted by $\mu$ \\
    \end{tabular}
  \end{table}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Dropout: theory vs practice}
  \begin{itemize}
    \item Computing the complete ensemble is to expensive in practice!
    \item Instead we approximate it.
  \end{itemize}
  \begin{algorithm}[H]
  \caption{Training a neural network with dropout}
    \begin{algorithmic}[1]
      \State Define parent network and initialize weights
      \For{each training sample: }
      \State Draw mask $\mu$
      \State Compute forward pass for $network_{\mu}$
      \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Update the weights of $network_{\mu}$, e.g. by performing a gradient step with weight decay}
      \EndFor
    \end{algorithmic}
  \end{algorithm}
  \begin{itemize}
    \item For prediction: use weight scaling rule.
  \end{itemize}
\end{frame}
% $\theta_{\mu, (i+1)} = \theta_{\mu, (i)} - \alpha \cdot \frac{\delta\Lxy_{\mu}}{\delta \theta_{\mu, (i)}}$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{copy pasta}
% <<mxnet, size = "scriptsize", cache = FALSE, eval = FALSE, echo = TRUE>>=
%     drop0 = mx.symbol.Dropout(data, p = dropoutInputValues)
%     fc1 = mx.symbol.FullyConnected(drop0, name = "fc1", num_hidden = 512)
%     act1 = mx.symbol.Activation(fc1, name = "relu1", act_type = "relu")
%     drop1 = mx.symbol.Dropout(act1, p = dropoutLayerValues)
%     fc2 = mx.symbol.FullyConnected(drop1, name = "fc2", num_hidden = 512)
%     act2 = mx.symbol.Activation(fc2, name = "relu2", act_type = "relu")
%     drop2 = mx.symbol.Dropout(act2, p = dropoutLayerValues)
%     fc3 = mx.symbol.FullyConnected(drop2, name = "fc3", num_hidden = 512)
%     act3 = mx.symbol.Activation(fc3, name = "relu3", act_type = "relu")
%     drop3 = mx.symbol.Dropout(act3, p = dropoutLayerValues)
%     fc4 = mx.symbol.FullyConnected(drop3, name = "fc4", num_hidden = 10)
%     softmax = mx.symbol.SoftmaxOutput(fc4, name = "sm")
% @
% \end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Dropout}
  \begin{itemize}
    \item Weight scaling rule (Hinton et al. (2012)):
    \item[]
      \begin{itemize}
        \item Approximate $p_{ensemble}$ by inspection of the \textbf{complete model}
        \item Multiply shared weights of the trained model coming out of unit (neuron) $i$ by $p$
      \end{itemize}
  \end{itemize}
  \begin{figure}
    \includegraphics[width=10cm]{plots/dropout_neuron.png}
    \caption{(Goodfellow et al. (2016))}
  \end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Dropout - Example}
  \begin{itemize}
    \item To demonstrate how dropout can easily improve generalization we compute various models 
  \end{itemize}
  \begin{figure}
    \centering
      \includegraphics[width=10cm]{plots/mxnet_regulariation_2.png}
  \end{figure}
  \begin{itemize}
    \item We compute all models of the Cartesian product between the variables dropoutInputValues and dropoutLayerValues. That is $$(0, 0.2, 0.4, 0.6) \times (0, 0.1, 0.2, 0.3, 0.4, 0.5)$$
  \end{itemize}
  
\framebreak

<<fig.height=5.5>>=
require("ggplot2")
require("reshape2")

mnist_dropout_dropoutTrain = read.csv("code/mnist_dropout_dropoutTrain", header = TRUE, check.names = FALSE)
dropoutTrain = melt(mnist_dropout_dropoutTrain, id = "epoch")

ggplot(data = dropoutTrain, aes(x = epoch, y = value, colour = variable)) +
  geom_line() +
  scale_y_continuous(name = "train error", limits = c(0, 0.1)) + 
  scale_x_continuous(labels = function (x) floor(x), 
    name = "epochs") + theme(legend.title.align = 0.5) +
  labs(colour = "Dropout rate:\n(Input; Hidden Layers)")
@

\framebreak

<<fig.height=5.5>>=
require("ggplot2")
require("reshape2")

mnist_dropout_dropoutTest = read.csv("code/mnist_dropout_dropoutTest", header = TRUE, check.names = FALSE)
dropoutTest = mnist_dropout_dropoutTest[, -c(8:25)]

dropoutTest = melt(dropoutTest, id = "epoch")

ggplot(data = dropoutTest, aes(x = epoch, y = value, colour = variable)) +
  geom_line() +
  scale_y_continuous(name = "test error", limits = c(0, 0.05)) + 
  scale_x_continuous(labels = function (x) floor(x), 
    name = "epochs", limits = c(150, 200)) + theme(legend.title.align = 0.5) +
  labs(colour = "Dropout rate:\n(Input; Hidden Layers)")
@

\framebreak

<<fig.height=5.5>>=
require("ggplot2")
require("reshape2")

mnist_dropout_dropoutTest = read.csv("code/mnist_dropout_dropoutTest", header = TRUE, check.names = FALSE)
dropoutTest = mnist_dropout_dropoutTest[, -c(2:7, 14:25)]
dropoutTest = melt(dropoutTest, id = "epoch")

ggplot(data = dropoutTest, aes(x = epoch, y = value, colour = variable)) +
  geom_line() +
  scale_y_continuous(name = "test error", limits = c(0, 0.05)) + 
  scale_x_continuous(labels = function (x) floor(x), 
    name = "epochs", limits = c(150, 200)) + theme(legend.title.align = 0.5) +
  labs(colour = "Dropout rate:\n(Input; Hidden Layers)")
@

\framebreak

<<fig.height=5.5>>=
require("ggplot2")
require("reshape2")

mnist_dropout_dropoutTest = read.csv("code/mnist_dropout_dropoutTest", header = TRUE, check.names = FALSE)
dropoutTest = mnist_dropout_dropoutTest[, -c(2:13, 20:25)]
dropoutTest = melt(dropoutTest, id = "epoch")

ggplot(data = dropoutTest, aes(x = epoch, y = value, colour = variable)) +
  geom_line() +
  scale_y_continuous(name = "test error", limits = c(0, 0.05)) + 
  scale_x_continuous(labels = function (x) floor(x), 
    name = "epochs", limits = c(150, 200)) + theme(legend.title.align = 0.5) +
  labs(colour = "Dropout rate:\n(Input; Hidden Layers)")
@

\framebreak

<<fig.height=5.5>>=
require("ggplot2")
require("reshape2")

mnist_dropout_dropoutTest = read.csv("code/mnist_dropout_dropoutTest", header = TRUE, check.names = FALSE)
dropoutTest = mnist_dropout_dropoutTest[, -c(2:19)]
dropoutTest = melt(dropoutTest, id = "epoch")

ggplot(data = dropoutTest, aes(x = epoch, y = value, colour = variable)) +
  geom_line() +
  scale_y_continuous(name = "test error", limits = c(0, 0.05)) + 
  scale_x_continuous(labels = function (x) floor(x), 
    name = "epochs", limits = c(150, 200)) + theme(legend.title.align = 0.5) +
  labs(colour = "Dropout rate:\n(Input; Hidden Layers)")
@

\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Dropout, weight decay or both?}

<<fig.height=5.5>>=
require("ggplot2")

errorPlot = read.csv("code/mnist_visualization_model_1_error", header = TRUE)

ggplot(data = errorPlot, aes(x = epoch, y = value, colour = variable)) +
  geom_line() +
  scale_y_continuous(name = "error", limits = c(0, 0.2)) + 
  scale_x_continuous(labels = function (x) floor(x), 
    name = "epochs", limits = c(0, 500)) + theme(legend.title.align = 0.5) + 
  labs(colour = "Unregularized model")
@

\framebreak

<<fig.height=5.5>>=
require("ggplot2")
require("plyr")

errorPlot = read.csv("code/mnist_visualization_model_2_error", header = TRUE)
errorPlot$variable = revalue(errorPlot$variable, c("test error dropout" = "test error"))
errorPlot$variable = revalue(errorPlot$variable, c("training error dropout" = "training error"))

ggplot(data = errorPlot, aes(x = epoch, y = value, colour = variable)) +
  geom_line() +
  scale_y_continuous(name = "error", limits = c(0, 0.2)) + 
  scale_x_continuous(labels = function (x) floor(x), 
    name = "epochs", limits = c(0, 500)) + theme(legend.title.align = 0.5) + 
  labs(colour = "Dropout model: \n (0.4; 0.2)")
@

\framebreak

<<fig.height=5.5>>=
require("ggplot2")
require("plyr")

errorPlot = read.csv("code/mnist_visualization_model_3_error", header = TRUE)
errorPlot$variable = revalue(errorPlot$variable, c("test error dropout + wd" = "test error"))
errorPlot$variable = revalue(errorPlot$variable, c("training error dropout + wd" = "training error"))

ggplot(data = errorPlot, aes(x = epoch, y = value, colour = variable)) +
  geom_line() +
  scale_y_continuous(name = "error", limits = c(0, 0.2)) + 
  scale_x_continuous(labels = function (x) floor(x), 
    name = "epochs", limits = c(0, 500)) + theme(legend.title.align = 0.5) + 
  labs(colour = "Weight decay \n and \n dropout model: \n (0.4; 0.2)")
@

\framebreak

<<fig.height=5.5>>=
require("plyr")

errorPlot = read.csv("code/mnist_visualization_model_4_error", header = TRUE)
errorPlot$variable = revalue(errorPlot$variable, c("test error dropout + wd" = "test error"))
errorPlot$variable = revalue(errorPlot$variable, c("training error dropout + wd" = "training error"))

ggplot(data = errorPlot, aes(x = epoch, y = value, colour = variable)) +
  geom_line() +
  scale_y_continuous(name = "error", limits = c(0, 0.2)) + 
  scale_x_continuous(labels = function (x) floor(x), 
    name = "epochs", limits = c(0, 500)) + theme(legend.title.align = 0.5) +
  labs(colour = "Weight decay \n only model")
@

\framebreak

<<fig.height=5.5>>=
require("ggplot2")

errorPlot = read.csv("code/mnist_visualization_model_total_error", header = TRUE)

ggplot(data = errorPlot, aes(x = epoch, y = value, colour = variable)) +
  geom_line() +
  scale_y_continuous(name = "test error", limits = c(0, 0.2)) + 
  scale_x_continuous(labels = function (x) floor(x), 
    name = "epochs", limits = c(0, 500)) + theme(legend.title.align = 0.5) +
  labs(colour = "Test error \n comparison")
@

\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Dataset Augmentation}
  \begin{itemize}
    \item Problem: low generalization because high ratio of $$\frac{\text{complexity of the model}}{\text{\#train data}}$$
    \item Idea: artificially increase the train data.
      \begin{itemize}
        \item Limited data supply $\rightarrow$ create \enquote{fake data}! 
      \end{itemize}
    \item Increase variation in inputs \textbf{without} changing the labels.
    \item Application:
      \begin{itemize}
        \item Image and Object recognition (rotation, scaling, pixel translation, flipping, noise injection, vignetting, color casting, lens distortion, injection of random negatives)
        \item Speech recognition (speed augmentation, vocal tract perturbation)
      \end{itemize}
  \end{itemize}
\framebreak
  \begin{figure}
    \centering
      \includegraphics[width=7cm]{plots/data_augmentation_1.png}
      \caption{(Wu et al. (2015))}
  \end{figure}
\framebreak
  \begin{figure}
    \centering
      \includegraphics[width=6cm]{plots/data_augmentation_2.png}
      \caption{(Wu et al. (2015))}
  \end{figure}
  $\Rightarrow$ careful when rotating digits (6 will become 9 and vice versa)!
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Artificial Noise}
  \begin{itemize}
    \item Intentionally inject noise to the model, to make it more robust to small pertubations in the inputs.
    \item This method may force the model to \enquote{grow} weights in regions of flat minima.
      \begin{itemize}
        \item Thus, the noisy model may not find perfect minima but its approximations lie in a flatter surrounding.
      \end{itemize}
    \item Bishop (1995) shows that the injection of artifical noise has the same effect as norm penalization strategies.
    \item In practice, it is common to apply noise to the outputs. 
      \begin{itemize}
        \item This strategy is termed label smoothing as it incorporates a small noise term on the labels of the classification outputs. The intuition is to account for possible errors in the labeling process.
      \end{itemize}
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}
\frametitle{References}
\footnotesize{
\begin{thebibliography}{99}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Ian Goodfellow et al., 2016]{1} Ian Goodfellow, Yoshua Bengio and Aaron Courville (2016)
\newblock Deep Learning
\newblock \emph{\url{http://www.deeplearningbook.org/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Hastie et al., 2009]{2} Trevoe Hastie, Robert Tibshirani and Jerome Friedman (2009)
\newblock The Elements of Statistical Learning
\newblock \emph{\url{https://statweb.stanford.edu/\%7Etibs/ElemStatLearn/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Hinton et al., 2012]{3} Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky Ilya Sutskever and Ruslan Salakhutdinov (2012)
\newblock Improving neural networks by preventing co-adaptation of feature detectors
\newblock \emph{\url{http://arxiv.org/abs/1207.0580}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Wu et al., 2015]{4} Wu Ren, Yan Shengen, Shan Yi, Dang Qingqing and Sun Gang (2015)
\newblock Deep Image: Scaling up Image Recognition
\newblock \emph{\url{https://arxiv.org/abs/1501.02876}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Bishop, Chris M., 1995]{5} Bishop, Chris M. (1995)
\newblock Training with Noise is Equivalent to Tikhonov Regularization
\newblock \emph{\url{https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/bishop-tikhonov-nc-95.pdf}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{thebibliography}
}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlecture