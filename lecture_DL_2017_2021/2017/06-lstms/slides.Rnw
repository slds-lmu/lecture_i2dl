<<setup-child, include = FALSE>>=
library(knitr)
set_parent("../style/preamble.Rnw")
knitr::opts_chunk$set(cache=TRUE)
@

\lecturechapter{7}{Sequence Modeling: Recurrent and Recursive Nets}
\lecture{Deeplearning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{

\frametitle{RNNs - What for?}
  Suppose we would like to process sequential inputs, such as
  \begin{itemize}
    \item Text data %(e.g. for text recognition)
    \item Audio data %(e.g. for natural language processing)
  \end{itemize}
  Can we do that with a classic dense net?
  %Is it possible for a dense net?
  %How would a dense neural network deal with them?
  \begin{itemize}
    \item[]
  \end{itemize}
  \begin{centering}
  \begin{minipage}{0.42\textwidth}
    \begin{figure}
        \only<1-2>{\includegraphics[width=5.5cm]{plots/neuralnet2.png}}
        \caption{A dense architecture. \textcolor{white}{bla bla bla blabla blabla blabla blabla blabla bla}}
    \end{figure}
  \end{minipage}\hfill
  \begin{minipage}{0.57\textwidth}
  \vspace{-0.3cm}
    \begin{itemize}
      \only<1>{\item[] \textcolor{white}{bla}} % stupid trick to get rid of compiling error
      \only<2>{\item[] Hardly, the major drawbacks of these models are:} %The major drawbacks of these models for sequential data are:}
      \only<2>{\begin{itemize}
        \only<2>{\item \textbf{a fixed input length}. \\ The length of sequential inputs can vary!}
        \only<2>{\item \textbf{the assumption of independent training samples}. \\ For sequential inputs, there are short and long term temporal dependencies!}
      \end{itemize}}

    \end{itemize}
    
  \end{minipage}
  \end{centering}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{RNNs - Sentiment Analysis}
  \begin{itemize}
    \item Suppose we would like to train a model to read a sentence and extract the year the narrator went to munich:
    \begin{itemize}
      %\item \enquote{$\underbrace{\text{I went to Munich in 2009}}_{\text{24 characters}}$}
      %\item[]
      %\item \enquote{$\underbrace{\text{In 2009, I went to Munich}}_{\text{25 characters}}$}
      \item \enquote{I went to Munich in 2009}
      \item \enquote{In 2009, I went to Munich}
    \end{itemize}
    \item A standard dense network would have separate parameters for each input feature. Thus it would need to learn all of the rules of the language separately at each position in the sentence!
      \item To overcome this issue, we introduce \textbf{recurrent neural networks}!
      \item In order to go from a standard dense to such a recurrent net, we need to take advantage of an idea we have already learned in the CNN chapter: \textbf{parameter sharing}.
  \end{itemize}
\framebreak
  \begin{itemize}
    \item Parameter sharing enables us to apply the model to examples of different forms (here: different lengths)!
    \item If we had separate parameters for each value of the input data, we could not generalize to sequence lengths not seen during training.
    \item Parameter sharing is specifically important, when a particular piece of information might occur at multiple positions within the input sequence.
    % \item Recurrent networks share parameters in a different way than CNNs: 
    % \begin{itemize}
    %   \item[] each member of the output is a function of the previous members of the output.
    %   \item[] each member of the output is produced using the same update rule applied to the previous outputs
    % \end{itemize}
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{RNNs - Generate sequences}
  \begin{itemize}
  \item Suppose we only had a vocabulary of four possible letters: \\
  $$\text{\enquote{h}, \enquote{e}, \enquote{l} and \enquote{o}}$$
  \item Our goal is to train a neural net on the sequence \enquote{hello}.
  \item Our training sequence \enquote{hello} is in actually a source of 4 separate training examples:
    \begin{itemize}
      \item The probability of \enquote{e} should be likely, given the context of \enquote{h}
      \item \enquote{l} should be likely in the context of \enquote{he}
      \item \enquote{l} should \textbf{also} be likely, given the context of \enquote{hel}
      \item and finally \enquote{o} should be likely, given the context of \enquote{hell}
    \end{itemize}
  \item So how to realize that?
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{RNNs - Generate sequences}
  \begin{figure}
      \only<1>{\includegraphics[width=6.5cm]{plots/nlp1a.png}}%
      \only<2>{\includegraphics[width=6.5cm]{plots/nlp1b.png}}%
      \only<3>{\includegraphics[width=6.5cm]{plots/nlp1c.png}}%
      \only<4>{\includegraphics[width=6.5cm]{plots/nlp1d.png}}%
  \end{figure}
  \begin{itemize}
    \only<1>{\item[] The probability of \enquote{e} should be likely, given the context of \enquote{h}} 
    \only<2>{\item[] The probability of \enquote{l} should be likely, given in the context of \enquote{he}} 
    \only<3>{\item[] The probability of \enquote{l} should \textbf{also} be likely, given in the context of \enquote{hel}}
    \only<4>{\item[] The probability of \enquote{o} should be likely, given the context of \enquote{hell}}
  \end{itemize}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{RNNs - Generate sequences}

The RNN has a 4-dimensional input and output. The exemplary hidden layer consists of 3 neurons. This diagram shows the activations in the forward pass when the RNN is fed the characters \enquote{hell} as input. The output contains confidences the RNN assigns for the next character.
  \begin{itemize}
    \item[]
  \end{itemize}
  \begin{minipage}{0.55\textwidth}
    \begin{figure}
        \only<1>{\includegraphics[width=5.5cm]{plots/nlp1.png}}%
        \only<2>{\includegraphics[width=5.5cm]{plots/nlp2.png}}%
    \end{figure}
  \end{minipage}%\hfill
  \begin{minipage}{0.45\textwidth}
  %\vspace{-0.3cm}
  
    \begin{itemize}
      \only<1>{\item[] \textcolor{white}{Our goal is to increase the confidence for the correct letters (green digits) and decrease the confidence of all others.}} 
      \only<2>{\item[] Our goal is to increase the confidence for the correct letters (green digits) and decrease the confidence of all others.} 
    \end{itemize}
  \end{minipage}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{natural language processing (NLP)}
  \begin{itemize}
    \item What we just did is called natural language processing (NLP)
    \item Input: word/character, encoded as one-hot vector
    \item Output: probability distribution over words given previous words $$P(y^{[1]}, \dots, y^{[\tau]}) = \displaystyle \prod_{i=1}^{\tau} P(y^{[i]}|y^{[1]}, \dots, y^{[i-1]})$$
    \item[] $\Rightarrow$ given a sequence of previous characters, ask the RNN to model the probability distribution of the next character in the sequence!
    \item[]
    \item[]
    \item[]
    \item Time to formalize RNNs...
  \end{itemize}
\end{vbframe}
\frame{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frametitle{Recurrent neural networks}
  \center
  \begin{figure}%
    \only<1>{\includegraphics[width=7cm]{plots/recurrent_neural_network_1_unfolded.png}}%
    \only<2-3>{\includegraphics[width=7cm]{plots/recurrent_neural_network_1.png}}%
    \only<4>{\includegraphics[width=7cm]{plots/recurrent_neural_network_2.png}}%
    \only<5-6>{\includegraphics[width=7cm]{plots/recurrent_neural_network_3.png}}%
    \only<7>{\includegraphics[width=7cm]{plots/recurrent_neural_network_4.png}}%
  \end{figure}%
  \vspace{-0.2cm}
  \begin{itemize}
    \only<1>{\item On the left, the computational graph for a dense net. A loss function $L$ measures how far each
    $\hat{y}$ is from the corresponding training target $y$. On the right, a potential unfolded network.}
    \only<2-3>{\item In order to derive RNNs we have to extend our notation.}
    \only<3>{\begin{itemize}
      \only<3>{\item So far, we mapped some inputs $x$ to outputs $\hat{y}$:}
      \only<3>{\item[] $\hat{y} = \sigma(c + Vz) = \sigma(c + V\sigma(b + Ux))$}
      \only<3>{\item[] ..with $V$ and $U$ being weight matrices.}
    \end{itemize}}
    \only<4>{\item A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor.}
    \only<4>{\item RNNs are networks with loops, allowing information to persist.}
    \only<5>{\item Things might become more clear if we unfold the architecture.}
    \only<5>{\item Instead of hidden layer, we call $z$ the state of the system at time $t$.}
    \only<5>{\item The state contains information about the whole past sequence.}
    \only<6>{\item We went from 
      \begin{eqnarray*} 
        \hat{y} &=& \sigma(c + V\sigma(b + Ux)) \text{ to } \\
        \hat{y}^{[t]} &=& \sigma(c + V\sigma(b + Wz^{[t-1]} + Ux^{[t]}))
      \end{eqnarray*}}
    \only<7>{\item Potential computational graph for time state $t$:}
    \only<7>{\item[] $$\hat{y}^{[t]} = \sigma(c + V\sigma(b + Wz^{[t-1]} + Ux^{[t]}))$$ }
  \end{itemize}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{

\frametitle{Backpropagation through time}

  \center
  \only<1>{\includegraphics[width=5cm]{plots/rnn_backpropagation1.png}}%
  \only<2>{\includegraphics[width=5cm]{plots/rnn_backpropagation2.png}}%

  \begin{itemize}

    \only<1-2>{\item To carry out backpropagation for an arbitrary RNN, we simply compute:}
    \only<1>{\item[] $$\textcolor{white}{\frac{\delta L}{\delta z^1} = \frac{\delta L}{\delta z^{t}} \frac{z^{t}}{\delta z^{t-1}} \dots \frac{\delta z^2}{\delta z^1}}$$}

    \only<2>{\item[] $$\frac{\delta L}{\delta z^1} = \frac{\delta L}{\delta z^{t}} \frac{z^{t}}{\delta z^{t-1}} \dots \frac{\delta z^2}{\delta z^1}$$}

  \end{itemize}

}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{Some more sophisticated applications}
  \begin{figure}
      \only<1>{\includegraphics[width=9.5cm]{plots/image_caption2.png}}%
      \only<2>{\includegraphics[width=9.5cm]{plots/image_caption.png}}%
  \end{figure}
\textbf{Figure:} Show and Tell: A Neural Image Caption Generator (Oriol Vinyals et al. 2014). A language generating RNN tries to describe in brief the content of different images.
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{Some more sophisticated applications}
  \begin{figure}
      \only<1>{\includegraphics[width=8.5cm]{plots/seq2seq.png}}%
      \only<2>{\includegraphics[width=8.5cm]{plots/seq2seq2.png}}%
  \end{figure}
\textbf{Figure:} Neural Machine Translation (seq2seq): Sequence to Sequence Learning with Neural Networks (Ilya Sutskever et al. 2014). An encoder converts a source sentence into a \enquote{meaning} vector which is passed through a decoder to produce a translation.
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Some more sophisticated applications}
  \begin{figure}
    \centering
    \includegraphics[width=10cm]{plots/hand_writing_generation.png}
    \caption{Generating Sequences With Recurrent Neural Networks (Alex Graves, 2013). Top row are real data, the rest are generated by various RNNs.}
  \end{figure}
\framebreak
  \begin{figure}
    \centering
    \includegraphics[width=11.5cm]{plots/emotion_from_audio_data.png}
    \caption{Convolutional and recurrent nets for detecting emotion from audio data (Namrata Anand \& Prateek Verma, 2016). We already had this example in the CNN chapter!}
  \end{figure}  
\framebreak
  \begin{figure}
    \centering
    \includegraphics[width=11.5cm]{plots/visually_indicated_sounds.png}
    \caption{\href{https://www.youtube.com/watch?v=0FW99AQmMc8&feature=youtu.be&t=61}{Visually Indicated Sounds} (Andrew Owens et al. 2016). A model to synthesize plausible impact sounds from silent videos.}
  \end{figure}   
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{Nlp - Example}
%       Example: 
%   \begin{itemize}
%     \item Suppose we only had a vocabulary of four possible letters: \enquote{h}, \enquote{e}, \enquote{l} and \enquote{o}
%     \item We want to train an RNN on the training sequence \enquote{hello}.
%     \item This training sequence is in fact a source of 4 separate training examples:
%       \begin{itemize}
%         \item The probability of \enquote{e} should be likely given the context of \enquote{h}
%         \item \enquote{l} should be likely in the context of \enquote{he}
%         \item \enquote{l} should also be likely given the context of \enquote{hel}
%         \item and \enquote{o} should be likely given the context of \enquote{hell}
%       \end{itemize}
%   \end{itemize}
% \end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \frame{
% 
% \frametitle{Nlp - Example}
% 
% The RNN has a 4-dimensional input and output. The exemplary hidden layer consists of 3 neurons. This diagram shows the activations in the forward pass when the RNN is fed the characters \enquote{hell} as input. The output contains confidences the RNN assigns for the next character.
%   \begin{itemize}
%     \item[]
%   \end{itemize}
%   \begin{minipage}{0.55\textwidth}
%     \begin{figure}
%         \only<1>{\includegraphics[width=5.5cm]{plots/nlp1.png}}%
%         \only<2>{\includegraphics[width=5.5cm]{plots/nlp2.png}}%
%     \end{figure}
%   \end{minipage}%\hfill
%   \begin{minipage}{0.45\textwidth}
%   %\vspace{-0.3cm}
%   
%     \begin{itemize}
%       \only<1>{\item[] \textcolor{white}{Our goal is to increase the confidence for the correct letters (green digits) and decrease the confidence of all others (we could also use a softmax activation to squash the digits to probabilities $\in [0,1]$).}} 
%       \only<2>{\item[] Our goal is to increase the confidence for the correct letters (green digits) and decrease the confidence of all others (we could also use a softmax activation to squash the digits to probabilities $\in [0,1]$).} 
%     \end{itemize}
%   \end{minipage}
%   
% }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{Word Embeddings (Word2vec)}
%   \begin{minipage}{0.4\textwidth}
%     \begin{itemize}
%       \item Data Sparsity: 
%       \item[]
%       $$\text{man} \to \begin{bmatrix}
%                                   0\\
%                                   \vdots\\
%                                   0\\
%                                   1\\
%                                   0\\
%                                   \vdots\\
%                                   0
%                         \end{bmatrix} \to
%                         \begin{bmatrix}
%                                   0.35\\
%                                   -0.83\\
%                                   \vdots\\
%                                   0.11\\
%                                   3.2
%                         \end{bmatrix}
%       $$
%   
%     \end{itemize}
%   \end{minipage}
%   \begin{minipage}{0.55\textwidth}
%     \begin{itemize}
%       \item[]
%     \end{itemize}
%     \begin{figure}
%       \includegraphics[width=4.5cm]{plots/word2vec.png}%
%       \caption*{https://www.tensorflow.org/tutorials/word2vec/}
%     \end{figure}    
%   \end{minipage}
% \end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Long-Term Dependencies}
  \begin{itemize}
    \item The challenge of learning long-term dependencies in RNNs often leads to gradients
     that either \textbf{vanish} (most of the time) or \textbf{explode} (rarely, but with much damage to the optimization).
    \item That happens simply because we propagate errors over very many stages backwards.
    \item Even if we assume that the parameters are such that the recurrent network is stable (can store memories, with gradients not exploding), the difficulty with long-term dependencies arises from the exponentially smaller weights given to long-term interactions (involving the multiplication of many Jacobians) compared to short-term ones.
  \end{itemize}
\framebreak
  \begin{itemize}
    \item The \textbf{vanishing gradient problem} is heavily dependent on the parameter initialization method, but in particular on the choice of the activation functions.
    \begin{itemize}
      \item For example, the sigmoid maps a real number into a \enquote{small} range (i.e. $[0, 1]$).
      \item As a result, large regions of the input space are mapped into a very small range.
      \item Even a huge change in the input will produce a small change in the output. Hence, the gradient will be small.
      \item This becomes even worse when we stack multiple layers of such non-linearities on top of each other (For instance, the first layer maps a large input to a smaller output region, which will be mapped to an even smaller region by the second layer, which will be mapped to an even smaller region by the third layer and so on..).
      \item We can avoid this problem by using activation functions which do not have the property of \enquote{squashing} the input.
      \item The most popular choice is obviously the Rectified Linear Unit (ReLU) which maps $x$ to $max(0,x)$.
      \item The really nice thing about ReLU is that the gradient is either $0$ or $1$, which means it never saturates. Thus, gradients can't vanish.
      \item The downside of this is that we can obtain a \enquote{dead} ReLU. It will always return $0$  and consequently never learn because the gradient is not passed through.
    \end{itemize}
    \item To avoid exploding gradients, we simply clip the norm of the gradient at some threshold $h$ (see chapter 3): $$\text{if  } ||\nabla W|| > \text h: \nabla W \leftarrow \frac{h}{||\nabla W||} \nabla W $$
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{long short-term memory - LSTM}
  \begin{itemize}
    \item The LSTM provides a different way of dealing with vanishing gradients.
    \item A cell state $s^{[t]}$ is introduced, which can be manipulated by different gates to forget old information, add new information and read information out of it.
    \item Each gate is a vector of the same size as the cell state and each element of the vector is a
number between $0$ and $1$, with $0$ meaning \enquote{let nothing pass} and 1 \enquote{let everything pass}.
    \item The gates are computed as a parametrized function of the previous hidden state $z^{[t-1]}$ and the input at the current time step $x^{[t]}$ multiplied by gate-specific weights and typically squashed through a sigmoid function into the range of $[0, 1]$.
    \item The cell state allows the recurrent neural network to keep information over long time ranges and therefore overcome the vanishing gradient problem.
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
\frametitle{long short-term memory - LSTM}
  \begin{figure}
    \only<1>{\includegraphics[width=8.5cm]{plots/vanilla_rnn_vs_lstm_1.png}}%
    \only<2>{\includegraphics[width=8.5cm]{plots/vanilla_rnn_vs_lstm_2.png}}%
  \end{figure}
  \begin{itemize}
    \only<1-2>{\item Untill now, we simply computed $$z^{[t]} = \sigma(b + Wz^{[t-1]} + Ux^{[t]})$$}
    \only<2>{\item Now we introduce the LSTM cell (this is where the fun begins)}
  \end{itemize}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \frame{
% \frametitle{long short-term memory - LSTM}
%   \begin{itemize}
%     \item[]
%   \end{itemize}
%   \begin{minipage}{0.51\textwidth}
%     \only<1-2>{\includegraphics[width=2.5cm]{plots/vanilla_rnn.png}}%
%     \begin{itemize}
%       \only<1-2>{\item Untill now, we simply computed $$z^{[t]} = \sigma(b + Wz^{[t-1]} + Ux^{[t]})$$}
%       \only<2>{\item Now we introduce the lstm cell (this is where the fun begins).}
%     \end{itemize}
%   \end{minipage}\hfill
%   \vspace{-0.7cm}
%   \begin{minipage}{0.41\textwidth}
%     \only<1>{\includegraphics[width=2.5cm]{plots/vanilla_lstm1.png}}%
%     \only<2>{\includegraphics[width=2.5cm]{plots/vanilla_lstm2.png}}%
%     \begin{itemize}
%       \only<1-2>{\item[] \textcolor{white}{with} $$\textcolor{white}{z^{(t)} = \sigma(b + Wz^{[t-1]} + Ux^{[t]})}$$}
%       \only<2>{\item[] \textcolor{white}{Now we introduce the lstm cell (this is where the fun begins).}}
%     \end{itemize}
%   \end{minipage}
% }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{

\frametitle{long short-term memory - LSTM}

  \center
  \only<1>{\includegraphics[width=4.75cm]{plots/lstm1a.png}}%
  \only<2-3>{\includegraphics[width=4.75cm]{plots/lstm1b.png}}%
  \only<4>{\includegraphics[width=4.75cm]{plots/lstm1c.png}}%
  \only<5-7>{\includegraphics[width=4.75cm]{plots/lstm3b.png}}%
  \only<8-9>{\includegraphics[width=4.75cm]{plots/lstm4.png}}%
  
  \begin{itemize}

    \only<1>{\item The key to LSTMs is the cell state $s^{[t]}$}
    \only<1>{\item The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates}
    \only<2>{\item The cell state $s^{[t]}$ is computed as a function of the previous cell state $s^{[t-1]}$, multiplied by the forget gate $f^{[t]}$. $$s^{[t]} = f^{[t]} s^{[t-1]}, \text{ with } f^{[t]} \in [0,1]$$}
    \only<3>{\item Think of a model trying to predict the next word based on all the previous ones. The cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old one.}
    \only<4>{\item We obtain the forget gate by computing 
    $$f^{[t]} = \sigma(b^{f} + W^{f} z^{[t-1]} + U^{f} x^{[t]})$$}
    \only<4>{\item $\sigma()$ is obviously a sigmoid, to squash the values to $[0,1]$}
    \only<5>{\item Now it is time to add new information $i^{[t]}$ into the cell state. }
    \only<5>{\item We also incorporate information of the previous hidden state $z^{[t-1]}$}
    \only<5>{\item In the case of the language model, this is where we add the new information about the gender of our subject.}
    \only<6>{\item The cell recurrent connection needs a function whose derivatives sustain for a long span to address the vanishing gradient problem}
    \only<6>{\item $\tilde{s}^{[t]} = tanh(b + W z^{[t-1]} + U x^{[t]}) \in [-1, 1]$}
    \only<6>{\item $i^{[t]} = \sigma(b^i + W^i z^{[t-1]} + U^i x^{[t]}) \in [0,1]$}
    \only<7>{\item Now we can finally update the cell state $s^{[t]}$: 
    $$s^{[t]} = f^{[t]} s^{[t-1]} + i^{[t]} \tilde{s}^{[t]}$$}
    \only<7>{\item[] By the way: this does not mean that our lstm is complete! }
    \only<8>{\item In order to complete the lstm cell, one final ingredient is missing}
    \only<8>{\item The output will be a filtered version of our cell state}
    \only<8>{\item First, we run a sigmoid layer which decides what parts of the cell state we're going to output: $q^{[t]} = \sigma(b^q + W^q z^{[t-1]} + U^q x^{[t]}$ }
    %\only<8>{\item }
    \only<9>{\item Finally, the new state $z^{[t]}$ is then a function of the LSTM cell state, multiplied by the output gate: $$z^{[t]} = q^{[t]} \cdot tanh(s^{[t]})$$}

  \end{itemize}

}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{RNN with output recurrence}
  \begin{figure}
    \includegraphics[width=5.5cm]{plots/output_recurrence.png}
  \end{figure}
  \begin{itemize}
    \item Such an RNN is less powerful (can express a smaller set of functions).
    \item However, it may be easier to train because each time step it can be trained in isolation from the others, allowing greater parallelization during training.
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Teacher Forcing}
  \begin{itemize}
    \item Teacher forcing is a procedure that emerges from the maximum likelihood criterion, in which during training the model receives the ground truth output $y^{[t]}$ as input at time $[t + 1]$.
  \end{itemize}
  \begin{minipage}{0.51\textwidth}
    \begin{figure}
      \includegraphics[width=3.8cm]{plots/teacher_forcing_train.png}
    \end{figure}  
    \begin{itemize}
      \item At training time
    \end{itemize}
  \end{minipage}
  \begin{minipage}{0.45\textwidth}
    \begin{figure}
      \includegraphics[width=3.8cm]{plots/teacher_forcing_test.png}
    \end{figure} 
    \begin{itemize}
      \item At testing time
    \end{itemize}
  \end{minipage}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{One-output RNN}
  \begin{itemize}
    \item Recurrent Neural Networks do not need to have an output at each time step, instead they can
only have outputs at a few time steps.
    \item A common variant is an RNN with only one output at the end of the sequence.
    \item Information from the whole input sequence is incorporated into the final hidden state, which is then used to create an output, e.g. a sentiment (\enquote{positive}, \enquote{neutral} or \enquote{negative}) for a movie review.
    \item  Other applications of such an architecture are sequence labeling, e.g. classify an article into different categories (\enquote{sports}, \enquote{politics} etc.)
  \end{itemize}
\framebreak
  \begin{figure}
    \includegraphics[width=6.5cm]{plots/one_output_rnn.png}
    \caption{A Recurrent Neural Network with one output at the end of the sequence. Such a
model can be used for sentiment analysis ($<$eos$>$ = \enquote{end of sequence}).}
  \end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Deep RNNs}
  \begin{itemize}
    \item Another generalization of the simple RNN are bidirectional RNNs.
    \item These allow to process sequential data depending on both past and future inputs, e.g. an application predicting missing words, which probably depend on both preceding and following words.
    \item One RNN processes inputs in the forward direction from $x^{[1]}$ to $x^{[T]}$ computing a sequence of hidden states $(z^{[1]}, \dots, z^{(T)})$, another RNN in the backward direction from $x^{[T]}$ to $x^{[1]}$ computing hidden states $(g^{[T]}, \dots, g^{[1]})$
    \item Predictions are then based on both hidden states, which could be concatenated.
    \item With connections going back in time the whole input sequence must be known in advance
to train and infer from the model.
    \item Bidirectional RNNs are often used for the encoding of a sequence in machine translation.
  \end{itemize}
\framebreak  
  \begin{figure}
    \includegraphics[width=5.5cm]{plots/bidirectional_rnn.png}
    \caption{A bidirectional RNN consists of a forward RNN processing inputs from left to right
and a backward RNN processing inputs backwards in time.}
  \end{figure} 
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Recursive neural networks}
  \begin{itemize}
    \item Recursive Neural Networks are a generalization of Recurrent Neural Networks. 
    \item A tree structure instead of a chain structure is used for the computations of the RNN.
    \item A fixed set of weights is repeatedly applied to the nodes of the tree.
    \item Recursive neural networks have been successfully applied to sentiment analysis!
  \end{itemize}
\framebreak  
  \begin{figure}
    \includegraphics[width=5.8cm]{plots/recursive_neural_network.png}
    \caption{A recursive neural network}
  \end{figure} 
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Encoder-Decoder Network}
  \begin{itemize}
    \item Standard RNNs operate on input and output sequences of the same length.
    \item But for many interesting tasks such as question answering or machine translation the network needs to map an input sequence to an output sequence of different length.
    \item This is what an encoder-decoder (also called sequence-to-sequence architecture) enables us to do!
    \item An input/encoder-RNN processes the input sequence of length $n_x$ and omits a fixed-length context vector $C$, usually the final hidden state or a simple function thereof.
    \item One time step after the other information from the input sequence is processed, added to the hidden state and passed forward in time through the recurrent connections between hidden states in the encoder.
    \item  The context summarizes important information from the input sequence, e.g. the intent of a question in an question answering task or the meaning of a text in the case of machine translation.
    \item The decoder RNN uses this information to predict the output, a sequence of length $n_y$, which could vary from $n_x$. 
    \item In natural language processing the decoder is a language model with recurrent connections between the output at one time step and the hidden state at the next time step as well as recurrent connections between the hidden states:
    $$p(y^{[1]}, \dots, y^{[y_n]}|x^{[1]}, \dots, x^{[x_n]}) = \displaystyle \prod_{t=1}^{n_y} p(y^{[t]}|C; y^{[1]}, \dots, y^{[t-1]})$$ with $C$ being the context-vector.
    \item This architecture is now jointly trained to minimize the translation
error given a source sentence.
    \item Each conditional probability is then $$p(y^{[t]}|y^{[1]}, \dots, y^{[t-1]};C) = f(y^{[t-1]}, g^{[t]}, C)$$ where f is a non-linear function, e.g. the tanh and $g^{[t]}$ is the hidden state of the decoder network.
    \item Encoder-decoder architectures are often used for machine translation, where they excel phrasebased translation models.
  \end{itemize}
\framebreak
  \begin{figure}
    \includegraphics[width=7cm]{plots/encoder_decoder_network.png}
    \caption{Encoder-decoder allows an RNN to process different length input and output sequences. In the first part of the network information from the input is encoded in the context, here the final hidden state, which is then passed on to every hidden state of the decoder, which produces the target sequence.}
  \end{figure} 
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Attention}
  \begin{itemize}
    \item In a classical decoder-encoder RNN all information about the input sequence must be incorporated into the final hidden state, which is then passed as an input to the decoder network.
    \item With a long input sequence this fixed-sized context vector is unlikely to capture all relevant information about the past.
    \item It allows the decoder network to focus on different parts of the input sequence by adding connections from all hidden states of the encoder to each hidden state of the decoder
    \item At each point in time a set of weights is computed, which determine, how to combine the hidden states of the encoder into a context vector $c_i$, which holds the necessary information to predict the correct output.
    \item Each hidden state contains mostly information from recent inputs, in the case of a bidirectional RNN to encode the input sequence, a hidden state contains information from recent preceding and following inputs.
  \end{itemize}
\framebreak
  \begin{figure}
    \includegraphics[width=6.5cm]{plots/attention.png}
    \caption{Attention at $t=t+1$}
  \end{figure}
\framebreak
  \begin{figure}
    \includegraphics[width=6.5cm]{plots/attention2.png}
    \caption{Attention at $t=t+2$}
  \end{figure}
\framebreak
  \begin{figure}
    \includegraphics[width=5cm]{plots/attention3.png}
    \caption{Attention for image captioning: the attention mechanism tells the network roughly which pixels to pay attention to when writing the text (Kelvin Xu al. 2015)}
  \end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}
\frametitle{References}
\footnotesize{
\begin{thebibliography}{99}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Ian Goodfellow et al., 2016]{1} Ian Goodfellow, Yoshua Bengio and Aaron Courville (2016)
\newblock Deep Learning
\newblock \emph{\url{http://www.deeplearningbook.org/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Oriol Vinyals et al., 2014]{2} Oriol Vinyals, Alexander Toshev, Samy Bengio and Dumitru Erhan (2014)
\newblock Show and Tell: A Neural Image Caption Generator
\newblock \emph{\url{https://arxiv.org/abs/1411.4555}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Alex Graves, 2013]{3} Alex Graves (2013)
\newblock Generating Sequences With Recurrent Neural Networks
\newblock \emph{\url{https://arxiv.org/abs/1308.0850}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Namrata Anand and Prateek Verma, 2016]{4} Namrata Anand and Prateek Verma (2016)
\newblock Convolutional and recurrent nets for detecting emotion from audio data
\newblock \emph{\url{http://cs231n.stanford.edu/reports/2015/pdfs/Cs_231n_paper.pdf}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Andrew Owens et al., 2016]{5} Andrew Owens, Phillip Isola, Josh H. McDermott, Antonio Torralba, Edward H. Adelson and  William T. Freeman (2015)
\newblock Visually Indicated Sounds
\newblock \emph{\url{https://arxiv.org/abs/1512.08512}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Kelvin Xu al., 2016]{6} Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel and  Yoshua Bengio (2015)
\newblock Show, Attend and Tell: Neural Image Caption Generation with Visual Attention
\newblock \emph{\url{https://arxiv.org/abs/1502.03044}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{thebibliography}
}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlecture