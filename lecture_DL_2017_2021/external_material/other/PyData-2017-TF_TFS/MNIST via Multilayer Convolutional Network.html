<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>MNIST via Multilayer Convolutional Network - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta name="robots" content="nofollow">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/img/favicon.ico"/>
<script>window.settings = {"enableNotebookNotifications":true,"enableSshKeyUI":false,"defaultInteractivePricePerDBU":0.4,"enableOnDemandClusterType":true,"enableAutoCompleteAsYouType":[],"devTierName":"Community Edition","enableJobsPrefetching":true,"workspaceFeaturedLinks":[{"linkURI":"https://docs.databricks.com/index.html","displayName":"Documentation","icon":"question"},{"linkURI":"https://docs.databricks.com/release-notes/product/index.html","displayName":"Release Notes","icon":"code"},{"linkURI":"https://docs.databricks.com/spark/latest/training/index.html","displayName":"Training & Tutorials","icon":"graduation-cap"}],"enableClearStateFeature":false,"dbcForumURL":"http://forums.databricks.com/","enableProtoClusterInfoDeltaPublisher":true,"serverlessDefaultSparkVersion":"2.2.x-scala2.11","maxCustomTags":45,"serverlessDefaultMaxWorkers":20,"enableInstanceProfilesUIInJobs":true,"nodeInfo":{"node_types":[{"support_ssh":false,"spark_heap_memory":4800,"instance_type_id":"r3.2xlarge","spark_core_oversubscription_factor":8.0,"node_type_id":"dev-tier-node","description":"Community Optimized","support_cluster_tags":false,"container_memory_mb":6000,"node_instance_type":{"instance_type_id":"r3.2xlarge","provider":"AWS","local_disk_size_gb":160,"compute_units":26.0,"number_of_ips":14,"local_disks":1,"reserved_compute_units":3.64,"gpus":0,"memory_mb":62464,"num_cores":8,"local_disk_type":"AHCI","max_attachable_disks":0,"supported_disk_types":[{"ebs_volume_type":"GENERAL_PURPOSE_SSD"},{"ebs_volume_type":"THROUGHPUT_OPTIMIZED_HDD"}],"reserved_memory_mb":4800},"memory_mb":6144,"is_hidden":false,"category":"Community Edition","num_cores":0.88,"support_port_forwarding":false,"support_ebs_volumes":false,"is_deprecated":false}],"default_node_type_id":"dev-tier-node"},"enableDatabaseSupportClusterChoice":true,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":999999,"serverlessClusterProductName":"Serverless Pool","enableTableHandler":false,"maxEbsVolumesPerInstance":10,"isAdmin":true,"deltaProcessingBatchSize":1000,"timerUpdateQueueLength":100,"enableLargeResultDownload":true,"maxElasticDiskCapacityGB":5000,"serverlessDefaultMinWorkers":2,"zoneInfos":[{"id":"us-west-2c","isDefault":true},{"id":"us-west-2b","isDefault":false},{"id":"us-west-2a","isDefault":false}],"enableCustomSpotPricingUIByTier":false,"serverlessClustersEnabled":false,"enableEBSVolumesUIForJobs":true,"enablePublishNotebooks":true,"enableMaxConcurrentRuns":true,"enableJobAclsConfig":false,"enableFullTextSearch":false,"enableElasticSparkUI":false,"enableNewClustersCreate":true,"clusters":true,"allowRunOnPendingClusters":true,"fileStoreBase":"FileStore","enableSshKeyUIInJobs":true,"enableDetachAndAttachSubMenu":false,"configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableAdminPasswordReset":false,"enableResetPassword":true,"maxClusterTagValueLength":255,"enableJobsSparkUpgrade":true,"perClusterAutoterminationEnabled":false,"enableNotebookCommandNumbers":true,"sparkVersions":[{"key":"1.6.3-db2-hadoop2-scala2.10","displayName":"Spark 1.6.3-db2 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-aba860a0ffce4f3471fb14aefdcb1d768ac66a53a5ad884c48745ef98aeb9d67","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.1-db5-scala2.11","displayName":"Spark 2.1.1-db5 (Scala 2.11)","packageLabel":"spark-image-08d9fc1551087e0876236f19640c4a83116b1649f15137427d21c9056656e80e","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1 (Hadoop 1)","packageLabel":"spark-image-f710650fb8aaade8e4e812368ea87c45cd8cd0b5e6894ca6c94f3354e8daa6dc","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.2.x-scala2.11","displayName":"3.0 beta (Scala 2.11)","packageLabel":"spark-image-c29cc066f8182f63337d06fbdb5d0d42656a2456da28d7a6c6eba8ce6829d145","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.0-db2-scala2.11","displayName":"Spark 2.1.0-db2 (Scala 2.11)","packageLabel":"spark-image-267c4490a3ab8a39acdbbd9f1d36f6decdecebf013e30dd677faff50f1d9cf8b","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.1.x-gpu-scala2.11","displayName":"Spark 2.1 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-f43ddf8ad9acfd71338f5a51345f077173d236016ad7e39ffd6f698403acd4ea","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.0-ubuntu15.10-scala2.10","displayName":"Spark 2.0.0 (Scala 2.10)","packageLabel":"spark-image-073c1b52ace74f251fae2680624a0d8d184a8b57096d1c21c5ce56c29be6a37a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db3-scala2.10","displayName":"Spark 2.0.2-db3 (Scala 2.10)","packageLabel":"spark-image-584091dedb690de20e8cf22d9e02fdcce1281edda99eedb441a418d50e28088f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.11","displayName":"Spark 2.1.0-db1 (Scala 2.11)","packageLabel":"spark-image-e8ad5b72cf0f899dcf2b4720c1f572ab0e87a311d6113b943b4e1d4a7edb77eb","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.1.1-db4-scala2.11","displayName":"Spark 2.1.1-db4 (Scala 2.11)","packageLabel":"spark-image-52bca0ca866e3f4243d3820a783abf3b9b3b553edf234abef14b892657ceaca9","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db2-scala2.10","displayName":"Spark 2.1.0-db2 (Scala 2.10)","packageLabel":"spark-image-a2ca4f6b58c95f78dca91b1340305ab3fe32673bd894da2fa8e1dc8a9f8d0478","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.x-ubuntu15.10-hadoop1","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.2-db4-scala2.11","displayName":"Spark 2.0.2-db4 (Scala 2.11)","packageLabel":"spark-image-7dbc7583e8271765b8a1508cb9e832768e35489bbde2c4c790bc6766aee2fd7f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-image-21d1cac181b7b8856dd1b4214a3a734f95b5289089349db9d9c926cb87d843db","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-gpu-scala2.11","displayName":"Spark 2.0 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-968b89f1d0ec32e1ee4dacd04838cae25ef44370a441224177a37980d539d83a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.3-db1-hadoop2-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-eaa8d9b990015a14e032fb2e2e15be0b8d5af9627cd01d855df728b67969d5d9","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db2-hadoop1-scala2.10","displayName":"Spark 1.6.3-db2 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-14112ea0645bea94333a571a150819ce85573cf5541167d905b7e6588645cf3b","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop2","displayName":"Spark 1.6.2 (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.1-ubuntu15.10-hadoop2","displayName":"Spark 1.6.1 (Hadoop 2)","packageLabel":"spark-image-4cafdf8bc6cba8edad12f441e3b3f0a8ea27da35c896bc8290e16b41fd15496a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db2-scala2.10","displayName":"Spark 2.0.2-db2 (Scala 2.10)","packageLabel":"spark-image-36d48f22cca7a907538e07df71847dd22aaf84a852c2eeea2dcefe24c681602f","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-ubuntu15.10-scala2.11","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.11, deprecated)","packageLabel":"spark-image-8e1c50d626a52eac5a6c8129e09ae206ba9890f4523775f77af4ad6d99a64c44","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-859e88079f97f58d50e25163b39a1943d1eeac0b6939c5a65faba986477e311a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.1-db4-scala2.10","displayName":"Spark 2.1.1-db4 (Scala 2.10)","packageLabel":"spark-image-c7c0224de396cd1563addc1ae4bca6ba823780b6babe6c3729ddf73008f29ba4","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.11","displayName":"Spark 2.0.2-db1 (Scala 2.11)","packageLabel":"spark-image-c2d623f03dd44097493c01aa54a941fc31978ebe6d759b36c75b716b2ff6ab9c","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db4-scala2.10","displayName":"Spark 2.0.2-db4 (Scala 2.10)","packageLabel":"spark-image-859e88079f97f58d50e25163b39a1943d1eeac0b6939c5a65faba986477e311a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.1-db5-scala2.10","displayName":"Spark 2.1.1-db5 (Scala 2.10)","packageLabel":"spark-image-74133df2c13950431298d1cab3e865c191d83ac33648a8590495c52fc644c654","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2 (Hadoop 1)","packageLabel":"spark-image-c9d2a8abf41f157a4acc6d52bc721090346f6fea2de356f3a66e388f54481698","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.2.x-scala2.10","displayName":"3.0 beta (Scala 2.10)","packageLabel":"spark-image-3d735029d7732dbd8fa1c01ced355310df03f39534e7b12475ebb9e5551c6a64","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"3.0.x-scala2.11","displayName":"3.0 beta (Scala 2.11)","packageLabel":"spark-image-c29cc066f8182f63337d06fbdb5d0d42656a2456da28d7a6c6eba8ce6829d145","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-scala2.11","displayName":"Spark 2.0 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-7dbc7583e8271765b8a1508cb9e832768e35489bbde2c4c790bc6766aee2fd7f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.x-scala2.10","displayName":"Spark 2.1 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-74133df2c13950431298d1cab3e865c191d83ac33648a8590495c52fc644c654","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db3-scala2.10","displayName":"Spark 2.1.0-db3 (Scala 2.10)","packageLabel":"spark-image-25a17d070af155f10c4232dcc6248e36a2eb48c24f8d4fc00f34041b86bd1626","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db2-scala2.11","displayName":"Spark 2.0.2-db2 (Scala 2.11)","packageLabel":"spark-image-4fa852ba378e97815083b96c9cada7b962a513ec23554a5fc849f7f1dd8c065a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0 (Hadoop 1)","packageLabel":"spark-image-40d2842670bc3dc178b14042501847d76171437ccf70613fa397a7a24c48b912","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.1-db1-scala2.11","displayName":"Spark 2.0.1-db1 (Scala 2.11)","packageLabel":"spark-image-10ab19f634bbfdb860446c326a9f76dc25bfa87de6403b980566279142a289ea","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db3-scala2.11","displayName":"Spark 2.0.2-db3 (Scala 2.11)","packageLabel":"spark-image-7fd7aaa89d55692e429115ae7eac3b1a1dc4de705d50510995f34306b39c2397","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop1-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-d50af1032799546b8ccbeeb76889a20c819ebc2a0e68ea20920cb30d3895d3ae","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.10","displayName":"Spark 2.0.2-db1 (Scala 2.10)","packageLabel":"spark-image-654bdd6e9bad70079491987d853b4b7abf3b736fff099701501acaabe0e75c41","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-ubuntu15.10","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.10, deprecated)","packageLabel":"spark-image-a659f3909d51b38d297b20532fc807ecf708cfb7440ce9b090c406ab0c1e4b7e","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.1-db1-scala2.10","displayName":"Spark 2.0.1-db1 (Scala 2.10)","packageLabel":"spark-image-5a13c2db3091986a4e7363006cc185c5b1108c7761ef5d0218506cf2e6643840","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.1.x-scala2.11","displayName":"Spark 2.1 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-08d9fc1551087e0876236f19640c4a83116b1649f15137427d21c9056656e80e","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.10","displayName":"Spark 2.1.0-db1 (Scala 2.10)","packageLabel":"spark-image-f0ab82a5deb7908e0d159e9af066ba05fb56e1edb35bdad41b7ad2fd62a9b546","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"3.0.x-scala2.10","displayName":"3.0 beta (Scala 2.10)","packageLabel":"spark-image-3d735029d7732dbd8fa1c01ced355310df03f39534e7b12475ebb9e5551c6a64","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.0-ubuntu15.10","displayName":"Spark 1.6.0 (Hadoop 1)","packageLabel":"spark-image-10ef758029b8c7e19cd7f4fb52fff9180d75db92ca071bd94c47f3c1171a7cb5","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.x-ubuntu15.10-hadoop2","displayName":"Spark 1.6.x (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.0-ubuntu15.10-scala2.11","displayName":"Spark 2.0.0 (Scala 2.11)","packageLabel":"spark-image-b4ec141e751f201399f8358a82efee202560f7ed05e1a04a2ae8778f6324b909","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.1.0-db3-scala2.11","displayName":"Spark 2.1.0-db3 (Scala 2.11)","packageLabel":"spark-image-ccbc6b73f158e2001fc1fb8c827bfdde425d8bd6d65cb7b3269784c28bb72c16","upgradable":true,"deprecated":false,"customerVisible":true}],"enableRestrictedClusterCreation":true,"enableFeedback":true,"enableClusterAutoScaling":false,"enableUserVisibleDefaultTags":true,"defaultNumWorkers":0,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","enableNotebookRefresh":false,"accountsOwnerUrl":"https://accounts.cloud.databricks.com/registration.html#login","driverStdoutFilePrefix":"stdout","defaultNodeTypeToPricingUnitsMap":{"r3.2xlarge":2,"i3.4xlarge":6,"class-node":1,"m4.2xlarge":1.5,"r4.xlarge":1,"m4.4xlarge":3,"r4.16xlarge":16,"Standard_DS11":0.5,"p2.8xlarge":16,"m4.10xlarge":8,"r3.8xlarge":8,"r4.4xlarge":4,"dev-tier-node":1,"c3.8xlarge":4,"r3.4xlarge":4,"i2.4xlarge":6,"m4.xlarge":0.75,"r4.8xlarge":8,"r4.large":0.5,"Standard_DS12":1,"development-node":1,"i2.2xlarge":3,"g2.8xlarge":6,"i3.large":0.75,"memory-optimized":1,"m4.large":0.375,"p2.16xlarge":24,"i3.8xlarge":12,"i3.16xlarge":24,"Standard_DS12_v2":1,"Standard_DS13":2,"Standard_DS11_v2":0.5,"Standard_DS13_v2":2,"c3.2xlarge":1,"c4.2xlarge":1,"i2.xlarge":1.5,"compute-optimized":1,"c4.4xlarge":2,"i3.2xlarge":3,"c3.4xlarge":2,"g2.2xlarge":1.5,"p2.xlarge":2,"m4.16xlarge":12,"c4.8xlarge":4,"i3.xlarge":1.5,"r3.xlarge":1,"r4.2xlarge":2,"i2.8xlarge":12},"enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"enableEBSVolumesUI":false,"sanitizeMarkdownHtml":true,"metastoreServiceRowLimit":1000000,"enableIPythonImportExport":true,"enableClusterTagsUIForJobs":true,"enableClusterTagsUI":false,"enableNotebookHistoryDiffing":true,"branch":"2.48","accountsLimit":3,"enableSparkEnvironmentVariables":true,"enableX509Authentication":false,"enableStructuredStreamingNbOptimizations":true,"enableNotebookGitBranching":true,"local":false,"enableNotebookLazyRenderWrapper":false,"enableClusterAutoScalingForJobs":false,"enableStrongPassword":false,"displayDefaultContainerMemoryGB":6,"enableNotebookCommandMode":true,"disableS3TableImport":false,"deploymentMode":"production","useSpotForWorkers":true,"enableNonPollingTableCall":true,"enableUserInviteWorkflow":true,"enableStaticNotebooks":true,"enableCssTransitions":true,"serverlessEnableElasticDisk":true,"minClusterTagKeyLength":1,"showHomepageFeaturedLinks":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterAclsConfig":false,"useTempS3UrlForTableUpload":false,"notifyLastLogin":false,"enableSshKeyUIByTier":false,"defaultAutomatedPricePerDBU":0.2,"enableNotebookGitVersioning":true,"files":"files/","feedbackEmail":"feedback@databricks.com","enableDriverLogsUI":true,"enableWorkspaceAclsConfig":false,"dropzoneMaxFileSize":2047,"enableNewClustersList":false,"enableNewDashboardViews":true,"driverLog4jFilePrefix":"log4j","enableSingleSignOn":true,"enableMavenLibraries":true,"displayRowLimit":1000,"deltaProcessingAsyncEnabled":true,"enableSparkEnvironmentVariablesUI":false,"defaultSparkVersion":{"key":"2.1.x-scala2.10","displayName":"Spark 2.1 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-74133df2c13950431298d1cab3e865c191d83ac33648a8590495c52fc644c654","upgradable":true,"deprecated":false,"customerVisible":true},"enableCustomSpotPricing":false,"enableMountAclsConfig":false,"defaultAutoterminationMin":180,"useDevTierHomePage":true,"enableClusterClone":true,"enableNotebookLineNumbers":true,"enablePublishHub":false,"notebookHubUrl":"http://hub.dev.databricks.com/","showSqlEndpoints":false,"enableNotebookDatasetInfoView":false,"enableClusterAclsByTier":false,"databricksDocsBaseUrl":"https://docs.databricks.com/","cloud":"AWS","disallowAddingAdmins":true,"enableSparkConfUI":true,"featureTier":"DEVELOPER_BASIC_TIER","mavenCentralSearchEndpoint":"http://search.maven.org/solrsearch/select","enableOrgSwitcherUI":true,"clustersLimit":1,"enableJdbcImport":true,"enableElasticDisk":false,"logfiles":"logfiles/","enableWebappSharding":true,"enableClusterDeltaUpdates":true,"enableSingleSignOnLogin":false,"ebsVolumeSizeLimitGB":{"GENERAL_PURPOSE_SSD":[100,4096],"THROUGHPUT_OPTIMIZED_HDD":[500,4096]},"enableMountAcls":false,"requireEmailUserName":true,"dbcFeedbackURL":"mailto:feedback@databricks.com","enableMountAclService":true,"serverlessClustersByDefault":false,"enableWorkspaceAcls":false,"maxClusterTagKeyLength":127,"gitHash":"3594d0006cfff297612107c9065dcba3695eed9c","showWorkspaceFeaturedLinks":true,"signupUrl":"https://databricks.com/try-databricks","allowFeedbackForumAccess":true,"enableImportFromUrl":true,"enableMiniClusters":true,"enableNewJobList":true,"enableNewTableUI":true,"enableDebugUI":false,"enableStreamingMetricsDashboard":true,"allowNonAdminUsers":true,"enableSingleSignOnByTier":false,"enableJobsRetryOnTimeout":true,"useStandardTierUpgradeTooltips":true,"staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/","enableSpotClusterType":true,"enableSparkPackages":true,"dynamicSparkVersions":true,"enableClusterTagsUIByTier":false,"enableNotebookHistoryUI":true,"enableClusterLoggingUI":true,"enableDatabaseDropdownInTableUI":true,"showDebugCounters":false,"enableInstanceProfilesUI":false,"enableFolderHtmlExport":true,"homepageFeaturedLinks":[{"linkURI":"https://docs.databricks.com/_static/notebooks/gentle-introduction-to-apache-spark.html","displayName":"Introduction to Apache Spark on Databricks","icon":"img/home/Python_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/databricks-for-data-scientists.html","displayName":"Databricks for Data Scientists","icon":"img/home/Scala_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/structured-streaming-python.html","displayName":"Introduction to Structured Streaming","icon":"img/home/Python_icon.svg"}],"enableClusterStart":false,"enableEBSVolumesUIByTier":false,"singleSignOnComingSoon":false,"upgradeURL":"https://accounts.cloud.databricks.com/registration.html#login","maxAutoterminationMinutes":10000,"autoterminateClustersByDefault":true,"notebookLoadingBackground":"#fff","sshContainerForwardedPort":2200,"enableServerAutoComplete":true,"enableStaticHtmlImport":true,"enableInstanceProfilesByTier":false,"showForgotPasswordLink":true,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"minAutoterminationMinutes":10,"accounts":true,"useOnDemandClustersByDefault":true,"useFramedStaticNotebooks":false,"enableNewProgressReportUI":true,"enableAutoCreateUserUI":true,"defaultCoresPerContainer":4,"showTerminationReason":true,"enableNewClustersGet":true,"showPricePerDBU":false,"showSqlProxyUI":true,"enableNotebookErrorHighlighting":true};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":15647574355542,"name":"MNIST via Multilayer Convolutional Network","language":"python","commands":[{"version":"CommandV1","origId":15647574355544,"guid":"ad6bb7fb-ac27-465f-b033-12394ea90f68","subtype":"command","commandType":"auto","position":1.5,"command":"%md ## MNIST via Multilayer Convolution Network\nThis notebook is based on the MNIST for Deep Learning Experts using TensorFlow\n\nSource: [mnist_deep.py](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/mnist/mnist_deep.py)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1498534890425,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"8b907d7d-528e-469b-8b3e-1eaf7df6e1e7"},{"version":"CommandV1","origId":15647574355545,"guid":"84c9bf8b-ab0a-4338-8479-3c7fb43c0cf1","subtype":"command","commandType":"auto","position":1.625,"command":"%md ## Cluster set-up\n\nTensorFrames is available as a Spark Package. To use it on your cluster, create a new library with the Source option \"Maven Coordinate\", using \"Search Spark Packages and Maven Central\" to find \"spark-deep-learning\". Then [attach the library to a cluster](https://docs.databricks.com/user-guide/libraries.html). To run this notebook, also create and attach the following libraries: \n* via PyPI: tensorflow\n* via Spark Packages: tensorframes\n\nThe latest version of TensorFrames is compatible with Spark versions 2.0 or higher and works with any instance type (CPU or GPU).","commandVersion":0,"state":"finished","results":null,"errorSummary":"<span class=\"ansired\">SyntaxError</span><span class=\"ansired\">:</span> invalid syntax","error":null,"workflows":[],"startTime":0,"submitTime":1498534890437,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"00d3c724-2e40-4370-886b-a1174fec245d"},{"version":"CommandV1","origId":15647574355714,"guid":"eea3f135-0c7d-48c7-8a18-d3b22948da41","subtype":"command","commandType":"auto","position":1.765625,"command":"%md\n\n## Convolution, pooling, weights and biases\n\nThe first method helps to create a 2-D convolutional layer for a specified input ```x``` and specified kernel size ```W```. The ```strides``` parameter specifies the *movement* of the kernel on the original image: in this case each convolution will *skip* one pixel at a time. The ```padding``` parameter keeps the output size of the features ```SAME``` as the input.\n\nThe pooling layers will be created using the ```max_pool(...)``` method. Each of our pooling layers has a 2x2 kernel and will skip---the ```strides``` parameter---2 rows and 2 columns at a time. The ```padding``` parameter here will apply padding if any downsampling results in a fractional number of pixels.\n\nEvery neuron (apart from those in the input layer) is connected to its predecessors via a number of links, each with a specific weight attached to it. The code below initializes the weights to a random number drawn from a ```truncated_normal``` distribution with standard deviation of 0.1. The ```shape``` parameter is used by the method's logic to determine how many weights to create.\n\nJust like the weights, each neuron has some *tuneable* bias. The ```bias_variable(...)``` method initializes the biases for the neurons to a ```constant``` equal to 0.1; these biases change as we train the network.","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ef08fe84-bddf-49fa-986a-fc056271e7cf"},{"version":"CommandV1","origId":15647574355548,"guid":"dadbdf5f-604f-4edd-9994-9ce67798804d","subtype":"command","commandType":"auto","position":1.78125,"command":"# Convolution and Pooling\ndef conv2d(x, W):\n  \"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\"\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\n\ndef max_pool_2x2(x):\n  \"\"\"max_pool_2x2 downsamples a feature map by 2X.\"\"\"\n  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                        strides=[1, 2, 2, 1], padding='SAME')\n\n# Weight Initialization\ndef weight_variable(shape):\n  \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n  initial = tf.truncated_normal(shape, stddev=0.1)\n  return tf.Variable(initial)\n\n\ndef bias_variable(shape):\n  \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n  initial = tf.constant(0.1, shape=shape)\n  return tf.Variable(initial)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1498628331206,"submitTime":1498628331129,"finishTime":1498628331234,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"466a46a9-a7c0-49cd-8614-9c6fe4e6be2c"},{"version":"CommandV1","origId":15647574355688,"guid":"fd03b6f9-74ad-4c13-86e7-bd4fa6dae26e","subtype":"command","commandType":"auto","position":1.875,"command":"%md\n\n## Create the model\n\nThe ```deepnn(...)``` method builds the artificial neural network we will use to classify the digits. \n\nHere's the overview of the structure of the network we will use:\n\n![](http://tomdrabas.com/data/PyData/ConvStructure_NoTanh.png)\n\n1. The input layer is a grey scale image of 28x28 pixels. This is accomplished with the ```tf.reshape(...)``` method. The first parameter to the function is the *tensor* we want to reshape, and the other one specifies the desired shape of the output.\n2. Convolution layer 1 maps one grayscale image to 32 feature maps using 5x5 kernels using ReLU (rectifier) activation function\n3. Pooling layer 1 down samples image by 2x so you have a 14x14 matrix \n4. Convolution layer 2 maps 32 feature maps to 64 using ReLU (rectifier) activation function\n5. Pooling layer 2  down samples by 2x with 64 images of 7x7 (vs. 14x14)\n6. The fully connected feed-forward part maps the 64 features of 7x7 pixels to an array of 1024 neurons that then get passed throught the ```argmax(...)``` method to come up with an actual output with one activated neuron. The ```matmul(...)``` method is nothing more than a matrix multiplication applied to an input matrix and corresponding matrix of weights.\n","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"05d6609b-89ae-44f0-b5dd-472606d5f667"},{"version":"CommandV1","origId":15647574355547,"guid":"5797c9b9-74ad-4b87-9dd2-8dbb4cec5f3c","subtype":"command","commandType":"auto","position":2.0,"command":"def deepnn(x):\n  \"\"\"deepnn builds the graph for a deep net for classifying digits.\n  Args:\n    x: an input tensor with the dimensions (N_examples, 784), where 784 is the\n    number of pixels in a standard MNIST image.\n  Returns:\n    A tuple (y, keep_prob). y is a tensor of shape (N_examples, 10), with values\n    equal to the logits of classifying the digit into one of 10 classes (the\n    digits 0-9). keep_prob is a scalar placeholder for the probability of\n    dropout.\n  \"\"\"\n  # Reshape to use within a convolutional neural net.\n  # Last dimension is for \"features\" - there is only one here, since images are\n  # grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n  x_image = tf.reshape(x, [-1, 28, 28, 1])\n\n  # First convolutional layer - maps one grayscale image to 32 feature maps.\n  W_conv1 = weight_variable([5, 5, 1, 32])\n  b_conv1 = bias_variable([32])\n  h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n  \n  # Pooling layer - downsamples by 2X.\n  h_pool1 = max_pool_2x2(h_conv1)\n\n  # Second convolutional layer -- maps 32 feature maps to 64.\n  W_conv2 = weight_variable([5, 5, 32, 64])\n  b_conv2 = bias_variable([64])\n  h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n  \n  # Second pooling layer.\n  h_pool2 = max_pool_2x2(h_conv2)\n\n  # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n  # is down to 7x7x64 feature maps -- maps this to 1024 features.\n  W_fc1 = weight_variable([7 * 7 * 64, 1024])\n  b_fc1 = bias_variable([1024])\n\n  h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n  h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n  # Dropout - controls the complexity of the model, prevents co-adaptation of\n  # features.\n  keep_prob = tf.placeholder(tf.float32)\n  h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n  # Map the 1024 features to 10 classes, one for each digit\n  W_fc2 = weight_variable([1024, 10])\n  b_fc2 = bias_variable([10])\n\n  y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n  return y_conv, keep_prob","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1498628331077,"submitTime":1498628331059,"finishTime":1498628331202,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"55e40385-fe39-4077-8525-49557eac2f10"},{"version":"CommandV1","origId":15647574355713,"guid":"5b73c6f1-9f7d-42cb-ad8d-a64cdc8f22cd","subtype":"command","commandType":"auto","position":2.5,"command":"%md\n## Training the model\n\nThis is the main script of the tutorial where we use all that we have defined so far. \n\nFirst, we read in the ```MNIST_data``` and specify that the output is to be ```one_hot``` encoded. Next, we create the placeholders for our input and output. Using the previously defined ```deepnn(...)``` method we create our DNN to be trained. \n\nThe goal of training our network is to minimze the ```cross_entropy```; the ```cross_entropy``` is defined as an average of the outputs from the ```softmax``` layer. We are using the ```AdamOptimizer``` with a specified learning rate of 0.0001. \n\nTo determine if the network has produced a correct prediction we use the ```argmax(...)``` method that returns an index associated with the maximum value in our output layer; this is then compared with the *desired* signal using the ```equal(...)``` method. \n\nThe overall accuracy is calculated as a mean of the correct and incorrect responses.\n\nTo train our network we use batches of 50 images; the weights are adjusted only after each batch iteration. ","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"c13bf6ca-a525-478c-819a-35c8b6583e54"},{"version":"CommandV1","origId":15647574355549,"guid":"9c308d86-2488-47d6-922f-0f2553eb3e32","subtype":"command","commandType":"auto","position":4.5,"command":"# Import TensorFlow\nimport tensorflow as tf\n\n# Import MNIST digit images data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n\n# Create the model\nx = tf.placeholder(tf.float32, [None, 784])\n\n# Define loss and optimizer\ny_ = tf.placeholder(tf.float32, [None, 10])\n\n# Build the graph for the deep net\ny_conv, keep_prob = deepnn(x)\n\ncross_entropy = tf.reduce_mean(\n  tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\ncorrect_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n# Launch the model\nsess = tf.InteractiveSession()\n\n# Initialize the variables\ntf.global_variables_initializer().run()\n\n# By default, should have the range go to 20,000 \nfor i in range(1500):\n  batch = mnist.train.next_batch(50)\n  if i % 100 == 0:\n    train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})\n    print('step %d, training accuracy %g' % (i, train_accuracy))\n  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})  ","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Extracting MNIST_data/train-images-idx3-ubyte.gz\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\nstep 0, training accuracy 0.08\nstep 100, training accuracy 0.86\nstep 200, training accuracy 0.92\nstep 300, training accuracy 0.9\nstep 400, training accuracy 0.98\nstep 500, training accuracy 0.9\nstep 600, training accuracy 0.94\nstep 700, training accuracy 0.94\nstep 800, training accuracy 0.9\nstep 900, training accuracy 0.94\nstep 1000, training accuracy 0.92\nstep 1100, training accuracy 0.98\nstep 1200, training accuracy 0.98\nstep 1300, training accuracy 0.94\nstep 1400, training accuracy 0.96\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<span class=\"ansired\">ValueError</span>: Dimensions must be equal, but are 32 and 6 for &apos;add&apos; (op: &apos;Add&apos;) with input shapes: [?,28,28,32], [6].","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">ValueError</span>                                Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-3-57771f8a80b5&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">     13</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     14</span> <span class=\"ansired\"># Build the graph for the deep net</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 15</span><span class=\"ansiyellow\"> </span>y_conv<span class=\"ansiyellow\">,</span> keep_prob <span class=\"ansiyellow\">=</span> deepnn<span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     16</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     17</span> cross_entropy = tf.reduce_mean(\n\n<span class=\"ansigreen\">&lt;ipython-input-1-609934e9144c&gt;</span> in <span class=\"ansicyan\">deepnn</span><span class=\"ansiblue\">(x)</span>\n<span class=\"ansigreen\">     18</span>   W_conv1 <span class=\"ansiyellow\">=</span> weight_variable<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">5</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">5</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">32</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     19</span>   b_conv1 <span class=\"ansiyellow\">=</span> bias_variable<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">6</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 20</span><span class=\"ansiyellow\">   </span>h_conv1 <span class=\"ansiyellow\">=</span> tf<span class=\"ansiyellow\">.</span>nn<span class=\"ansiyellow\">.</span>relu<span class=\"ansiyellow\">(</span>conv2d<span class=\"ansiyellow\">(</span>x_image<span class=\"ansiyellow\">,</span> W_conv1<span class=\"ansiyellow\">)</span> <span class=\"ansiyellow\">+</span> b_conv1<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     21</span>   h_tanh1 <span class=\"ansiyellow\">=</span> tf<span class=\"ansiyellow\">.</span>nn<span class=\"ansiyellow\">.</span>tanh<span class=\"ansiyellow\">(</span>h_conv1<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     22</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.pyc</span> in <span class=\"ansicyan\">binary_op_wrapper</span><span class=\"ansiblue\">(x, y)</span>\n<span class=\"ansigreen\">    836</span>           <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    837</span>             <span class=\"ansigreen\">raise</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 838</span><span class=\"ansiyellow\">       </span><span class=\"ansigreen\">return</span> func<span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">,</span> y<span class=\"ansiyellow\">,</span> name<span class=\"ansiyellow\">=</span>name<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    839</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    840</span>   <span class=\"ansigreen\">def</span> binary_op_wrapper_sparse<span class=\"ansiyellow\">(</span>sp_x<span class=\"ansiyellow\">,</span> y<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.pyc</span> in <span class=\"ansicyan\">add</span><span class=\"ansiblue\">(x, y, name)</span>\n<span class=\"ansigreen\">     65</span>     A <span class=\"ansiyellow\">&#96;</span>Tensor<span class=\"ansiyellow\">&#96;</span><span class=\"ansiyellow\">.</span> Has the same type <span class=\"ansigreen\">as</span> <span class=\"ansiyellow\">&#96;</span>x<span class=\"ansiyellow\">&#96;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     66</span>   &quot;&quot;&quot;\n<span class=\"ansigreen\">---&gt; 67</span><span class=\"ansiyellow\">   </span>result <span class=\"ansiyellow\">=</span> _op_def_lib<span class=\"ansiyellow\">.</span>apply_op<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;Add&quot;</span><span class=\"ansiyellow\">,</span> x<span class=\"ansiyellow\">=</span>x<span class=\"ansiyellow\">,</span> y<span class=\"ansiyellow\">=</span>y<span class=\"ansiyellow\">,</span> name<span class=\"ansiyellow\">=</span>name<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     68</span>   <span class=\"ansigreen\">return</span> result<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     69</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc</span> in <span class=\"ansicyan\">apply_op</span><span class=\"ansiblue\">(self, op_type_name, name, **keywords)</span>\n<span class=\"ansigreen\">    765</span>         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n<span class=\"ansigreen\">    766</span>                          input_types<span class=\"ansiyellow\">=</span>input_types<span class=\"ansiyellow\">,</span> attrs<span class=\"ansiyellow\">=</span>attr_protos<span class=\"ansiyellow\">,</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 767</span><span class=\"ansiyellow\">                          op_def=op_def)\n</span><span class=\"ansigreen\">    768</span>         <span class=\"ansigreen\">if</span> output_structure<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    769</span>           outputs <span class=\"ansiyellow\">=</span> op<span class=\"ansiyellow\">.</span>outputs<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc</span> in <span class=\"ansicyan\">create_op</span><span class=\"ansiblue\">(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)</span>\n<span class=\"ansigreen\">   2506</span>                     original_op=self._default_original_op, op_def=op_def)\n<span class=\"ansigreen\">   2507</span>     <span class=\"ansigreen\">if</span> compute_shapes<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 2508</span><span class=\"ansiyellow\">       </span>set_shapes_for_outputs<span class=\"ansiyellow\">(</span>ret<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2509</span>     self<span class=\"ansiyellow\">.</span>_add_op<span class=\"ansiyellow\">(</span>ret<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2510</span>     self<span class=\"ansiyellow\">.</span>_record_op_seen_by_control_dependencies<span class=\"ansiyellow\">(</span>ret<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc</span> in <span class=\"ansicyan\">set_shapes_for_outputs</span><span class=\"ansiblue\">(op)</span>\n<span class=\"ansigreen\">   1871</span>       shape_func <span class=\"ansiyellow\">=</span> _call_cpp_shape_fn_and_require_op<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1872</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 1873</span><span class=\"ansiyellow\">   </span>shapes <span class=\"ansiyellow\">=</span> shape_func<span class=\"ansiyellow\">(</span>op<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1874</span>   <span class=\"ansigreen\">if</span> shapes <span class=\"ansigreen\">is</span> None<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1875</span>     raise RuntimeError(\n\n<span class=\"ansigreen\">/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc</span> in <span class=\"ansicyan\">call_with_requiring</span><span class=\"ansiblue\">(op)</span>\n<span class=\"ansigreen\">   1821</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1822</span>   <span class=\"ansigreen\">def</span> call_with_requiring<span class=\"ansiyellow\">(</span>op<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 1823</span><span class=\"ansiyellow\">     </span><span class=\"ansigreen\">return</span> call_cpp_shape_fn<span class=\"ansiyellow\">(</span>op<span class=\"ansiyellow\">,</span> require_shape_fn<span class=\"ansiyellow\">=</span>True<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1824</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1825</span>   _call_cpp_shape_fn_and_require_op <span class=\"ansiyellow\">=</span> call_with_requiring<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc</span> in <span class=\"ansicyan\">call_cpp_shape_fn</span><span class=\"ansiblue\">(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)</span>\n<span class=\"ansigreen\">    608</span>     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n<span class=\"ansigreen\">    609</span>                                   input_tensors_as_shapes_needed<span class=\"ansiyellow\">,</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 610</span><span class=\"ansiyellow\">                                   debug_python_shape_fn, require_shape_fn)\n</span><span class=\"ansigreen\">    611</span>     <span class=\"ansigreen\">if</span> <span class=\"ansigreen\">not</span> isinstance<span class=\"ansiyellow\">(</span>res<span class=\"ansiyellow\">,</span> dict<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    612</span>       <span class=\"ansired\"># Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/local/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc</span> in <span class=\"ansicyan\">_call_cpp_shape_fn_impl</span><span class=\"ansiblue\">(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)</span>\n<span class=\"ansigreen\">    674</span>       missing_shape_fn <span class=\"ansiyellow\">=</span> True<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    675</span>     <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 676</span><span class=\"ansiyellow\">       </span><span class=\"ansigreen\">raise</span> ValueError<span class=\"ansiyellow\">(</span>err<span class=\"ansiyellow\">.</span>message<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    677</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    678</span>   <span class=\"ansigreen\">if</span> missing_shape_fn<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">ValueError</span>: Dimensions must be equal, but are 32 and 6 for &apos;add&apos; (op: &apos;Add&apos;) with input shapes: [?,28,28,32], [6].</div>","workflows":[],"startTime":1498628331238,"submitTime":1498628331207,"finishTime":1498628494379,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ab7f04c4-08e0-4828-8568-751a8f76d34a"},{"version":"CommandV1","origId":15647574355715,"guid":"b63dce60-7714-495d-ae92-2927c06199b7","subtype":"command","commandType":"auto","position":4.6015625,"command":"%md\n\nFinally, we test the accuracy of our trained network. ","commandVersion":0,"state":"error","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"68a7cd60-6e6f-4014-bbed-378d493e37e9"},{"version":"CommandV1","origId":15647574355550,"guid":"e66f50a0-22a3-4a4a-9667-3617a06b4610","subtype":"command","commandType":"auto","position":4.703125,"command":"# Take first 500 images\nprint('test accuracy %g' % accuracy.eval(feed_dict={x: mnist.test.images[:500, :784], y_: mnist.test.labels[:500,:10], keep_prob: 1.0}))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">test accuracy 0.966\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[],"datasetInfos":[]},"errorSummary":"<span class=\"ansired\">NameError</span>: name &apos;accuracy&apos; is not defined","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-4-fa0a52c32a26&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> <span class=\"ansired\"># Take first 500 images</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 2</span><span class=\"ansiyellow\"> </span><span class=\"ansigreen\">print</span><span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;test accuracy %g&apos;</span> <span class=\"ansiyellow\">%</span> accuracy<span class=\"ansiyellow\">.</span>eval<span class=\"ansiyellow\">(</span>feed_dict<span class=\"ansiyellow\">=</span><span class=\"ansiyellow\">{</span>x<span class=\"ansiyellow\">:</span> mnist<span class=\"ansiyellow\">.</span>test<span class=\"ansiyellow\">.</span>images<span class=\"ansiyellow\">[</span><span class=\"ansiyellow\">:</span><span class=\"ansicyan\">500</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">:</span><span class=\"ansicyan\">784</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> y_<span class=\"ansiyellow\">:</span> mnist<span class=\"ansiyellow\">.</span>test<span class=\"ansiyellow\">.</span>labels<span class=\"ansiyellow\">[</span><span class=\"ansiyellow\">:</span><span class=\"ansicyan\">500</span><span class=\"ansiyellow\">,</span><span class=\"ansiyellow\">:</span><span class=\"ansicyan\">10</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> keep_prob<span class=\"ansiyellow\">:</span> <span class=\"ansicyan\">1.0</span><span class=\"ansiyellow\">}</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: name &apos;accuracy&apos; is not defined</div>","workflows":[],"startTime":1498628494384,"submitTime":1498628331268,"finishTime":1498628494707,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"44c3772b-37f4-44ce-8eb9-0c63dbac83b4"},{"version":"CommandV1","origId":15647574355551,"guid":"f8de7ab9-bd24-442e-8c42-d0e215dc9adf","subtype":"command","commandType":"auto","position":9.0,"command":"%md\nIn fact, the accuracy could be much higher if we set it to higher iterations (e.g. 20000) and tested it against the full test dataset\n```\nstep 19500, training accuracy 1\nstep 19600, training accuracy 1\nstep 19700, training accuracy 1\nstep 19800, training accuracy 1\nstep 19900, training accuracy 1\ntest accuracy 0.9895\n```","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1498628331318,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"aae15611-35c2-4c1c-a5df-9559f30c5532"}],"dashboards":[],"guid":"c9ba5411-9217-4d6d-9e87-6336d81dca0c","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>
