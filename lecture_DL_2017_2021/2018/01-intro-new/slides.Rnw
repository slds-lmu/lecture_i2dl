%Blank Lecture
%This file is a child of preamble.Rnw in the style folder
%if you want to add stuff to the preamble go there to make
%your changes available to all childs


<<setup-child, include = FALSE>>=
library(knitr)
set_parent("../style/preamble.Rnw")
@

<<size = "scriptsize", include=FALSE>>=
source("code/functions.R")
@

\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-nn}

\lecturechapter{1}{Introduction to Deep Learning}
\lecture{Deeplearning}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {Applications of Deep Learning}
  \begin{figure}
    \centering
      \scalebox{0.95}{\includegraphics{plots/maskrcnn.png}}
  \end{figure}
   \hspace{3cm}     \textbf{Machine Vision} (Credit : Kaiming He)
\end{frame}

\begin{frame}  {Applications of Deep Learning}
  \vspace{5mm}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/nmt.png}}
  \end{figure}
    \hspace{4cm} \textbf{Machine Translation}
\end{frame}

\begin{frame}  {Applications of Deep Learning}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/speech_goog.jpg}}

  \end{figure}
  \hspace{1cm} \textbf{Speech Recognition and Generation} (Source : Google)
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%SINGLE NEURON%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{frame} {Intro to ML 1*}
%   \begin{itemize}
%     \item The universe, laws and patterns
%     \item Approximate the function f
%     \item Learn from experience
%   \end{itemize}
% \end{frame}
% 
% \begin{frame} {Intro to ML 2*}
%   \begin{itemize}
%     \item SL vs UL
%     \item Features
%     \item labels, regression, classification
%   \end{itemize}
% \end{frame}

\begin{frame} {Introduction}
  \begin{itemize}
    \item All machine learning algorithms consist of three key components:
    \vspace{4mm}
    \item The \textbf{hypothesis space}:
    \begin{itemize}
      \item This is basically the search space of the algorithm. It is the predefined set of functions from which the algorithm picks a function/model that is the best fit to the data.
    \end{itemize}
    \vspace{4mm}
    \item \textbf{Objective function}:
    \begin{itemize}
      \item A metric by which to evaluate models in the hypothesis space.
      \item The model returned by the algorithm must perform well on \textit{unseen} data.
    \end{itemize}
    \vspace{4mm}
    \item \textbf{Optimizer}:
      \begin{itemize}
        \item A method/algorithm to find the "right" model.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame} {Introduction}
  \begin{itemize}
    \vspace{10mm}
    \item Neural networks are fundamentally a special kind of hypothesis space (\textit{very}) loosely inspired by the organisation of neurons in biological brains.
    \vspace{5mm}
    \item This lecture is about the nature of this hypothesis space.
    \vspace{5mm}
    \item Some (important!) added tricks related to optimization will be covered in later lectures.
  \end{itemize}
\end{frame}

% \begin{frame} {Logistic Regression*}
%   \begin{itemize}
%     \item In order to better understand the types of functions that neural networks can represent, we begin by briefly examining a very simple machine learning algorithm : logistic regression. REWORD
%     \vspace{3mm}
%     \item In a binary classification task, the goal is to map an input $x$ to a binary target variable $y$.
%     \vspace{3mm}
%     \item It models y as {TODO}    
%       \begin{itemize}
%         \item Weights {TODO}
%         \item The bias term must not be confused with the statistical bias. It actually is an intercept parameter.
%     \item Logistic regression example {TODO}
%         \item Sigmoid function {TODO}
%       \end{itemize}
%   \end{itemize}
% \end{frame}

% \begin{frame} {Logistic Regression}
%   \begin{itemize}
%     \item Sigmoid function $$ \sigma(v) = \frac{1}{(1+\exp (-s \cdot v))} $$
%     <<echo=FALSE, fig.height=2.7>>=
% library(ggplot2)
% logfun = function(v, s) {
%   1 / (1 + exp(- s * v))
% }
% x = seq(-10, 10, 0.1)
% stretch = c(0.25, 1, 10)
% y = sapply(stretch, function(s) {
%   sapply(x, logfun, s = s)
% })
% df = data.frame(y = as.vector(y), x = rep(x, length(stretch)),
%   s = as.factor(rep(stretch, each = length(x))))
% 
% logfun.q = ggplot(df, aes(x = x, y = y, color = s)) + geom_line(size=1)
% logfun.q = logfun.q + scale_y_continuous(name = NULL)
% logfun.q = logfun.q + scale_x_continuous(name = NULL)
% logfun.q = logfun.q + theme(axis.title = element_text(size = 14L, face = "bold"),
%   plot.margin = unit(c(0, 0, 0, 0), "cm"))
% logfun.q
% @
%     \item \small{This "squashes" the output to be between 0 and 1.}
%   \end{itemize}
% \end{frame}
% 
% \begin{frame} {Logistic Regression}
%   \begin{itemize}
%   \item Some important properties of the sigmoidal logistic function include:
%   \item[]
%   \item[]
%   \begin{itemize}
%     \item limits: $$\lim_{v \to -\infty} \sigma(v) = 0 \text{ and } \lim_{v \to \infty} \sigma(v) = 1$$
%     \item the derivative for $s = 1$: $$\frac{\delta\sigma(v)}{\delta v}=\frac{\exp(v)}{(1+\exp(v))^2} = \sigma(v)(1-\sigma(v))$$
%     \item for any s: $$\sigma(v) \text{ is symmetrical in } (0, 0.5)$$
%   \end{itemize}
% \end{itemize}
% \end{frame}
% 
% \begin{frame} {Types of boundaries that can be learned}
%   \begin{itemize}
%     \item Different values of $\mathbf{w}$ and $b$ map to different linear decision boundaries separating the two classes.
%     \vspace{5mm}
%     \begin{figure}
%     \centering
%       \scalebox{1}{\includegraphics{plots/lin_bound.png}}
%   \end{figure}
%     \vspace{5mm}
%   Note : Only two dimensions pictured here
%   \end{itemize}
% \end{frame}
% 
% \begin{frame} {Types of boundaries that can be learned}
% \begin{itemize}
%     \item As already stated, the bias term must not be confused with the statistical bias.
%     \item It is actually an intercept parameter, which puts the decision boundary at the correct position in the learned space.
%     % Deeplearningbook page 110: This terminology derives from the point of view that the output of the transformation is biased toward being b in the absence of any input.
%   \end{itemize}
%     \begin{figure}
%       \centering
%         \includegraphics[width=8.5cm]{plots/bias.png}
%     \end{figure}
% \end{frame}

\begin{frame} {A Single Neuron}
  \begin{itemize}
    \vspace{5mm}
    \item In order to better understand the types of functions that neural networks can represent, let's begin with a very simple model : logistic regression.
    \vspace{5mm}
    \item We've seen that the hypothesis space of logistic regression can be written as $f(\mathbf{x}) = \tau(w_1x_1 + w_2x_2 + w_3x_3 + b)$, where $\tau$ is the logistic sigmoid.
    \vspace{5mm}
    \item It is very straightforward to represent this function $f(\mathbf{x})$ graphically as a neuron.
    \vspace{5mm}
    \item Note : $\wtw$ and $b$ together constitute $\theta$.
  \end{itemize}
\end{frame}

\begin{frame} {A Single Neuron}
  \begin{itemize}
    \item In logistic regression : $f(\mathbf{x}) = \tau(w_1\textcolor{red}{x_1} + w_2\textcolor{red}{x_2} + w_3\textcolor{red}{x_3} + b)$.
    \item First, the features of $\mathbf{x}$ are represented by the nodes in the "input layer".
    \begin{figure}
    \centering
      \scalebox{0.7}{\includegraphics{plots/neurep_one.png}}
  \end{figure}
    \item In general, a $p$-dimensional input vector $\mathbf{x}$ will be represented by $p$ nodes in the input layer.
  \end{itemize}
\end{frame}

\begin{frame} {A Single Neuron}
  \begin{itemize}
    \item In logistic regression, $f(\mathbf{x}) = \tau(\textcolor{red}{w_1}x_1 + \textcolor{red}{w_2}x_2 + \textcolor{red}{w_3}x_3 + b)$.
    \item Next, the weights $\mathbf{w}$ are represented by the edges from the input layer.
    \begin{figure}
    \centering
      \scalebox{0.7}{\includegraphics{plots/neurep_two.png}}
  \end{figure}
    \item The bias term is implicit. It is not shown/represented visually.
  \end{itemize}
\end{frame}

\begin{frame} {A Single Neuron}
  \begin{itemize}
    \item Recall : In logistic regression, $\textcolor{red}{f(\mathbf{x}) = \tau(w_1x_1 + w_2x_2 + w_3x_3 + b)}$.
    \item Finally, the computation $\tau(w_1x_1 + w_2x_2 + w_3x_3 + b)$ is represented by the neuron in the "output layer".
    \begin{figure}
    \centering
      \scalebox{0.7}{\includegraphics{plots/neurep_three.png}}
  \end{figure}
    \item \small{Because this single neuron represents exactly the same hypothesis space as logistic regression, it can only learn linear decision boundaries.}
  \end{itemize}
\end{frame}

\begin{frame} {A Single Neuron}
  \begin{itemize}
    \item Therefore, a neuron is just a graphical representation of a function, but it's a very specific kind of function.
    \vspace{4mm}
    \item Every neuron performs a 2-step computation:
      \begin{itemize}
        \item Step 1: Compute the weighted sum of inputs (with bias).
        \item Step 2: Apply an \textbf{activation function} to the sum.This can be used for a non-linear transformation of the input.
      \end{itemize}
    \vspace{4mm}
    \item With a single neuron, the activation function serves to constrain the output to the desired range of values.
    \vspace{4mm}
    \item However, we'll see that activation functions serve a far more important purpose. They are one of the main reasons that neural networks can represent extremely complicated functions.
  \end{itemize}
\end{frame}

\begin{frame} {A Single Neuron}
  \begin{itemize}
    \item \small{One very nice thing about the graphical representation of the function is that you can picture the input vector being "fed" to the neuron on the left followed by a sequence of computations being performed from left to right.}\small{This is called a "\textbf{forward pass}"}.
  \end{itemize}
  \begin{figure}
    \centering
      \scalebox{0.70}{\includegraphics{plots/neuron_one.png}}
      \caption{Weights (and bias) of the neuron.}
  \end{figure}
\end{frame}

\begin{frame} {A Single Neuron}
  \begin{itemize}
    \item \small{One very nice thing about the graphical representation of the function is that you can picture the input vector being "fed" to the neuron on the left followed by a sequence of computations being performed from left to right.}\small{This is called a "\textbf{forward pass}"}.
  \end{itemize}
  \begin{figure}
    \centering
      \scalebox{0.70}{\includegraphics{plots/neuron_two.png}}
      \caption{Feed the input on the left.}
  \end{figure}
\end{frame}

\begin{frame} {A Single Neuron}
  \begin{itemize}
    \item \small{One very nice thing about the graphical representation of the function is that you can picture the input vector being "fed" to the neuron on the left followed by a sequence of computations being performed from left to right}.\small{This is called a "\textbf{forward pass}".}
  \end{itemize}
  \begin{figure}
    \centering
      \scalebox{0.70}{\includegraphics{plots/neuron_three.png}}
      \caption{Step 1 : Compute the weighted sum.}
  \end{figure}
\end{frame}

\begin{frame} {A Single Neuron}
  \begin{itemize}
    \item \small{One very nice thing about the graphical representation of the function is that you can picture the input vector being "fed" to the neuron on the left followed by a sequence of computations being performed from left to right}.\small{This is called a "\textbf{forward pass}".}
  \end{itemize}
  \begin{figure}
    \centering
      \scalebox{0.70}{\includegraphics{plots/neuron_six.png}}
      \caption{Step 2 : Apply the activation function.}
  \end{figure}
\end{frame}

\begin{frame} {A Single Neuron}
  \begin{itemize}
    \item \small{One very nice thing about the graphical representation of the function is that you can picture the input vector being "fed" to the neuron on the left followed by a sequence of computations being performed from left to right}.\small{This is called a "\textbf{forward pass}".}
  \end{itemize}
  \begin{figure}
    \centering
      \scalebox{0.75}{\includegraphics{plots/neuron_seven.png}}
  \end{figure}
\end{frame}

\begin{frame} {A Single Neuron}
  \begin{itemize}
    \item \small{Even though all neurons compute a weighted sum in the first step, there is considerable flexibility in the type of activation function used in the second step.
    \item For example, setting the activation function to the identity function allows a neuron to represent linear regression.}
  \begin{figure}
    \centering
      \scalebox{0.70}{\includegraphics{plots/neuron_five.png}}
  \end{figure}  \end{itemize}
\end{frame}

\begin{frame} {A Single Neuron : Optimization}
  \begin{itemize}
    \item   
To optimize this model, we have to minimize a loss function $\Lxy$, where $y$ corresponds to the ground truth and $f(x)$ to the networks prediction.
    \item For regression, we typically use the L2 loss (rarely L1): $$\Lxy = \frac{1}{2}(y - f(x))^2$$
    \item For binary classification, we typically apply the cross entropy (binomial loss): 
     $$\Lxy = -\frac{1}{n} \sum_{i=1}^{n} \Big[y_i log \ f(x) + (1 - y_i) log(1 - f(x)) \Big]$$
    \item For a single neuron, in both cases, the loss function is convex and the global optimum can be found with an iterative algorithm like gradient descent. \small{(Note : In the case of regression, the solution can also be found analytically using the 'normal equations'.)}
  \end{itemize}
\end{frame}  

\begin{frame} {Neural Networks}
  \begin{itemize}
    \vspace{2mm}
    \item So, we have a nice graphical way of representing simple functions/models like logistic regression. Why is that useful?
    \vspace{5mm}
    \item It's useful because this visual metaphor allows us to use such individual neurons as building blocks of considerably more complicated functions.
    \vspace{5mm}
    \item Therefore, networks of neurons can represent extremely complex hypothesis spaces.
    \vspace{5mm}
    \item Most importantly, it allows us to define the "right" kinds of hypothesis spaces to learn functions that are more common in our universe in a data-efficient way. (see Lin, Tegmark et al. 2016)
  \end{itemize}
\end{frame}

\begin{frame} {Neural Networks}
  \begin{itemize}
    \item \small{But why do we need more complicated functions? Isn't logistic regression enough?}
    \item \small{Because a single neuron is restricted to learning only linear decision boundaries, it's performance on the task below will be quite poor}
    \begin{figure}
    \centering
      \scalebox{0.25}{\includegraphics{plots/cartesian.png}}
  \end{figure}
    \item \small{However, if the original features are transformed(for example, from Cartesian to Polar coord.), the neuron can easily separate the classes.}
    \begin{figure}
    \centering
      \scalebox{0.25}{\includegraphics{plots/polar.png}}
  \end{figure}
  \end{itemize}
\end{frame}

\begin{frame} {Neural Networks : The Basic idea}
  \small{Instead of classifying the data in the original representation, ...}
    \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/repold_f.png}}
  \end{figure}
\end{frame}

\begin{frame} {Neural Networks : The Basic idea}
   \small{we classify it in the new feature space.}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/repnew_f.png}}
  \end{figure}
\end{frame}

\begin{frame} {Neural Networks : The Basic idea}
   \small{we classify it in the new feature space.}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/repnew_f.png}}
  \end{figure}
  \small{Analogously, }
  \begin{figure}
    \centering
      \scalebox{0.75}{\includegraphics{plots/oldrep_n_f.png}}
  \end{figure}
\end{frame}

\begin{frame} {Neural Networks : The Basic idea}
   \small{we classify it in the new feature space.}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/repnew_f.png}}
  \end{figure}
  \small{Analogously, }
  \begin{figure}
    \centering
      \scalebox{0.75}{\includegraphics{plots/newrep_n_f.png}}
  \end{figure}
\end{frame}

\begin{frame} {Neural Nets : Representation Learning}
  \begin{itemize}
    \vspace{5mm}
    \item Therefore, it is \textit{very} critical to feed a classifier the "right" features in order for it to perform well.
    \vspace{7mm}
    \item Before deep learning (DL) took off, features for tasks like machine vision and speech recognition were "hand-designed" by domain experts.This step of the machine learning pipeline is called "feature engineering".
    \vspace{7mm}
    \item The single biggest reason DL is so important is that it automates feature engineering.This is called \textbf{"representation learning"}.
  \end{itemize}
\end{frame}

\begin{frame} {Neural Nets : Representation Learning}
  \begin{itemize}
    \item How does DL automate feature engineering?
    \vspace{3mm}
    \item In neural networks, we represent the new features as intermediate neurons called "\textbf{hidden neurons}".
    \vspace{3mm}
    \item Different weights correspond to different features and "good" weights are learned from the data in an end-to-end fashion.
    \vspace{3mm}
    \item The final neuron will now be referred to as the "output neuron".
    \vspace{3mm}
    \item The classifier can then learn a linear decision boundary in the transformed space (a similar argument applies to regression).
    \vspace{3mm}
    \item It's very important to note that both the intermediate feature transformations and the final classifier are learned from data simultaneously.
  \end{itemize}
\end{frame}

\begin{vbframe} {Neural Networks : Single Hidden Layer}
 \begin{figure}
      \centering
        \includegraphics[width=10.2cm]{plots/neuralnet2.png}
        \caption{Structure of a single hidden layer, feed-forward neural network for regression or binary classification problems (bias term omitted).}
    \end{figure}
\framebreak
  \begin{itemize}
    \item The input $\mathbf{x}$ is a column vector with dimensions $p \times 1$
    \item $W$ is a weight matrix with dimensions $p \times m$:
    $$
    W =
     \begin{pmatrix}
      w_{1,1} & w_{1,2} & \cdots & w_{1,m} \\
      w_{2,1} & w_{2,2} & \cdots & w_{2,m} \\
      \vdots  & \vdots  & \ddots & \vdots  \\
      w_{p,1} & w_{p,2} & \cdots & w_{p,m}
     \end{pmatrix}
    $$
    \item For example, to obtain $z_1$, we pick the first column of $W$:
    $$
    W_1 =
     \begin{pmatrix}
      w_{1,1} \\
      w_{2,1} \\
      \vdots  \\
      w_{p,1}
     \end{pmatrix}
    $$
    and compute $z_1 = \sigma(W_1^T \mathbf{x} + b_1)$, where $b_1$ is the bias of the first hidden neuron.
  \end{itemize}
\framebreak
  \textbf{General notation}:
  \begin{itemize}
    \vspace{4mm}
    \item The network has $m$ hidden neurons $z_1, \dots, z_m$, with:
    $$ z_j = \sigma(W_j^T \mathbf{x} + b_j), \quad j = 1,\ldots,m$$
    \begin{itemize}
    \item $z_{in,j}  = W_j^T \mathbf{x} + b_j$
    \vspace{4mm}
    \item $z_{out,j} = \sigma(z_{in,j}) = \sigma(W_j^T \mathbf{x} + b_j)$
    \end{itemize}
    \vspace{4mm}
    \item Vectorized notation:
      \begin{itemize}
        \item $ \hidz_{in} = (z_{in,1}, \dots, z_{in,m})^T = W^T \mathbf{x} + \biasb$
          \begin{itemize}
            \item Note : $W^T \mathbf{x}$ = $(\mathbf{x}^TW)^T$
          \end{itemize}
        \item $ \hidz = \hidz_{out} = \sigma(\hidz_{in}) = \sigma(W^T \mathbf{x} + \biasb)$
          \begin{itemize}
            \item Activation function (hidden layer) $\sigma$ is applied element-wise to $\hidz_{in}$.
          \end{itemize}
      \end{itemize}
    \end{itemize}
\framebreak
  \textbf{General notation}:
  \begin{itemize}
    \vspace{4mm}
    \item For regression or binary classification: one output unit $f$, where,
      \begin{itemize}
        \item $f_{in} = \wtu^T \hidz + c$ , and,
          \begin{itemize}
            \item This is a linear combination of derived features.
            \item $c$ is the bias of the output neuron.
          \end{itemize}
        \item $f(\mathbf{x})= f_{out} = \tau(f_{in}) = \tau(\wtu^T \hidz + c)$ , where $\tau$ is the output activation.
      \end{itemize}
      \vspace{4mm}
    \item For regression $\tau$ is the identity function.
      \vspace{4mm}
    \item For binary classification, $\tau$ is a sigmoid.
  \end{itemize}
\framebreak
  \textbf{General notation : Multiple inputs}
  \begin{itemize}
    \item It's possible to feed multiple inputs to a neural network simultaneously.
    \vspace{4mm}
    \item The inputs $\mathbf{x}^{(i)}$, for $i \in \nset$, are arranged as rows in the \textbf{design matrix} $X$.
    \begin{itemize}
      \item $X$ is a ($n \times p$) matrix.
    \end{itemize}
    \vspace{4mm}
    \item The weighted sum in the hidden layer is now computed as $XW + B$, where,
      \begin{itemize}
        \item $W$, as usual, is a ($p \times m$) matrix, and,
        \vspace{2mm}
        \item $B$ is a ($n \times m$) matrix containing the bias vector $\biasb$ (duplicated) as the rows of the matrix.
      \end{itemize}
    \vspace{4mm}
    \item The \textit{matrix} of hidden activations $Z = \sigma(XW + B)$
    \begin{itemize}
      \item $Z$ is a ($n \times m$) matrix.
    \end{itemize}
  \end{itemize}
\framebreak
  \textbf{General notation : Batch processing}
  \begin{itemize}
    \vspace{15mm}
    \item The final output of the network, which contains a prediction for each input, is $\tau(Z\wtu + C)$, where,
      \begin{itemize}
        \vspace{2mm}
        \item $\wtu$ is the vector of weights of the output neuron, and,
        \vspace{2mm}
        \item $C$ is a ($n \times 1$) matrix whose elements are the (scalar) bias $c$ of the output neuron.
      \end{itemize}
  \end{itemize}
\end{vbframe}

\begin{frame} {Single Hidden Layer : Example}
  \begin{itemize}
    \item Weights (and biases) of the network.
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/sinlay_one.png}}
  \end{figure}
  \end{itemize}
\end{frame}

\begin{frame} {Single Hidden Layer : Example}
  \begin{itemize}
    \item Weights (and biases) of the network.
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/sinlay_two.png}}
  \end{figure}
  \end{itemize}
\end{frame}

\begin{frame} {Single Hidden Layer : Example}
\begin{itemize}
    \item Weights (and biases) of the network.
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/sinlay_three.png}}
  \end{figure}
  \end{itemize}
\end{frame}

\begin{frame} {Single Hidden Layer : Example}
  \begin{itemize}
    \item Weights (and biases) of the network.
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/sinlay_four.png}}
  \end{figure}
  \end{itemize}
\end{frame}

\begin{frame} {Single Hidden Layer : Example}
\begin{itemize}
    \item Weights (and biases) of the network.
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/sinlay_five.png}}
  \end{figure}
  \end{itemize}
\end{frame}

\begin{frame} {Single Hidden Layer : Example}
  \begin{itemize}
    \item Weights (and biases) of the network.
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/sinlay_six.png}}
  \end{figure}
  \end{itemize}
\end{frame}

\begin{frame} {Single Hidden Layer : Example}
  \begin{itemize}
  \item Forward pass through the shallow neural network.
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/sinlay_seven.png}}
  \end{figure}
  \end{itemize}
\end{frame}

\begin{frame} {Single Hidden Layer : Example}
  \begin{itemize}
  \item Forward pass through the shallow neural network.
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/sinlay_eight.png}}
  \end{figure}
  \end{itemize}
\end{frame}

\begin{frame} {Single Hidden Layer : Example}
  \begin{itemize}
    \item Forward pass through the shallow neural network.
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/sinlay_nine.png}}
  \end{figure}
  \end{itemize}
\end{frame}

\begin{frame} {Single Hidden Layer : Example}
  \begin{itemize}
    \item Forward pass through the shallow neural network.  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/sinlay_ten.png}}
  \end{figure}
  \end{itemize}
\end{frame}

\begin{frame} {Single Hidden Layer : Example}
  \begin{itemize}
    \item Forward pass through the shallow neural network.  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/sinlay_eleven.png}}
  \end{figure}
  \end{itemize}
\end{frame}

\begin{frame} {Single Hidden Layer : Example}
  \begin{itemize}
    \item Forward pass through the shallow neural network.
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/sinlay_twelve.png}}
  \end{figure}
  \end{itemize}
\end{frame}

\begin{frame} {Hidden Layer : Activation Function}
  \begin{itemize}
    \item It's important to note that if the hidden layer doesn't have a non-linear activation, the network can only learn linear decision boundaries.
    \item To drop the bias term in the notation, let's add a feature $x_0$ to $\mathbf{x}$ which always takes the value '1'. 
      \begin{itemize}
        \item $\tilde{\mathbf{x}} = (x_0,\ldots, x_p)^T$
      \end{itemize}
    \item $\tilde{W}$ is now the weight matrix W with the bias $\biasb$ as an additional row (the first row)
      \begin{equation*}. 
        \begin{split}
        f & = \tau(\wtu^T \hidz + c) \\
         & = \tau(\wtu^T (\sigma(W^T \mathbf{x} + \biasb)) + c) \\
         & = \tau(\wtu^T (\sigma(\tilde{W} \tilde{\mathbf{x}})) + c) \\
         & = \tau((\wtu^T\tilde{W}^{T}) \tilde{\mathbf{x}})) + c) \\
         & = \tau((\mathbf{v})^T \tilde{\mathbf{x}}) + c)
        \end{split}
      \end{equation*}
      where, $ \mathbf{v} = \tilde{W}\wtu$.
  \end{itemize}
\end{frame}

\begin{frame} {Hidden Layer : Activation Function}
  \begin{blocki}{Activation function $\sigma$:}
    \item Currently the most popular choice is the ReLU (rectified linear unit):
    $$ \sigma (v) = max(0,v) $$
  \end{blocki}
<<echo=FALSE, fig.height=3>>=
library(ggplot2)
library(reshape2)
relu <- function(x) sapply(x, function(z) max(0,z))
x <- seq(from=-5, to=5, by=0.1)
fits <- data.frame(x=x, relu = relu(x))
long <- melt(fits, id.vars="x")
ggplot(data=long, aes(x=x, y=value, group=variable, colour=variable))+
  geom_line(size=1.5) + scale_y_continuous(name = NULL) + 
  scale_x_continuous(name = NULL) + theme(legend.position="none")
@
\end{frame}

\begin{frame} {Hidden Layer : Activation Function}
  \begin{itemize}
    \item Some important properties of the relu function include:
    \item[]
    \item[]
    \begin{itemize}
      \item limits: $$\lim_{v \to -\infty} \sigma(v) = 0 \text{ and } \lim_{v \to \infty} \sigma(v) = \infty$$
      \item derivative: 
      $$\frac{\delta\sigma(v)}{\delta v} =
        \begin{cases}
                                       1 & \text{if $v > 0$} \\
                                       0 & \text{else}
        \end{cases}
      $$
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame} {Hidden Layer : Activation Function}
  \begin{blocki}{Activation function $\sigma$:}
    \item Another choice might be the hyperbolic tangent:
    $$ \sigma (v) = tanh(v) = \frac{sinh(v)}{cosh(v)} = 1 - \frac{2}{exp(2v) + 1}$$
  \end{blocki}
<<echo=FALSE, fig.height=3>>=
library(ggplot2)
library(reshape2)
relut <- function(x) sapply(x, function(z) tanh(z))
x <- seq(from=-3, to=3, by=0.1)
fits <- data.frame(x=x, relut = relut(x))
long <- melt(fits, id.vars="x")
ggplot(data=long, aes(x=x, y=value, group=variable, colour=variable))+
  geom_line(size=1.2) + scale_y_continuous(name = NULL) + 
  scale_x_continuous(name = NULL) + theme(legend.position="none")
@
\end{frame}

\begin{frame} {Hidden Layer : Activation Function}
  \begin{itemize}
    \item Some important properties of the hyperbolic tangent function include:
    \item[]
    \item[]
    \begin{itemize}
      \item limits: $$\lim_{v \to -\infty} \sigma(v) = -1 \text{ and } \lim_{v \to \infty} \sigma(v) = 1$$
      \item derivative: $$\frac{\delta\sigma(v)}{\delta v} = 1 - tanh^2(v)$$
      \item symmetry: $$\sigma(v) \text{ is symmetrical in } (0, 0)$$
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame} {Hidden Layer : Activation Function}
  \begin{blocki}{Activation function $\sigma$:}
  \item Of course, as seen in the previous example, the sigmoid function can be used even in the hidden layer:
    $$ \sigma(v) = \frac{1}{(1+\exp (-s \cdot v))} $$
  \end{blocki}
<<echo=FALSE, fig.height=2.7>>=
library(ggplot2)
logfun = function(v, s) {
  1 / (1 + exp(- s * v))
}
x = seq(-10, 10, 0.1)
stretch = c(0.25, 1, 10)
y = sapply(stretch, function(s) {
  sapply(x, logfun, s = s)
})
df = data.frame(y = as.vector(y), x = rep(x, length(stretch)),
  s = as.factor(rep(stretch, each = length(x))))

logfun.q = ggplot(df, aes(x = x, y = y, color = s)) + geom_line(size=1)
logfun.q = logfun.q + scale_y_continuous(name = NULL)
logfun.q = logfun.q + scale_x_continuous(name = NULL)
logfun.q = logfun.q + theme(axis.title = element_text(size = 14L, face = "bold"),
  plot.margin = unit(c(0, 0, 0, 0), "cm"))
logfun.q
@
\end{frame}
\begin{frame}{Hidden Layer : Activation Function}
  \begin{itemize}
    \item Some important properties of the sigmoidal logistic function include:
    \item[]
    \item[]
    \begin{itemize}
      \item limits: $$\lim_{v \to -\infty} \sigma(v) = 0 \text{ and } \lim_{v \to \infty} \sigma(v) = 1$$
      \item the derivative for $s = 1$: $$\frac{\delta\sigma(v)}{\delta v}=\frac{\exp(v)}{(1+\exp(v))^2} = \sigma(v)(1-\sigma(v))$$
      \item for any s: $$\sigma(v) \text{ is symmetrical in } (0, 0.5)$$
    \end{itemize}
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Example: XOR Problem}
  \begin{itemize}
    \item Suppose we have four data points $$X = \{[0,0]^T, [0,1]^T, [1,0]^T, [1,1]^T \}$$
    \item The XOR gate (exclusive or) returns true, when an odd number of inputs are true:
  \end{itemize}
  \begin{table}
    \centering
      \begin{tabular}{ccc}
        \textbf{$x_1$}  & \textbf{$x_2$}  & \textbf{XOR} $= y$ \\
        \hline
        \hline
        $0$             &   $0$           &  $0$ \\
        $0$             &   $1$           &  $1$ \\
        $1$             &   $0$           &  $1$ \\
        $1$             &   $1$           &  $0$
      \end{tabular}
  \end{table}
  \begin{itemize}
    \item Can you learn the target function with a logistic regression model? \\
    % (Aside from statistical generalization, we just want to learn the training data!)
  \end{itemize}
\framebreak
  \begin{minipage}{0.45\textwidth}
    \begin{itemize}
      \item Logistic regression cannot solve this problem. %and will always output $0.5$. \\
      In fact, any model using simple hyperplanes for separation can't (including a single neuron).
      \lz
      \item A small neural net can easily solve the problem by transforming the space!
    \end{itemize}
  \end{minipage}
  \begin{minipage}{0.5\textwidth}
    \includegraphics{plots/xor1.png}%
  \end{minipage}\hfill
\framebreak
  \begin{itemize}
    \item Consider the following model:
  \end{itemize}
    \begin{figure}
      \centering
        \includegraphics[width=8cm]{plots/xor_rep.png}%
        \caption{A neural network with two neurons in the hidden layer. The matrix W describes the mapping from x to z. The vector u from z to y.}
    \end{figure}
\framebreak
  \begin{itemize}
    \item Let us treat the XOR task as a regression problem and apply ReLU as activation function. We can represent the models architecture by the following equation: 
  \end{itemize}
  \begin{eqnarray*}
    f(\mathbf{x}|\theta) &=& f(\mathbf{x}| W, \biasb, \wtu, c) \\ 
                &=& \wtu^T\sigma(W^T \mathbf{x}+\biasb)+c \\
                &=& \wtu^T max\{0, W^T \mathbf{x}+\biasb\} + c
  \end{eqnarray*}
  \begin{itemize}
    \item So how many parameters does our model have?
    \begin{itemize}
      \item In a fully connected neural net, the number of connections between the nodes equals our parameters: $$\underbrace{(2 \times 2)}_{W} + \underbrace{(2 \times 1)}_{\biasb} + \underbrace{(2 \times 1)}_{\wtu} + \underbrace{(1)}_{c} = 9$$
    \end{itemize}
  \end{itemize}
\framebreak
  \begin{eqnarray*}
   \text{Let} \ W = \begin{pmatrix}
      1 & 1 \\
      1 & 1
    \end{pmatrix}, \
      \biasb = \begin{pmatrix}
      0 \\
      -1
    \end{pmatrix}, \
      \wtu = \begin{pmatrix}
      1 \\
      -2
    \end{pmatrix}, \
      c = 0
  \end{eqnarray*}
  \begin{eqnarray*}
    X = \begin{pmatrix}
      0 & 0 \\
      0 & 1 \\
      1 & 0 \\
      1 & 1
    \end{pmatrix}, \
    XW = \begin{pmatrix}
      0 & 0 \\
      1 & 1 \\
      1 & 1 \\
      2 & 2
    \end{pmatrix}, \
      XW + B = \begin{pmatrix}
        0 & -1 \\
        1 & 0 \\
        1 & 0 \\
        2 & 1
    \end{pmatrix}
  \end{eqnarray*}
%\vspace{4mm}
 \footnotesize{Note : X is a $(n \times p)$ design matrix in which the \textit{rows} correspond to the data points. W, as usual, is a $(p \times m)$ matrix where each \textit{column} corresponds to a single (hidden) neuron. B is a ($n \times m$) matrix with $\biasb$ duplicated along the rows.}
 \begin{figure}
    \centering
      \scalebox{0.6}{\includegraphics{plots/rowcol.png}}
  \end{figure}
 \framebreak
 \normalsize{
 \begin{eqnarray*}
   \text{Let} \ W = \begin{pmatrix}
      1 & 1 \\
      1 & 1
    \end{pmatrix}, \
      \biasb = \begin{pmatrix}
      0 \\
      -1
    \end{pmatrix}, \
      \wtu = \begin{pmatrix}
      1 \\
      -2
    \end{pmatrix}, \
      c = 0
  \end{eqnarray*}
  \begin{eqnarray*}
  X = \begin{pmatrix}
      0 & 0 \\
      0 & 1 \\
      1 & 0 \\
      1 & 1
    \end{pmatrix}, \
    XW = \begin{pmatrix}
      0 & 0 \\
      1 & 1 \\
      1 & 1 \\
      2 & 2
    \end{pmatrix}, \
      XW + B = \begin{pmatrix}
        0 & -1 \\
        1 & 0 \\
        1 & 0 \\
        2 & 1
    \end{pmatrix}
  \end{eqnarray*}
  \begin{eqnarray*}
    Z = max\{0, XW+B\}
    &=&
    \begin{pmatrix}
      0 & 0 \\
      1 & 0 \\
      1 & 0 \\
      2 & 1
    \end{pmatrix}
  \end{eqnarray*}
  \begin{itemize}
    \item Note that we computed all examples at once.
  \end{itemize}

\framebreak
  \begin{minipage}{0.45\textwidth}
    \begin{itemize}
      \item The input points are mapped into transformed space to
        \begin{eqnarray*}
          Z = \begin{pmatrix}
              0 & 0 \\
              1 & 0 \\
              1 & 0 \\
              2 & 1
          \end{pmatrix}
        \end{eqnarray*}
      \item[] which is easily separable.
    \end{itemize}
  \end{minipage}
  \begin{minipage}{0.5\textwidth}
    \includegraphics<1>{plots/xor2_2.png}%
  \end{minipage}\hfill
  
\framebreak
  \begin{minipage}{0.45\textwidth}
    \begin{itemize}
      \item The input points are mapped into transformed space to
        \begin{eqnarray*}
          Z = \begin{pmatrix}
              0 & 0 \\
              1 & 0 \\
              1 & 0 \\
              2 & 1
          \end{pmatrix}
        \end{eqnarray*}
      \item[] which is easily separable.
    \end{itemize}
  \end{minipage}
  \begin{minipage}{0.5\textwidth}
    \includegraphics{plots/xor2.png}%
  \end{minipage}\hfill
  
\framebreak
  \begin{itemize}
    \item In a final step we have to multiply the activated values of matrix Z with the vector u:
  \end{itemize}
  \begin{eqnarray*}
    f(\mathbf{x}; W, \biasb, \wtu, c) =
    \begin{pmatrix}
      0 & 0 \\
      1 & 0 \\
      1 & 0 \\
      2 & 1
    \end{pmatrix}
    \begin{pmatrix}
      1 \\
      -2
    \end{pmatrix}
    &=&
    \begin{pmatrix}
      0 \\
      1 \\
      1 \\
      0
    \end{pmatrix}
  \end{eqnarray*}
  \begin{itemize}
    \item This solves the XOR problem perfectly!
  \end{itemize}
  \begin{table}
    \centering
      \begin{tabular}{ccc}
        \textbf{$x_1$}  & \textbf{$x_2$}  & \textbf{XOR} = y\\
        \hline
        \hline
        $0$             &   $0$           &  $0$ \\
        $0$             &   $1$           &  $1$ \\
        $1$             &   $0$           &  $1$ \\
        $1$             &   $1$           &  $0$
      \end{tabular}
  \end{table}}
\end{vbframe}

\begin{frame} {Neural Networks : Optimization}
  \begin{itemize}
    \item In this simple example we actually \enquote{guessed} the values of the parameters for $W$, $\biasb$, $\wtu$ and $c$.
    \vspace{3mm}
    \item That won't work for more sophisticated problems!
    \vspace{3mm}
    \item To learn the right weights(and biases), we once again have to rely on iterative algorithms like gradient descent.
    \vspace{3mm}
    \item An added complication is that the loss function is no longer convex. Therefore, gradient descent can only find a local minimum.
    \vspace{3mm}
    \item An extremely efficient method to compute gradients called backpropogation will be covered in the next lecture.
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Universal approximation property}

  \textbf{Theorem.}
  Let $\sigma : \R \to \R$ be a continuous, non-constant, bounded, and
  monotonically increasing function. Let $C \subset \R^p$ be compact,
  and let $\continuous(C)$ denote the space of continuous functions $C \to \R$.
  Then, given a function $g \in \continuous(C)$ and an accuracy $\varepsilon > 0$,
  there exists a hidden layer size $m \in \N$ and a set of coefficients
  $W_j \in \R^p$, $u_j, b_j \in \R$
  (for $j \in \{1, \dots, m\}$), such that
  $$
    f : C \to \R \,;\quad f(\mathbf{x}) = \sum_{j=1}^m u_j \cdot \sigma \Big( W_j^T \mathbf{x} + b_j \Big)
  $$
  is an $\varepsilon$-approximation of $g$, that is,
  $$
    \|f - g\|_{\infty} := \max_{x \in C} |f(\mathbf{x}) - g(\mathbf{x})| < \varepsilon
    \enspace.
  $$

  The theorem extends trivially to multiple outputs.

  \framebreak

  \textbf{Corollary.}
  Neural networks with a single sigmoidal hidden layer and linear
  output layer are universal approximators.

  \begin{itemize}
    \item This means that for a given target function $g$ there exists a
    sequence of networks $\big( f_k \big)_{k \in \N}$ that converges
    (pointwise) to the target function.
    \vspace{2mm}
    \item Usually, as the networks come closer and closer to $g$, they
    will need more and more hidden neurons.
    \vspace{2mm}
    \item A network with fixed layer sizes can only model a subspace of all
    continuous functions. Its dimensionality is limited by the number
    of weights.
    \vspace{2mm}
    \item The continuous functions form an infinite dimensional vector space.
    Therefore arbitrarily large hidden layer sizes are needed.
  \end{itemize}

  \framebreak

  \begin{itemize}
  \item Why is universal approximation a desirable property?
  \vspace{2mm}
  \item Recall the definition of a Bayes optimal hypothesis $h^* : X \to Y$.
    It is the best possible hypothesis (model) for the given problem:
    it has minimal loss averaged over the data generating distribution.
  \vspace{2mm}
  \item So ideally we would like the neural network (or any other
    learner) to approximate the Bayes optimal hypothesis.
  \vspace{2mm}
  \item Usually we do not manage to learn $h^*$.
  \vspace{2mm}
  \item This is because we do not have enough (infinite) data. We have
    no control over this, so we have to live with this limitation.
  \vspace{2mm}
  \item But we do have control over which model class we use.
  \end{itemize}

  \framebreak

  \begin{itemize}
    \vspace{10mm}
    \item Universal approximation $\Rightarrow$ approximation error tends
    to zero as hidden layer size tends to infinity.
    \vspace{5mm}
    \item Positive approximation error implies that no matter how good
    the data, we cannot find the optimal model.
    \vspace{5mm}
    \item This bears the risk of systematic under-fitting, which can be avoided with a universal model class.
  \end{itemize}

  \framebreak

  \begin{itemize}
    \vspace{5mm}
    \item As we know, there are also good reasons for restricting the model class.
    \vspace{5mm}
    \item This is because a flexible model class with universal approximation
    ability often results in over-fitting, which is no better than
    under-fitting.
    \vspace{5mm}
    \item Thus, \enquote{universal approximation $\Rightarrow$ low approximation error}, but at the risk of a substantial learning error.
    \vspace{5mm}
    \item In general, models of intermediate flexibility give the best predictions.
    For neural networks this amounts to a reasonably sized hidden layer.
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame} {Neural Nets : Regression/Classification}
  \begin{itemize}
    \vspace{15mm}
    \item Let's look at a few examples of the types of functions and decisions boundaries learnt by neural networks(with a \textbf{single} hidden layer) of various sizes.
      \begin{itemize}
        \item "size" here refers to the number of neurons in the hidden layer.
      \end{itemize}
    \vspace{5mm}
    \item The number of "iterations" in the following slides is the number of steps of (stochastic) gradient descent performed.
  \end{itemize}
\end{frame}

% \begin{vbframe}{Regression: 100 training iterations}
% <<echo=FALSE, warning=FALSE, message=FALSE, results="hide">>=
% 
% library("mlr")
% set.seed(1234L)
% n = 50L
% x = sort(10 * runif(n))
% y = sin(x) + 0.2 * rnorm(x)
% df = data.frame(x = x, y = y)
% tsk = makeRegrTask("sine function example", data = df, target = "y")
% plotLearnerPrediction("regr.nnet", tsk, size = 1L, maxit = 100)
% 
% plotLearnerPrediction("regr.nnet", tsk, size = 2L, maxit = 100)
% 
% plotLearnerPrediction("regr.nnet", tsk, size = 3L, maxit = 100)
% 
% plotLearnerPrediction("regr.nnet", tsk, size = 4L, maxit = 100)
% 
% plotLearnerPrediction("regr.nnet", tsk, size = 5L, maxit = 100)
% 
% plotLearnerPrediction("regr.nnet", tsk, size = 6L, maxit = 100)
% 
% plotLearnerPrediction("regr.nnet", tsk, size = 100L, maxit = 100)
% 
% @
% \end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Regression: 1000 training iterations}
<<echo=FALSE, warning=FALSE, message=FALSE, results="hide">>=

library("mlr")
set.seed(1234L)
n = 50L
x = sort(10 * runif(n))
y = sin(x) + 0.2 * rnorm(x)
df = data.frame(x = x, y = y)
tsk = makeRegrTask("sine function example", data = df, target = "y")
plotLearnerPrediction("regr.nnet", tsk, size = 1L, maxit = 1000)

plotLearnerPrediction("regr.nnet", tsk, size = 2L, maxit = 1000)

plotLearnerPrediction("regr.nnet", tsk, size = 3L, maxit = 1000)

plotLearnerPrediction("regr.nnet", tsk, size = 4L, maxit = 1000)

plotLearnerPrediction("regr.nnet", tsk, size = 5L, maxit = 1000)

plotLearnerPrediction("regr.nnet", tsk, size = 6L, maxit = 1000)

plotLearnerPrediction("regr.nnet", tsk, size = 10L, maxit = 1000)

@
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Classification: 500 training iterations}
<<echo=FALSE, warning=FALSE, message=FALSE, results="hide">>=

library("mlr")
library("mlbench")
set.seed(1234L)
spirals = mlbench.spirals(500,1.5,0.05)
spirals = data.frame(cbind(spirals$x, spirals$classes))
colnames(spirals) = c("x1","x2","class")
spirals$class = as.factor(spirals$class)
task = makeClassifTask(data = spirals, target = "class")
lrn = makeLearner("classif.nnet")
plotLearnerPrediction("classif.nnet", task, size = 1L, maxit = 500)

plotLearnerPrediction("classif.nnet", task, size = 2L, maxit = 500)

plotLearnerPrediction("classif.nnet", task, size = 3L, maxit = 500)

plotLearnerPrediction("classif.nnet", task, size = 5L, maxit = 500)

plotLearnerPrediction("classif.nnet", task, size = 10L, maxit = 500)

plotLearnerPrediction("classif.nnet", task, size = 30L, maxit = 500)

plotLearnerPrediction("classif.nnet", task, size = 50L, maxit = 500)

@
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame} {Representations}
%   \begin{itemize}
%     \item Regression and Classification examples
%   \end{itemize}
% \end{frame}
% 


\begin{frame} {Multi-class Classification}
  \vspace{20mm}
  \begin{itemize}
    \item We've only considered regression and binary classification problems so far
    \vspace{5mm}
    \item How can we get a neural network to perform multiclass classification?
  \end{itemize}
\end{frame}

\begin{frame} {Multi-class Classification}
  \begin{itemize}
    \item The first step is to add additional neurons to the output layer.
    \item Each neuron in the layer will represent a specific class (number of neurons in the output layer = number of classes).
    \begin{figure}
      \centering
      \scalebox{0.75}{\includegraphics[width=10.2cm]{plots/neuralnet.png}}
        \caption{\footnotesize Structure of a single hidden layer, feed-forward neural network for g-class classification problems (bias term omitted).}
    \end{figure}
  \end{itemize}
\end{frame}

\begin{frame} {Multi-class Classification}
    \vspace{5mm}
    \begin{blocki}{Notation:}
    \item For $g$-class classification, $g$ output units: $$\mathbf{f} = (f_1, \dots, f_g)$$
    \vspace{4mm}
    \item $m$ hidden neurons $z_1, \dots, z_m$, with:
    $$ z_j = \sigma(W_j^T \mathbf{x}), \quad j = 1,\ldots,m $$
 %   \vspace{4mm}
    \item Compute linear combinations of derived features $z$:
    $$ f_{in,k} = U_k^T \hidz, \quad \hidz=(z_1,\dots, z_m)^T, \quad k = 1,\ldots,g$$
  \end{blocki}
\end{frame}

\begin{frame} {Multi-class Classification}
  \begin{itemize}
    \item The second step is to apply a Softmax activation function to the output layer.
    \vspace{4mm}
    \item This gives us a probability distribution over g different possible classes:
    $$ f_{out,k} = \tau_k(f_{in,k}) = \frac{\exp(f_{in,k})}{\sum_{k'=1}^g\exp(f_{in,k'})}$$
    \vspace{2mm}
    \item This is the same transformation used in softmax regression!
    \vspace{4mm}
    \item Derivative $ \frac{\delta\tau(\mathbf{f}_{in})}{\delta \mathbf{f}_{in}} = diag(\tau(\mathbf{f}_{in})) - \tau(\mathbf{f}_{in}) \tau(\mathbf{f}_{in})^T $
    \vspace{4mm}
    \item It is a \enquote{smooth} approximation of the argmax operation,
        so $\tau((1, 1000, 2)^T) \approx (0, 1, 0)^T$ (picks out 2nd element!).
  \end{itemize}
\end{frame}

\begin{frame} {Multi-class Classification : Example}
  \begin{itemize}
    \item Forward pass (Hidden : Sigmoid, Output : Softmax).
  \end{itemize}
  \begin{figure}
    \centering
      \scalebox{0.95}{\includegraphics{plots/softie_one.png}}
  \end{figure}
\end{frame}

\begin{frame} {Multi-class Classification : Example}
  \small{Forward pass (Hidden : Sigmoid, Output : Softmax)}.
  \begin{figure}
    \centering
      \scalebox{0.95}{\includegraphics{plots/softie_two.png}}
  \end{figure}
\end{frame}

\begin{frame} {Multi-class Classification : Example}
  \begin{itemize}
    \item Forward pass (Hidden : Sigmoid, Output : Softmax).
  \end{itemize}
  \begin{figure}
    \centering
      \scalebox{0.95}{\includegraphics{plots/softie_three.png}}
  \end{figure}
\end{frame}

\begin{frame} {Multi-class Classification : Example}
  \begin{itemize}
    \item Forward pass (Hidden : Sigmoid, Output : Softmax).
  \end{itemize}
  \begin{figure}
    \centering
      \scalebox{0.95}{\includegraphics{plots/softie_four.png}}
  \end{figure}
\end{frame}

\begin{frame} {Multi-class Classification : Example}
  \begin{itemize}
    \item Forward pass (Hidden : Sigmoid, Output : Softmax).
  \end{itemize}
  \begin{figure}
    \centering
      \scalebox{0.95}{\includegraphics{plots/softie_five_a.png}}
  \end{figure}
\end{frame}

\begin{frame} {Multi-class Classification : Example}
  \begin{itemize}
    \item Forward pass (Hidden : Sigmoid, Output : Softmax).
  \end{itemize}
  \begin{figure}
    \centering
      \scalebox{0.95}{\includegraphics{plots/softie_five_b.png}}
  \end{figure}
\end{frame}

\begin{frame} {Multi-class Classification : Example}
  \begin{itemize}
    \item Forward pass (Hidden : Sigmoid, Output : Softmax).
  \end{itemize}
  \begin{figure}
    \centering
      \scalebox{0.95}{\includegraphics{plots/softie_six.png}}
  \end{figure}
\end{frame}

\begin{frame} {Multi-class Classification : Example}
  \begin{itemize}
    \item Forward pass (Hidden : Sigmoid, Output : Softmax).
  \end{itemize}
  \begin{figure}
    \centering
      \scalebox{0.95}{\includegraphics{plots/softie_seven.png}}
  \end{figure}
\end{frame}

\begin{frame} {Softmax Loss}
  \begin{itemize}
    \vspace{5mm}
    \item The loss function for a softmax classifier is,
    $$L(\mathbf{y}, \mathbf{f}(\mathbf{x})) = - \sum_{i=1}^{n} \log ( \frac{\exp(f_{in,k})}{\sum_{k'=1}^g\exp(f_{in,k'})})$$
      where k is the "correct" class of $i$'th datapoint.
    \vspace{5mm}
    \item This is equivalent to the cross entropy loss when the label vector $y$ is one-hot coded. (eg. $\mathbf{y} = (0,0,1,0)^T$)
    \vspace{5mm}
    \item Again, there is no analytic solution. Therefore, we use gradient descent.
  \end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% XOR problem with sigmoid activation function %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{XOR Problem 2}
%   \begin{eqnarray*}
%     z = \frac{1}{1+exp(-W^Tx+c)}
%     &=&
%     \begin{pmatrix}
%       -0.5 & 0.269 \\
%       0.731 & -0.5 \\
%       0.731 & -0.5 \\
%       0.881 & 0.731
%     \end{pmatrix} \\
%     &=&
%     \begin{pmatrix}
%       -1.038 \\
%       1.731 \\
%       1.731 \\
%       -0.581
%     \end{pmatrix}
%   \end{eqnarray*}
%   \begin{eqnarray*}
%     \hat{y} = \frac{1}{1+exp(-w^Tx+b)}
%     &=&
%     \begin{pmatrix}
%       0.26 \\
%       0.85 \\
%       0.85 \\
%       0.36
%     \end{pmatrix}
%   \end{eqnarray*}
% \end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Shallow Neural Nets : Summary}
  \begin{itemize}
    \item We have seen that neural networks are far more flexible than linear models. Furthermore, neural networks are able to approximate any continuous function.
      \begin{itemize}
        \item Yet, in reality, there is no way to make full use of the universal approximation property. The learning algorithm will usually not find the best possible model. At best it finds a locally optimal model. 
      \end{itemize}
    \item The XOR example showed us how neural networks extract features to transform the space and actually learn a kernel (learn a representation).
    \item Neural networks can perfectly fit noisy data. Thus, neural networks are endangered to over-fit. This is particularly true for a model with a huge hidden layer.
    \item Fitting neural networks with sigmoidal activation function is nothing else but fitting many weighted logistic regressions!
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Deep feedforward networks}
  \begin{itemize}
    \vspace{15mm}
    \item We will now extend the model class once again, such that we allow an arbitrary amount of $h$ layers.
    \vspace{5mm}
    \item For more than one hidden layer, we call such graphs \textbf{deep feedforward networks}.
  \end{itemize}
\framebreak
  \begin{itemize}
%     \item We can characterize those models by the following chain structure: $$f(x) = g(f_{(k)}(f_{(k-1)}(f_{(k-2)}(\ldots(f_{(1)}(x))\ldots)$$ where $f_{(1)}$ corresponds to the first and $f_{(k)}$ to the last layer of the network.
%     \item We can characterize those models by the following chain structure: $$f(\mathbf{x}) = \tau(\sigma^{(k)}(\sigma^{(k-1)}(\sigma^{(k-2)}(\ldots(\sigma^{(1)}(W^{(1)T}\mathbf{x} + \biasb^1)))$$ where $\sigma^{(1)}$ corresponds to the first and $\sigma^{(k)}$ to the last layer of the network.
%   \end{itemize}
% \framebreak
%   \begin{itemize}
%     \item We will now extend the model class once again, such that we allow an arbitrary amount of $K$ layers.
%       \begin{itemize}
%         \item For more than one hidden layer, we call such graphs deep feedforward networks.
%       \end{itemize}
    % \item We can characterize those models by the following chain structure: $$f(\mathbf{x}) = \tau(\sigma^{(K)}(\sigma^{(K-1)}(\sigma^{(K-2)}(\ldots(\sigma^{(1)}(W^{(1)T}\mathbf{x} + \biasb^{(1)})))$$ where $\sigma^{(1)}$ corresponds to the first and $\sigma^{(k)}$ to the last (hidden) layer of the network.
        \item We can characterize those models by the following chain structure: $$f(\mathbf{x}) = \tau \circ \phi \circ \sigma^{(h)} \circ \phi^{(h)} \circ \sigma^{(h-1)} \circ \phi^{(h-1)} \circ \ldots \circ \sigma^{(1)} \circ \phi^{(1)}$$ where $\sigma^{(i)}$ and $\phi^{(i)}$ are the activation function and the weighted sum of hidden layer $i$, respectively. $\tau$ and $\phi$ are the corresponding components of the output layer.

%(W^{(1)T}\mathbf{x} + \biasb^{(1)})

    \vspace{5mm}
    \item Each hidden layer has: 
      \begin{itemize}
        \vspace{2mm}
        \item an associated weight matrix $W^{(i)}$, bias $\biasb^{(i)}$ and activations $\hidz^{(i)}$ for $i \in 1 \ldots h$
        \vspace{2mm}
        \item $\hidz^{(i)} = \sigma^{(i)}(\phi^{(i)}) = \sigma^{(i)}(W^{(i)T}\hidz^{(i - 1)} + \biasb^{(i)})$ , where $\hidz^{(0)} = \mathbf{x}$.
      \end{itemize}
    \vspace{5mm}
    \item Again, without non-linear activations in the hidden layers, the network can only learn linear decision boundaries.
  \end{itemize}
\framebreak
  \begin{figure}
    \centering
      \includegraphics[width=10.5cm]{plots/deepneuralnet.png}
      \caption{Structure of a deep neural network with $h$ layers (bias term omitted).}
  \end{figure}
% \framebreak
%   \begin{itemize}
%     \item Mathematically, we observe the following mappings: 
%   \end{itemize}
%   \begin{eqnarray*}
%     &f_1(x):& \R^P \to \R^{M_1} \\
%     &f_2(f_1):& \R^{M_1} \to \R^{M_2} \\
%     &...& \\
%     &f_H(..):& \R^{M_{H-1}} \to \R^{M_H}, \ \forall h = 2,\dots,H \\
%     &g(..):& \R^{M_H} \to \R^{K}
%   \end{eqnarray*}
%   \begin{figure}
%     \centering
%       \includegraphics[width=5cm]{plots/deepneuralnet.png}
%   \end{figure}
\end{vbframe}  

% \begin{frame} {Deep feedforward networks : Matrix}
%   \begin{itemize}
%     \item Each layer has an associated weight matrix W
%     \item and hidden activations z1, z2,z3,
%     
%   \end{itemize}
% \end{frame}
% 
% \begin{frame} {Why deep}
%   \begin{itemize}
%     \item The best way to think of these layers is as representing hierarchies of features where earlier layers represent the simpler features and later layers represent more complex features.
%     \item Edges, nose, face
%   \end{itemize}
% \end{frame}

% \begin{frame} {DL}
%   \item A whole bunch of linear layers is just a linear layer
% \end{frame}
% 
% \begin{frame} {DL}
%   \item Not only is DL a way to automate representation, it is a clever way of doing it.
%   \item Bloody good prior
%   \item That's why our brains are organized in a similar way
%   \item NFL theorem
%   \item Why depth matters
% \end{frame}
% 
% \begin{frame} {DL}
%   \item Why depth matters : Empirical
% \end{frame}
% 
% \begin{frame} {DL}
%   \item Why depth matters : Theoretical
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe} {Deep feedforward networks : Example}
  \begin{figure}
    \centering
      \scalebox{0.95}{\includegraphics{plots/deepnet_one.png}}
  \end{figure}
\framebreak
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/deepnet_two.png}}
  \end{figure}
\framebreak
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/deepnet_three.png}}
  \end{figure}
\framebreak
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/deepnet_four.png}}
  \end{figure}
\framebreak
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/deepnet_five.png}}
  \end{figure}
\framebreak
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/deepnet_six.png}}
  \end{figure}
\framebreak
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/deepnet_seven.png}}
  \end{figure}
\framebreak
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/deepnet_eight.png}}
  \end{figure}
\framebreak
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/deepnet_nine.png}}
  \end{figure}
\framebreak
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/deepnet_ten.png}}
  \end{figure}
\framebreak
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/deepnet_eleven.png}}
  \end{figure}
\framebreak
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/deepnet_twelve.png}}
  \end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Why add more layers?}
\begin{itemize}
  \item Each layer in a feed-forward neural network adds its own degree of non-linearity to the model.
\end{itemize}
  \begin{figure}
    \centering
      \includegraphics[width=10.5cm]{plots/folding}
      \caption{An intuitive, geometric explanation of the exponential advantage of deeper networks formally (Mont\'{u}far et al. (2014)).}
  \end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \vspace{30mm}
  \hspace{15mm} \LARGE{A Brief History of Neural Networks}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{A brief history of neural networks}
  
  \begin{itemize}
    \item \pkg{1943:} The first artificial neuron, the "Threshold Logic Unit (TLU)", was proposed by Warren McCulloch \& Walter Pitts.
    \begin{itemize}
      \item In this model the neuron fires a $+1$ if the input exceeds a certain threshold $\phi$.
      \item However, this model didn't have adjustable weights, so learning could only be achieved by changing the threshold $\phi$.
    \end{itemize}
    % Source: https://en.wikipedia.org/wiki/Artificial_neuron#History
  \end{itemize}
\framebreak
  \begin{itemize}
    \item \pkg{1957:} The perceptron was invented by Frank Rosenblatt. 
  \end{itemize}
    \begin{figure}
      \centering
        \includegraphics[width=4cm]{plots/mark_i_perceptron.png}
        \caption{The Mark I Perceptron (https://en.wikipedia.org/wiki/Perceptron)}
    \end{figure}
\framebreak
  \begin{itemize}
    \item \pkg{1960:} ADALINE by Bernard Widrow \& Tef Hodd.
    \begin{itemize}
      \item Weights are now adjusted according to the weighted sum of the inputs.
    \end{itemize}
    % Source: https://en.wikipedia.org/wiki/ADALINE
    \item \pkg{1965:} Group method of data handling (also known as polynomial neural networks) by Alexey Ivakhnenko. The first general working learning algorithm for supervised deep feedforward multilayer perceptrons.
    % Source: http://people.idsia.ch/~juergen/firstdeeplearner.html (J?rgen Schmidhubers website)
    %         https://en.wikipedia.org/wiki/Group_method_of_data_handling
    \item \pkg{1969:} The first "AI Winter" kicked in.
    \begin{itemize}
      \item Marvin Minsky \& Seymour Papert proved that a perceptron cannot solve the XOR-Problem (linear separability).
      \item Less funding $\Rightarrow$ Standstill in AI/DL research
      % Source: https://en.wikipedia.org/wiki/Perceptrons_(book)#The_XOR_affair
      %         https://en.wikipedia.org/wiki/AI_winter
    \end{itemize}
  \end{itemize}
\framebreak
  \begin{itemize}
    \item \pkg{1985:} Multi-layered perceptron with backpropagation by David Rumelhart, Geoffrey Hinton and Ronald Williams.
    % Source: https://en.wikipedia.org/wiki/Backpropagation#History
    % Results: This was when Rumelhart, Williams, and Hinton demonstrated back propagation in a neural network could provide "interesting" distribution representations. Philosophically, this discovery brought to light the question within cognitive psychology of whether human understanding relies on symbolic logic (computationalism) or distributed representations (connectionism). http://www.dataversity.net/brief-history-deep-learning/
    \begin{itemize}
      \item Method to efficiently compute derivatives of differentiable composite functions.
      \item Backpropagation was developed already in 1970 by Seppo Linnainmaa.
      % Source: https://en.wikipedia.org/wiki/Seppo_Linnainmaa
    \end{itemize}
    \item \pkg{1985:} The second "AI Winter" kicked in.
    \begin{itemize}
      \item Overly optimistic/exaggerated expectations concerning potential of AI/DL.
      \item Angering investors, the phrase "AI" even reached a pseudoscience status.
      \item Kernel machines and graphical models both achieved good results on many important tasks.
      \item Some of the fundamental mathematical difficulties in modeling long sequences were identified.
      % Source http://www.dataversity.net/brief-history-deep-learning/
    \end{itemize}
    \item \pkg{2006:} Age of deep neural networks began.
    \begin{itemize}
      \item Geoffrey Hinton showed that a kind of neural network called deep belief network could be efficiently trained using a strategy called greedy layer-wise pretraining.
      \item This wave of neural networks research popularized the use of the term deep learning to emphasize that researchers were now able to train deeper neural networks than had been possible before.
      \item At this time, deep neural networks outperformed competing AI systems based on other machine learning technologies as well as hand-designed functionality.
    \end{itemize}
%  \end{itemize}
\framebreak
      \item  Why now and not earlier?
      \begin{itemize}
    \vspace{2mm}
    \item Significantly bigger datasets.
    \vspace{2mm}
    \item Better algorithms (optimization chapter).
      \begin{itemize}
        \item Vanishing gradient problem (relu).
      \end{itemize}
      \vspace{2mm}
    \item Better regularization (regularization chapter).
    \vspace{2mm}
    \item Unsupervised pretraining (autoencoder chapter).
    \vspace{2mm}
    \item More layers inevitably lead to a significant increase of parameters.
      \begin{itemize}
        \item Back then, processing power was simply not capable to handle such huge amounts of parameters. \\
        $\Rightarrow$ Nowadays, deep neural networks are trained on GPUs (graphic processing units), not on CPUs (central processing units).
      \end{itemize}
  \end{itemize}
\end{itemize}
  \framebreak
  \vspace{15mm}
  \begin{figure}
    \centering
      \scalebox{1.1}{\includegraphics{plots/dl_timeline.png}}
      \caption{Credit : Favio Vazquez}
  \end{figure}
  \framebreak
  \begin{figure}
    \centering
      \includegraphics[width=10cm]{plots/dataset_size_over_time.png}
      \caption{Dataset sizes over time (Goodfellow et al. (2016))}
  \end{figure}
\framebreak
  \begin{figure}
    \centering
      \includegraphics[width=10cm]{plots/network_size_over_time.png}
      \caption{Network sizes over time. 1: Perceptron, 5: Recurrent neural network for speech recognition, 8: LeNet-5, 10: Deep belief network, 20: GoogLeNet. For more details, see: Goodfellow et al. (2016)}
  \end{figure}
\end{vbframe}

% \begin{frame} {Why now and not earlier?(Can delete)}
% 
% %https://alisha17.github.io/machine-learning/2017/12/15/benchmarks.html
% \begin{figure}
%     \centering
%       \scalebox{0.75}{\includegraphics{plots/whynow_gpu.jpg}}
%       \tiny{\\Source : dl4j}
%   \end{figure}
% \end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{Number of parameters in a neural network}
%   \begin{itemize}
%     \item For a fix input and output, a neural network has 
%     $$M_1 \cdot M_2 \cdot ... \cdot M_H$$ 
%     parameters in its hidden layers.
%     \lz
%     \item or just
%     $$M^H$$ 
%     for the same amount of neurons in each layer (note that we omitted bias terms).
%   \end{itemize}
% \framebreak
% <<echo=FALSE, fig.height=5.5>>=
% library(ggplot2)
% logfun = function(v, s) {
%   v^s
% }
% x = seq(0, 10, 0.1)
% stretch = c(2, 3, 4)
% y = sapply(stretch, function(s) {
%   sapply(x, logfun, s = s)
% })
% 
% df = data.frame(y = as.vector(y), 
%   x = rep(x, length(stretch)),
%   s = as.factor(rep(stretch, each = length(x))))
% 
% hl = ggplot(df, 
%   aes(x = x, y = y, color = s)) + 
%   geom_line(size = 1) +
%   scale_y_continuous(name = "# parameters") +
%   scale_x_continuous(labels = function (x) floor(x), 
%     name = "# neurons in each hidden layer") +
%   theme(axis.title = element_text(size = 14L, 
%     face = "bold"),
%     plot.margin = unit(c(0, 0, 0, 0), "cm"), 
%     legend.position = c(0.25, 0.78), 
%     legend.background = element_rect(fill="transparent")) +
%   labs(colour = "number of\nhidden layers") 
% 
% hl
% @
% \end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{Initializing of the parameters}
%   \begin{itemize}
%     \item This topic is basically part of neural network optimization and not yet fully understood.
%     \item The choice of initial weights strongly influences the speed of convergence as well as optimality (ending up in a local or global minima).
%     \lz
%     \item Common strategies for weight initialization are:
%       \begin{itemize}
%         \item Drawing from a standard normal distribution.
%         \item Drawing from an uniform distribution with zero mean and sqrt(number of parameters) (LeCun et al. (1998)).
%         \item Unsupervised layerwise pre-training (Hinton and Salakhutdinov (2006)).
%       \end{itemize}
%   \end{itemize}
% \end{vbframe}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{Introduction to MXNet}
%   \begin{itemize}
%     \item Open-source deep learning framework written in C++ and cuda (used by Amazon for their Amazon Web Services)
%     \item Scalable, allowing fast model training
%     \item Supports flexible model programming and multiple languages (C++, Python, Julia, Matlab, JavaScript, Go, \textbf{R}, Scala, Perl)
%     \item Installation instructions for different operating systems: \url{http://mxnet.io/get_started/install.html}
%   \end{itemize}
% \end{vbframe}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{Kaggle challenge digit recognizer}
%     \begin{itemize}
%       \item The MNIST database is a large database of handwritten digits (black and white) that is commonly used for benchmarking various image processing algorithms.
%       \item It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.
%       \item  There have been a number of scientific papers on attempts to achieve the lowest error rate. One paper, using a hierarchical system of convolutional neural networks (chapter 4), manages to get an error rate of only 0.23 percent.
%     \end{itemize}
% \framebreak
%   \begin{figure}
%     \centering
%       \includegraphics[width=10cm]{plots/mnist.png}
%       \caption{Snipped from the mnist data set (LeCun and Cortes (2010)).}
%   \end{figure}
%   \begin{itemize}
%     \item 70k image data of handwritten digits with $28 \times 28$ pixels.
%     \item Classification task with 10 classes (e.g. 0, 1, 2, ..., 9).
%     \item In R: the darch package gives an easy option to access the data.
%     \item[] ...but for our example, we use another source with a more difficult train/test split.
%   \end{itemize}
% \framebreak
%   \begin{itemize}
%     \item Since competing with others is more fun, we dare ourselves to face the mnist kaggle challenge.
%     \item Therefor, we download the data sets (train.csv and test.csv) from \url{https://www.kaggle.com/c/digit-recognizer/data}.
%     \item We obtain $42.000$ images for training and $28.000$ for testing.
%   \end{itemize}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%% READ ME!!! %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % The following slides include lots of code chunks which have been     %
% % temporarily disabled (eval = FALSE, echo = FALSE) and replaced by    %
% % screenshots of the corresponding outputs (to maintain colorization). %
% % Else, one would need a working version of mxnet (and a fast CPU/GPU) %
% % to compile the code in a finite amount of time.                      %
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   % \begin{figure}
%   %   \centering
%   %     \includegraphics[width=12cm]{plots/mxnet_codechunk_1.png}
%   % \end{figure}
% <<mxnet1, size = "small", cache = TRUE, eval = FALSE, echo = TRUE>>=
% # assign the location of the data as your wd()
% 
% train = read.csv("train.csv", header = TRUE)
% test = read.csv("test.csv", header = TRUE)
% 
% train = data.matrix(train)
% test = data.matrix(test)
% @
% \framebreak
%   \begin{figure}
%     \centering
%       \includegraphics[width=11cm]{plots/mxnet_codechunk_2.png}
%   \end{figure}
% <<mxnet2, size = "normalsize", cache = TRUE, eval = FALSE, echo = FALSE>>=
% # Split data into matrix containing features and
% # vector with labels
% train.x = train[, -1]
% train.y = train[, 1]
% 
% # normalize to (0,1) and transpose data
% train.x = t(train.x/255)
% dim(train.x)
% 
% test = t(test/255)
% 
% table(train.y)
% @
% \framebreak
%   \begin{itemize}
%     \item Now we define the architecture of our model.
%   \end{itemize}
%   % \begin{figure}
%   %   \centering
%   %     \includegraphics[width=11cm]{plots/mxnet_codechunk_3.png}
%   % \end{figure}
% <<mxnet3, size = "scriptsize", cache = TRUE, eval = FALSE, echo = TRUE>>=
% require("mxnet")
% 
% data = mx.symbol.Variable(name = "data")
% 
% layer1 = mx.symbol.FullyConnected(data = data, name = "layer1",
%   num_hidden = 10L)
% activation1 = mx.symbol.Activation(data = layer1, name = "activation1",
%   act_type = "relu")
% layer2 = mx.symbol.FullyConnected(data = activation1, name = "layer2",
%   num_hidden = 10L)
% activation2 = mx.symbol.Activation(data = layer2, name = "activation2",
%   act_type = "relu")
% layer3 = mx.symbol.FullyConnected(data = activation2, name = "layer3",
%   num_hidden = 10L)
% softmax = mx.symbol.SoftmaxOutput(data = layer3, name = "softmax")
% @
% \framebreak
%   \begin{minipage}{0.45\textwidth}
%     \begin{itemize}
%       \item Mxnet enables us to easily visualize the models architecture
%     \end{itemize}
%   % \begin{figure}
%   %   \centering
%   %     \includegraphics[width=6cm]{plots/mxnet_codechunk_4a.png}
%   % \end{figure}
% <<mxnet4, size = "footnotesize", cache = TRUE, eval = FALSE, echo = TRUE>>=
% graph.viz(model$symbol)
% @
%   \end{minipage}
%   \begin{minipage}{0.45\textwidth}
%     \begin{figure}
%       \centering
%         \includegraphics[width=1.5cm]{plots/mxnet_codechunk_4b.png}
%     \end{figure}
%   \end{minipage}
% \framebreak
%   \begin{itemize}
%     \item In a final step, we have to assign some parameters.
%   \end{itemize}
% 
%   % \begin{figure}
%   %   \centering
%   %     \includegraphics[width=11cm]{plots/mxnet_codechunk_5.png}
%   % \end{figure}
% <<mxnet5, size = "footnotesize", cache = TRUE, eval = FALSE, echo = TRUE>>=
% devices = mx.cpu()
% 
% mx.set.seed(1337)
% 
% model = mx.model.FeedForward.create(
%   symbol = softmax,
%   X = train.x, y = train.y,
%   ctx = devices,
%   num.round = 10L, array.batch.size = 100L,
%   learning.rate = 0.05,
%   eval.metric = mx.metric.accuracy,
%   initializer = mx.init.uniform(0.07),
%   epoch.end.callback = mx.callback.log.train.metric(100L))
% @
% \framebreak
%   \begin{figure}
%     \centering
%       \includegraphics[width=10.5cm]{plots/mxnet_codechunk_6.png}
%   \end{figure}
% <<mxnet6, size = "scriptsize", warning = FALSE, cache = TRUE, eval = FALSE, echo = FALSE>>=
% require("mxnet")
% 
% train = read.csv("train.csv", header = TRUE)
% test = read.csv("test.csv", header = TRUE)
% train = data.matrix(train)
% test = data.matrix(test)
% train.x = train[,-1]
% train.y = train[,1]
% train.x = t(train.x/255)
% test = t(test/255)
% data = mx.symbol.Variable("data")
% layer1 = mx.symbol.FullyConnected(data, name = "layer1",num_hidden = 10)
% activation1 = mx.symbol.Activation(layer1, name = "activation1", act_type = "relu")
% layer2 = mx.symbol.FullyConnected(activation1, name = "layer2", num_hidden = 10)
% activation2 = mx.symbol.Activation(layer2, name = "activation2", act_type = "relu")
% layer3 = mx.symbol.FullyConnected(activation2, name = "layer3", num_hidden = 10)
% softmax = mx.symbol.SoftmaxOutput(layer3, name = "softmax")
% devices = mx.cpu()
% mx.set.seed(1337)
% model = mx.model.FeedForward.create(softmax, X = train.x, y = train.y,
%   ctx = devices, num.round = 10, array.batch.size = 100,
%   learning.rate = 0.05, momentum = 0.9,
%   eval.metric = mx.metric.accuracy,
%   initializer = mx.init.uniform(0.07),
%   epoch.end.callback = mx.callback.log.train.metric(100))
% @
%   \begin{itemize}
%     \item After 10 epochs, our neural network begins to stagnate at a training accuracy of roughly $93.5\%$
%     \item Following up, we use the model to predict the test data.
%   \end{itemize}
% \framebreak
%   \begin{figure}
%     \centering
%       \includegraphics[width=11cm]{plots/mxnet_codechunk_7.png}
%   \end{figure}
% <<mxnet7, size = "scriptsize", warning = FALSE, cache = TRUE, eval = FALSE, echo = FALSE>>=
% preds = predict(model, test)
% # this yields us predicted probabilities for all 10 classes
% dim(preds)
% 
% # we choose the maximum to obtain quantities for each class
% pred.label = max.col(t(preds)) - 1
% table(pred.label)
% @
% \framebreak
%   \begin{itemize}
%     \item Finally we want to submit our predictions on kaggle to see how good we performed.
%     \item Thus, we save our results in a csv file and upload it on \url{https://www.kaggle.com/c/digit-recognizer/submit}
%   \end{itemize}
%   % \begin{figure}
%   %   \centering
%   %     \includegraphics[width=11cm]{plots/mxnet_codechunk_8.png}
%   % \end{figure}
% <<mxnet8, size = "footnotesize", cache = TRUE, eval = FALSE, echo = TRUE>>=
% submission = data.frame(ImageId = 1:ncol(test), Label = pred.label)
% 
% write.csv(submission, file = 'submission.csv', row.names = FALSE,
%   quote = FALSE)
% @
% \framebreak
%   \begin{itemize}
%     \item After making your submission, you should see something like this:
%   \end{itemize}
%   \begin{figure}
%     \centering
%       \includegraphics[width=10.5cm]{plots/mxnet_codechunk_9.png}
%   \end{figure}
%   \begin{itemize}
%     \item For this competition Kaggle uses accuracy (score) to messure each participants performance.
%     \item While the ratio of the train to test data makes the problem really difficult, $89.843\%$ is still a very bad result and we would like to improve our performance.
%   \end{itemize}
% \framebreak
%   \begin{minipage}{0.45\textwidth}
%     \begin{itemize}
%       \item Let us try the following, much larger, network (all other parameters remain the same):
%     \end{itemize}
%   \end{minipage}
%   \begin{minipage}{0.45\textwidth}
%     \begin{figure}
%       \centering
%         \includegraphics[width=1.5cm]{plots/mxnet_codechunk_10.png}
%     \end{figure}
%   \end{minipage}
% \framebreak
%   \begin{figure}
%     \centering
%       \includegraphics[width=11cm]{plots/mxnet_codechunk_11.png}
%   \end{figure}
%   \begin{itemize}
%     \item Rerunning the training with the new architecture, this model yields us a training accuracy of $99.39\%$ and a test accuracy of $96.514\%$.
%   \end{itemize}
% \end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{vbframe} {Optimization (Preview)}
%   \begin{blocki}{Loss of a neural network:}
%     \item To optimize a neural network, we have to minimize a loss function $\Lxy$, where $y$ corresponds to the ground truth and $f(x)$ to the networks prediction.
%     \item For regression, we typicall use the L2 loss (rarely L1): $$\Lxy = \frac{1}{2}(y - f(x))^2$$
%     \item For classification we typically apply the cross entropy (binomial loss): 
%      $$\Lxy = -\frac{1}{n} \sum_{i=1}^{n} \Big[y_i log \ f(x) + (1 - y_i) log(1 - f(x)) \Big]$$
%     \item Thingy about gradient descent.
%   \end{blocki}
% \framebreak
%   \begin{itemize}
%     \item The term cross-entropy is widely used for the negative log-likelihood of a bernoulli or softmax distribution, but that is a misnomer.
%     \begin{itemize}
%       \item Any loss consisting of a negative log-likelihood is a cross-entropy between the empirical distribution of the training data and the probability distribution defined by model! 
%       \item For example, the mean squared error is the cross-entropy between the empirical distribution and a Gaussian model.
%     \end{itemize}
%     \item Thus, maximum likelihood estimation is as an attempt to make the model distribution match the empirical distribution.
%   \end{itemize}
% \end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%   TO DELETE   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{vbframe}{Supervised and unsupervised learning}
%   \begin{itemize}
%     \item Dealing with supervised problems means we know the ground truth of the data (i.e. we have labeled training data).
%     \begin{itemize}
%       \item Our goal is to learn an output $y$ (real valued or categorical) based on its features: $\hat{y} = f(x)$.
%       \item Those features are the elements of the input vectors $x_1, x_2, ..., x_p$. 
%       %(the columns of the data frame).
%       \item Upon the prediction we can compute the loss to evaluate our model.
%     \end{itemize}
%     \item Unsupervised on the other hand means that we do not know the ground truth of the data.
%     \begin{itemize}
%       \item Thus, there is no possibility to evaluate the the model against the ground truth.
%     \end{itemize}
%   \end{itemize}
% \end{vbframe}
% 
% \begin{vbframe}{Shallow Neural Nets : Single Hidden Layer}
%   We will now extend the perceptron:\\
%   \begin{itemize}
%     \item Our perceptron from before only had one neuron $z$.
%     \item Extension: $m$ neurons $z_1, \dots, z_m$
%     \item The neurons are arranged in layers:\\
%     \begin{itemize}
%       \item The first layer is the input layer.
%       \item The intermediate layer is called \enquote{hidden layer},
%       because its output is not observed directly.
%       \item The last layer is the output layer.
%     \end{itemize}
%     \item Each neuron is connected to all neurons in the previous and next layer over directed links (fully connected layers!). Directed means that information can only be passed in one direction. We'll call such graphs
%     \enquote{feed-forward neural networks}.
%   \end{itemize}
% \framebreak
%   \begin{blocki}{Output function $\tau(u^T z)$:}
%     \item For regression one may use any function mapping from $\R \to \R$, for example the identity function:
%     $$ \tau(u^T z) = u^T z $$
%     \item For binary classification one could apply the sigmoidal logistic function.
%   \end{blocki}
% \end{vbframe}
% 
% \begin{vbframe}{Single Neuron}
%     \begin{figure}
%       \centering
%         \includegraphics[width=10cm]{plots/perceptron_neu.png}
%         \caption{The perceptron with a single neuron z, input units $x_1, x_2,... ,x_p$, weights $w_1, w_2,... ,w_p$ and a constant bias term b.}
%     \end{figure}
% \framebreak
% \begin{itemize}
%   \item The features $x = (x_1,\ldots, x_p)^T$ are connected with the neuron $z$ via links.
%   \item The bias term must not be confused with the statistical bias. It actually is an intercept parameter.
%   \item We may add a column containing only ones to the feature matrix, so we can drop the bias term in notation ($x = (x_0,\ldots, x_p)^T$).
%   \item The weights $w = (w_0,\ldots,w_p)$ control the impact of each input unit (i.e. feature) on the prediction.
%   \item $\sigma$ is called activation function. It can be used for a non-linear transformation of the input.
%   \item So the neuron can be expressed as: $z = \sigma(w^Tx) = \hat{y}$ and:
%   $$
%   \sigma(w^Tx) =
%   \begin{cases}
%    1 & w^Tx \geq \phi \\
%    0 & otherwise \\
%   \end{cases}
%   $$
% \end{itemize}
% \end{vbframe}
% 
% \begin{vbframe}
% \begin{minipage}{0.5\textwidth}
%   \begin{itemize}
%     \item Linear classifier: \\
%     \lz
%     2 weights: linear boundary:
%     $$w_{1} x_1 + w_{2} x_2 = \phi$$
%     \item Weights are learned by a delta-update rule.
%     \item Only linearly separable problems can be learned with a perceptron.
%   \end{itemize}
% \end{minipage}\hfill
% \begin{minipage}{0.45\textwidth}
%   \begin{figure}
%     \centering
%       \includegraphics[width=5cm]{plots/linearclassifier.png}
%       %\caption{The Mark I Perceptron (https://en.wikipedia.org/wiki/Perceptron)}
%   \end{figure}
% \end{minipage}
% \framebreak
%   \begin{blocki}{Choices for $\sigma$:}
%     \item If we choose the identity our model collapses to a simple linear regression:
%     $$
%     y = \sigma(w^Tx) = w^Tx
%     $$
%     \item Using the logistic function gives us:
%     $$
%     y = \sigma(w^Tx) = \frac{1}{1 + \exp(-w^Tx)}
%     $$
%     This is just logistic regression!
%   \end{blocki}
% \framebreak
%   \begin{itemize}
%     \item As already stated, the bias term must not be confused with the statistical bias.
%     \item It is actually an intercept parameter, which puts the decision boundary at the correct position in the learned space.
%     % Deeplearningbook page 110: This terminology derives from the point of view that the output of the transformation is biased toward being b in the absence of any input.
%   \end{itemize}
%     \begin{figure}
%       \centering
%         \includegraphics[width=8.5cm]{plots/bias.png}
%     \end{figure}
% \end{vbframe}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}
\frametitle{References}
\footnotesize{
\begin{thebibliography}{99}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Mont\'{u}far et al., 2014]{1} Guido Mont\'{u}far, Razvan Pascanu, Kyunghyun Cho and Yoshua Bengio (2014)
\newblock On the Number of Linear Regions of Deep Neural Networks
\newblock \emph{\url{https://arxiv.org/pdf/1402.1869.pdf}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Yann LeCun and Corinna Cortes, 2010]{2} Yann LeCun and Corinna Cortes (2010)
\newblock MNIST handwritten digit database 
\newblock \emph{\url{http://yann.lecun.com/exdb/mnist/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Yann LeCun et al., 1998]{3} Yann Lecun, Leon Bottou, Genevieve B. Orr and Klaus-Robert Mller (1998)
\newblock Efficient BackProp
\newblock \emph{\url{http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Geoffrey Hinton and Ruslan Salakhutdinov, 2006]{4} Geoffrey Hinton and Ruslan Salakhutdinov (2006)
\newblock Reducing the Dimensionality of Data with Neural Networks
\newblock \emph{\url{https://www.cs.toronto.edu/\%7Ehinton/science.pdf}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Ian Goodfellow et al., 2016]{5} Ian Goodfellow, Yoshua Bengio and Aaron Courville (2016)
\newblock Deep Learning
\newblock \emph{\url{http://www.deeplearningbook.org/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibitem[Lin, Tegmark et al. 2016]{6} Henry W. Lin, Max Tegmark, and David Rolnick (2016)
\newblock Why does deep and cheap learning work so well?
\newblock \emph{\url{https://arxiv.org/pdf/1608.08225.pdf}}
\end{thebibliography}
}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlecture
