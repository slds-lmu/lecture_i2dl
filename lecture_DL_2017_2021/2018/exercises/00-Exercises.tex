%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
%\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{verbatimbox}
\usepackage{graphicx}
\usepackage{float}


\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps
\usepackage{hyperref}
\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

\input{commands.tex}

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height
\title{	
\normalfont \normalsize 
\textsc{Faculty of Mathematics, Ruhr University Bochum} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Deep Learning\\{\Large Exercise Sheet 1}\\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Asja Fischer} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title
\section{In Class}
\begin{enumerate} 
\item \textbf{Classification and Regression}
\begin{itemize}
\item[a)]Give an example for a classification and a regression problem one could solve with supervised machine learning.
\item[b)]Do you know a loss function that could be used for a classification problem? What kind of loss function could be used for a regression problem?
\end{itemize}


\item \textbf{Risk and Empirical Risk}

Let $f: \mathcal{X} \rightarrow \mathcal{Y}$ be a model, $P_{x,y}$ the data underlying distribution, and $L(y, f(x))$ a loss function. Recall the definition of the risk  $\mathcal{R}(f)$ and empirical risk 
 $\mathcal{R}(f)_{\text{emp}}$, and show that
 $$
 \mathbb{E}[\mathcal{R}(f)_{\text{emp}}] = \mathcal{R}(f)
 $$
Which property of the expectation is used?  And what follows for $mathcal{R}(f)_{\text{emp}}$ from the central limit theorem? 


\item \textbf{Linear Regression}
\begin{itemize}
\item[a)] Consider a linear regression model $y= X\theta$  with feature-data-matrix $X \in \mathbb{R}^{n \times (p+1)}$, parameters $\theta\in \mathbb{R}^{p+1}$, and label vector $y \in \mathbb{R}^{n}$. Derive the normal equations, that is, the parameters that minimize the mean squared error (MSE). 
\item[b)] Show that minimizing the MSE and maximizing the log-likelihood (for linear regression with normal distributed noise) leads to the same parameters. 
\end{itemize}

\item \textbf{Logistic Regression}

Consider a logistic regression model 
$$
\pi(x) = P(y=1|x) = \frac{\exp(\theta^Tx)}{1+\exp(\theta^Tx)}
$$
Derive an expression for the log-likelihood of $\theta$ given a data set $(x^{(1)}, y^{(1)}), \dots, (x^{(n)}, y^{(n)})$.





\item \textbf{Underfitting and Overfitting}

%\begin{itemize}
%\item[a)] What are underfitting and overfitting?
%\item[b)] 
You are training three classifiers on a training set. The plots in Figure~\ref{fig:model1}, %\ref{fig:model2} and \ref{fig:model3} 
show the evolution of their classification error on training and test set during training. Do the models underfit, overfit or model the data well?
\begin{figure}[H]
%\centering
\hspace{-.5cm}
\includegraphics[width=.4\textwidth]{images/model1}%
\includegraphics[width=.4\textwidth]{images/model2}%
\includegraphics[width=.4\textwidth]{images/model3}
\caption{Training and test  error during iterative optimization. Left: model 1, middle: model 2, right: model 3. }
\label{fig:model1}
\end{figure}
%

\end{enumerate}


\section{At Home}
\begin{enumerate}
\item If you want to familiarise yourself with Python and its machine learning libraries, please go through the tutorial at \url{http://scikit-learn.org/stable/tutorial/basic/tutorial.html}. % installing Python (and scikit-learn?) 
%\item under-overfitting with a concrete example under usage of Python (scikit-learn?) 
\end{enumerate}
\end{document}
