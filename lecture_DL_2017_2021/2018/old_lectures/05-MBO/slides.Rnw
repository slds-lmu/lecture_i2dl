<<setup-child, include = FALSE>>=
library(knitr)
set_parent("../style/preamble.Rnw")
knitr::opts_chunk$set(cache=TRUE)
@

\lecturechapter{5}{Model-Based Optimization}
\lecture{Deeplearning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% smbo %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Sequential model-based optimization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Expensive Black-Box Optimization}
  \vspace{1cm}
  \begin{minipage}{0.33\linewidth}
    \begin{figure}
    \centering
      \includegraphics[width = \linewidth]{plots/gears.png}
    \end{figure}
  \end{minipage}
  \begin{minipage}{0.65\linewidth}
    \begin{align}
        y &= f(\boldsymbol{x}) \ , \quad f: \mathbb{X} \rightarrow \mathbb{R} \\
        \boldsymbol{x}^* &= \argmin\limits_{\boldsymbol{x} \in \mathbb{X}} f(\boldsymbol{x})
    \end{align}
    \begin{itemize}
        \item{$y$}, target value
        \item{$\boldsymbol{x} \in \mathbb{X} \subset \mathbb{R}^d$}, domain
        \item{$f(\boldsymbol{x})$} function with considerably long runtime
        \item{Goal:} Find optimum $\boldsymbol{x}^*$
    \end{itemize}
  \end{minipage}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Sequential model-based optimization}
  \begin{block}{}
    \begin{itemize}
      \item Setting: Expensive black-box problem $f: x \rightarrow \mathbb{R} = min!$
      \item Classical problem: Computer simulation with a bunch of control parameters and
        performance output; or algorithmic performance on 1 or more problem instances;
        we often optimize ML pipelines
      \item Idea: Let's approximate $f$ via regression!
    \end{itemize}
  \end{block}
\framebreak
  \begin{block}{Generic MBO Pseudo Code}
    \begin{itemize}
      \item Create initial space filling design and evaluate with $f$
      \item In each iteration:
        \begin{itemize}
          \item Fit regression model on all evaluated points to \\
                predict $\hat{f}x$ and uncertainty $\hat{s}(x)$
          \item Propose point via infill criterion
            $$ EI(x)\uparrow \;\Longleftrightarrow\; \hat{f}(x) \downarrow \; \wedge \; \hat{s}(x) \uparrow $$
          \item Evaluate proposed point and add to design
          \item EGO proposes kriging (aka Gaussian Process) and EI\\
            \textit{Jones 1998, Efficient Global Opt. of Exp. Black-Box Functions}
        \end{itemize}
    \end{itemize}
  \end{block}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Latin Hypercube Designs}
  \begin{figure}
  \centering
    \includegraphics[height = 4cm]{plots/initdes.png}
  \end{figure}
  \begin{itemize}
    \item Initial design to train first regression model
    \item Not too small, not too large
    \item LHS / maximin designs: Min dist between points is maximized
    \item But: Type of design usually has not the largest effect on MBO, and unequal distances between points
      could even be beneficial
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Infill Criteria: Expected Improvement}
  \begin{itemize}
    \small
    \item Define improvement at $x$ over best visited point
          with $y=f_{min}$  as random variable
          $I(x) = |f_{min} - Y(x)|^+ $
    \item For kriging $Y(x) \sim N(\hat{f}(x), \hat{s}^2(x))$ (given $x=x$)
    \item Now define $EI(x) = E[ I(x)| x = x ]$
    \item Expectation is integral over normal density starting at $f_{min}$
    \item Alternative: Lower confidence bound (LCB) $\hat{f}(x) - \lambda\hat{s}(x)$
    \end{itemize}
  Result: $EI(x) = \left( f_{min} - \hat{f}(x) \right) \Phi \left( \frac{f_{min} - \hat{f}(x))}{\hat{s}(x)} \right) +
           \hat{s}(x) \phi \left( \frac{f_{min} - \hat{f}(x)}{\hat{s}(x)} \right)$
  \begin{figure}[b]
    \includegraphics[width = \textwidth, height = 3.3cm]{plots/Grafik3}
  \end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Focussearch}
    \begin{itemize}
      \item EI optimization is multimodal and not that simple
      \item But objective is now cheap to evaluate
      \item Many different algorithms exist, from gradient-based methods with restarts to
        evolutionary algorithms
      \item We use an iterated, focusing random search coined \enquote{focus search}
      \item In each iteration a random search is performed
      \item We then shrink the constraints of the feasible region towards the best point in the current
        iteration (focusing) and iterate, to enforce local convergence
      \item Whole process is restarted a few times
      \item Works also for categorical and hierarchical params
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \begin{figure}[H]
  \centering %page 1,10
    \only<1>{\includegraphics[page=1, width=\linewidth]{plots/mbo-example0-1.pdf}}%
    \only<2>{\includegraphics[page=1, width=\linewidth]{plots/mbo-example1-1.pdf}}%
    \only<3>{\includegraphics[page=1, width=\linewidth]{plots/mbo-example2-1.pdf}}%
    \only<4>{\includegraphics[page=1, width=\linewidth]{plots/mbo-example3-1.pdf}}%
    \only<5>{\includegraphics[page=1, width=\linewidth]{plots/mbo-example4-1.pdf}}%
    \only<6>{\includegraphics[page=1, width=\linewidth]{plots/mbo-example20-1.pdf}}%
  \end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{mlrMBO: Model-Based Optimization Toolbox}
\begin{minipage}{0.4\linewidth}
  \begin{itemize}
  \small
    \item Any regression from mlr
    \item Arbtritrary infill
    \item Single - or multi-crit
    \item Multi-point proposal
    \item Via parallelMap and batchtools
    runs on many parallel backends and clusters
    \item Algorithm configuration
    \item Active research
  \end{itemize}
\end{minipage}
\begin{minipage}{0.55\linewidth}
    \includegraphics[width = \textwidth]{plots/mlrMBO1.pdf}
\end{minipage}
  \begin{center}
    \begin{itemize}
    \footnotesize
      \item mlr:
      \url{https://github.com/mlr-org/mlr}
      \item mlrMBO:
      \url{https://github.com/mlr-org/mlrMBO}
      \item mlrMBO Paper on arXiv (under review)
      \url{https://arxiv.org/abs/1703.03373}
    \end{itemize}
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Benchmark MBO on artificial test functions}
  \begin{itemize}
    \item Comparison of mlrMBO on multiple different test functions
      \begin{itemize}
        \item Multimodal
        \item Smooth
        \item Fully numeric
        \item \textit{Well known}
      \end{itemize}
    \item We use GPs with
      \begin{itemize}
        \item LCB with $\lambda = 1$
        \item Focussearch
        \item 200 iterations
        \item 25 point initial design, created by LHS sampling
      \end{itemize}
      \item Comparison with
    \begin{itemize}
        \item Random search
        \item CMAES
        \item other MBO implementations in R
    \end{itemize}
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{MBO GP vs. competitors in 5D}
  \begin{center}
    \includegraphics[width = 0.7\textwidth]{plots/mbo_y_single-1.pdf}
  \end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%% challenges 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Interesting Challenges}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Challenge: The correct surrogate?}
  \begin{itemize}
    \item GPs are very much tailored to what we want to do, due to their spatial structure
    in the kernel and the uncertainty estimator.
    \item But GPs are rather slow. And (fortunately) due to parallization (or speed-up tricks like subsampling)
    we have more design points to train on.
    \item Categorical features are also a problem in GPs
    (although methods exist, usually by changing the kernel)
    \item Random Forests handle categorical features nicely, are much faster. But they don't rely on a
    spatial kernel and the uncertainty estimation is much more heuristic / may not represent what we want.
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Challenge: Time Heterogeneity}
  \begin{itemize}
    \item Complex configuration spaces across many algorithms results in vastly different runtimes
    in design points.
    \item Actually just the RBF-SVM tuning can result in very different runtimes.
    \item We don't care how many points we evaluate, we care about total walltime of the configuration.
    \item The option to subsample further complicates things.
    \item Parallelization further complicates things.
    \item Option: Estimate runtime as well with a surrogate, integrate it into acquisition function.
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%% hpo %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ML Model Selection and Hyperparameter Optimization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Automatic Model Selection}
  \begin{block}{Prior approaches:}
    \begin{itemize}
    \small
      \item Looking for the silver bullet model \\
      $\leadsto$ \textcolor{blue}{Failure} \\
      \item Exhaustive benchmarking / search \\
      $\leadsto$ \textcolor{blue}{Very expensive, often contradicting results}
      \item Meta-Learning:\\
      $\leadsto$ \textcolor{blue}{Good meta-features are hard to construct} \\
      $\leadsto$ \textcolor{blue}{IMHO: Gets more interesting when combined with SMBO}
    \end{itemize}
  \end{block}
  \begin{block}{Goal for AutoML:}
    \begin{itemize}
      \small
      \item Data dependent
      \item Automatic
      \item Include every relevant modeling decision
      \item Efficient
      \item Learn on the model-settings level!
    \end{itemize}
  \end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{From Normal SMBO to Hyperarameter Tuning}
  \begin{itemize}
    \item Objective function is resampled performance measure
    \item Parameter space  $\theta \in \Theta$\\
    might be discrete and dependent / hierarchical
    \item No derivative for $f(\cdot, \theta)$, black-box
    \item Objective is stochastic / noisy
    \item Objective is expensive to evaluate
    \item In general we face a problem of algorithm configuration:
    \item $\leadsto$ \textcolor{blue}{Usual approaches: racing or model-based / bayesian optimization}
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{From Normal SMBO to Hyperarameter Tuning}
  \begin{figure}[H]
  \centering
  \includegraphics[width=0.65\textwidth]{plots/chain.pdf}
 \end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Complex Parameter Space}
\vspace{-0.5cm}
\begin{figure}[t]
\center
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
                    thick,circ/.style={circle,draw,font=\sffamily\scriptsize},
                    rect/.style={rectangle,draw,font=\sffamily\scriptsize}]
  \node[rect] (20) at (3, 4.5) {Parameter Set};
  \node[circ] (18) at (0, 3.5) {cl.weights};
  \node[circ]  (1) at (6, 3.5) {learner};
  \node[rect] (19) at (-0.5, 2) {$2^{[-7,...,7)}$};
  \node[rect]  (2) at (2, 2) {randomForest};
  \node[rect]  (3) at (4, 2) {L2 LogReg};
  \node[rect]  (4) at(6, 2) {svm};
  \node[circ]  (5) at (0, 0.5) {mtry};
  \node[circ]  (6) at (2, 0.5) {nodesize};
  \node[circ]  (7) at (4, 0.5) {cost};
  \node[circ]  (8) at (6, 0.5) {cost};
  \node[circ]  (9) at(8, 2) {kernel};
  \node[rect] (10) at (8.5, 1){radial};
  \node[rect] (17) at (7, 1){linear};
  \node[circ] (11) at(8, 0) {$\gamma$};
  \node[rect] (12) at (-0.5, -1) {$\{0.1p,..., 0.9p\}$};
  \node[rect] (13) at (2, -1) {$\{1,..., 0.5n\}$};
  \node[rect] (14) at (4, -1) {$2^{[-15, 15]}$};
  \node[rect] (15) at (6, -1) {$2^{[-15, 15]}$};
  \node[rect] (16) at (8, -1) {$2^{[-15, 15]}$};
  \path[every node/.style={font=\sffamily\small}]
    (1) edge node {}(2)
        edge node {}(3)
        edge node {}(4)
    (2) edge node {}(5)
        edge node {}(6)
    (3) edge node {}(7)
    (4) edge node {}(8)
        edge node {}(9)
    (5) edge node {}(12)
    (6) edge node {}(13)
    (7) edge node {}(14)
    (8) edge node {}(15)
    (9) edge node {}(10)
        edge node {}(17)
    (10) edge node {}(11)
    (11) edge node {}(16)
    (18) edge node {}(19)
    (20) edge node {}(1)
         edge node {}(18);
\end{tikzpicture}
\end{figure}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Complex Parameter Space}
\vspace{-0.5cm}
\begin{figure}[t]
\center
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
                    thick,circ/.style={circle,draw,font=\sffamily\scriptsize},
                    rect/.style={rectangle,draw,font=\sffamily\scriptsize}]
  \node[circ]  (1) at (3.5, 3) {neural net};
  \node[rect]  (2) at (1, 2) {Layer};
  \node[rect]  (3) at (5.5, 3.5) {global};
  \node[rect] (35) at (7.5, 3.2){batch size};
  \node[rect] (39) at (5.5, 4.5){weight decay};
  \node[rect] (58) at (6.5, 2){...};
  \node[rect]  (4) at(4, 1.5) {optimizer};
  \node[rect]  (5) at (-1, 1) {conv};
  \node[rect]  (30) at (-1, 3) {other..};
  \node[rect]  (20) at (-1, 0) {activation};
  \node[rect, draw, align=left]  (21) at (-1, -1) {batch norm, \\ dropout};
  \node[rect]  (6) at (1.5, 1) {dense};
  \node[rect]  (7) at (7.5, 4.2) {epochs};
  \node[rect]  (8) at (4, 0.5) {SGD};
  \node[rect]  (9) at(6, 1) {adaptive};
  \node[rect] (10) at (6.5, -0.2){RMSprop};
  \node[rect] (11) at (7.5, 0.5){...};
  \node[rect] (17) at (5.2, 0.2){adam};
  \node[rect] (12) at (-1, -2) {pooling};
  \node[rect] (22) at (1.5, 0) {activation};
  \node[rect, draw, align=left] (23) at (1.5, -1) {batch norm, \\ dropout};
  \node[rect] (15) at (3.4, -1) {momentum};
  \node[rect] (31) at (5.3, -1) {learning rate};
  \node[circ] (50) at (3, -2.5) {output};
  \node[rect] (54) at (6, -2.5) {loss function};
  \node[circ] (51) at (-1.5, 4.5) {input};
  \node[rect] (52) at (1, 4.5) {preprocessing};
  \node[rect] (53) at (1, 3.5) {dropout};
  %\node[draw, align=left] {This is a\\demonstration.};
  \path[every node/.style={font=\sffamily\small}]
    (1) edge node {}(2)
        edge node {}(3)
        edge node {}(4)
    (2) edge node {}(5)
        edge node {}(6)
    (3) edge node {}(7)
    (4) edge node {}(8)
        edge node {}(9)
    (5) edge node {}(20)
    (6) edge node {}(22)
    (22) edge node {}(23)
    (8) edge node {}(15)
    (9) edge node {}(10)
        edge node {}(11)
        edge node {}(17)
    (20) edge node {}(21)
    (2) edge node {}(30)
    (8) edge node {}(31)
    (3) edge node {}(35)
    (51) edge node {}(52)
    (52) edge node {}(1)
    (12) edge node {}(50)
    (23) edge node {}(50)
    (52) edge node {}(53)
    (53) edge node {}(1)
    (50) edge node {}(54)
    (3) edge node {}(58)
    (3) edge node {}(39)
    (12) edge[bend left=95] node [left] {} (2)
    (23) edge[bend left=75] node [left] {} (2)
    (21) edge node {}(12);

\end{tikzpicture}
\end{figure}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{From Normal SMBO to Hyperarameter Tuning}

  \begin{itemize}
  \item Initial design: LHS principle can be extended, or just use random
  \item Focus search: Can be (easily) extended, as it is based on random search.
    To zoom in for categorical parameters we randomly drop a category for each param
    which is not present in the currently best configuration.
\end{itemize}


  \begin{itemize}
  \item Few approaches for GPs with categorical params exist (usually with new covar kernels), not very established
\item Alternative: Random regression forest (mlrMBO, SMAC)
  \item Estimate uncertainty / confidence interval for mean response by
  efficient bootstrap technique\footnote{Sexton et al, \enquote{Standard errors for bagged and random forest estimators, 2009.}}, or jackknife, so we can define $EI(x)$ for the RF
  \item Dependent params in mlrMBO: Imputation
  \item Many of the current techniques to handle these problems are (from a theoretical standpoint) somewhat crude
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Hyperparameter Tuning}
 \begin{itemize}
    \item Still common practice: grid search\\
    Simple example for a neural network with one conv and two dense layers (mnist):
    \begin{itemize}
      \item $ \text{dropout(input)} \in (0.1, 0.9)$
      \item $ \text{momentum} \in (0, 0.99)$
      \item Evaluate a grid of $10^2 = 100$ combinations
    \end{itemize}
    \item Bad beacause:
    \begin{itemize}
      \item optimum might be "off the grid"
      \item lots of evaluations in bad areas
      \item lots of costy evaluations
    \end{itemize}
    \item How bad?
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{mxnet in mlr}
<<mxnet1, size = "scriptsize", cache = TRUE, eval = FALSE, echo = TRUE>>=
# libs (currently mxnet is only available in the mxnet branch)
devtools::install_github("mlr-org/mlr", force = TRUE, ref = "mxnet")
require(mlr)
require(mlrMBO)
require(gridExtra)

# learner (note that mlr generates the output layer automatically)
lrn = makeLearner("classif.mxff",
  layers = 2, num.layer1 = 30, num.layer2 = 128,
  act1 = "relu", act2 = "relu", act.out = "softmax",
  conv.layer1 = TRUE, conv.data.shape = c(28,28),
  conv.kernel1 = c(2,2), conv.stride1 = c(2,2),
  pool.kernel1 = c(2,2), pool.stride1 = c(2,2),
  begin.round = 1, num.round = 100, array.batch.size = 128)

@
\framebreak
<<mxnet2, size = "footnotesize", cache = TRUE, eval = FALSE, echo = TRUE>>=
# task
task = makeClassifTask(data = mnist, target = "label")

# param set
ps = makeParamSet(
  makeNumericParam(id = "dropout.input", lower = 0.1, upper = 0.9),
  makeNumericParam(id = "momentum", lower = 0, upper = 0.99))

# make a grid search
grid.ctrl = makeTuneControlGrid(resolution = 5)
rdesc = makeResampleDesc("CV", iters = 3)
res.grid = tuneParams(lrn, task, rdesc, par.set = ps,
  control = grid.ctrl, show.info = FALSE)
op.df.gs = as.data.frame(res.grid$opt.path)

@
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Hyperparameter Tuning}
  \begin{center}
    \includegraphics[width=0.5\textwidth]{plots/grid_search_2.png}
  \end{center}
  \vspace{-0.2cm}
  \begin{itemize}
    \small
    \item Because of budget restrictions grid might even be smaller!
    \item Unpromising area quite big!
    \item Lots of costly evaluations!
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{mxnet in mlr}
  \begin{itemize}
    \item With \textbf{mlrMBO} it is not hard to do it better!
  \end{itemize}
<<mxnet3, size = "footnotesize", cache = TRUE, eval = FALSE, echo = TRUE>>=
# make MBO
mbo.ctrl = makeMBOControl()
# iters = 10 results in 4d + 10 evaluations (here 18)
mbo.ctrl = setMBOControlTermination(mbo.ctrl, iters = 10)
surrogate.lrn = makeLearner("regr.km", predict.type = "se")
ctrl = mlr:::makeTuneControlMBO(learner = surrogate.lrn,
  mbo.control = mbo.ctrl, same.resampling.instance = FALSE)
rdesc = makeResampleDesc("CV", iters = 3)
res.mbo = tuneParams(lrn, task, rdesc, par.set = ps,
  control = ctrl, show.info = FALSE)

@
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Hyperparameter Tuning}
\vfill
  \centering
  \includegraphics[width=9cm]{plots/grid_search_vs_MBO_2.png}
\vfill
\begin{itemize}
  \item The best grid search obtains an mmce.test.mean of $0.275$.
  \item MBO manages to get an mmce.test.mean of $0.183$
\end{itemize}
\end{frame}
% \begin{frame}{Hyperparameter Tuning}
% \vfill
% \includegraphics[width=\textwidth]{plots/res2.png}
% \vfill
% \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{HPOlib}
\begin{itemize}
\item HPOlib is a set of standard benchmarks for hyperparameter optimizer
\item Allows comparison with
\begin{itemize}
\item Spearmint
\item SMAC
\item Hyperopt (TPE)
\end{itemize}
\item Benchmarks:
\begin{itemize}
\item Numeric test functions (similar to the ones we've seen bevor)
\item Numeric machine learning problems (lda, SVM, logistic regression)
\item Deep neural networks and deep belief networks with $15$ and $35$ parameters.
\end{itemize}
\item For benchmarks with discrete and dependent parameters (hpnnet, hpdbnet) a random forest with standard error estimation is used.
\end{itemize}
\end{frame}

\begin{frame}{MBO: HPOlib}
\begin{center}
  \includegraphics[width = 0.7\textwidth]{plots/hpolib-1.pdf}
\end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%% hpo %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Deep Learning Configuration Example}
  \begin{itemize}
  \item Dataset: CIFAR-10 (60000 32x32 images with 3 color channels; 10 classes)
  \item Configuration of a deep neural network (mxnet)
  \item Size of parameter set: 30, including number of hidden layers, activation functions,
    regularization, convolution layer setting, etc.
  \item Split: 2/3 training set, 1/6 test set, 1/6 validation set
  \item Time budget per tuning run: 4.5h (16200 sec)
  \item Surrogate: Random forest
  \item Acquisition: LCB with $\lambda = 2$
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Deep Learning Configuration Example}
  \begin{figure}[H]
  \centering
  \includegraphics[width = 0.65\textwidth]{plots/cifar10_mbo.png}
 \end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Hyperband}
  \begin{itemize}
    \small
    \item Hyperband tries to tackle the B to n problem (\enquote{budget and amount of configurations to compare}).
    Particularly interesting for neural networks.
    \item Suppose we compare 81 random architectures: how many epochs until we eliminate \enquote{bad} architectures?
    \item Naturally, if we eliminate after a few epochs, architectures with high learning rate will dominate.
    \item Hyperband tries to solve the issue by considering different brackets with different B and n.
  \end{itemize}
  \begin{figure}[H]
    \centering
    \includegraphics[width = 0.5\textwidth]{plots/hyperband.png}
    \caption{\small Hyperband with 5 brackets: n: number of configurations, r: budget/epochs for each config. For default values, only the best third of the configurations are kept and retrained until the next elimination phase. In the end only one config for each bracket remains (Lisha Li et al. (2016)).}
  \end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Architecture search}
\framebreak
  \begin{itemize}
    \item Despite the success of neural networks, they are still hard to design.
    \item Barret Zoph \& Quoc V. Le (2016) propose neural architecture search with reinforcement learning
  \end{itemize}
  \begin{figure}[H]
    \centering
    \includegraphics[width = 0.5\textwidth]{plots/paper1.png}
    \caption{Overview of architecture search (Barret Zoph \& Quoc V. Le (2016)).}
  \end{figure}
  \begin{itemize}
    \item Another noteworthy approach is \enquote{Designing Neural Network Architectures using Reinforcement Learning} by Bowen Baker et al. (2017).
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \begin{vbframe}
 \frametitle{References}
 \footnotesize{
 \begin{thebibliography}{99}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \bibitem[Lisha Li et al., 2016]{1} Lisha Li, Kevin G. Jamieson, Giulia DeSalvo, Afshin Rostamizadeh
 and Ameet Talwalkar (2016)
 \newblock Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization
 \newblock \emph{\url{https://arxiv.org/abs/1603.06560}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \bibitem[Barret Zoph \& Quoc V. Le 2016]{2} Barret Zoph and Quoc V. Le (2016)
 \newblock Neural Architecture Search with Reinforcement Learning
 \newblock \emph{\url{https://arxiv.org/abs/1611.01578}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \bibitem[Bowen Baker et al., 2017]{3} Bowen Baker, Otkrist Gupta Nikhil Naik and Ramesh Raskar(2017)
 \newblock Designing Neural Network Architectures using Reinforcement Learning
 \newblock \emph{\url{https://arxiv.org/abs/1611.02167}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{thebibliography}
}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\endlecture

