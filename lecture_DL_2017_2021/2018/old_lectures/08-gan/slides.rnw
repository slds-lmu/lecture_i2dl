%Blank Lecture
%This file is a child of preamble.Rnw in the style folder
%if you want to add stuff to the preamble go there to make
%your changes available to all childs

<<setup-child, include = FALSE>>=
library(knitr)
set_parent("../style/preamble.Rnw")
@


\lecturechapter{8}{Generative Adversarial Networks}
\lecture{Fortgeschrittene Computerintensive Methoden}

% \begin{frame} {Notation}
% \end{frame}

\begin{frame} {Unsupervised Learning}
  
  There are two main goals of \textbf{unsupervised learning}:
  \begin{itemize}
  \vspace{4mm}
    \item Representation Learning
      \begin{itemize}
        \item Learning the right features for the task.
        \item Applications :
          \begin{itemize}
            \item Dimensionality reduction
            \item Transfer learning / Semi-supervised learning
          \end{itemize}
      \end{itemize}
      \vspace{5mm}
    \item Generative Models / Density Estimation
      \begin{itemize}
        \item Given a training set $S = (\mathbf{x}_1, \mathbf{x}_2, \ldots \mathbf{x}_n)$ where each $\mathbf{x}_i \sim p(\mathbf{x})$, the goal is to estimate $p(\mathbf{x})$.
        \item Applications : 
          \begin{itemize}
            \item Generating music, videos, volumetric models for 3D printing, synthetic data for learning algorithms, etc.
          \end{itemize}
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame} {Generative Models : A 1D Example}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/gen_oned.png}}
      \tiny{\\Source : scikit-learn}
  \end{figure}
\end{frame}

\begin{frame} {Deep Generative Models}
  \begin{itemize}
   \vspace{8mm}
   \item Today, the three most actively researched approaches to deep generative models are : 
      \begin{itemize}
        \item Generative Adversarial Networks
        \item Autoregressive models
        \item Variational Autoencoders
      \end{itemize}
    \vspace{6mm}
    \item Note:
      \begin{itemize}
        \item It's important to bear in mind that generative models are not a solved problem. 
        \item There are many interesting hybrid models that combine two or more of these approaches.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame} {Explosive growth in the number of (named) GAN papers}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/named_gans.png}}
      \tiny{\\Credit : hindupuravinash}
  \end{figure}
\end{frame} 

\begin{frame} {Generating Images from Text}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/text_to_img.png}}
      \tiny{\\Source : Zhang, Xu et al (2017)}
  \end{figure}
\end{frame}

\begin{frame} {Semantic Labels --> Images}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/sem_to_img.png}}
      \tiny{\\Source : Wang, Liu et al (2017)}
  \end{figure}
\end{frame}

\begin{frame} {Image Inpainting}
  \vspace{8mm}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/inpainting.png}}
      \tiny{\\Source : Demir et al (2018)}
  \end{figure}
\end{frame}

\begin{frame} {None of these are real!}
  \vspace{8mm}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/fake_celeb.png}}
      \tiny{\\Source : Karras et al (2018)}
  \end{figure}
\end{frame}

\begin{frame} {What's a GAN?}
  \begin{figure}
    \centering
      \scalebox{0.75}{\includegraphics{plots/gan.png}}
      \tiny{\\Source : dl4j}
  \end{figure}
  \begin{itemize}
    \item In its simplest form, a GAN consists of two deep neural networks:
      \begin{itemize}
        \item The Generator 
        \item The Discriminator
      \end{itemize}
    \item The Generator is fed a random noise vector which it transforms into a fake sample in a given domain.
    \item The Discriminator is fed both real and fake samples and outputs a number between 0 and 1 indicating the probability of the input being real.
  \end{itemize}
\end{frame}

\begin{frame} {What's a GAN?}
  \begin{figure}
    \centering
      \scalebox{0.5}{\includegraphics{plots/gan.png}}
      \tiny{\\Source : dl4j}
  \end{figure}
  \begin{itemize}
    \item The goal of the Generator is to fool the discriminator into thinking that the synthesized samples are real
    \vspace{4mm}
    \item The goal of the Discriminator is to accurately recognize real samples and not be fooled by the Generator.
    \vspace{4mm}
    \item This sets off an arms race. As the Generator gets better at producing realistic samples, the Discriminator is forced to get better at detecting the fake samples which in turn forces the Generator to get even better at producing realistic samples and so on.
  \end{itemize}
\end{frame}

\begin{frame} {GANs : Intuition}
\vspace{15mm}
  ""The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistinguishable from the genuine articles."\\
\hspace{45mm} -Ian Goodfellow
\end{frame}

\begin{frame} {Adversarial Training}
  \begin{itemize}
  \vspace{12mm}
    \item So, GANs have intuitive appeal and produce state-of-the-art results but the reason they're such an important breakthrough is that they introduce a whole new paradigm to training deep neural networks : Adversarial Training.
    \vspace{4mm}
    \item Compared to plain-vanilla gradient descent, we'll see throughout the lecture that adversarial training is a whole different beast; one that we haven't figured out how to tame (yet).
    \vspace{4mm}
    \item First, let's look at a simple example.
  \end{itemize}
\end{frame}

\begin{frame} {Adversarial Training : A Toy Example}
  \begin{figure}
    \centering
      \scalebox{0.3}{\includegraphics{plots/adv_xy.png}}
  \end{figure}
  \begin{itemize}
    \item Consider the function $f(x,y) = xy$, where $x$ and $y$ are both scalars.
    \item Player A can control $x$ and Player B can control $y$
    \item The loss:
      \begin{itemize}
        \item Player A : $L_{A}(x,y) = xy$
        \item Player B : $L_{B}(x,y) = -xy$
      \end{itemize}
    \item This can be rewritten as $L(x,y) = \min \limits_x \max \limits_y xy$
    \item What we have here is a simple zero-sum game with its characteristic minimax loss
  \end{itemize}
\end{frame}

\begin{frame} {Possible behaviour \#1 : Convergence}
  \begin{figure}
    \centering
      \scalebox{0.3}{\includegraphics{plots/adv_xy.png}}
  \end{figure}
  \begin{itemize}
    \item The partial derivatives of the losses are:
     \begin{equation*}
       \frac {\partial{L_{A}}}{\partial x} = y \text{ , }
       \frac {\partial{L_{B}}}{\partial y} = -x
     \end{equation*}
    \item In adversarial training, both players perform gradient descent on their respective losses.
    \item To perform simultaneous gradient descent, we update $x$ with $x - \alpha \cdot y$ and $y$ with $y + \alpha \cdot x$ simultaneously in one iteration, where $\alpha$ is the learning rate.
  \end{itemize}
\end{frame}

\begin{frame} {Possible behaviour \#1 : Convergence}
  \begin{figure}
    \centering
      \scalebox{0.3}{\includegraphics{plots/adv_xy.png}}
  \end{figure}
  \begin{itemize}
    \item In order for simulteanous gradient descent to converge to a fixed point, both gradients have to be simultaneously 0.
    \item They are both (simultaneously) zero only for the point (0,0).
    \item This is a saddle point of the function $f(x,y) = xy$.
    \begin{itemize}
       \item The fixed point for a minimax game is typically a saddle point.
      \item Such a fixed point is an example of a Nash Equilibrium.
    \end{itemize}
    \item In adversarial training, convergence to a fixed point is \textbf{NOT} guaranteed.
  \end{itemize}
\end{frame}

\begin{frame} {Possible behaviour \#2 : Cycles}
  \begin{figure}
    \centering
      \scalebox{0.3}{\includegraphics{plots/adv_cycle.png}}
      \tiny{\\Credit : Goodfellow}
      \caption{\footnotesize Simultaneous gradient descent with an infinitesimal step size can result in a circular orbit in the parameter space}
  \end{figure}
  \begin{itemize}
    \item A Discrete Example : A never-ending game of Rock-Paper-Scissors where player A chooses 'Rock' $\rightarrow$ player B chooses 'Paper' $\rightarrow$ A chooces 'Scissors' $\rightarrow$ B chooses 'Rock' $\rightarrow$ ...
    \item In GAN training, this happens when the generator keeps switching the category of the samples generated without a noticeable improvement in image quality for any category.
  \end{itemize}
\end{frame}

\begin{frame} {Possible behaviour \#3 : Chaotic behaviour}
  \begin{figure}
    \centering
      \scalebox{0.6}{\includegraphics{plots/sim_grad.png}}
      \tiny{\\Credit : Lilian Weng}
      \caption{\footnotesize A simulation of our example for updating x to minimize xy and updating y to minimize -xy. The learning rate eta = 0.1. With more iterations, the oscillation grows more and more unstable.}
  \end{figure}
  \begin{itemize}
    \item Once $x$ and $y$ have different signs, every following gradient update causes huge oscillation and the instability gets worse in time, as shown in the figure.
    \item \textbf{Moral :} Adversarial training is highly unpredictable. It can go in circles(as in the previous slide) or become chaotic.
  \end{itemize}
\end{frame}


\begin{frame} {Minimax Loss for GANs}
  \begin{tcolorbox}
  $\min \limits_G \max \limits_D V(D,G) = \E_{\mathbf{x}}[\log D(\mathbf{x})] - \E_{\mathbf{z}}[\log (1 - D(G(\mathbf{z})))]$
  \end{tcolorbox}
   \begin{itemize}
  \vspace{6mm}
      \item $p_{data(x)}$ is our target, the data distribution
  \vspace{6mm}
      \item Because a neural network(the generator) is a deterministic function, we feed a random vector $\mathbf{z}$ to the generator to induce variability in its outputs
  \vspace{6mm}
      \item $p_{z}(\mathbf{z})$ is usually a uniform distribution or an isotropic Gaussian. This is typically fixed and cannot change during training.
    
   \end{itemize}
\end{frame}

\begin{frame} {Minimax Loss for GANs}
  \begin{tcolorbox}
  $\min \limits_G \max \limits_D V(D,G) = \E_{\mathbf{x}}[\log D(\mathbf{x})] - \E_{\mathbf{z}}[\log (1 - D(G(\mathbf{z})))]$
  \end{tcolorbox}
  \begin{itemize}
  \vspace{6mm}
    \item $G(\mathbf{z})$ is the output of the Generator for a given noise vector $\mathbf{z}$
  \vspace{6mm}
      \item $D(\mathbf{x})$ is the output of the Discriminator for a real sample $\mathbf{x}$
  \vspace{6mm}
      \item $D(G(\mathbf{z}))$ is the output of the Discriminator for a fake sample $G(\mathbf{z})$ synthesized by the Generator.
  \end{itemize}
\end{frame}


\begin{frame} {Minimax Loss for GANs}
  \begin{tcolorbox}
  $\min \limits_G \max \limits_D V(D,G) = \E_{\mathbf{x}}[\log D(\mathbf{x})] - \E_{\mathbf{z}}[\log (1 - D(G(\mathbf{z})))]$
  \end{tcolorbox}
  \begin{itemize}
    \item Rougly speaking, $\E_{\mathbf{x}}[\log D(\mathbf{x})]$ is the "log-probability" of correctly classifying real data points as real
  \vspace{2mm}
    \item $\E_{\mathbf{z}}[\log (1 - D(G(\mathbf{z})))]$ is the "log-probability" of correctly classifying fake samples as fake
  \vspace{2mm}
    \item Therefore, with each gradient update, the discriminator tries to push $D(\mathbf{x})$ toward 1 and $D(G(\mathbf{z})))$ toward 0. This is the same as maxinizing V(D,G).
  \vspace{2mm}
    \item The generator, on the other hand, only has control over $D(G(\mathbf{z}))$ and tries to push that toward 1 with each gradient update. This is the same as minimizing V(D,G).
  \end{itemize}
\end{frame}

\begin{frame} {GAN training : Pseudocode}
  \begin{algorithm}[H]
  \footnotesize
    \caption{Minibatch stochastic gradient descent training of GANs. The number of steps to apply to the discriminator,$k$,is a hyperparameter}
    \begin{algorithmic}[1]
      \For{number of training iterations}
        \For{k steps}
          \State Sample minibatch of $m$ noise samples $\{\mathbf{z}^{(1)} \ldots \mathbf{z}^{(m)}$\} from the noise prior $p_g(\mathbf{z})$
          \State Sample minibatch of $m$ examples $\{\mathbf{x}^{(1)} \ldots \mathbf{x}^{(m)}$\} from the data \item[]
 generating distribution $p_{data}(\mathbf{x})$.
          \State Update the discriminator by ascending its stochastic gradient: \item[]
  \hspace{2.5 cm}          $\nabla_{{\theta}_d} \frac {1}{m} \sum \limits_{i=1} \limits^{m} \left [ \log D(\mathbf{x}^{(i)}) + \log (1 - D(G(\mathbf{z}^{(i)}))) \right]$
            % + \log (1 - D(G(\mathbf{z}^{(i)})))}\right]$
        \EndFor
        \State Sample minibatch of $m$ noise samples $\{\mathbf{z}^{(1)} \ldots \mathbf{z}^{(m)}$\} from the noise prior $p_g(\mathbf{z})$
        \State Update the generator by descending its stochastic gradient: \item[]
   \hspace{2.5 cm}       $\nabla_{{\theta}_d} \frac {1}{m} \sum \limits_{i=1} \limits^{m} \log (1 - D(G(\mathbf{z}^{(i)})))$
      \EndFor
    \end{algorithmic}
  \end{algorithm}
\end{frame}

\begin{frame} {GAN training : Illustration}
  \begin{figure}
    \centering
      \scalebox{0.75}{\includegraphics{plots/gan_training.png}}
      \tiny{\\Source : Pascual et al (2017)}
  \end{figure}
  \begin{itemize}
    \item For $k$ steps, G's parameters are frozen and D back-props to increase accuracy.
    \item Finally, D's parameters are frozena nd G back-props to make D misclassify.
    \item Note that G gets to peek at D's internals(from the back-propogated errors) but D doesn't get to peek at G.
  \end{itemize}
\end{frame}

\begin{frame} {Non-stationary loss surface}
   \begin{itemize}
     \item Once again, it is extremely important to note that from the perspective of one of the players, the loss surface changes every time the other person makes a move. 
  \vspace{2mm}
     \item This is in stark contrast to the (full batch) gradient descent case where the loss surface is stationary no matter how many iterations of gradient descent are performed.
  \vspace{2mm}
     \item It's \textit{somewhat} analogous to a game of football played by two players where the goalposts for a given player changes position every time the opponent kicks the ball and vice versa! 
   \vspace{2mm}
    \item The only way this game ends(\textit{if} it ends, that is) is if the ball goes through both players' goalposts simultaneously.
   \vspace{2mm}
    \item This is what a saddle point represents. It is simultaneously a minimum for the first player and a maximum for the second player.

   \end{itemize}
 \end{frame}

\begin{frame} {Illustration of Convergence}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/illus_conv_one.png}}
      \tiny{\\Credit : Mark Chang}
  \end{figure}
\end{frame}

\begin{frame} {Illustration of Convergence : Final Step}
  \begin{figure}
    \centering
      \scalebox{0.9}{\includegraphics{plots/illus_conv_two.png}}
      \tiny{\\Credit : Mark Chang}
  \end{figure}
  Such convergence is not guaranteed, however.
\end{frame}

\begin{frame} {Optimal Discriminator}

  For G fixed, the optimal discriminator D is:
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/opt_discriminator.png}}
      \tiny{\\Credit : Mark Chang}
  \end{figure}
  Note : This is never achieved in practice because the discriminator has a finite capacity and is trained on a finite amount of data.
\end{frame}

\begin{frame} {Toy Example : Estimating a 1D Gaussian}
  \vspace{10mm}
  \begin{itemize}
    \item Our target distribution is a simple Gaussian with mean 4 and standard deviation of 0.5.
    \begin{figure}
    \centering
      \scalebox{0.66}{\includegraphics{plots/toy_gaussian.png}}
      \tiny{\\Source : Aylien}
  \end{figure}
  \end{itemize}
\end{frame}

\begin{frame} {Toy Example : Estimating a 1D Gaussian}
  \begin{itemize}
    \item Our generator
      \begin{itemize}
        \item Takes a scalar noise input
        \item Has only a single hidden layer with a softplus activation
        \item The output layer has only a single neuron with a linear activation
      \end{itemize}
  \vspace{4mm}
    \item Our discriminator
      \begin{itemize}
        \item Has 3 hidden layers with 'tanh' activations
        \item The output layer once again has a single neuron with a sigmoid activation
      \end{itemize}
  \vspace{4mm}
    \item Loss : Non-saturating Loss
  \vspace{4mm}
    \item Optimizer : Gradient Descent with exponential learning rate decay
  \end{itemize}
\end{frame}

\begin{frame} {Toy Example : Estimating a 1D Gaussian}
  
  \begin{itemize}
  \item After training, the the two distributions look like this:
  \begin{figure}
    \centering
      \scalebox{0.75}{\includegraphics{plots/gan_gaussian_post.png}}
      \tiny{\\Source : Aylien}
  \end{figure}
  \item This makes sense intuitively.If the generator just produces the mean value of the real data in this simple example, then it is going to be quite likely to fool the discriminator.
  \end{itemize}
\end{frame}



\begin{frame} {Implicit divergence measures}
  \begin{itemize}
    \item Recall that the goal of generative modelling is to learn $p_{\text{data}}(x)$.
    \vspace{2mm}
    \item In order to understand the differences between different generative models, it is sometimes helpful to consider them from the angle of \textbf{divergence measures}.
    \vspace{2mm}
    \item A divergence measure quantifies the "distance" between distribution. In other words, it is a measure of how different one distribution is from another.
    \vspace{2mm}
    \item There are many different divergence measures that one can use here(such as Kullback Liebler divergence)
    \vspace{2mm}
    \item One thing that all such measures have in common is that they are 0 if and only if the two distributions are equal to each other(otherwise, they are all positive).
  \end{itemize}
\end{frame}
    
\begin{frame} {Implicit divergence measures}
  \begin{itemize}
    \small{\item One approach to training generative models, then, is to explicitly minimize the distance between $p_{\text{data}}(x)$ and the model distribution $p_g(x)$ according to some divergence measure.
    \vspace{2mm}
    \item In the GAN approach however, there is no such explicit divergence measure. Nevertheless, it turns out that the minimax GAN loss does indeed implicitly minimize a divergence measure(more on this later)
    \vspace{2mm}
    \item If our generator has the capacity to model $p_{\text{data}}(x)$ perfectly, the choice of divergence doesn't matter much because they all acheive their minimum(that is,0) when $p_g(x) = p_{\text{data}}(x)$
    \vspace{2mm}
    \item However, it is not likely that that the generator, which is parametrized by weights, is capable of perfectly modelling an arbitrary  $p(x)$
    \vspace{2mm}
    \item In such a scenario, the choice of divergence matters because the parameter settings that miniminize of the various divergence measures differ.}
  \end{itemize}
\end{frame}

\begin{frame} {Implicit divergence measures}
  \begin{figure}
    \centering
      \scalebox{0.9}{\includegraphics{plots/implicit_div.png}}
      \tiny{\\Credit : Aiden NIbali}
  \end{figure}
  \begin{itemize}
    \item In this simplified 1-dimensional case, $p_{\text{data}}(x)$ is a bimodal distribution, but $p_g(x)$ only has the modelling capacity of a single Gaussian.
    \vspace{2mm}
    \item Therefore, based on the divergence measure, $p(x)$ can either fit a single mode really well('mode-seeking') or attempt to cover both modes('mode-covering'.
  \end{itemize}
\end{frame}

\begin{frame} {Optimal Generator (Advanced.Optional.)}
  \begin{itemize}
    \item Under some assumptions, it can be shown that the generator minimizes a divergence measure between distributions known as the Jensen-Shannon divergence.
    \begin{align*}
      JS(p_X|p_G(Z)) & = KL(p_X||\frac{p_x+p_{G(Z)}}{2}) + KL(p_G(Z) || \frac {p_x + p_{G(Z)}}{2}) \\
        KL(p_X|P_{G(Z)}) & = E_{p_X}[log \frac {p_X(x)}{p_{G(Z)}(x)}]
    \end{align*}
  \vspace{2mm}
    \item A generator with sufficient capacity successfully learns the target distribution
  \vspace{2mm}
    \item Otherwise, it exhibits "mode-seeking" behaviour.
  \vspace{2mm}
    \item Researchers originally speculated that this behaviour is the reason GANs produce stunningly realistic samples.
  \end{itemize}
\end{frame}

\begin{frame} {Trade-offs made by some common divergences}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/div_tradeoffs.png}}
      \tiny{\\Source : Theis et al 2016}
      \caption{\small : An isotropic Gaussian distribution was fit to data drawn from a mixture of Gaussians by either minimizing Kullback-Leibler divergence (KLD), maximum mean discrepancy (MMD), or
Jensen-Shannon divergence (JSD). The different fits demonstrate different tradeoffs made by the
three measures of distance between distributions.}
  \end{figure}
\end{frame}

\begin{frame} {Non-Saturating Loss}
  \begin{figure}
    \centering
      \scalebox{0.85}{\includegraphics{plots/ns_loss.png}}
      \tiny{\\Credit : Daniel Seita}
  \end{figure}
  \begin{itemize}
    \item It was discovered that a relatively strong discriminator could completely dominate the generator.
    \item The reason for this is that when trained witht he Minimax loss, as the Discriminator gets good at identifying fake images, that is, as $D(G(\mathbf{z}))$ approaches 0, the gradient of $J^(G)$ vanishes
    
  \end{itemize}
\end{frame}

\begin{frame} {Non-Saturating Loss}
  \begin{figure}
    \centering
      \scalebox{0.85}{\includegraphics{plots/ns_loss.png}}
      \tiny{\\Credit : Daniel Seita}
  \end{figure}
  \begin{itemize}
    \item Non-Saturating(NS) Loss = $J^{(G)} = - \frac{1}{2} \E [\log D(G(\mathbf{x}))]$
    \item In contrast to the Minimax Loss(MM), when the discriminator gets good at identifying fake images, the magnitude of the gradient of $J^{(G)}$ increases and the Generator is able to learn to produce better images in successive iterations.
  \end{itemize}
\end{frame}

\begin{frame} {Other losses}
  \vspace{10mm}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/other_losses.png}}
      \tiny{\\Source : Lucic et al 2016}
  \end{figure}
\end{frame}

\begin{frame} {GANs as black box models}
  \begin{itemize}
    \item GANs are black-box models because it is not possible to explicitly compute the probability $p_g(\mathbf{x})$
    \item Therefore, classic measures, such as log-likelihood on the test set, cannot be evaluated.
    \item As a consequence, many researchers focused on qualitative comparisons, such as comparing the visual quality of samples. However, such approaches are subjective and potentially misleading.
    \item To remedy this, new evaluation metrics such as Inception Score(IS) and Frechet Inception Distance(FID) were proposed. However, these are not without their shortcomings.
  \end{itemize}
\end{frame}

\begin{frame} {Are all GANs created equal? }
  \begin{itemize}
    \item Both IS and FID are incapable of detection overfitting: a \textit{memory} GAN which simply stores all training samples would score perfectly under both measures.
    \item The authors of the paper propose an \textit{approximation} to precision and recall for GANs
    \item Precision measures the fraction of relevant retrieved instances among the retrieved instances.If the samples from the model $p_g$ are (on average) close to the manifold, its precision is high.
    \item Recall measures the fraction of retrieved instances among relevant instances. High recall implies that the generator can recover (i.e. generate something close to) \textit{any} sample from the manifold.
    \item $F_1$ score is the harmonic average of precision and recall.
  \end{itemize}
\end{frame}

\begin{frame} {Are all GANs created equal? }
  \begin{figure}
    \centering
      \scalebox{0.6}{\includegraphics{plots/areallgans_pr.png}}
      \tiny{\\Source : Lucic et al 2016}
      \caption{Low recall = Lacking in diversity. Low precision = Fails to capture convexity}
  \end{figure}
\end{frame}

\begin{frame} {Are all GANs created equal?}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/areallgans_ch.png}}
      \tiny{\\Source : Lucic et al 2016}
      \caption{The plot shows the distribution of the maximum $F_1$ score achievable for a fixed budget with a 95\% confidence interval. For each budget, we estimate the mean and confidence interval(of the mean) using 5000 bootstrap resamples out of 100 runs.}
  \end{figure}
  \begin{itemize}
    \item When optimizing for $F_1$ score, both $NSGAN$ and $WGAN$ enjoy high precision and recall.
  \end{itemize}
\end{frame}



\begin{frame} {Mode Collapse}
  \begin{itemize}
  \vspace{4mm}
    \item One of the most common problems encountered while training GANs is the phenomenon called 'mode collapse'.
  \vspace{4mm}
    \item This is when the generator either produces only a single sample or a small variety of samples for any latent vector $z$.
   \vspace{4mm}
   \item It is important to note that the true distribution of images , in any domain, is almost always multi-modal. In other words, it tends to have multiple "peaks" that are the result of the natural groupings present in the domain.
    \begin{itemize}
  \vspace{2mm}
      \item For example, the distribution over hand-written digits can have 10 modes - one for each digit.
    \end{itemize}
  \end{itemize}
  \end{frame}
  
\begin{frame} {Mode Collapse}
  \begin{itemize}
    \item There is nothing explicit in the objective function of GANs that encourages the generator to produce samples from a diverse set of nodes.
    \begin{itemize}
      \item Heuristic example : If a counterfeiter is able to successfully fool the police by printing \$50 bills, he/she has no incentive to also print \$20 bills or \$100 bills and risk getting caught.
    \end{itemize}
    \item Therefore, while training, the generator often ends up producing samples that are close to a single mode or a small handful of modes and fails to learn the true distribution of the data.
  \end{itemize}
\end{frame}

\begin{frame} {Mode Collapse}
  \begin{figure}
    \centering
      \scalebox{0.9}{\includegraphics{plots/modecol_toy.png}}
      \tiny{\\Source : Metz et al 2016}
      \caption{An illustration of the mode collapse problem on a two-dimensional toy
dataset.}
  \end{figure}
  \begin{itemize}
    \item In the top row, we see the target distribution $p_{\text{data}}$ that the model should learn. It is a mixture of Gaussians in a two-dimensional space.
    \item In the lower row, we see a series of different distributions learned over time as the GAN is trained.
    \item The generator only ever produces a single mode at a time, cycling between different modes as the discriminator learns to reject each one.
  \end{itemize}
\end{frame}

\begin{frame} {Mode Collapse : Visual Example}
  \begin{figure}
    \centering
      \scalebox{0.75}{\includegraphics{plots/modecol_vis.png}}
      \tiny{\\Source : Reed et al 2016}
  \end{figure}
\end{frame}

\begin{frame} {Mode Collapse : Solutions}
This is a very active area of research and there are \textit{many} different approaches to tackling this problem. Some examples are:
  \begin{itemize}
    \vspace{7mm}
    \item 1) Minibatch discrimination : This method computes the "closeness" between samples in a given minibatch and the discriminator then relies on this additional information to decide whether the samples are real of fake.
    \vspace{10mm}

    \item 2) Ensemble apprach : Instead of trying to figure out ways to get a single GAN to generate sampeles from all possible nodes, one approach is to train an ensemble of GANs where:
  \begin{itemize}
    \item Individual GANs produce samples only from a subset of the modes, and,
    \item The GANs in the ensemble \textit{collectively} cover all the modes in the data distribution.
  \end{itemize}
  \end{itemize}
  \end{frame}

\begin{frame} {Mode Collapse : Solutions}
This is a very active area of research and there are many different approaches to tackling this problem. Some examples are:
  \begin{itemize}
  \vspace{10mm}
    \item 3) Unrolled GANs : "Instead of the generator learning to fool the current discriminator, it learns to maximally fool the discriminator after it has a chance to respond, thus taking counterplay into account. Intuitively, this prevents players of the GAN game from making moves which are easily countered."
  \end{itemize}
\end{frame}

\begin{frame} {Unrolled GANs}
  \begin{itemize}
    \item The GAN learning problem is to find the optimal parameters $\theta^*_G$ for a generator function $G(z;\theta_G)$ in a minimax objective,
      \begin{align*}
      \theta^*_G & = \argmin_{\theta_G} \max_{\theta_D} f(\theta_g,\theta_D) \\
                & =  \argmin_{\theta_G} f(\theta_G,\theta^*_D(\theta_G)) \\
      \theta^*_D(\theta_G) & = \argmax_{\theta_D}f(\theta_G,\theta_D)
      \end{align*}
    where f is the minimax loss, for example.
    \vspace{4mm}
    \item Explicitly solving for the optimal discriminator parameters $\theta^*_D(\theta_G)$ for every update step of generator G is computationally infeasible. Therefore, we alternately update $\theta_G$ and $\theta_D$ every finite number of steps $k$.
  \end{itemize}
       \tiny{\hspace{8cm}Source : Metz et al 2016}
\end{frame}

 \begin{frame} {Unrolled GANs}
   \begin{itemize}
     \item A local optimum of the discriminator parameters $\theta^*_D$ can be expressed as the fixed point of an iterative optimization procedure,
     \begin{align*}
       \theta^0_D &= \theta_D \\
       \theta^{(k+1)}_D &= \theta^k_D + \eta^k \frac {df(\theta_G,\theta^k_D)} {d\theta^k_D} \\
       \theta^*_D(\theta_G) &= \lim_{k\to\infty}\theta^k_D
     \end{align*}
     where $\eta^k$ is the learning rate schedule.
     \vspace{4mm}
     \item By unrolling for $K$ steps, we create a surrogate objective for the update of the generator,
       \begin{tcolorbox}
         \begin{equation*}
           f_k(\theta_G,\theta_D) = f(\theta_G, \theta^K_D(\theta_G,\theta_D))
         \end{equation*}
       \end{tcolorbox}
   \end{itemize}
       \tiny{\hspace{8cm}Source : Metz et al 2016}
 \end{frame}

 \begin{frame} {Unrolled GANs}
   \begin{itemize}
      \item When K = 0, the surrogate objective corresponds exactly to the standard GAN objective, while as $K \to \infty$ it corresponds to the true generator objective function $f(\theta_G,\theta^*_D(\theta_G,\theta_D(G)))$
    \vspace{6mm}
     \item The generator and discriminator parameter updates using this surrogate loss are
      \begin{equation*}
        \theta_G \leftarrow \theta_G - \eta \frac {df_K(\theta_G,\theta_D)} {d\theta_G}
      \end{equation*}
      \begin{equation*}
        \theta_D \leftarrow \theta_D + \eta \frac {d f(\theta_G,\Theta_D)}{d\theta_D}
      \end{equation*}
   \end{itemize}
       \tiny{\hspace{8cm}Source : Metz et al 2016}
 \end{frame}

\begin{frame} {Unrolled GANs}
  \begin{figure}
    \centering
      \scalebox{0.9}{\includegraphics{plots/unrolled_gan.png}}
     \caption{An illustration of the computation graph for an unrolled GAN with 3 unrolling steps.}
  \end{figure}
  \begin{itemize}
    \item The generator update in Equation <10> involves backpropagating the generator gradient (blue arrows) through the unrolled optimization.
    \item Each step $k$ in the unrolled optimization uses the gradients of $f_k$ w.r.t $\theta^k_D$, as described in Eqn.<7> and indicated by green arrows.
    \item The discriminator update in Eqn.<11> does not depend on the unrolled optimization (red arrow)
  \end{itemize}
       \tiny{\hspace{8cm}Source : Metz et al 2016}
\end{frame}

\begin{frame} {Unrolled GANs}
 To better understand the behaviour of the surrogate loss $f_k(\theta_G,\theta_D)$, we examine its gradient with respect to the generator parameters $\theta_G$,
  \begin{equation*}
    \frac {df_K(\theta_G,\theta_D)}{d\theta_G} = \frac {\partial f(\theta_G,\theta^K_D(\theta_G,\theta_D))}{\partial \theta_G} + \frac {\partial f(\theta_G,\theta^K_D(\theta_G,\theta_D))}{\partial \theta^K_D(\theta_G,\theta_D)} \frac{d \theta^K_D(\theta_G,\theta_D)}{d\theta_G}
  \end{equation*}
  \begin{itemize}
    \item Standard GAN training corresponds exactly to updating the generator parameters using only the first term in this gradient, with $\theta^k_D(\theta_G,\theta_D)$ being the parameters resulting from the discriminator update step.
    \vspace{4mm}
    \item An optimal generator for any fixed discriminator is a delta function at the $x$ to which the discriminator assigns the highest probability.
    \begin{itemize}
      \item Therefore, in standard GAN training, each generator update is a partial collapse towards a delta function.
    \end{itemize}
  \end{itemize}
       \tiny{\hspace{8cm}Source : Metz et al 2016}
\end{frame}

\begin{frame} {Unrolled GANs}
 To better understand the behaviour of the surrogate loss $f_k(\theta_G,\theta_D)$, we examine its gradient with respect to the generator parameters $\theta_G$,
  \begin{equation*}
    \frac {df_K(\theta_G,\theta_D)}{d\theta_G} = \frac {\partial f(\theta_G,\theta^K_D(\theta_G,\theta_D))}{\partial \theta_G} + \frac {\partial f(\theta_G,\theta^K_D(\theta_G,\theta_D))}{\partial \theta^K_D(\theta_G,\theta_D)} \frac{d \theta^K_D(\theta_G,\theta_D)}{d\theta_G}
  \end{equation*}
  \begin{itemize}
    \item The second term captures how the discriminator would react to a change in the generator. It reduces the tendency of the generator to engage in mode collapse.
    \vspace{4mm}
    \item For instance, the second term reflects that as the generator collapses towards a delta function, the discriminator reacts and assigns lower probability to that state, increasing the generator loss.
    \begin{itemize}
      \item This extra information helps the generator spread its mass to make the next D step
less effective instead of collapsing to a point.
    \end{itemize}
  \end{itemize}
       \tiny{\hspace{8cm}Source : Metz et al 2016}
\end{frame}

\begin{frame} {Unrolled GANs}
  \begin{figure}
    \centering
      \scalebox{1.1}{\includegraphics{plots/unrolled_gauss.png}}
  \caption{\footnotesize Unrolling the discriminator stabilizes GAN training on a toy 2D mixture of Gaussians dataset. Columns show a heatmap of the generator distribution after increasing numbers of training steps. The final column shows the data distribution. The top row shows training for a GAN with
10 unrolling steps. Its generator quickly spreads out and converges to the target distribution. The
bottom row shows standard GAN training. The generator rotates through the modes of the data
distribution. It never converges to a fixed distribution, and only ever assigns significant probability
mass to a single data mode at once.}
\end{figure}
       \tiny{\hspace{8cm}Source : Metz et al 2016}
\end{frame}

\begin{frame} {A Cambrian Explosion of GANs}
  \begin{figure}
    \centering
      \scalebox{0.9}{\includegraphics{plots/cambrian_exp.png}}
  \end{figure}
\end{frame}

\begin{frame} {DC-GAN : Deep Convolutional GAN}
  \begin{itemize}
    \item In the history of GANs, the DC-GAN is of special importance because:
      \begin{itemize}
        \item Compared to the original GAN, the samples produced by DC-GAN were of much higher quality.
        \item The architectural changes introduced in the paper grealty improved the stability of training and were widely adopted by GAN researchers.
      \end{itemize}
    \end{itemize}
    \begin{figure}
    \centering
      \scalebox{0.66}{\includegraphics{plots/dcgan_comp.png}}
  \end{figure}
\end{frame}

\begin{frame} {DCGAN Generator : A Closer Look}
  \begin{figure}
    \centering
      \scalebox{0.9}{\includegraphics{plots/dcgan_gen.png}}
      \tiny{\\Source : Radford et al 2015}
  \end{figure}
  \begin{itemize}
    \small{\item The first layer("Project and reshape") takes a 100-dimensional latent vector $z$( sampled from a uniform distribution), and converts it to a 4 $\times$ 4 $\times$ 1024 tensor. 
    \item A series of four transpose convolutions then convert this high level representation into a 64 $\times$ 64 pixel image.
    \item Note : Instead of pooling layers, a stride of 2 is used.}
  \end{itemize}
\end{frame}

\begin{frame} {DCGAN : Architectural Guidelines}
  \begin{itemize}
    \item Replace any pooling layers with strided convolutions (discriminator) and transpose convolutions (generator).
      \begin{itemize}
        \item This enables the network to learn its own spatial downsampling.
      \end{itemize}
    \vspace{2mm}
    \item Use batchnorm in both the generator and the discriminator.
    \vspace{2mm}
    \item Remove fully connected hidden layers for deeper architectures.
    \vspace{2mm}
    \item Use ReLU activation in generator for all layers except for the output, which uses Tanh.
    \vspace{2mm}
    \begin{figure}
    \centering
      \scalebox{0.3}{\includegraphics{plots/leaky_relu.png}}
    \end{figure}
    \item Use LeakyReLU(see figure) activation in the discriminator for all layers(in order to avoid sparse gradients).
  \end{itemize}
\end{frame}

\begin{frame} {Batch-norm in CNNs : A note of caution}
  \begin{itemize}
    \small{\item Batch normalization in a fully-connected(FC) layer is different from batch-normalization in a convolutional layer.
    \vspace{2mm}
    \item To apply batchnorm to a FC layer, means and standard deviations(for a given batch) are computed separately for each neuron in the layer. Therefore, the \textbf{neurons in such a layer are modified by different amounts} by the procedure.
    \vspace{2mm}
    \item However, this can be problematic in convolutional layers because the activations for a given filter carry important spatial information. Independenlty modifying the activation for each neuron in the filter can destroy this spatial pattern. Therefore, in a convolutional layer, means and standard deviations are computed for each filter map as a whole, rather than for each neuron separately.
    \vspace{2mm}
    \item As a consequence, \textbf{each neuron in a given filter is modified by the same amount} and the overall spatial pattern for that filter is preserved.}
  \end{itemize}
\end{frame}


\begin{frame} {DCGAN : Vector Arithmetic in the Latent Space}
  \begin{figure}
    \centering
      \scalebox{0.72}{\includegraphics{plots/dcgan_vec.png}}
      \tiny{\\Source : Radford et al 2015}
  \end{figure}
  \begin{itemize}
    \item For each column, the Z vectors of samples are averaged. Arithmetic was then performed on the mean vectors creating a new vector Y.
    \item The center sample on the right hand side is produced by feeding Y as input to the generator.
    \item Uniform noise sampled with scale +/- 0.25 was added to Y to produce the 8 other samples.
  \end{itemize}
\end{frame}

\begin{frame} {Conditional GANs : Motivation}
  \begin{itemize}
    \item In an ordinary GAN, the only thing that is fed to the generator is random noise $\mathbf{z}$. A conditional GAN allows you to have more control over the samples produced by the generator.
    \item This makes it very easy to work with multiple modalities.
    \item For example, a generator conditioned on text input(in addition to noise $\mathbf{z}$) can be trained to generate the image described by the text.
  \end{itemize}
\end{frame}

\begin{frame} {Conditional GANs : Architecture}
  \begin{figure}
    \centering
      \scalebox{0.75}{\includegraphics{plots/cgan_arch.png}}
      \tiny{\\Credit : Guim Perarnau}
  \end{figure}
  \begin{itemize}
    \item In a conditional GAN, additional information in the form of vector Y is fed to both the generator and the discriminator.
    \item Z will then end up encoding all the variations that are not encoded in Y.
    \item For example, let's suppose Y encodes th digit of a hand-written number(from 0 to 9). Then, Z would encode all the other variations that are not encoded in Y. That could be, for example, the style of the number(size,weight,rotation,etc)
  \end{itemize}
\end{frame}

\begin{frame} {Conditional GANs : Loss}
\end{frame}

\begin{frame} {Conditional GANs : Example}
  \vspace{10mm}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/cgan_mnist.png}}
      \tiny{\\Source : Mirza et al 2014}
  \end{figure}
\end{frame}

\begin{frame} {Conditional GANs : More Examples}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/congan.png}}
       \tiny{\\Source : Isola et al 2016}
 \end{figure}
\end{frame}

\begin{frame} {GANs as learned loss functions}
  \begin{itemize}
    \vspace{8mm}
    \item In many machine vision applications, such as colorizing grayscale photos or converting a low-res photo to high res, researchers have to design custom loss functions for each task.
        \begin{itemize}
          \item The L2 loss only measures the elementwise distance between pixels in the photos. This makes it extremely unsuitable for machine vision tasks...
        \end{itemize}
    \vspace{5mm}
    \item In fact, it is often unclear exactly what loss to use because any such loss will have to match the perceptual/semantic distance that is apparent to the human eye.
  \end{itemize}
\end{frame}

\begin{frame} { GANs as learned loss functions}
  \begin{figure}
    \centering
      \scalebox{0.4}{\includegraphics{plots/gan_euclid_a.png}}
      \caption{\footnotesize Computing the Euclidean distances yields $d(a,b) = 54$ and $d(a,c) = 49$}
 \end{figure}

 \begin{figure}
    \centering
      \scalebox{0.8}{\includegraphics{plots/gan_euclid_b.png}}
  \end{figure}
 These examples show that distance between images in L2 terms can be very different from the "perceptual/semantic" distance.
\end{frame}

\begin{frame} {GANs as learned loss functions}
  \begin{itemize}
    \vspace{12mm}
    \item Therefore, it is extremely difficult to design exactly the right loss function for a given task and even if such a loss is painstakingly hand-crafted, it is unlikely to be useful for a different task.
    \vspace{12mm}
    \item GANs automate this whole process and produce custom losses that are prefectly tailored to the tasks.
  \end{itemize}
\end{frame}

\begin{frame} { GANs as learned loss functions}
  \begin{itemize}
  \item Our task here is to colorize gray-scale images. The discriminator then outputs a probability indicating whether it thinks the colorized image is real or fake.
    \begin{figure}
      \centering
        \scalebox{0.6}{\includegraphics{plots/learnedloss_1.png}}
   \end{figure}
   \item From the persective of the generator, the entire discriminator can be viewed as part of some complicated loss function.
     \begin{figure}
        \centering
          \scalebox{0.6}{\includegraphics{plots/learnedloss_2.png}}
     \end{figure}
  \item Therefore, GANs can be seen as an automated way to "learn" the right loss function for a given task.
  \end{itemize}
  \tiny{\hspace{8cm} Credit : Alexei Efros}
\end{frame}

\begin{frame} {Latent Space Goal 1 : Manifold Learning}
  \begin{figure}
    \centering
      \scalebox{0.75}{\includegraphics{plots/manifold.png}}
  \end{figure}
  \begin{itemize}
    \item \textbf{Data manifold }: In many high-dimensional spaces(for example, the space of all 256 X 256 grayscale photos of human faces), most of the probability "mass" lies on a much lower dimensional (non-linear) manifolg.
    \item A key goal of generative modelling is to map the vectors in the (lower-dimensional) latent space to vectors in the lower-dimensional data manifold.
  \end{itemize}
\end{frame}

\begin{frame} {Manifold Learning : VAE example}
  \begin{figure}
    \centering
      \scalebox{0.5}{\includegraphics{plots/vae_mnist.png}}
  \end{figure}
  \begin{itemize}
    \item Even though MNIST is 28 X 28 dimensional, we see above that most of the variation is captured by the 2-dimensional latent space of a Variational Autoencoder
  \end{itemize}
\end{frame}

\begin{frame} {Goal 2 : Disentangled representations}
  \begin{itemize}
    \item In addition to learning a target distribution, a desirable goal for a generative model(or any other model) is to learn "disentangled representations".
    \vspace{2mm}
    \item As a simple example, two important "factors of variation" in images of human faces are the facial expression and the identity of the person. Different joint configurations of these two high-level factors can result in a wide variety of images.
   \vspace{2mm}
   \item In deep learning, each neuron would ideally learn a specific higher-level or abstract concept on its own without depending on other neurons.
   \vspace{2mm}
   \item Such disentanglement of features allows for better generalization and easier transfer learning.
   \end{itemize}
\end{frame}
  
  \begin{frame} {Goal 2 : Disentangled representations}
    \begin{itemize}
      \item Even if a regular GAN successfully learns the data-generating distribution(such as that of human faces), the mapping from the latent space to the space of images can be highly entagled. 
      \vspace{4mm}
      \item In other words, none of the individual components of the latent vector $z$ might correspond to important factors of variation (such as age, identity, gender, etc)
      \vspace{4mm}
      \item On the other hand, if the components of the latent vector encoded such high-level features, then it would be possible to obtain any desired image from the generator by tweaking the values of the relevant components before feeding it to the generator.
     
  \end{itemize}
\end{frame}

\begin{frame} {Disentangling representations : cGAN}
  \begin{figure}
    \centering
      \scalebox{0.75}{\includegraphics{plots/disent_cgan.png}}
      \tiny{\\Credit : Guim Perarnau}
  \end{figure}
  \begin{itemize}
    \item A conditional GAN "mimics" this property because it is trained on the additional information in the vector Y which explicitly consists of higher-level(disentangled) features.
  \end{itemize}
\end{frame}

\begin{frame} {InfoGAN}
  \begin{itemize}
    \item We've seen that conditional GANs encode disentangled features in a rather trivial way. InfoGANs accomplish this in a much more general way.
    \item The key difference here is that they achieve this in a \textit{completely} unsupervised manner. In other words, there is no vector Y that explicitly contains label information.
    \item to achieve this, the researchers split the latent vector $Z$ into two parts : C and Z'.
      \begin{itemize}
        \item C will (eventually) encode the important higher-level disentangled features.
        \item The role of Z' is to encode the unstructured noise of the distribution. 
      \end{itemize}
    \item Once again, it is important to note that C doesn't originally encode any higher-level features. The training objective essentially "forces" the GAN to encode them in C.
  \end{itemize}
\end{frame}

\begin{frame} {InfoGAN : Training objective}
  \begin{itemize}
    \item In information theory, mutual information between $X$ and $Y$, $I(X;Y)$, measures the "amount of information" learned from knowledge of random variable Y about the other random variable X.
  \vspace{4mm}
    \item If X and Y are independent, then $I(X;Y) =0$, because knowing one variable reveals nothing about the other; by contrast, if $X$ and $Y$ are related by a deterministic, invertible function, then maximal mutual information is attained.
  \vspace{4mm}
    \item The InfoGAN training objective encourages high mutual information between C and the generator distribution. In other words, if C changes, the generated image needs to change, too. This ensures that the information contained in C is not lost in the generation process.
    \tiny{\\ \hspace{8cm} Credit : Guim Perarnau}
  \end{itemize}
\end{frame}

\begin{frame} {InfoGAN : Disentangling representations}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/disent_infogan.png}}
      \tiny{\\Source : Chen et al 2016}
  \end{figure}
\end{frame}

% \begin{frame} {Progressive growing of GANs}
%   \begin{figure}
%     \centering
%       \scalebox{0.8}{\includegraphics{plots/prog_gan.png}}
%   \end{figure}
%   \begin{itemize}
%     \item As the training advances, we incrementally add layers to G and D, thus increasing the spatial resolution of the generated images. 
%     \item All existing layers remain trainable throughout the process.
%     \item This technique currently produces the sharpest and highest-resolution images.
%   \end{itemize}
% \end{frame}
% 
% \begin{frame} {Training GANs is more art than science}
%   \vspace{10mm}
%   "You spend a lot of time tuning hyperparameters before you get good performance. Overall, I would say that GANs today remind me of the state of deep learning in 2010 or so. Since about 2012, we've known that if you just use ReLUs, backprop and gradient descent with something like momentum, and later Adam, you get pretty good results. Back in 2010, we spent a lot of time fiddling with hyperparameters for things like unsupervised pre-training and markov chains for boltzmann machines and things like that. I think in a few more years, we'll know recipes that just reliably get good results out of GANs but at the moment, we don't have those recipes yet. So there's a lot more manual effort and trial and error that goes into getting them to work."\\
% \hspace{45mm} -Ian Goodfellow
% \end{frame}

\begin{frame} {Diving Deeper}
  \begin{itemize}
    \item Does the generator really learn the target distribution?
      \begin{itemize}
        \item \url{https://arxiv.org/abs/1706.08224}
      \end{itemize}
    \item Under what conditions does training converge?
      \begin{itemize}
        \item \url{https://robotics.eecs.berkeley.edu/~sastry/pubs/Pdfs\%20of\%202013/RatliffCharacterization2013.pdf}
        \item \url{https://arxiv.org/pdf/1705.10461.pdf}
        \item \url{https://openreview.net/pdf?id=Hk4_qw5xe}
      \end{itemize}
    \item How are GANs and Variational Autoencoders related?
      \begin{itemize}
        \item \url{https://arxiv.org/pdf/1706.00550.pdf}
      \end{itemize}
  \end{itemize}
\end{frame}



% \begin{frame}{Citation}
% 
%     This statement requires citation \cite{tobin1964commercial}
% 
%     But the way I want is \footnote{Tobin, James. Commercial banks as creators of" money.". Cowles Foundation for Research in Economics at Yale University, 1964.} %\footnote{tobin1964commercial}
% \end{frame}


% \begin{frame}
% fgjgf
% 
% According to \cite{Karpathy2016} , good stuff
% \end{frame}

\begin{frame}[allowframebreaks]
  \frametitle<presentation>{References}    
  \begin{thebibliography}{10}    
  \setbeamertemplate{bibliography item}[book]
  \bibitem{Autor1990}
    A.~Autor.
    \newblock {\em Introduction to Giving Presentations}.
    \newblock Klein-Verlag, 1990.
  \setbeamertemplate{bibliography item}[article]
  \bibitem{Jemand2000}
    S.~Jemand.
    \newblock On this and that.
    \newblock {\em Journal of This and That}, 2(1):50--100, 2000.
  
  \setbeamertemplate{bibliography item}[online]
  \bibitem{Karpathy2016}
    A.~Karpathy.
    \newblock {\em Pong from Pixels}.
    \newblock {\url{http://karpathy.github.io/2016/05/31/rl/}}
  
  \bibitem[Yann Dauphin et al., 2014]{2} Yann Dauphin, Razvan Pascanu, {\c{C}}aglar G{\"{u}}l{\c{c}}ehre, Kyunghyun Cho, Surya Ganguli, Yoshua Bengio (2014)
\newblock Identifying and attacking the saddle point problem in high-dimensional non-convex optimization
\newblock \emph{\url{https://arxiv.org/abs/1406.2572}}

  \setbeamertemplate{bibliography item}[book]
\bibitem[Ian Goodfellow et al., 2016]{1} Ian Goodfellow, Yoshua Bengio and Aaron Courville (2016)
\newblock Deep Learning
\newblock \emph{\url{http://www.deeplearningbook.org/}}

  \end{thebibliography}
  
  
  
    
      % \bibitem{bl}Business Logic layer, \url{http://en.wikipedia.org/wiki/Business_logic_layer}, 23 12 2011.

\end{frame}


\begin{vbframe} %frame with breaks and verbatim

\end{vbframe}

\endlecture
