%Blank Lecture
%This file is a child of preamble.Rnw in the style folder
%if you want to add stuff to the preamble go there to make
%your changes available to all childs

<<setup-child, include = FALSE>>=
library(knitr)
set_parent("../style/preamble.Rnw")
@


\lecturechapter{9}{Introduction to Reinforcement Learning (RL1)}
\lecture{Fortgeschrittene Computerintensive Methoden}

\begin{frame} 
  \vspace{20mm}
  Material for the slides is taken from the book, "Reinforcement Learning : An Introduction", by Richard S. Sutton and Andrew G. Barto
  \vspace{5mm}
  \\
  The book is freely available to download here : http://incompleteideas.net/book/the-book-2nd.html
\end{frame}

\begin{frame} {Notation}
    \vspace{3mm}
    \makebox[1.5cm]{$\epsilon$}  probability of taking a random action in an $\epsilon$-greedy policy\par
    \makebox[1.5cm]{$\alpha$}  step-size parameter\par
    \makebox[1.5cm]{$\gamma$}  discount-rate parameter\par
    
    \vspace{5mm}
    
  In a multi-arm bandit problem: \par
    \makebox[1.5cm]{$k$}  number of actions (arms)\par
    \makebox[1.5cm]{$q_*(a)$}  true value (expected reward) of action $a$\par
    \makebox[1.5cm]{$Q_t(a)$}  estimate at time $t$ of $q_*(a)$\par
    
    \vspace{5mm}
    
  In a Markov Decision process: \par
    \makebox[1.5cm]{$s,s'$}  states\par
    \makebox[1.5cm]{$a$}  an action\par
    \makebox[1.5cm]{$r$}  a reward\par
    \makebox[1.5cm]{$\mathcal{S}$}  set of all nonterminal states\par
    \makebox[1.5cm]{$\mathcal{A}$}  set of all actions\par
    \makebox[1.5cm]{$\mathcal{R}$}  set of all possible rewards, a finite subset of $\mathbb{R}$\par
\end{frame}

\begin{frame} {Notation}
    \vspace{3mm}
    \makebox[2cm]{$\pi(a|s)$}  probability of taking action $a$ in state $s$ under policy $\pi$\par
    \makebox[2cm]{$\pi(a|s,\boldsymbol\theta)$}  probability of taking action $a$ in state $s$ given parameter $\theta$\par
    
    \vspace{4mm}
    
    \makebox[2cm]{$p(s',r|s,a)$}  probability of transition to state $s'$ with reward $r$, from state $s$ and action $a$\par
    \makebox[2cm]{$p(s'|s,a)$}  probability of transition to state $s'$, from state $s$ taking action $a$\par
    \makebox[2cm]{$r(s,a,s')$}  expected immediate reward on transition from $s$ to $s'$ under action $a$\par
        
    \vspace{4mm}

    \makebox[2cm]{$v_{\pi}(s)$}  value of state $s$ under policy $\pi$(expected return)\par
    \makebox[2cm]{$v_*(s)$}  value of state s under the optimal policy\par
    \makebox[2cm]{$q_{\pi}(s,a)$}  value of taking action $a$ in state $s$ under policy $\pi$\par
    
    \makebox[2cm]{$V, V_t$}  array estimates of state-value function $v_{\pi}$ or $v_*$\par
    \makebox[2cm]{$Q,Q_t$}  array estimates of actin-value function $q_{\pi}$ or $q_{*}$\par
\end{frame}

\begin{frame} {Notation}
    \vspace{3mm}
    \makebox[2cm]{$\mathbf{w}, \mathbf{w_t}$}  $d$-dimensional vector of weights\par
    \makebox[2cm]{$\hat{v}(s,a,\mathbf{w})$}  approximate value of state $s$ given weight vector $\mathbf{w}$\par
    \makebox[2cm]{$\hat{q}(s,a,\mathbf{w})$}  discount-rate parameter\par
    \makebox[2cm]{$\mathbf{x}(s)$}  vector of features visible when in state $s$\par
    
    \vspace{6mm}
    \makebox[2cm]{$\boldsymbol\theta , \boldsymbol\theta_t$}  parameter vector of target policy\par
    \makebox[2cm]{$\pi_{\theta}$}  policy corresponding to parameter $\boldsymbol{\theta}$\par
    \makebox[2cm]{$J(\pi),J(\boldsymbol\theta)$}  performance measure for policy $\pi$\par
    \makebox[2cm]{$h(s,a,\boldsymbol\theta)$}  a preference for selecting action $a$ in state $s$ based on $\boldsymbol\theta$\par
    
\end{frame}


\begin{frame} {Context for RL}
  \begin{itemize}
    \vspace{8mm}
    \item \textbf{\textit{Learning from interaction}} is a foundational idea underlying nearly all theories of learning and intelligence in human beings.
    \vspace{5mm}
    \item When an infant plays, waves its arms, or looks about, it has \textbf{no explicit teacher}, but it does have a direct sensorimotor connection to its environment.
    \vspace{5mm}
    \item Exercising this connection produces a wealth of information about cause and effect, about the \textbf{consequences of actions}, and about what to do in order to achieve goals.
  \end{itemize}
\end{frame}

\begin{frame} {Context for RL}
  \begin{itemize}
    \vspace{8mm}
    \item Throughout our lives, such interactions are undoubtedly a major source of knowledge about our environment and ourselves.
    \vspace{5mm}
    \item Whether we are learning to drive a car or to hold a conversation, we are acutely aware of how our environment responds to what we do, and we seek to influence what happens through our behavior.
    \vspace{5mm}
    \item Analogously, Reinforcement Learning allows an agent to learn the optimal behaviour based on feedback from the environment.
  \end{itemize}
\end{frame}

\begin{frame} {Context for RL}
  \begin{itemize}
    \item The basic idea is simply to capture the most important aspects of the real problem facing a learning agent interacting over time with its environment to achieve a goal.
    \vspace{4mm}
    \item A learning agent must be able to sense the \textbf{state} of its environment to some extent and must be able to take \textbf{actions} that affect the state. The agent also must have a \textbf{goal} or goals relating to the state of the environment.
    \vspace{4mm}
    \item The basic mathematical framework for reinforcement learning, known as \textbf{Markov Decision Processes}, are intended to include just these three aspects -- \textit{sensation, action,} and \textit{goal} -- in their simplest possible forms without trivializing any of them.
    \vspace{4mm}
    \item Any method that is well suited to solving such problems can be considered a Reinforcement Learning method.
  \end{itemize}
\end{frame}

\begin{frame} {Context for RL}
    \begin{itemize}
      \item More concretely, Reinforcement Learning is learning \textit{what to do} - how to map situations/states to actions - so as to maximize a numerical reward signal.
      \vspace{4mm}
      \item The learner is \textit{not} told which actions to take, but instead must discover which actions yield the most reward by trying them.
      \vspace{4mm}
      \item In the most interesting and challenging cases, actions may affect not only the immediate reward but also the next situation and, through that, all subsequent rewards.
      \vspace{4mm}
      \item These two characteristics - \textbf{trial-and-error search} and \textbf{delayed reward} are the two most important distinguishing features of reinforcement learning.
    \end{itemize}
\end{frame}

\begin{frame} {RL vs Supervised learning}
  \begin{itemize}
    \item Yet another important feature distinguishing reinforcement learning from other types of learning is that it uses training information that \textit{evaluates} the actions taken rather than \textit{instructs} by giving correct actions.
    \vspace{4mm}
    \item This is what creates the need for active exploration, for an explicit search for good behavior. 
    \vspace{4mm}
    \item \textit{Purely evaluative feedback} indicates how good the action taken was, but not whether it was the best or the worst action possible. 
    \vspace{4mm}
    \item \textit{Purely instructive feedback}, on the other hand, indicates the correct action to take, independently of the action actually taken. This kind of feedback is the basis of supervised learning.
  \end{itemize}
\end{frame}

\begin{frame} {Application : Robotics}
  \vspace{5mm}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/rl_app_robo.png}}
      \tiny{\\Credit : Texas Instruments}

  \end{figure}
\end{frame}

\begin{frame} {Application : Atari/GTA5/Starcraft}
  \begin{figure}
    \centering
      \scalebox{0.8}{\includegraphics{plots/rl_app_atari.png}}
      \tiny{\\Credit : Keon}

    \end{figure}
\end{frame}

\begin{frame} {Application : Architecture Search}
  \begin{figure}
    \centering
      \scalebox{0.8}{\includegraphics{plots/neu_arch_search.png}}
      \tiny{\\Credit : Zoph,Le}

    \end{figure}
\end{frame}

\begin{frame} {Multi-Armed Bandits}
  \begin{itemize}
    \item We begin by examining a simplified Reinforcement Learning setting known as the "multi-armed bandit" in which:
      \begin{itemize}
        \item There is only a \textit{single} state
        \item \textit{Multiple} actions are possible in this state
      \end{itemize}
  \end{itemize}
  \begin{figure}
    \centering
      \scalebox{0.35}{\includegraphics{plots/s_bandit.png}}
      \caption{\footnotesize The term "bandit" refers to a slot machine ("multi-armed bandit" = slot machine with multiple levers)}
    \end{figure}
\end{frame}

\begin{frame} {Multi-Armed Bandits}
  \vspace{4mm}
  \begin{itemize}
    \item Consider the following learning problem ($k$-armed bandit):
      \vspace{5mm}
      \begin{itemize}
        \item You are faced repeatedly with a choice among $k$ different options, or actions.
          \begin{itemize}
            \item Each "arm" represents a single action.
          \end{itemize}
        \vspace{5mm}
        \item After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected.
        \vspace{5mm}
        \item Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.
      \end{itemize}
  \end{itemize}

\end{frame}

\begin{frame} {Multi-Armed Bandits}
  \vspace{7mm}
  \begin{itemize}
    \item In our $k$-armed bandit problem, each of the $k$ actions has an expected or mean reward given that that action is selected; let us call this the \textit{value} of that action.
    \vspace{5mm}
    \item We denote the action selected on time step $t$ as $A_t$, and the corresponding reward as $R_t$.
    \vspace{5mm}
    \item The value of an action $a$, denoted $q_*(a)$ is the expected reward given that $a$ is selected : \\
      \begin{equation*}
        q_*(a) =  \E[R_t | A_t = a]
      \end{equation*}
  \end{itemize}
\end{frame}

\begin{frame} {Multi-Armed Bandits}
  \vspace{7mm}
  \begin{itemize}
    \item If you \textit{knew} the value of each action, then it would be trivial to solve the $k$-armed bandit problem: you would always select the action with highest value.
    \vspace{5mm}
    \item We assume that you \textit{do not know} the action values with certainty, although you may have estimates.
    \vspace{5mm}
    \item We denote the estimated value of action $a$ at time
  step $t$ as $Q_t(a)$. We would like $Q_t(a)$ to be close to $q_*(a)$.
  \end{itemize}
\end{frame}


\begin{frame}{Action-Value}

\vspace{8mm}
  To estimate action values, we simply take the "sample average" of the rewards.

  \begin{equation*}
  Q_t(a) = \frac{\text{sum of rewards when $a$ taken prior to $t$}}{\text{number of times $a$ taken prior to $t$}} = \frac {\sum_{i=1}^{t-1}R_i\cdot \mathbbm{1}_{A_i=a}}{\sum_{i=1}^{t-1}\mathbbm{1}_{A_i=a}}
  \end{equation*}

  \begin{itemize}
    \item If the denominator is zero, then we instead denote $Q_t$ to be some default value, such as $0$.
    \item As the denominator
  goes to infinity, by the law of large numbers, $Q_t(a)$ converges to $q_*(a)$.
  \end{itemize}

\end{frame}

\begin{frame}{Explore-Exploit Dilemma}
  \vspace{5mm}
\begin{itemize}
  \item If you maintain estimates of the action values, then at any time step there is at least one action whose estimated value is greatest. We call these the \textit{\textbf{greedy}} actions.
    \begin{itemize}
      \item $A_t = \argmax \limits_{a} Q_t(a)$ \hspace{5mm} (ties broken arbitrarily)
    \end{itemize}
  \vspace{5mm}
  \item When you select one of these greedy actions, we say that you are \textit{\textbf{exploiting}} your current knowledge of the values of the actions.
  \vspace{5mm}
  \item If instead you select one of the non-greedy actions, then we say you are \textit{\textbf{exploring}}, because this enables you to improve your estimate of the non-greedy action's value.
  \end{itemize}
\end{frame}

\begin{frame} {Explore-Exploit Dilemma}
  \vspace{5mm}
  \begin{itemize}
    \item Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run.
    \vspace{7mm}
    \item Neither exploration nor exploitation can be pursued exclusively without failing at the task.
    \vspace{7mm}
    \item The agent must try a variety of actions and progressively favor those that appear to be best.
  \end{itemize}
\end{frame}

\begin{frame} {Examples of Explore-Exploit dilemmas}
  \begin{itemize}
  \vspace{4mm}
    \item Restaurant selection
      \begin{itemize}
        \item Exploration : go to your favourite restaurant
        \item Exploitation : try a new restaurant
      \end{itemize}
    \vspace{5mm}
    \item Online ad placement
      \begin{itemize}
        \item Exploitation : show the most successful advertisement
        \item Exploration : show a different random advertisement
      \end{itemize}
    \vspace{5mm}
    \item Oil drilling
      \begin{itemize}
        \item Exploration : drill at the best known location
        \item Exploitation : drill at a new location
      \end{itemize}
      \hspace{8cm} \tiny{Credit : CMU}
  \end{itemize}
\end{frame}

\begin{frame} {$\epsilon$-greedy policies}
  \vspace{3mm}
  \begin{itemize}
    \item Greedy action selection
      \begin{itemize}
        \item always exploits current knowledge to maximize immediate reward
        \vspace{4mm}
        \item spends no time at all sampling apparently inferior actions to see if they might really be better.
      \end{itemize}
      \vspace{8mm}
    \item A simple alternative($\epsilon$-greedy):
      \begin{itemize}
        \item behave greedily most of the time, BUT,
        \vspace{4mm}
        \item every once in a while, say with small probability $\epsilon$, instead select randomly from among all the actions with equal probability, independently of the action-value estimates.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame} {$\epsilon$-greedy policies}
  \vspace{3mm}
  \begin{itemize}
    \item Therefore,
      \begin{itemize}
      \vspace{3mm}
        \item All nongreedy actions are given minimal probability of selection, $\frac{\epsilon}{|\mathcal{A}(s)|}$, and,
        \vspace{3mm}
        \item The remaining bulk of the probability, $1-\epsilon+\frac{\epsilon}{|\mathcal{A}(s)|}$, is given to the greedy action.
      \end{itemize}
      \vspace{8mm}
    \item Key advantage : In the limit as the number of steps increases, every action will be sampled an infinite number of times, thus ensuring that all the $Q_t(a)$ converge to $q^*(a)$.
  \end{itemize}
\end{frame}

\begin{frame} {10-Armed Testbed}
  \begin{itemize}
    \item To roughly assess the relative effectiveness of the greedy and $\epsilon$-greedy methods, we compared them numerically on a suite of test problems.
    \vspace{4mm}
    \item This was a set of 2000 randomly generated k-armed bandit problems with $k$ = 10.
    \vspace{4mm}
    \item For any learning method, we can measure its performance and behavior as it improves with experience over 1000 time steps when applied to one of the bandit problems. This makes up one \textit{run}.
    \vspace{4mm}
    \item Repeating this for 2000 independent runs, each with a different bandit problem, we obtained measures of the learning algorithm's average behavior.
  \end{itemize}
\end{frame}

\begin{frame} {10-Armed Testbed : Example}
\begin{figure}
    \centering
      \scalebox{0.75}{\includegraphics{plots/10arm-testbed.png}}
      \caption{\footnotesize An example bandit problem from the 10-armed testbed. The true value $q_*(a)$ of each of the ten actions was selected according to a normal distribution with mean zero and unit variance, and then the actual rewards were selected according to a mean $q_*(a)$ unit variance normal distribution, as suggested by these gray distributions.}
  \end{figure}
\end{frame}

\begin{frame} {10-Armed Testbed - Results}
  \begin{figure}
    \centering
      \scalebox{0.85}{\includegraphics{plots/10arm_results1.png}}
  \end{figure}
  \begin{itemize}
    \item The greedy method:
      \begin{itemize}
        \vspace{2mm}
        \item improved slightly faster than the other methods at the very beginning, but then leveled off at a lower level because it often got stuck performing suboptimal actions
        \vspace{2mm}
        \item achieved a reward-per-step of only about 1, compared with the best possible of about 1.55 on this testbed.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame} {10-Armed Testbed - Results}
  \begin{figure}
    \centering
      \scalebox{0.85}{\includegraphics{plots/10arm_results2.png}}
  \end{figure}
  \begin{itemize}
    \item The $\epsilon = 0.1$ method:
      \begin{itemize}
        \vspace{2mm}
        \item explored more, and usually found the optimal action earlier, but,
        \vspace{2mm}
        \item it never selected that action more than 91\% of the time.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame} {10-Armed Testbed - Results}
  \begin{figure}
    \centering
      \scalebox{0.75}{\includegraphics{plots/10arm_results2.png}}
  \end{figure}
  \begin{itemize}
    \item The $\epsilon = 0.01$ method:
      \begin{itemize}
        \vspace{2mm}
        \item improved more slowly, but,
        \vspace{2mm}
        \item eventually would perform better than the $\epsilon = 0.1$ method on both performance measures shown in
the figures.
      \end{itemize}
      \vspace{2mm}
    \item Note : It is also possible to reduce $\epsilon$ over time to try to get the best of both high and low values.
  \end{itemize}
\end{frame}

\begin{frame} {Incremental formula - Context}
  \begin{itemize}
    \item The obvious implementation of the sample-average method discussed earlier would be to maintain a record of all the rewards and then perform this computation whenever the estimated value was needed.
    \vspace{5mm}
    \item However, if this is done, then the memory and computational requirements would grow over time as more rewards are seen.
    \vspace{5mm}
    \item Each additional reward would require additional memory to store it and additional computation to compute the sum in the numerator.
    \vspace{5mm}
    \item As you might suspect, this is not really necessary. It is easy to devise incremental formulas for updating averages with small, constant computation required to process each new reward.
  \end{itemize}
\end{frame}

\begin{frame} {Sample Mean - Incremental Formula}
  \begin{equation*}
    \begin{split}
      Q_{n+1} & = \frac{1}{n} \sum_{i=1}^{n} R_i \\
              & = \frac{1}{n} \left( R_n + \sum_{i=1}^{n-1} R_i \right) \\
              & = \frac{1}{n} \left( R_n + (n-1) \frac{1}{n-1} \sum_{i=1}^{n-1} R_i \right) \\
              & = \frac{1}{n} \left( R_n + (n-1) Q_n \right) \\
              & = \frac{1}{n} \left( R_n + nQ_n - Q_n \right) \\
              & = Q_n + \frac{1}{n} [R_n - Q_n]
    \end{split}
  \end{equation*}

This implementation requires memory only for $Q_n$ and $n$, and only the small computation (eqn.\#) for each new reward.

\end{frame}

\begin{frame} {Target - OldEstimate}
  \begin{itemize}
    \item The update rule (eqn.\#) is of a form that occurs frequently in RL algorithms.
    \vspace{3mm}
    \item The general form is:
  \end{itemize}
      \begin{tcolorbox}
    $NewEstimate \leftarrow OldEstimate + StepSize[Target\,-\,OldEstimate]$
      \end{tcolorbox}
  \begin{itemize}
    \vspace{3mm}
    \item The expression $[Target - OldEstimate]$ is an \textit{error} in the estimate. it is reduced by taking a step toward the "Target".
    \vspace{3mm}
    \item The target is presumed to indicate a desirable direction in which to move, though it may be noisy.
  \end{itemize}
\end{frame}

\begin{frame} {Tracking a non-stationary problem}
  \vspace{10mm}
  \begin{itemize}
    \item The averaging methods discussed so far are appropriate for stationary bandit problems,that is, for problems in which the reward probabilities do not change over time.
    \vspace{10mm}
    \item When the problem is non-stationary, it makes more sense to give more weight to recent rewards than to long-past rewards.
  \end{itemize}
\end{frame}

\begin{frame} {Tracking a non-stationary problem}

  \begin{itemize}
    \item One of the most popular ways of doing this is by using a constant step-size parameter.
  \end{itemize}

  \begin{equation*}
    \begin{split}
      Q_{n+1} & = Q_{n} + \alpha [R_{n} - Q_{n}]\\
              & = \alpha R_{n} + (1 - \alpha) Q_{n}\\
              & = \alpha R_{n} + (1 - \alpha) [\alpha R_{n-1} + (1 - \alpha)Q_{n-1}]\\
              & = \alpha R_{n} + (1 - \alpha)\alpha R_{n-1} + (1 - \alpha)^2 Q_{n-1}\\
              & = \alpha R_{n} + (1 - \alpha) \alpha R_{n-1} + (1-\alpha)^2\alpha R_{n-2} + (1-\alpha)^{n-1}\alpha R_{1} + (1 - \alpha)^{n} Q_{1}\\
              & = (1 - \alpha)^{n}Q_{1} + \sum \limits_{i = 1} \limits^{n} \alpha (1 - \alpha)^{n-i}R_{i}.
     \end{split}
  \end{equation*}
\end{frame}

\begin{frame} {Tracking a non-stationary problem}
  \vspace{7mm}
  \begin{itemize}
    \item This is called a \textit{weighted average} because the sum of the weights is $(1 - \alpha)^n + \sum \limits_{i = 1} \limits^n \alpha(1 - \alpha)^{n-i}=1$.
    \vspace{4mm}
    \item The weight, $\alpha(1 - \alpha)^{n-i}$, given to reward $R_i$ depends on how many rewards ago, $n-i$, it was observed.
    \vspace{4mm}
    \item The quantity $1-\alpha$ is less than 1, and thus the weight given to $R_i$ decreases as the number of intervening rewards increases.
    \vspace{4mm}
    \item The weight decays exponentially according to the exponent on $1-\alpha$
  \end{itemize}
\end{frame}

\begin{frame} {Intro to Markov Decision Processes}
  \begin{itemize}
    \item Thus far, we have only considered a rather simple setting in which there is only a single state.
    \vspace{5mm}
    \item In order to consider more general scenarios in Reinforcment Learning, we need the machinery of \textbf{Markov Decision Processes} (MDPs)
    \vspace{5mm}
    \item An MDP provides a convenient mathematical framework for modelling the decision-making process of an agent in a (typically) non-deterministic environment.
    \vspace{5mm}
    \item It is a considerable abstraction of the problem of goal-directed learning from interaction.
  \end{itemize}
\end{frame}

\begin{frame} {Intro to Markov Decision Processes}

  \begin{itemize}
    \item The MDP framework proposes that any problem of learning goal-directed behavior can be reduced to three signals passing back and forth between an agent and its environment: 
    \begin{itemize}
      \vspace{4mm}
      \item one signal to represent the choices made by the agent (the \textbf{actions}),
      \vspace{4mm}
      \item one signal to represent the basis on which the choices are made (the \textbf{states}), and,
      \vspace{4mm}
      \item one signal to define the agent's goal (the \textbf{rewards}). 
    \end{itemize}
    \vspace{4mm}
    \item This framework may not be suffcient to represent \textit{all} decision-learning problems usefully, but it has proved to be widely useful and applicable.
  \end{itemize}
\end{frame}

\begin{frame} {MDPs : Reward}
      \vspace{6mm}
        \begin{itemize}
          \vspace{4mm}
          \item The reward characterizes the goal of the learning-agent.
          \vspace{4mm}
          \item It is a scalar number received by the agent upon taking a specific action in a specific state.
          \vspace{4mm}
          \item Reward signals may be stochastic.
          \vspace{4mm}
          \item \textbf{Delayed rewards} : Actions may affect not only the immediate reward but also the next situation, and through that, all subsequent rewards.
        \end{itemize}
\end{frame}

\begin{frame} {MDPs : Reward}

 Examples of how rewards are used:
 \begin{itemize}
    \vspace{4mm}
    \item To make a robot learn to walk, researchers have provided reward on each time step proportional to the robot's forward motion. 
    \vspace{4mm}
    \item In making a robot learn how to escape from a maze, the reward is often $1$ for every time step that passes prior to escape; this encourages the agent to escape as quickly as possible. 
    \vspace{4mm}
    \item To make a robot learn to find and collect empty soda cans for recycling, one might give it a reward of zero most of the time, and then a reward of $+1$ for each can collected.
    \vspace{4mm}
    \item One might also want to give the robot negative rewards when it bumps into things or when somebody yells at it.
  \end{itemize}
\end{frame}

\begin{frame} {MDPs : Reward}

  \begin{itemize}
    \item It is critical that the rewards we set up truly indicate what we want accomplished.
    \vspace{4mm}
    \item In particular, the reward signal is not the place to impart to the agent prior knowledge about how to achieve what we want it to do.
    \vspace{4mm}
    \item For example, a chessplaying agent should be rewarded only for actually winning, not for achieving subgoals such as taking its opponent's pieces or gaining control of the center of the board.
    \vspace{4mm}
    \item If achieving these sorts of subgoals were rewarded, then the agent might find a way to achieve them without achieving the real goal. 
    \vspace{4mm}
    \item In this case, it might find a way to take the opponent's pieces even at the cost of losing the game.
  \end{itemize}
\end{frame}

\begin{frame} {Simple MDP example }
\begin{figure}
    \centering
      \scalebox{0.4}{\includegraphics{plots/cartpole.png}}
      \tiny{\\Source : CMU}
  \end{figure}
        
  \begin{itemize}
    \item Example 1 : Cartpole
      \begin{itemize}
        \item States: Pole angle and angular velocity
        \item Actions: Move left, right
        \item Rewards: 0 while balancing, -1 for imbalance
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame} {Simple MDP example}
  \begin{figure}
    \centering
      \scalebox{0.5}{\includegraphics{plots/bioreactor.png}}
       \tiny{\\Source : Quora}
  \end{figure}

   \begin{itemize}
   \item Example 2 : Bioreactor
      \begin{itemize}
        \item States : Sensor readings, Ingredients in the vat
        \item Actions : Target temperature , target stirring rate
        \item Rewards : Rate at which the desired chemical is produced by the bioreactor
      \end{itemize}

    \end{itemize}
\end{frame}

\begin{frame} {The MDP Sequence}

  \begin{figure}
    \centering
      \scalebox{0.85}{\includegraphics{plots/MDP.png}}
  \end{figure}

  \vspace{8mm}
  \begin{itemize}

    \item The agent and the environment interact in a sequence of discrete time-steps : $ t = 0,1,2,3...$
  \end{itemize}

\end{frame}

\begin{frame} {The MDP Sequence}
   \begin{figure}
    \centering
      \scalebox{0.75}{\includegraphics{plots/MDP.png}}
  \end{figure}

  \begin{itemize}
    \item At each time-step $t$, the agents observes the current state $S_t \in \mathcal{S}$  and takes an action $A_t \in \mathcal{A}(s)$ that is available in that state
    \begin{itemize}
        \item We will not consider the case of partial observability.
      \end{itemize}
    \item One time-step later, in part as a consequence of its action, the agent receives a numerical \textit reward, $R_{t+1} \in \mathcal{R} \subset \mathbb{R}$.
    \item This results in the sequence/trajectory : \\
        \begin{tcolorbox}
          \indent $S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, ....$
        \end{tcolorbox}
  \end{itemize}
\end{frame}

\begin{frame} {MDPs}
  \vspace{5mm}
  \begin{itemize}
    \item The MDP framework is abstract and  flexible and can be applied to many different problems in many different ways.
    \vspace{8mm}
    \item For example, 
      \begin{itemize}
        \vspace{3mm}
        \item the time steps need not refer to fixed intervals of real time; they can refer to arbitrary successive stages of decision making and acting.
        \vspace{3mm}
        \item the actions can be low-level controls, such as the voltages applied to the motors of a robot arm, or high-level decisions, such as whether or not to have lunch or to go to graduate school.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame} {MDP formalism : Dynamics}
  \begin{itemize}
    \item In a finite MDP, the sets of states, actions and rewards ($\mathcal{S}$, $\mathcal{A}$ and $\mathcal{R}$) all have a finite number of elements.
    \item Therefore, the random variables $R_t$ and $S_t$ have discrete probability distributions that depend only on the previous state and action (Markov property).
    \item That is, for particular values of these random variables, $s' \in \mathcal{S}$ and $r \in \mathcal{R}$, there is a probability of those values occuring at time t, given particular values of the preceding state and action:\\
      \begin{tcolorbox}
          $p(s',r|s,a) = Pr\left\{ \mathcal{S}_t = s', \mathcal{R}_t = r | \mathcal{S}_{t-1} = s, \mathcal{A}_{t-1} = a\right\}$
      \end{tcolorbox}
      for all $s',s \in \mathcal{S}, r \in \mathcal{R}$,and $a \in A(s)$.
    \item Naturally,
      \begin{tcolorbox}
        $ \sum \limits_{s' \in \mathcal{S}} \sum \limits_{r \in \mathcal{R}} p(s',r|s,a) = 1, \text{for all } s \in \mathcal{S}, a \in \mathcal{A}(s)$.
      \end{tcolorbox}
  \end{itemize}
\end{frame}

\begin{frame} {MDP formalism}
  \begin{itemize}
    \item The probabilities given by the four-argument function $p$ completely characterize the dynamics of a finite MDP. It is the \textbf{model} of the environment.
    \item A model makes it possible to decide on a course of action by considering possible future states \textit{before} they are actually experienced.
    \item From it, one can compute anything else one might want to know about the environment 
    
    \item For example, the \textit{state-transition probabilities} ($p : \mathcal{S} \times \mathcal{S} \times \mathcal{A} \rightarrow [0,1]$)
      \begin{itemize}
        \item 
        \begin{tcolorbox}
          $p(s'|s,a) = Pr\left\{ \mathcal{S}_t = s' | \mathcal{S}_{t-1} = s, \mathcal{A}_{t-1} = a\right\} = \sum \limits_{r \in \mathcal{R}} p(s',r|s,a)$
        \end{tcolorbox}
      \end{itemize}
    
  \end{itemize}
\end{frame}

\begin{frame} {MDP formalism : Expected reward}
  \begin{itemize}
    \item Expected reward for state-action pairs ($r : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$) :
          \begin{tcolorbox}
            \begin{equation*}
              r(s,a) = \E[R_t | S_{t-1} = s, A_{t-1} = a] = \sum \limits_{r \in \mathcal{R}} r \sum \limits_{s' \in \mathcal{S}}p(s',r|s,a)
            \end{equation*}
          \end{tcolorbox}
          \pause
    \item Expected reward for state-action-state triples ($r : \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}$):
          \begin{tcolorbox}
            \begin{equation*}
              \begin{split}
                   r(s,a,s') & = \E[R_t | S_{t-1} = s, A_t = a, S_t = s'] \\
                             & = \sum \limits_{r \in \mathcal{R}} r \frac {p(s',r|s,a)}{p(s'|s,a)}
              \end{split}
            \end{equation*}
          \end{tcolorbox}
  \end{itemize}
\end{frame}



\begin{frame} {Returns}
  
  \begin{itemize}
    \vspace{5mm}
    \item The agent's goal is to \textbf{maximize the cumulative reward} it receives in the long run.
    \vspace{5mm}
    \item In general, we seek to maximize \textit{expected return}, where the return, denoted $G_t$, is defined as some specific function of the reward sequence $R_t, R_{t+1}, R_{t+2}, R_{t+3} , \ldots $.
    \vspace{5mm}
    \item In the simplest case, the \textbf{return} is the sum of the rewards:
      \begin{itemize}
        \vspace{2mm}
        \item $G_t = R_{t+1} + R_{t+2} + R_{t+3} + ... + R_T$
        \vspace{2mm}
        \item where T is the final time step.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame} {Episodic Tasks}
  \begin{itemize}
    \item This approach makes sense in applications in which there is a natural notion of final time step. Such tasks are called \textbf{episodic tasks}.
    \vspace{5mm}
    \item For example : plays of a game, trips through a maze, or any sort of repeated interaction.
    \vspace{5mm}
    \item Each episode ends in a special state called the \textit{terminal state}, followed by a reset to a standard starting state or to a sample from a standard distribution of starting states.
    \vspace{5mm}
    \item Note : The next episode begins independently of how the previous episode ended.
  \end{itemize}
\end{frame}


\begin{frame} {Continuing Tasks}
  \begin{itemize}
    \item In many cases, the agent-environment interaction does not break naturally into identifiable episodes, but goes on continually without limit.
    \vspace{3mm}
    \item We call these \textbf{continuing tasks}.
    \vspace{3mm}
    \item The return formulation in the previous slide is problematic for continuing tasks because the final time step would be $T = \infty$, and the return, which is what we're trying to maximize, could itself be infinite.
      \begin{itemize}
        \vspace{2mm}
        \item For example, suppose the agent receives a reward of +1 at each time step.
      \end{itemize}
  \vspace{3mm}
  \item To deal with this, we introduce the concept of \textit{discounting}(see next slide) to our framework.
  \end{itemize}
\end{frame}

\begin{frame} {Discounting}
  \begin{itemize}
    \item In this approach, the agent tries to select actions so that the sum of discounted rewards it receives over the future is maximized.
    \vspace{3mm}
    \item Expected \textit{discounted return} :
      \begin{tcolorbox}
        \begin{equation*}
            G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} \text{...} = \sum \limits_{k = 0} \limits^{\infty} \gamma^k R_{t+k+t}
        \end{equation*}
      \end{tcolorbox}
      \begin{itemize}
        \item $\gamma$ is a parameter, $0 \leq \gamma \leq 1$, called the \textit{discount rate}
      \end{itemize}
    \vspace{3mm}
    \item The discount rate determines the \textit{present} value of \textit{future} rewards.
      \begin{itemize}
        \item A reward reward received $k$ time-steps in the future is worth only $\gamma^{k-1}$ times what it would be if it were received immediately
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame} {Discounting}
  \begin{itemize}
    \item Expected \textit{discounted return} :
      \begin{tcolorbox}
        \begin{equation*}
            G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} \text{...} = \sum \limits_{k = 0} \limits^{\infty} \gamma^k R_{t+k+t}
        \end{equation*}
      \end{tcolorbox}
      \begin{itemize}
        \item $\gamma$ is a parameter, $0 \leq \gamma \leq 1$, called the \textit{discount rate}
      \end{itemize}
    \vspace{2mm}
    \item If $\gamma < 1$, the infinite sum has a finite value as long as the reward sequence ${R_k}$ is bounded.
    \vspace{2mm}
    \item If $\gamma = 0$, the agent only tries to maximize current reward and, therefore, behaves myopically
    \vspace{2mm}
    \item As $\gamma$ approaches $1$, the agent takes future rewards into account more seriously, i.e, becomes more farsighted
  \end{itemize}
\end{frame}

\begin{frame} {Recursive Returns}

  \begin{itemize}
    \vspace{7mm}
    \item Returns at successive time steps are related to each other in a way that is important for the theory and algorithms of reinforcement learning.
  \end{itemize}
  \begin{equation*}
    \begin{split}
      G_t & = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + ... \\
          & = R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + ...) \\
          & = R_{t+1} + \gamma G_{t+1}
    \end{split}
  \end{equation*}
  \begin{itemize}
      \item This works for all time-steps $t < T$, even if termination occurs at time $t+1$, if we define $G_T = 0$.
  \end{itemize}
\end{frame}

\begin{frame} {Policy}
  \vspace{5mm}
   Almost all reinforcement learning algorithms involve estimating \textit{value functions}--functions of states (or of state-action pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state). \\
  \vspace{5mm}
  The notion of "how good" here is defined in terms of future rewards that can be expected, or, to be precise, in terms of expected return. \\
  \vspace{5mm}
  Of course the rewards the agent can expect to receive in the future depend on what actions it will take. \\
  \vspace{5mm}
  Accordingly, \textit{value functions are defined with respect to particular ways of acting, called \textbf{policies}}.

\end{frame}

\begin{frame} {Policy}

  \begin{itemize}
    \vspace{4mm}
    \item Formally, a \textbf{policy} is a mapping from states to probabilities of selecting each possible action.
    \vspace{4mm}
    \item If the agent is following policy $\pi$ at time $t$, then $\pi(a|s)$ is the probability that $A_t = a$ if $S_t = s$.
    \vspace{4mm}
    \item The policy alone is sufficient to determine behaviour.
    \vspace{4mm}
    \item A policy may be stochastic.
      \begin{itemize}
          \vspace{2mm}
          \item In other words, given a state, an agent "samples" an action from a distribution over all actions available in that state.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame} {Value functions}

\vspace{7mm}

The \textit{\textbf{value}} of a state $s$ under a policy $\pi$, denoted as $v_{\pi}(s)$, is the expected return when starting in $s$ and following $\pi$ thereafter.
      \begin{tcolorbox}
        $v_{\pi}(s) = \E_{\pi}[G_t | S_t = s] = \E_{\pi}\left[ \sum \limits_{k=0} \limits^{\infty} \gamma^{k}R_{t+k+1} | S_t = s \right ]$
      \end{tcolorbox}

\vspace{10mm}

Similarly, we define the \textbf{action-value} of taking action $a$ in state $s$ under a policy $\pi$, denoted $q_{\pi}(s,a)$, as the expected return starting from $s$, taking the action $a$ and thereafter following policy $\pi$.
      \begin{tcolorbox}
        $q_{\pi}(s,a) = \E_{\pi}[G_t | S_t = s, A_t = a] = \E_{\pi}\left[ \sum \limits_{k=0} \limits^{\infty} \gamma^{k}R_{t+k+1} | S_t = s \right ]$
      \end{tcolorbox}
\end{frame}


\begin{frame}{Value functions}
      \begin{itemize}
        \item Roughly speaking, the value of a state is the total amount of reward an agent can accumulate over the future, starting from that state.
        \vspace{5mm}
        \item A state might always yield a low immediate reward but still have a high value because it is regularly followed by other states that yield high rewards.
          \begin{itemize}
            \item Or the reverse could be true.
          \end{itemize}
        
        \vspace{5mm}
        \item Rewards are directly given by the environment. Values, however, must be estimated from the rewards received.
          \begin{itemize}
            \item We'll see that this is quite a challenging task in practice!
          \end{itemize}
      \end{itemize}
\end{frame}

\begin{frame} {Key Elements}
  \begin{itemize}
    \vspace{10mm}
    \item{To summarize, the 3 key elements of Reinforcement Learning are :}
      \begin{itemize}
        \vspace{4mm}
        \item The \textbf{model} of the environment
        \vspace{4mm}
        \item The agent's \textbf{policy} : determines how the agent behaves
        \vspace{4mm}
        \item \textbf{Value} of a state/action : defines how "good"/"bad" a given state or state-action pair is
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame} {Simple MDP example}
  \begin{figure}
    \centering
      \scalebox{0.33}{\includegraphics{plots/qvalues_example1.png}}
      \tiny{\\Source : NVIDIA}
  \end{figure}
  Figure : A grid world problem with blocking states (black), where the goal is the bottom right corner.

  \begin{itemize}
    \item Each white cell is a possible state of the MDP.
    \item The cell marked G is the goal state.
    \item In each cell, the agent can choose between 4 actions/moves : Left, Right, Up or Down
    \item The agent cannot enter a blocking state or leave the grid by taking any of the actions.
  \end{itemize}
\end{frame}

\begin{frame} {Simple MDP example : Q-values}
  \begin{figure}
    \centering
      \scalebox{0.8}{\includegraphics{plots/gridworld_down.png}}
      \tiny{\\Source : NVIDIA}
  \end{figure}
  \begin{itemize}
    \item These are the q-values(w.r.t a given policy) for taking the action "Down" in a given state.
    \item Darker shades indicate higher Q-values.
    \item Naturally, choosing the action "Down" while in state A has a lower action-value than choosing "Down" while in state B.
  \end{itemize}
\end{frame}

\begin{frame} {Simple MDP example : Q-values}
  \begin{figure}
    \centering
      \scalebox{0.7}{\includegraphics{plots/qvalues_example.png}}
      \tiny{\\Source : NVIDIA}
  \end{figure}
  \begin{itemize}
    \item This figure shows the q-values of all 4 actions for the states in the MDP.
    \item For every state, a greedy policy chooses the action with the highest q-value.
  \end{itemize}
\end{frame}

\begin{frame} {Bellman Equation for $v_{\pi}$}
  \begin{itemize}
    \item A fundamental property of value functions used throughout reinforcement learning is that they satisfy \textbf{recursive relationships} similar to eqn<>.
    \item For any policy $\pi$ and any state $s$, the following consistency condition holds between the value of $s$ and the value of its possible successor states:
      \begin{tcolorbox}
        \begin{equation*}
          \begin{split}
            v_{\pi} & = \E_{\pi}[G_t | S_t = s] \\
                    & = \E_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s] \\
                    & = \sum \limits_{a} \pi(a|s) \sum \limits_{s'} \sum \limits_{r} p(s',r|s,a) \left[r + \gamma \E_{\pi} [G_{t+1} | S_{t+1} = s'] \right] \\
                    & = \sum \limits_{a} \pi(a|s) \sum \limits_{s',r} p(s',r|s,a) [r + \gamma v_{\pi} (s')] , \forall s \in \mathcal{S}
          \end{split}
        \end{equation*}
      \end{tcolorbox}
  \end{itemize}
\end{frame}

\begin{frame} {Bellman Equation for $v_{\pi}$}
  \begin{figure}
    \centering
      \scalebox{0.5}{\includegraphics{plots/bellman_v.png}}
  \end{figure}
  \begin{itemize} 
    \item The Bellman equation for $v_{\pi}$ expresses a relationship between the value of a state and the values of its successor states.
    \item Think of looking ahead from a state to its possible successor states, as suggested by the diagram to the right.
    \item Each open circle represents a state and each solid circle represents a state-action pair.
  \end{itemize}
\end{frame}

\begin{frame} {Bellman Equation for $v_{\pi}$}
  \begin{figure}
    \centering
      \scalebox{0.5}{\includegraphics{plots/bellman_v.png}}
  \end{figure}
  \begin{itemize} 
    \item Starting from state $s$, the root node at the top, the agent could take any of some set of actions-three are shown in the diagram- based on its policy $\pi$
    \item From each of these, the environment could respond with one of several next states, $s'$(two are shown in the figure)n along with a reward, $r$, depending on its dynamics given by the function $p$.
  \end{itemize}
\end{frame}

\begin{frame} {Bellman Equation for $v_{\pi}$}
  \begin{figure}
    \centering
      \scalebox{0.5}{\includegraphics{plots/bellman_v.png}}
  \end{figure}
  \begin{itemize} 
     \item The Bellman equation averages over all the posibilities, weighting each by its probability of occurring.
    \item It states that the value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way.
  \end{itemize}
\end{frame}

\begin{frame} {Bellman Equation for $q_{\pi}$}
  \begin{itemize}
    \item ddf
    % \small
      \begin{tcolorbox}
        \begin{equation*}
        \resizebox{1.05\hsize}{!}{$
          \begin{split}
            q_{\pi} & = \E_{\pi}[G_t | S_t = s, A_t = a] \\
                    & = \E_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a] \\
                    & = \sum \limits_{s'} \sum \limits_{r} p(s',r|s,a) \left[r + \gamma \E_{\pi} [G_{t+1} | S_{t+1} = s'] \right] \\
                    & = \sum \limits_{s',r} p(s',r|s,a) \left[r + \gamma \sum \limits_{a'} \pi(a'|s') \E_{\pi} [G_{t+1} | S_{t+1} = s', A_{t+1} = a'] \right] \\
                    & = \sum \limits_{s',r} p(s',r|s,a) \left[r + \gamma \sum \limits_{a'} \pi(a'|s') q_{\pi}(s',a') \right] , \forall s \in \mathcal{S} , a \in \mathcal{A}
          \end{split}$}
        \end{equation*}
      \end{tcolorbox}
  \end{itemize}
\end{frame}

\begin{frame} {Bellman Equation for $q_{\pi}$}
  \begin{figure}
    \centering
      \scalebox{0.5}{\includegraphics{plots/bellman_q.png}}
  \end{figure}
\end{frame}

\begin{frame} {Bellman Equations : Toy Example}
   \begin{figure}
    \centering
      \scalebox{0.5}{\includegraphics{plots/bellman_example.png}}
  \end{figure}
  \begin{itemize}
    \item The MDP has 7 states (A,B...G)
    \item Each non-terminal state has two possible actions("left" and "right")
    \item The environment is deterministic (that is, actions are deterministic)
  \end{itemize}
\end{frame}

\begin{frame} {Bellman Equations : Toy Example}
  \begin{figure}
    \centering
      \scalebox{0.5}{\includegraphics{plots/bellman_example.png}}
  \end{figure}
  \begin{itemize}
    \item Terminal states are D,E,F and G and each has value 0.
    \item Reward is 0 for entering a non-terminal state.
    \item Reward for entering D,E,F and G are 1,2,3 and 4, respectively.
  \end{itemize}
\end{frame}

\begin{frame} {Bellman Equations : Toy Example}
  \begin{itemize}
    \item The Bellman equation is :
    \begin{equation*}
      v_{\pi} = \sum \limits_{a} \pi(a|s) \sum \limits_{s',r} p(s',r|s,a) [r + \gamma v_{\pi} (s')]
    \end{equation*}
    \item In our example:
      \begin{itemize}
        \item $\pi(a|s) = 0.5$ , for any action in any state
        \item $\gamma$ is 1
        \item The sum over $s'$ and $r$ disappears because the environment is deterministic. In other words, an action taken in a state can only result in one possible reward and one possible (next) state.      \end{itemize}
      \item Therefore, in our example, the Bellman equation reduces to :
        \begin{equation*}
          v_{\pi} = \sum \limits_{a} \pi(a|s) [r + v_{\pi} (s')]
        \end{equation*}
  \end{itemize}
\end{frame}

\begin{frame} {Bellman Equations : Toy Example}
  \begin{figure}
    \centering
      \scalebox{0.2}{\includegraphics{plots/bellman_example.png}}
  \end{figure}

  \begin{itemize}
    \item According to the Bellman Equation, $v_{\pi}(A) = 0.5 * ( 0 + \textcolor{green}{v_{\pi}(B)}) + 0.5 * (0 + \textcolor{blue}{v_{\pi}(C)})$
    \item In the figure,
      \begin{itemize}
        \item $\textcolor{green}{v_{\pi}(B)} = 0.5 * (1 + v_{\pi}(D)) + 0.5 * (2 + v_{\pi}(E))
                          = 0.5 * 1 + 0.5 * 2 = 1.5$
        \item $\textcolor{blue}{v_{\pi}(C)} = 0.5 * (3 + v_{\pi}(F)) + 0.5 * (4 + v_{\pi}(G))
                          = 0.5 * 3 + 0.5 * 4 = 3.5$
      \end{itemize}
   \item Therefore, $v_{\pi}(A) = 0.5 * 1.5 + 0.5 * 3.5 = 2.5$
  \end{itemize}
\end{frame}

\begin{frame} {Optimal Policies}
  \vspace{5mm}
  \begin{itemize}
    \item Value functions define a partial ordering over policies.
    \vspace{5mm}
    \item A policy $\pi$ is defined to be better than or equal to a policy $\pi^{'}$ if its expected return is greater than or equal to that of $\pi^{'}$ for all states.
    \vspace{5mm}
    \item In other words, $\pi \geq \pi^{'}$ if and only if $v_{\pi}(s) \geq v_{\pi^{'}}(s)$ for all $s \in \mathcal{S}$.
    \vspace{5mm}
    \item There is always at least one policy that is better than or equal to all other policies. This is an \textit{optimal policy}
    \vspace{5mm}
    \item Although there may be more than one, we denote all the optimal policies by $\pi_{*}$.
  \end{itemize}
\end{frame}

\begin{frame} {Optimal value functions}

All optimal policies share the same state-value function, called the \textit{optimal state-value function}, denoted $v_{*}$, and defined as:
  \begin{itemize}
    \item $v_{*}(s) = \max \limits_{\pi} v_{\pi}(s)$ , for all $s \in \mathcal{S}$
  \end{itemize}
\vspace{7mm}
Optimal policies also share the same \textit{optimal action-value function}, denoted $q_{*}$, and defined as:
  \begin{itemize}
    \item $q_{*}(s,a) = \max \limits_{\pi} q_{\pi}(s,a)$ , for all $s \in \mathcal{S}$ and $a \in \mathcal{A}(s)$
  \end{itemize}
\vspace{7mm}
For the state-action pair $(s,a)$, this function gives the expected return for taking action $a$ in state $s$ and thereafter following an optimal policy. Thus, we can write $q_{*}$ in terms of $v_{*}$ as follows:
  \begin{itemize}
    \item $q_{*}(s,a) = \E[R_{t+1} + \gamma v_{*}(S_{t+1}) | S_t = s, A_t = a]$
  \end{itemize}

\end{frame}

\begin{frame} {Bellman Optimality Equation for $v_*$}
  \begin{itemize}
    \item Intuitively, the Bellman optimality equation expresses the fact that the value of a state under an optimal policy must equal the expected return for the best action from that state:
      \begin{tcolorbox}
        \begin{equation*}
          \begin{split}
            v_{*}(s) & = \max \limits_{a \in \mathcal{A}(s)} q_{\pi_{*}}(s,a) \\
                     & = \max \limits_{a} \E_{\pi_{*}} [G_t | S_t = s, A_t = a] \\
                     & = \max \limits_{a} \E_{\pi_{*}} [R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a] \\
                     & = \max \limits_{a} \E [R_{t+1} + \gamma v_{*}(S_{t+1}) | S_t = s, A_t = a] \\
                     & = \max \limits_{a} \E[R_{t+1} + \gamma v_{*} (S_{t+1} | S_{t} = s, A_{t} = a] \\
                     & = \max \limits_{a} \sum \limits_{s',r} p(s',r|s,a) [r + \gamma v_{*}(s')]
          \end{split}
        \end{equation*}
      \end{tcolorbox}

    \item Note : We do not refer to any specific policy above
  \end{itemize}
\end{frame}

\begin{frame} {Bellman Optimality Equation for $q_*$}
    \begin{tcolorbox}
      \begin{equation*}
        \begin{split}
          q_{*}(s,a) & = \E[R_{t+1} + \gamma \max \limits_{a'} q_{*}(S_{t+1},a')|S_t = s, A_t = a]\\
                     & = \sum \limits_{s',r}p(s',r|s,a)[r+\gamma \max \limits_{a'}q_{*}(s',a')]
        \end{split}
      \end{equation*}
    \end{tcolorbox}
    \begin{figure}
      \centering
        \scalebox{0.7}{\includegraphics{plots/bellman_opt.png}}
        \caption {\footnotesize Backup diagrams for $v_*$ and $q_*$}
      \begin{itemize}
        \item \small{These are the same as the backup diagrams for $v_{\pi}$ and $q_{\pi}$ presented earlier except that arcs have been added at the agent's choice points to represent that the maximum over that choise is taken rather than the expected value given some policy.}
      \end{itemize}
    \end{figure}
    
\end{frame}

\begin{frame} {Deriving the optimal policy from $v_*$}
  \begin{itemize}
    \item It is relatively easy to obtain an optimal policy from $v_{*}$
    \vspace{2mm}
    \item For each state $s$, there will be one or more actions at which the maximum  is obtained in the Bellman optimality equation. Any policy that assigns nonzero probability only to these actions is an optimal policy.
      \begin{itemize}
        \item You can think of this as a \textbf{one-step search}.
      \end{itemize}
    \vspace{2mm}
    \item If you have the optimal value function $v_{*}$, the actions that appear best after a one-step search will be optimal actions. In other words, any policy that is \textit{greedy} with repect to the optimal evaluation function $v_{*}$ is an optimal policy.
    \vspace{2mm}
    \item By means of $v_{*}$, the optimal expected long-term return is turned into a quantity that is locally and immediately available for each state. Hence, a
one-step-ahead search yields the long-term optimal actions.
  \end{itemize}
\end{frame}

\begin{frame} {Deriving the optimal policy from $q_*$}
  \begin{itemize}
    \item Having $q_{*}$ makes choosing optimal actions even easier.
    \vspace{2mm}
    \item With $q_{*}$, the agent does not even have to do a one-step-shead search : for any state $s$, it can simply find any action that maximizes $q_{*}(s,a)$.
    \vspace{2mm}
    \item The action-value function essentially caches the results of all one-step-ahead searches. It provides the optimal expected long-term return as a value that is locally and immediately available for each state-action pair.
    \vspace{2mm}
    \item Hence, having $q_{*}$ enables one to select optimal actions \textbf{without knowing anything about the environment's dynamics}.
      \begin{itemize}
        \item It is for this key reason that we will mainly study algorithms for learning action-values rather than state-values.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame} {Bird's eye view : 3 kinds of RL}
  \textbf{The goal of Reinforcement Learning is to find an optimal policy}. This can be accomplished in one of three ways:
 \begin{itemize}
    \vspace{4mm}
    \item 1) Model $\rightarrow$ Value $\rightarrow$ Policy
      \begin{itemize}
        \item Here, we are either given a model of the MDP or we learn it from data. Using the model, a value-funtion is learnt and a policy is extracted from the value-function.
        \item Examples : Dynamic-programming algorithms , Dyna
      \end{itemize}
    \vspace{4mm}
    \item 2) Value $\rightarrow$ Policy
      \begin{itemize}
        \item Here, we directly estimate the value function without using a model(primarily because the model is unknown) and extract a policy from the value function.
        \item \textbf{Monte-Carlo} methods, Temporal-Difference methods(including \textbf{Sarsa} and \textbf{Q-learning}).
      \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame} {Bird's eye view : 3 kinds of RL}

  \begin{itemize}
  \item 3) Policy
      \begin{itemize}
        \item Here, we directly search the space of policies to find an optimal one without considering models or value-functions.
        \item Examples : \textbf{Policy gradients}, Evolutionary algorithms.
        \vspace{15mm}
        \item In the rest of today's lecture we'll look at methods that directly estimate the value function and use the estimates to eventually arrive at the optimal policy.
        \item In the next lecture, we'll study methods that directly search the space of policies.
    \end{itemize}
  \end{itemize}
\end{frame}
\begin{frame} {Generalized Policy Iteration}

    \begin{figure}
      \centering
        \scalebox{0.5}{\includegraphics{plots/gpi_cycle.png}}
    \end{figure}

    \begin{itemize}
      \item Almost all reinforcement learning methods are well described as GPI. That
  is, all have identifiable policies and value functions, with the policy always being improved with respect to the value function and the value function always being driven toward the value function for the policy, as suggested by the diagram.
    \end{itemize}

\end{frame}

\begin{frame} {Generalized Policy Iteration}
  \vspace{5mm}
   Once a policy, $\pi$ , has been improved using $v_{\pi}$ to yield a better policy, $\pi^{'}$, we can then compute $v_{\pi^{'}}$ and improve it again to yield an even better $\pi^{''}$. We can thus obtain a sequence of monotonically improving policies and value functions:
    \begin{figure}
        \centering
          \scalebox{1}{\includegraphics{plots/gpi_seq.png}}
    \end{figure}
     where $\xrightarrow{\text{E}}$ denotes \textbf{\textit{policy evaluation}} and $\xrightarrow{\text{I}}$ denotes \textbf{\textit{policy improvement}}.
     \begin{itemize}
        \vspace{2mm}
        \item Each policy is guaranteed to be a strict improvement over the previous one(unless it is already optimal).
      \end{itemize}
\end{frame}

\begin{frame} {Generalized Policy Iteration}
  \begin{figure}
    \centering
      \scalebox{0.6}{\includegraphics[left]{plots/GPI_arrow.png}}
    \end{figure}
  \begin{itemize}
    \item The value function stabilizes only when it is consistent with the current policy, and the policy stabilizes only when it is greedy with respect to the current value function.
    \item Thus, both processes stabilize only when a policy has been found that is greedy with respect to its own evaluation function.
    \item This implies that the Bellman optimality equation holds, and thus that the policy and the value function are optimal.
  \end{itemize}
\end{frame}

\begin{frame} {Generalized Policy Iteration}
  \begin{figure}
    \centering
      \scalebox{0.6}{\includegraphics[left]{plots/GPI_arrow.png}}
    \end{figure}
  \begin{itemize}
    \item \small{The evaluation and improvement processes in GPI can be viewed as both competing and cooperating.}
    \item \small{They compete in the sense that they pull in opposing directions. Making the policy greedy with respect to the value function typically makes the value function incorrect for the changed policy, and making the value function consistent with the policy typically causes that policy no longer to be greedy.}
    \item \small{In the long run, however, these two processes interact to find a single joint solution: the optimal value function and an optimal policy.}
  \end{itemize}
\end{frame}



\begin{frame} {Monte Carlo : Intro}
  \begin{itemize}
    \vspace{12mm}
    \item Monte Carlo methods are ways of solving the reinforcement learning problem based on averaging \textbf{sample returns}.
    \vspace{4mm}
    \item These methods require \textit{only} experience -- sample sequences of states, actions, and rewards from actual or simulated interaction with an environment.
    \vspace{4mm}
    \item Learning from actual experience is striking because it requires no prior knowledge of the environment's dynamics, yet can still attain optimal behavior.
  \end{itemize}
\end{frame}

\begin{frame} {Monte Carlo}
  \begin{itemize}
    \item Recall that the action-value of a state-action pair is the expected return--expected cumulative future discounted reward--starting from that state-action pair.
    \vspace{5mm}
    \item \textbf{Key idea} : An obvious way to estimate it from experience, then, is simply to \textbf{average the returns} observed after visits to that state-action pair.
      \begin{itemize}
        \vspace{2mm}
        \item To ensure that well-defined returns are available, here we define Monte Carlo methods only for episodic tasks.
      \end{itemize}
    \vspace{5mm}
    \item Each return is an independent, identically distributed estimate of $q(s,a)$ with finite variance. By the law of large numbers, the sequence of averages of these estimates converges to their expected value.
  \end{itemize}
\end{frame}

\begin{frame} {Monte Carlo : Illustration}
  \begin{figure}
      \centering
        \scalebox{0.05}{\includegraphics{plots/mc_backup.png}}
  \end{figure}
  \begin{itemize}
    \item The white node in the figure represents a state and the solid black node represents a state-action pair.
    \item The square at the bottom is a terminal state.
    \item In order to estimate the action-value of a state-action pair, the agent begins the episode in the state-action pair and chooses actions by sampling from the policy until the episode ends(when a terminal state is reached).
    \item The (discounted) return received during the episode is then the "Target" in the update rule.
  \end{itemize}
\end{frame}

\begin{frame} {Monte Carlo}
  \begin{itemize}
    \vspace{10mm}
    \item \textbf{Policy evaluation} : Many episodes are experienced, with the approximate action-value function approaching the true function asymptotically.
    \vspace{5mm}
    \item \textbf{Policy improvement} is done by making the policy $\epsilon$-greedy with respect to the current action-value function.
    \vspace{5mm}
    \item Note : Only on the completion of an episode are value estimates and policies changed.
  \end{itemize}
\end{frame}

\begin{frame} {Monte Carlo : Pseudocode}
  \begin{algorithm}[H]
  \footnotesize
    \caption{Monte-Carlo $Q \approx q_*$}
    \begin{algorithmic}[1]

    \State Initialize, for all $s \in \mathcal{S}, a \in \mathcal{A}(s)$: \item[]
      $Q(s,a) \gets$ arbitrary \item[]
      $Returns(s,a) \gets $ empty list \item[]
      $\pi(a|s) \gets$ an arbitrary $\epsilon$-soft policy

      \Loop{ (forever)}
        \State Generate an episode using $\pi$
        \State For each pair $s,a$ appearing in the episode: \item[]
          \hspace{1cm} $G \gets$ the return that follows the first occurence of $s,a$ \item[]
          \hspace{1cm} Append $G$ to $Returns(s,a)$ \item[]
          \hspace{1cm} $Q(s,a) \gets$ average($Returns(s,a)$)
        \State For each $s$ in the episode: \item[]
          \hspace{1cm} $A^* \gets \argmax_a Q(s,a)$ \Comment{ties broken arbitrarily} \item[]
          \hspace{1cm} For all $a \in \mathcal{A}(s)$: \item[]
          \hspace{2cm} $$
\pi(a|s) =
\begin{cases}
1 - \epsilon + \frac {\epsilon} {|\mathcal{A}(s)|}, & \text{if } a = A^* \\
\frac {\epsilon} {|\mathcal{A}(s)|}, & \text{if } a \neq A^*
\end{cases}
$$
    \EndLoop
  \end{algorithmic}
\end{algorithm}
\end{frame}
\begin{frame} {Sarsa : Intro }
  \vspace{7mm}
  \begin{itemize}
    \item Like Monte Carlo methods, TD methods (such as Sarsa and Q-learning) can learn directly from raw experience without a model of the environment's dynamics.
    \vspace{10mm}
    \item Unlike Monte Carlo methods, however, \textbf{TD methods update estimates based in part on other learned estimates, without waiting for a final outcome}.
  \end{itemize}
\end{frame}

\begin{frame} {Sarsa : Update}
  \begin{itemize}
    \item As usual, we follow the pattern of generalized policy iteration (GPI), only this time \textbf{using Sarsa(defined below) for the policy evaluation part}.
    \vspace{3mm}
    \item \textbf{Policy improvement} is done by making the policy $\epsilon$-greedy (same as Monte Carlo)
    \vspace{3mm}
    \item Recall that an episode consists of an alternating sequence of states and state-action pairs:
    \begin{figure}
      \centering
      \scalebox{1}{\includegraphics{plots/sarsa_seq.png}}
    \end{figure}
    \vspace{3mm}
    \item The Sarsa update is : \\
      $Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha[\textcolor{red}{R_{t+1} + \gamma Q(S_{t+1},A_{t+1})} - Q(S_t,A_t)]$
   \end{itemize}
\end{frame}

\begin{frame} {Sarsa : Update}
  \begin{itemize}
    \item The Sarsa update is (previous slide) : \\
        $Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha[\textcolor{red}{R_{t+1} + \gamma Q(S_{t+1},A_{t+1})} - Q(S_t,A_t)]$
    \vspace{5mm}
    \item This update is done after every transition from a nonterminal state $S_t$. 
      \begin{itemize}
        \item Recall : In the Monte Carlo algorithm, the update is done only after the entire episode is experienced
      \end{itemize}
    \vspace{4mm}
    \item If $S_{t+1}$ is terminal, then $Q(S_t,A_t)$ is defined as zero.
    \vspace{4mm}
    \item This rule uses \textit{every} element of the quintuple of events, $(S_t,A_t,R_{t+1},S_{t+1},A_{t+1})$, that make up a transition from one state-action pair to the next. This is the reason the algorithm is named 'Sarsa'
  \end{itemize}
\end{frame}


\begin{frame} {Sarsa : Backup diagram}
  \begin{figure}
      \centering
        \scalebox{0.4}{\includegraphics{plots/mc_vs_sarsa.png}}
  \end{figure}
  \begin{itemize}
    \item Once again, with Monte Carlo methods one must wait until the end of an episode, because only then is the return known, whereas with Sarsa one need wait only one time step. 
  \end{itemize}
\end{frame}

\begin{frame} {Sarsa : Backup diagram}
  \begin{figure}
      \centering
        \scalebox{0.4}{\includegraphics{plots/mc_vs_sarsa.png}}
  \end{figure}
  \begin{itemize}
    \item Surprisingly often, not having to until the end of the episode turns out to be a critical consideration. 
    \begin{itemize}
      \item Some applications have very long episodes, so that delaying all learning until the end of the episode is too slow. 
      \item Other applications are continuing tasks and have no episodes at all.
    \end{itemize}
  \end{itemize}
\end{frame}


\begin{frame} {Sarsa : Pseudocode}
  \begin{algorithm}[H]
  \footnotesize
    \caption{Sarsa for estimating $Q \approx q_*$}
    \begin{algorithmic}[1]

    \State Initialize $Q(s,a)$ for all $s \in \mathcal{S}, a \in \mathcal{A}(s)$, arbitrarily
    \State Initialize $Q$(\textit{terminal-state}, $\cdot$) = 0
    \Loop{ (for each episode)}
      \State Initialize $S$
      \State Choose $A$ from $S$ using policy derived from $Q$ (e.g., $\epsilon$-greedy)
      \Repeat{ (for each step of the episode)}:
        \State Take action $A$, observe $R$, $S'$
        \State Choose $A'$ from $S'$ using policy derived from $Q$ (e.g., $\epsilon$-greedy)
        \State $Q(S,A) \leftarrow Q(S,A) + \alpha \left[{R + \gamma Q(S',A')} - Q(S,A) \right]$
        \State $S \gets S'$ ; $A \gets A'$;
      \Until{until $S$ is terminal}
    \EndLoop
  \end{algorithmic}
\end{algorithm}

\begin{itemize}
    \item \small{Sarsa converges with probability 1 to an optimal policy and action-value function as long as all state-action pairs are visited an infinite number of times and the policy converges in the limit to the greedy policy (which can be arranged, for example, with $\epsilon$-greedy policies by setting $\epsilon = \frac{1}{t}$).}
\end{itemize}

\end{frame}

\begin{frame} {n-step methods : intro}
  \begin{itemize}
    \vspace{5mm}
    \item We've seen that Monte Carlo methods perform an update for each state based
on the entire sequence of observed rewards from that state until the end of the episode. 
    \vspace{5mm}
    \item The one-step Sarsa method, on the other hand, performs the update based on just a single reward, using the action-value of the next state-action pair as a "proxy" for the remaining rewards.
    \vspace{5mm}
    \item However, neither of these methods is always the best. The best methods are often intermediate between the two extremes.
    \end{itemize}
\end{frame}

\begin{frame} {N-Step methods}
  \begin{itemize}
    \vspace{5mm}
    \item One kind of intermediate method, then, would perform an update based on an intermediate number of rewards: more than one, but less than all of them until termination.
    \vspace{5mm}
    \item For example, a \textit{two-step} update would be based on the first two rewards and the estimated action-value of the state-action pair two-steps later.
    \vspace{5mm}
    \item Similarly, we could have \textit{three-step} updates, \textit{four-step} updates, and so on...
  \end{itemize}
\end{frame}


\begin{frame} {n-step methods}
  \begin{figure}
      \centering
        \scalebox{0.75}{\includegraphics{plots/nstep_sarsa.png}}
    \end{figure}
  \begin{itemize}
    \item \small{The backup diagrams for the spectrum of $n$-step methods for state-action values. They range from the one-step update of Sarsa(0) to the up-until-termination update of the Monte Carlo method. In between are the $n$-step updates, based on $n$ steps of real rewards and the estimated value of the $n$th next state-action pair, all appropriately discounted.}
  \end{itemize}
\end{frame}

\begin{frame} {n-step methods}
  \vspace{4mm}
  The \textbf{n-step return} is :

    \begin{tcolorbox}
    $G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + ... + \gamma^{n-1}R_{t+n} + \gamma^{n}Q_{t+n-1}(S_{t+n},A_{t+n})$
    \end{tcolorbox}
    \begin{itemize}
      \item $n \geq 1, 0 \leq t < T - n$
      \item $G_{t:t+n} = G_{t} \text{ if }t+n \geq T$
    \end{itemize}

  \vspace{4mm}
  The \textbf{n-step Sarsa update} is:
  \begin{tcolorbox}
  $ Q_{t+n}(S_t,A_t) = Q_{t+n-1}(S_t,A_t) + \alpha[G_{t:t+n} - Q_{t+n-1}(S_t,A_t)]$
  \end{tcolorbox}
  \begin{itemize}
    \item $0 \leq t < T$
  \end{itemize}
\end{frame}

\begin{frame} {n-step methods}
  \begin{figure}
      \centering
        \scalebox{1}{\includegraphics{plots/nstep_sarsa_example.png}}
        \caption{\footnotesize Gridworld example of the speedup of policy learning due to the use of $n$-step methods.}
      \end{figure}
  \begin{itemize}
    \item \small{ The first panel shows the path taken by an agent in a single episode, ending at a location of high reward, marked by the G. In this example the values were all initially 0, and all rewards were zero except for a positive reward at G.}
  \end{itemize}
\end{frame}

\begin{frame} {n-step methods}
  \begin{figure}
      \centering
        \scalebox{1}{\includegraphics{plots/nstep_sarsa_example.png}}
        \caption{\footnotesize Gridworld example of the speedup of policy learning due to the use of $n$-step methods.}
      \end{figure}
  \begin{itemize}
    \item \small{The arrows in the second and third panels show which action values were strengthened as a result of this path by one-step and $n$-step Sarsa methods.}
    \item \small{The one-step method strengthens only the last action of the sequence of actions that led to the high  reward, whereas the $n$-step method strengthens the last $n$ actions of the sequence, so that much more is learned from the one episode.}
  \end{itemize}
\end{frame}

\begin{frame} {Q-learning : Update}

  \begin{itemize}
    \item One of the early breakthroughs in reinforcement learning was the development of an algorithm known as Q-learning (Watkins,1989)
    \vspace{4mm}
    \item The update rule for the Q-learning algorithm is:
      \begin{tcolorbox}
        \begin{equation*}
          Q(S_t,A_t) \rightarrow Q(S_t,A_t) + \alpha \left[\textcolor{red}{R_{t+1} + \gamma \max\limits_{a}Q(S_{t+1},a)} - Q(S_t,A_t) \right]
        \end{equation*}
      \end{tcolorbox}
  \begin{itemize}
    \vspace{3mm}
    \item Here, even before the agent chooses an action according the policy, the reward and action-value of the $argmax$ over policies is used as the target.
    \vspace{3mm}
    \item Therefore, $Q$ \textit{directly} approximates $q_{*}$, the \textbf{optimal} action-value function, \textbf{independently} of the policy being followed.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame} {Q-learning : Update}

  \begin{itemize}
    \item One of the early breakthroughs in reinforcement learning was the development of an algorithm known as Q-learning (Watkins,1989)
    \vspace{4mm}
    \item The update rule for the Q-learning algorithm is:
      \begin{tcolorbox}
        \begin{equation*}
          Q(S_t,A_t) \rightarrow Q(S_t,A_t) + \alpha \left[\textcolor{red}{R_{t+1} + \gamma \max\limits_{a}Q(S_{t+1},a)} - Q(S_t,A_t) \right]
        \end{equation*}
      \end{tcolorbox}
  \begin{itemize}
    \vspace{2mm}
    \item The policy still has an effect in that it determines which state-action pairs are visited and updated.
    \vspace{2mm}
    \item All that is required for correct convergence is that all pairs continue to be updated.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame} {Q-learning : Illustration}
  \begin{figure}
      \centering
        \scalebox{0.75}{\includegraphics{plots/sarsa_vs_qlearn.png}}
  \end{figure}

  \begin{itemize}
    \item In Sarsa, the agent chooses an action according to the policy and then uses the resulting reward and action-value as the target.
    \item Recall : In Q-learning, even before the agent chooses an action according the policy, the reward and action-value of the $argmax$ over policies is used as the target.
  \end{itemize}
\end{frame}

\begin{frame} {Q-learning : Pseudocode}
  \begin{algorithm}[H]
  \footnotesize
    \caption{Q-learning for estimating $\pi \approx \pi_*$}
    \begin{algorithmic}[1]
    
    \State Initialize $Q(s,a)$ for all $s \in \mathcal{S}, a \in \mathcal{A}(s)$, arbitrarily
    \State Initialize Q(\textit{terminal-state}, $\cdot$) = 0
    \Loop{ for each episode}
      \State Initialize $S$
      \Repeat{ for each step of the episode}:
        \State Choose $A$ from $S$ using policy derived from $Q$ (e.g., $\epsilon$-greedy)
        \State Take acction A, observe $R$, $S'$
        \State $Q(S,A) \leftarrow Q(S,A) + \alpha \left[{R + \gamma \max\limits_{a}Q(S',a)} - Q(S,A) \right]$
        \State $S \gets S'$
      \Until{until $S$ is terminal}
    \EndLoop
 %      \For{number of training iterations}
 %        \For{k steps}
 %          \State Sample minibatch of $m$ noise samples $\{\mathbf{z}^{(1)} \ldots \mathbf{z}^{(m)}$\} from the noise prior $p_g(\mathbf{z})$
 %          \State Sample minibatch of $m$ examples $\{\mathbf{x}^{(1)} \ldots \mathbf{x}^{(m)}$\} from the data \item[]
 % generating distribution $p_{data}(\mathbf{x})$.
 %          \State Update the discriminator by ascending its stochastic gradient: \item[]
 %  \hspace{2.5 cm}          $\nabla_{{\theta}_d} \frac {1}{m} \sum \limits_{i=1} \limits^{m} \left [ \log D(\mathbf{x}^{(i)}) + \log (1 - D(G(\mathbf{z}^{(i)}))) \right]$
 %            % + \log (1 - D(G(\mathbf{z}^{(i)})))}\right]$
 %        \EndFor
 %        \State Sample minibatch of $m$ noise samples $\{\mathbf{z}^{(1)} \ldots \mathbf{z}^{(m)}$\} from the noise prior $p_g(\mathbf{z})$
 %        \State Update the generator by descending its stochastic gradient: \item[]
 %   \hspace{2.5 cm}       $\nabla_{{\theta}_d} \frac {1}{m} \sum \limits_{i=1} \limits^{m} \log (1 - D(G(\mathbf{z}^{(i)})))$
 %      \EndFor
    \end{algorithmic}
  \end{algorithm}
\end{frame}

\begin{frame} {Q-learning : Toy example (Fix Errors)}
  \begin{figure}
    \centering
      \scalebox{0.7}{\includegraphics{plots/DQLE1.png}}
      \tiny{\\Source : Mnemosyne Studio}

  \end{figure}
  \begin{itemize}
    \item We have 5 rooms(numbered 0 - 4) in a building connected by doors.
    \item The outside of the building can be though of as 1 big room("room" 5)
  \end{itemize}
\end{frame}

\begin{frame} {Q-learning : Toy example (Fix Errors)}
  \begin{figure}
    \centering
      \scalebox{0.4}{\includegraphics{plots/DQLE2.png}}
      \tiny{\\Source : Mnemosyne Studio}
  \end{figure}
  \begin{itemize}
    \item This figure represents the rooms as a graph.
    \item Our goal is to reach "room 5", that is, exit the building.
    \item The doors that lead immediately to the outside have a reward of 100. Others doors have no reward.
    \item dfgdf
  \end{itemize}
\end{frame}

\begin{frame} {Q-learning : Toy example (Fix Errors)}
  \begin{figure}
    \centering
      \scalebox{0.4}{\includegraphics{plots/dqle_init.png}}
      \tiny{\\Source : Mnemosyne Studio}
  \end{figure}
  \begin{itemize}
    \item We initialize out Q matrix to 0.For the sake of simplicity, we assume that the number of states and possible actions is known beforehand.
    \item The rows represent the states and the columns represent the actions. The ijth item indicates the q-value of taking action j in state i.
    \item We set the learning rate $\alpha$ equal to 1. Therefore, the update rule is:
     $Q(S_t,A_t) \rightarrow \left[\textcolor{red}{R_{t+1} + \gamma \max\limits_{a}Q(S_{t+1},a)}\right]$
  \end{itemize}
\end{frame}

\begin{frame} {Q-learning : Toy example - Episode 1 (Fix Errors)}
Let's say we have state 3 as our initial state.
  \begin{itemize}
    \item We have 3 possible actions: go to state 1, 2 or 4.
    \item Because all three actions have the same estimated value(that is, 0), we choose randomly to go to state 1 as our action.
    \begin{itemize}
      \item Our update is $Q(3,1) = 0 + (0.8 * \text{Max}[Q(1,3),Q(1,5)])
       = Q(3,1) = 0 + (0.8 * \text{Max}[0,0]) = 0$
    \end{itemize}
    \item In state 1, we choose randomly and go to state 5 and the episode ends.
      \begin{itemize}
        \item We get a reward of 100 and the value of state 5 is 0.
        \item Therefore $Q(1,5) = 100 + (0.8 * 0) = 100$
      \end{itemize}
    \item This is our Q-matrix at the end of episode 1.
  \end{itemize}
\end{frame}

\begin{frame} {Q-learning : Toy example - Episode 2 (Fix Errors)}
Let's say that one again, episode 2 begins at state 3.
\begin{itemize}
  \item This time, we choose to go to 1 because that action has the highest current estimated value.
  \begin{itemize}
    \item We update Q(3,1) as $Q(3,1) = 0 + (0.8 * 100) = 80$
  \end{itemize}
  \item In state 3, we once again choose to go to sate 5 because that action has the highest current estimated value.
  \item Episode 2 ends with the following Q matrix:
\end{itemize}
\end{frame}

\begin{frame} {Q-learning : Toy example - Convergence (Fix Errors)}
  \begin{figure}
    \centering
      \scalebox{0.4}{\includegraphics{plots/dqle_conv.png}}
      \tiny{\\Source : Mnemosyne Studio}
  \end{figure}

For example, from initial State 2, the agent can use the matrix Q as a guide:
  \begin{itemize}
    \item From State 2 the maximum Q values suggests the action to go to state 3.
    \item From State 3 the maximum Q values suggest two alternatives: go to state 1 or 4.  Suppose we arbitrarily choose  to go to 1.
    \item From State 1 the maximum Q values suggests the action to go to state 5.
    \item Thus the sequence is 2 - 3 - 1 - 5
  \end{itemize}
\end{frame}

\begin{frame} {Function Approximation}
  \begin{itemize}
    \vspace{4mm}
    \item So far we have represented value function by a \textbf{lookup table}
      \begin{itemize}
        \item Every state s has an entry $v(s)$, or
        \item Every state-action pair (s,a) has an entry $q(s,a)$
      \end{itemize}
    \vspace{4mm}
    \item Problem with large MDPs:
      \begin{itemize}
        \item There are too many(possibly infinite) states and/or actions to store in memory
        \item It is too slow to learn the value of each state individually
      \end{itemize}
    \vspace{4mm}
    \item Solution:
      \begin{itemize}
        \item Estimate value function with \textbf{function approximation}
          \begin{tcolorbox}
            $\hat{v}(s,w) \approx v_r{\pi}(s)\\
            \text{or } \hat{q}(s,a,w) \approx q_{\pi}(s,a)$
          \end{tcolorbox}
      \end{itemize}
  \end{itemize}
  \hspace{8cm} \tiny{Source:CMU}
\end{frame}

 \begin{frame} {Function Approximation}
   \begin{itemize}
   \vspace{4mm}
     \item Represent state by a \textbf{feature vector}
     \begin{align}
        x(s) &= \begin{bmatrix}
              x_1(s) \\
               x_2(s) \\
               \vdots \\
               x_n(s)
         \end{bmatrix}
     \end{align}
     \vspace{6mm}
      \item For example
        \begin{itemize}
          \item Distance of robot from landmarks
          \item Trends in the stock market
          \item Piece and pawn configurations in chess
        \end{itemize}
   \end{itemize}
    \hspace{8cm} \tiny{Source:CMU}
 \end{frame}

\begin{frame} {Function Approximation}
  \begin{itemize}
  \vspace{4mm}
    \item $\hat{v}$ might be a \textbf{linear function} in features of the state, with \textbf{w} the vector of feature weights.
    \vspace{4mm}
    \item Or $\hat{v}$ might be the function computed by a \textbf{neural network}, with \textbf{w} the vector of connection weights in all the layers.
    \vspace{4mm}
    \item Or $\hat{v}$ might be the function computed by a \textbf{decision tree}, where \textbf{w} is all the numbers defining the split points and leaf values in the tree
    \vspace{3mm}
    \item Typically, the number of weights(the dimensionality of w) is much less than the number of states (d $\ll |\mathcal{S}|$) and \textit{\textbf{changing one weight changes the estimated value of many states.}}
  \end{itemize}
\end{frame}

\begin{frame} {Gradient descent}
  \begin{itemize}
    \item Goal : Find parameter vector $\mathbf{w}$ minimizing \textbf{mean-squared error} between the true action-value function $q_{\pi}(s,a)$ and its approximation $\hat{q}(s,a,\mathbf{w})$:
    \begin{tcolorbox}
      \begin{equation*}
        J(\mathbf{w}) = \E_{\pi}[(q_{\pi}(s,a) - \hat{q}(s,a,\mathbf{w}))^2]
      \end{equation*}
    \end{tcolorbox}
    \vspace{4mm}
    \item We adjust $\mathbf{w}$ in the direction of the negative gradient :
      \begin{tcolorbox}
        \begin{equation*}
          \begin{split}
            \Delta \mathbf{w} & = - \frac{1}{2} \alpha \nabla_{\mathbf{w}}J(\mathbf{w}) \\
                              & = \alpha \E_{\pi} [(q_{\pi}(s,a) - \hat{q}(s,a,\mathbf{w}))\nabla_{\mathbf{w}}\hat{q}(s,a,\mathbf{w})]
          \end{split}
        \end{equation*}
      \end{tcolorbox}
  \end{itemize}
    \hspace{8cm} \tiny{Source:CMU}

\end{frame}

\begin{frame} {SGD and Semi-gradient descent}
  \begin{itemize}
    \item Use stochastic gradient descent(SGD) to find a local minimum
      \begin{tcolorbox}
        \begin{equation*}
          \Delta \mathbf{w} = \alpha [(q_{\pi}(s,a) - \hat{q}(s,a,\mathbf{w}))\nabla_{\mathbf{w}}\hat{q}(s,a,\mathbf{w})]
        \end{equation*}
      \end{tcolorbox}
    \item We have assumed that the true action-value function $q_{\pi}(s,a)$ is given by a supervisor. However, in RL, there is no supervisor, only rewards.
    \item Therfore, in deep Q-learning,
      \begin{tcolorbox}
        \begin{equation*}
          \Delta \mathbf{w} = \alpha [\textcolor{red}{R + \gamma \max\limits_{a'}q(s',a',\mathbf{w})} - \hat{q}(s,a,\mathbf{w}))\nabla_{\mathbf{w}}\hat{q}(s,a,\mathbf{w})]
        \end{equation*}
      \end{tcolorbox}
  \item This is called a "semi-gradient" method because the expectation of the "stochastic gradients" is \textbf{not} the \textit{true} gradient.
  \end{itemize}
\end{frame}

\begin{frame} {Deep Q-learning : Context}
  \vspace{7mm}
  \begin{itemize}
    \item Learning to control agents directly from high-dimensional sensory inputs like vision and speech is one of the long-standing challenges of reinforcement learning (RL).
    \vspace{4mm}
    \item Before deep reinforcement learning, most successful RL applications that operate on these domains relied on hand-crafted features combined with linear value
functions or policy representations.
    \vspace{4mm}
    \item Deep neural networks are exceptionally good at learning the right representations for these tasks from high-dimensional data.
  \end{itemize}
  \hspace{8cm} \tiny{Credit : Mnih. et. al}

\end{frame}

\begin{frame} {Deep Q-learning : Challenges}
  \begin{itemize}
    \item RL algorithms must be able to learn from a scalar reward signal that is frequently sparse, noisy and delayed. The delay between actions and resulting rewards, which can be thousands of timesteps long, seems particularly daunting when compared to the direct association between inputs and targets found in supervised learning.
    \vspace{4mm}
    \item Most deep learning algorithms assume the data samples to be independent, while in reinforcement learning one typically encounters sequences of highly correlated states.
    \vspace{4mm}
    \item In RL the data distribution changes as the algorithm learns new behaviours, which can be problematic for deep learning methods that assume a fixed underlying distribution.
  \end{itemize}
  \hspace{8cm} \tiny{Credit : Mnih. et. al}

\end{frame}

\begin{frame} {Deep Q-learning}
  \begin{figure}
    \centering
      \scalebox{0.75}{\includegraphics{plots/DQN_naive.png}}
  \end{figure}
  \begin{itemize}
    \item The main drawback of the first architecture is that a separate forward pass is required to compute the Q-value of each action, resulting in a cost that scales linearly with the number of actions.
  \end{itemize}
  \hspace{8cm} \tiny{Credit : Mnih. et. al}
\end{frame}

\begin{frame} {Deep Q-learning : DQN}
    \begin{figure}
      \centering
        \scalebox{1}{\includegraphics{plots/DQN_deep.png}}
        \tiny{Credit : Mnih. et. al}
    \end{figure}
    \begin{itemize}
      \item Input state s is a stack of raw pixels from last 4 frames
      \item Output is Q(s,a) for 18 joystick/button positions
      \item Reward is change in score(wrong) for that step
    \end{itemize}
\end{frame}

\begin{frame} {Deep Q-learning : DQN}
  \begin{itemize}
    \item Mnih et al. used DQN to show how a single reinforcement learning agent can achieve high levels of performance in many different problems without relying on different problem-specific feature sets.
    \vspace{4mm}
    \item To demonstrate this, they let DQN learn to play 49 different Atari 2600 video games by interacting with a game emulator. 
    \vspace{4mm}
    \item For learning each game, DQN used the same raw input, the same network architecture, and the same parameter values (e.g., step-size, discount rate, exploration parameters, and
  many more specific to the implementation).
    \vspace{4mm}
    \item DQN achieved levels of play at or beyond human level on a large fraction of these games.
  \end{itemize}
\end{frame}



\begin{frame} {DQN : Pong}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/dqn_pong.png}} \\
      \tiny{Credit : Mnih}

  \end{figure}
\end{frame}

\begin{frame} {Deep Q-learning}
  \vspace{13mm}
  \begin{itemize}
    \item Mnih et. al modified the basic Q-learning procedure in 3 ways
    \vspace{5mm}
      \begin{itemize}
        \item Experience Replay
        \vspace{5mm}
        \item Target Q network
        \vspace{5mm}
        \item Clipped error term : Error remained in the interval [-1,+1]
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame} {Deep Q-learning : Experience replay}
  \begin{figure}
    \centering
      \scalebox{0.25}{\includegraphics{plots/dqn_replay.png}}\\
      \tiny{Credit : David Silver}
  \end{figure}
  \begin{itemize}
    \item This method stores the agent's experience at each time step in a replay memory that is accessed to perform the weight updates.
    \vspace{2mm}
    \item It works like this : After the game emulator executed action $A_t$ in a state represented by the image stack $S_t$, and returned reward $R_{t+1}$ and image stack $S_{t+1}$, it added the tuple ($S_{t},A_{t},R_{t+1},S_{t+1}$) to the replay memory.
    \vspace{2mm}
    \item This memory accumulated experiences over many plays of the same game.
  \end{itemize}
\end{frame}

\begin{frame} {Deep Q-learning : Experience Replay}
  \begin{itemize}
    \item At each time-step multiple Q-learning updates-a mini-batch-were performed based on experiences sampled uniformly at random from the replay memory.
    \vspace{3mm}
    \item Instead of $S_{t+1}$ becoming the new $S_t$ for the next episode as it would be in the usual form of Q-learning, a new unconnected experience was drawn from the replay memory to supply data for the next update.
    \vspace{3mm}
    \item The ability to use each stored experience for many updates allowed DQN to learn more efficiently from its experiences
    \vspace{3mm}
    \item Experience replay reduced the variance of the updates because successive updates were not correlated with one another as they would be with standard Q-learning.
  \end{itemize}
\end{frame}

\begin{frame} {Deep Q-learning : Target Q network}
\end{frame}

\begin{frame} {DQN : Performance}
  \vspace{3mm}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/dqn_stability.png}}\\
      \tiny{Credit : Mnih}
      \caption{The values in the table are the scores achieved in each game averaged over a 100 or so plays of the game.}
  \end{figure}
  The stability techniques \textbf{drastically} improved the performance of the network!
\end{frame}

\begin{frame} {Policy Gradients : Intro}
  \vspace{7 mm}
  \begin{itemize}
    \item So far in this class, almost all the methods have learned the values of actions and then selected actions based on their estimated action values; their policies would not even exist without the action-value estimates.
    \vspace{4mm}
    \item In this section we consider methods that instead learn a parameterized policy that can select actions without consulting a value function.
    \vspace{4mm}
    \item A value function may still be used to learn the policy parameter, but is not required for action selection.
  \end{itemize}
\end{frame}

\begin{frame} {Policy Gradients : Notation}
  \begin{itemize}
    \item We use the notation $\theta \in \R^{d}$ for the policy's parameter vector.
    \vspace{4mm}
    \item Thus we write $\pi(a|s,\theta) = Pr\{A_t = a| S_t = s, \theta_t = \theta\}$  for the probability that action $a$ is taken at time $t$ given that the environment is in state $s$ at time $t$ with parameter $\theta$.
    \vspace{4mm}
    \item If a method uses a learned value function as well, then the value function's weight vector is denoted $\text{\textbf{w}} \in \R^{d}$ as usual, as in $\hat v(s,\text{\textbf{w}})$
    \vspace{4mm}
    \item Policy gradient methods seek to maximize some performance measure $J(\theta)$, so their updates approximate gradient ascent in J:
      \begin{tcolorbox}
        $\theta_{t+1} = \theta_{t} + \widehat{\alpha \nabla J (\theta_{t})}$
      \end{tcolorbox}
  \end{itemize}
\end{frame}

\begin{frame} {Policy Gradients}
  \vspace{4mm}
  \begin{itemize}
    \item In policy gradient methods, the policy can be parameterized in any way, as long as $\pi(a|s,\theta)$ is differentiable w.r.t its parameters
    \vspace{4mm}
    \item If the action space is discrete and not too large, then a natural kind of parametrization is to form parametrized \textit{numerical preferences} $h(s,a,\theta) \in \R$ for each state-action pair.
    \vspace{4mm}
    \item The actions are then selected according to an exponential softmax distribution:
      \begin{equation*}
        \pi(a|s,\theta) = \frac {exp(h(s,a,\theta))}{\sum_{b}exp(h(s,b,\theta))}
      \end{equation*}
    
  \end{itemize}
\end{frame}

\begin{frame} {Policy Gradients : Advantages}
  \begin{itemize}
  \vspace{7mm}
    \item An immediate advantage of selecting actions according to the softmax in action preferences is that the approximate policy can approach a deterministic policy, whereas with $\epsilon$-greedy action selection over action values there is always an $\epsilon$ probability of selecting a random action.
    \vspace{10mm}
    \item The choice of policy parameterization is sometimes a good way of injecting prior knowledge about the desired form of the policy into the reinforcement learning system. 
    
  \end{itemize}
\end{frame}

\begin{frame} {Policy Gradients : Advantages}
  \begin{itemize}
  \vspace{7mm}
  \item In problems with significant function approximation, the best approximate policy may be stochastic.Action-value methods have no natural way of finding stochastic optimal policies, whereas policy approximating methods can.
  \vspace{10mm}
    \item Perhaps the simplest advantage that policy parameterization may have over action-value parameterization is that the policy may be a simpler function to approximate.
  \end{itemize}
\end{frame}

\begin{frame} {Policy Gradients : Stochastic Policies}
  \begin{figure}
    \centering
      \scalebox{0.66}{\includegraphics{plots/pg_small_corridor.png}}
  \end{figure}
  \begin{itemize}
    \item In this small corridor gridworld, the reward is -1 per step.
    \vspace{4mm}
    \item In each of the three nonterminal states there are only two actions, right and left.
    \vspace{4mm}
    \item These actions have their usual consequences in the first and third states (left causes no movement in the first state), but in the second state they are reversed, so that right moves to the left and left moves to the right.
  \end{itemize}
\end{frame}

\begin{frame} {Policy Gradients : Stochastic Policies}
  \begin{figure}
    \centering
      \scalebox{0.66}{\includegraphics{plots/pg_small_corridor.png}}
  \end{figure}
  \begin{itemize}
    \vspace{5mm}
    \item The problem is difficult because all the states appear identical under the function approximation.
    \vspace{5mm}
    \item In particular, we define $x(s, right) = [1,0]^{T}$ and $x(s,left) = [0,1]^{T}$, for all $s$.
  \end{itemize}
\end{frame}

\begin{frame} {Policy Gradients : Stochastic Policies}
  \begin{figure}
    \centering
      \scalebox{0.8}{\includegraphics{plots/pg_motiv.png}}
  \end{figure}
  \begin{itemize}
    \item An action-value method with $\epsilon$-greedy action selection is forced to choose between just two policies: choosing right or choosing left with high probability 1 - $\epsilon$/2 on all steps.
    \item If $\epsilon$ = 0.1, then these two policies achieve a value(at the start state) of less than -44 and -82, respectively, as shown in the graph.
  \end{itemize}
\end{frame}

\begin{frame} {Policy Gradients : Stochastic Policies}
  \begin{figure}
    \centering
      \scalebox{0.8}{\includegraphics{plots/pg_motiv.png}}
  \end{figure}
  \begin{itemize}
  \vspace{3mm}
    \item A method can do significantly better if it can learn a specific probability with which to select 'right'.
    \vspace{4mm}
    \item The best probability is about 0.59, which achieves a value of about -11.6.
  \end{itemize}
  \end{frame}
  
\begin{frame} {Score Function Gradient Estimator}
  \vspace{7mm}
  \begin{itemize}
    \item Policy Gradients are a special case of a more general \textit{score function gradient estimator}.
    \vspace{4mm}
    \item In the general case, we have an expression of the form $E_{p(x|\theta)}[f(x)]$ - that is, the expectation of some scalar valued score function f(x) under some probability distribution p(s;$\theta$).
    \vspace{4mm}
    \item Because the score is not a function of $\theta$, the only way to increase the expectation is by \textbf{shifting the distribution of x} in order to make samples with higher scores more likely.
  \end{itemize}
  \hspace{8cm} \tiny{Credit : Karpathy}
\end{frame}

\begin{frame} {Score Function Gradient Estimator}
  \begin{tcolorbox}
    \begin{equation*}
      \begin{split}
        \nabla_{\theta}\E_x[f(x)] & = \nabla_{\theta}\sum \limits_{x}p(x)f(x) \\
                                 & = \sum \limits_{x}\nabla_{\theta}p(x)f(x) \\
                                 & = \sum \limits_{x}p(x)\frac{\nabla_{\theta}p(x)}{p(x)}f(x) \\
                                 & = \sum \limits_{x}p(x)\nabla_{\theta}\log p(x)f(x) \\
                                 & = \E_x[f(x)\nabla_{\theta}\log p(x)]
      \end{split}
    \end{equation*}
  \end{tcolorbox}
 Basically, for a given x, f(x) is multiplied by the gradient of $\log p(x)$. Therefore, for a given x, if f(x) is positive, $\theta$ is "pulled" in the direction that increases the probability of x. And if f(x) is negative, $\theta$ is pulled in the opposite direction that lowers the (log) probability of x.
\hspace{8cm} \tiny{Credit : Karpathy}
\end{frame}

\begin{frame} {Score Function Gradient Estimator }
  \begin{figure}
    \centering
      \scalebox{0.95}{\includegraphics{plots/pg_pong_two.png}}
      \caption{\footnotesize{A visualization of the score function gradient estimator.\textit{Left}: A gaussian distribution and a few samples from it (blue dots). On each blue dot we also plot the gradient of the log probability with respect to the gaussian's mean parameter. The arrow indicates the direction in which the mean of the distribution should be nudged to increase the probability of that sample.}}
  \end{figure}
    \hspace{8cm} \tiny{Credit : Karpathy}

\end{frame}

\begin{frame} {Score Function Gradient Estimator 
}
  \begin{figure}
    \centering
      \scalebox{0.95}{\includegraphics{plots/pg_pong_two.png}}
      \caption{\footnotesize{A visualization of the score function gradient estimator.\textit{Middle}: Overlay of some score function giving -1 everywhere except +1 in some small regions (note this can be an arbitrary and not necessarily differentiable scalar-valued function). The arrows are now color coded because due to the multiplication in the update we are going to average up all the green arrows, and the negative of the red arrows.\textit{Right}: after parameter update, the green arrows and the reversed red arrows nudge us to left and towards the bottom. Samples from this distribution will now have a higher expected score, as desired.}}
  \end{figure}
    \hspace{8cm} \tiny{Credit : Karpathy}

\end{frame}


\begin{frame} {Comparison to Maximum Likelihood}
\end{frame}

\begin{frame} {Policy Gradients : Gradient (Fix Notation)}
  \begin{itemize}
    \vspace{4mm}
    \item We let $\tau$ denote a state-action sequence, $s_{0},a_{0},...,s_{H},a_{H}$.
    \vspace{4mm}
    \item We overload notation : $R(\tau) = \sum \limits_{t=0} \limits^{H} R(s_t,a_{t})$.
    \vspace{4mm}
    \item ..
      \begin{tcolorbox}
        \begin{equation*}
          U(\theta) = \E_{\pi_{\theta}}[\sum \limits_{t=0} \limits^{H} R(s_t,a_t)] = \sum \limits_{\tau} p(\tau;\theta)R(\tau)
        \end{equation*}
      \end{tcolorbox}
    \vspace{4mm}
    \item (Fix Error)Our goal is to find $\theta = \max \limits_{\theta} \sum\limits_{\tau} p(\tau;\theta)R(\tau)$
  \end{itemize}
\hspace{8cm} \tiny{Credit : Abbeel}
\end{frame}

\begin{frame} {Policy Gradients : Gradient}
  Taking the gradient of $U(\theta)$ w.r.t. $\theta$ gives :
  \begin{equation*}
    \begin{split}
      \nabla_{\theta}U(\theta) & = \nabla_{\theta} \sum \limits_{\tau} p(\tau;\theta)R(\tau) \\
      & = \sum \limits_{\tau} \nabla_{\theta} p(\tau;\theta)R(\tau) \\
      & = \sum \limits_{\tau} \frac{p(\tau;\theta)}{p(\tau;\theta)}\nabla_{\theta} p(\tau;\theta)R(\tau) = \sum \limits_{\tau} p(\tau;\theta) \frac {\nabla_{\theta} p(\tau;\theta)}{p(\tau;\theta)}R(\tau) \\
      & = \sum \limits_{\tau} p(\tau;\theta) \nabla_{\theta} \log p(\tau;\theta)R(\tau)
    \end{split}
  \end{equation*}
We approximate the empirical estimate for $m$ sample paths under policy $\pi_{\theta}$:
  \begin{tcolorbox}
    $\nabla_{\theta}U(\theta) \approx g = \frac{1}{m}\sum \limits_{i = 1}\limits^{m} \nabla_{\theta} \log p(\tau^{(i)};\theta)R(\tau^{(i)})$
  \end{tcolorbox}
  \hspace{8cm} \tiny{Credit : Abbeel}
\end{frame}

\begin{frame} {Policy Gradients : Intuition}
  \begin{figure}
    \centering
      \scalebox{0.8}{\includegraphics{plots/pg_intuition.png}}
  \end{figure}
  \begin{itemize}
    \item The gradient is \\
    $\nabla_{\theta}U(\theta) \approx g = \frac{1}{m}\sum \limits_{i = 1}\limits^{m} \nabla_{\theta} \log p(\tau^{(i)};\theta)R(\tau^{(i)})$
    \vspace{3mm}
    \item The gradient tries to
      \begin{itemize}
        \item Increase probability of paths with positive R
        \item Decrease probability of paths with negative R
      \end{itemize}
  \end{itemize}
  \hspace{8cm} \tiny{Credit : Abbeel}

\end{frame}

\begin{frame} {Decomposition into states and actions}
  \begin{equation*}
    \begin{split}
      \nabla_{\theta}p(\tau^{(i)};\theta) & = \nabla_{\theta}\log \left[\prod\limits_{t =0} \limits^{H} \underbrace{p(s_{t+1}^{(i)}|s_{t}^{(i)},a_{t}^{(i)})}_{\text{dynamics model}}\underbrace{\pi_{\theta}(a_{t}^{(i)}|s_{t}^{(i)})}_{\text{policy}}\right] \\
      & = \nabla_{\theta} \left[\sum \limits_{t=0}\limits^{H}\log p(s_{t+1}^{(i)}|s_{t}^{(i)},a_{t}^{(i)}) + \sum \limits_{t=0}\limits^{H}\log \pi_{\theta}(a_{t}^{(i)}|s_{t}^{(i)})\right] \\
      & = \nabla_{\theta} \sum \limits_{t=0}\limits^{H} \log \pi_{\theta}(a_{t}^{(i)}|s_{t}^{(i)}) \\
      & = \sum \limits_{t=0}\limits^{H}\underbrace{ \nabla_{\theta} \log \pi_{\theta}(a_{t}^{(i)}|s_{t}^{(i)})}_{\text{no dynamics model required!}}
    \end{split}
  \end{equation*}
  \hspace{8cm} \tiny{Credit : Abbeel}

\end{frame}

\begin{frame} {Policy Gradients : Pseudocode}
\end{frame}

\begin{frame} {Policy Gradients : Pong}
  \begin{figure}
    \centering
      \scalebox{0.6}{\includegraphics{plots/pg_pong.png}}
  \end{figure}
  \begin{itemize}
    \item The player controls one of the paddles and can either move it up or down
    \item Similarly, the computer controls the other paddle
    \item The goal is to bounce the ball past the other player
  \end{itemize}
      \hspace{8cm} \tiny{Credit : Karpathy}

\end{frame}

\begin{frame} {Pong : Policy Network}
  \begin{figure}
    \centering
      \scalebox{0.8}{\includegraphics{plots/pg_pong_one.png}}
  \end{figure}
  \begin{itemize}
    \item Every one of our inputs is an 80x80 difference image (current frame minus last frame)
    \item The network has a single fully connected hidden layer with 200 units.
    \item The final layer has a sigmoid activation(indicating whether to move the paddle up or down).
  \end{itemize}
      \hspace{8cm} \tiny{Credit : Karpathy}
\end{frame}

\begin{frame} {Policy Gradients}
  \begin{figure}
    \centering
      \scalebox{0.8}{\includegraphics{plots/pg_pong_three.png}}
  \end{figure}
  \begin{itemize}
    \item Cartoon diagram of 4 games of Pong. Each black circle is some game state (three example states are visualized on the bottom), and each arrow is a transition, annotated with the action that was sampled. 
    \item In this case we won 2 games and lost 2 games. 
    \item With Policy Gradients we would take the two games we won and slightly encourage every single action we made in that episode. 
    \item Conversely, we would also take the two games we lost and slightly discourage every single action we made in that episode.
  \end{itemize}
      \hspace{8cm} \tiny{Credit : Karpathy}

\end{frame}

\begin{frame} {Pong : Learned weights}
  \begin{figure}
    \centering
      \scalebox{0.6}{\includegraphics{plots/pg_hidden.png}}
      \caption {\footnotesize A collection of 20 (out of 200) neurons in a grid. White pixels are positive weights and black pixels are negative weights.}
  \end{figure}
  \begin{itemize}
    \item \small {Notice that several neurons are tuned to particular traces of bouncing ball, encoded with alternating black and white along the line.}
    \item \small{The ball can only be at a single spot, so these neurons are multitasking and will "fire" for multiple locations of the ball along that line.}
  \end{itemize}
      \hspace{8cm} \tiny{Credit : Karpathy}

\end{frame}

\begin{frame} {Neural Architecture Search}
  \begin{itemize}
    \vspace{10mm}
    \item The success of deep nets like GoogLeNet(Inception), Resnet, Xception, etc is primarily due to  innovations in network architecture
    \vspace{5mm}
    \item However, designing these architectures takes a lot of expert knowledge and ample time
    \vspace{5mm}
    \item Neural Architecture Search is a gradient based method for "automatically" finding good architectures.
  \end{itemize}
\end{frame}

\begin{frame} {Neural Architecture Search}
  \begin{figure}
    \centering
      \scalebox{0.8}{\includegraphics{plots/neu_arch_search_two.png}}
  \end{figure}
  \begin{itemize}
    \item  The structure and connectivity of a neural network can be typically specified by a variable-length string.
      \begin{itemize}
        \item "Filter Width : 5, Filter Height : 3, Num Filters : 24"
      \end{itemize}
    \vspace{2mm}
    \item It is therefore possible to use a recurrent network - the controller - to generate such string.
  \end{itemize}
        \hspace{8cm} \tiny{Credit : Zoph,Le}

\end{frame}

\begin{frame} {Neural Architecture Search}
  \begin{figure}
    \centering
      \scalebox{0.8}{\includegraphics{plots/neu_arch_search_two.png}}
  \end{figure}
  \begin{itemize}
    \item Training the network specified by the string - the "child network" - on the real data will result in an accuracy on a validation set.
    \item Using this accuracy as the reward signal, we can compute the policy gradient to update the controller. 
    \item  As a result, in the next iteration, the controller will give higher probabilities to architectures that receive high accuracies.  
  \end{itemize}
          \hspace{8cm} \tiny{Credit : Zoph,Le}

\end{frame}


\begin{frame} {Neural Architecture Search}
  \vspace{10mm}
  \begin{figure}
    \centering
      \scalebox{0.8}{\includegraphics{plots/neu_arch_search_one.png}}
      \caption{How our controller recurrent neural network samples a simple convolutional network. It
predicts filter height, filter width, stride height, stride width, and number of filters for one layer and
repeats. Every prediction is carried out by a softmax classifier and then fed into the next time step
as input.}
  \end{figure}
          \hspace{8cm} \tiny{Credit : Zoph,Le}

\end{frame}

\begin{frame} {Neural Architecture Search}
  \vspace{10mm}
  \begin{itemize}
    \item The list of tokens that the controller predicts can be viewed as a list of actions $a_{1:T}$ to design an architecture for a child network.
    \vspace{4mm}
    \item At convergence, this child network will achieve an accuracy R(on a held-out dataset) which we use as a reward signal.
    \vspace{4mm}
    \item To find the optimal architecture, we ask our controller to maximize its expected reward: \\
      \begin{equation*}
       J(\theta_{c}) = \E_{p(a_{1:T},\theta_{c})}[R]
      \end{equation*}
     where $\theta_{c}$ represents the parameters of the RNN.
     
  \end{itemize}
          \hspace{8cm} \tiny{Credit : Zoph,Le}

\end{frame}

\begin{frame} {Neural Architecture Search}
  \begin{itemize}
   \item Since the reward signal R is non-differentiable, we use the policy gradient: \\
    \begin{equation*}
    \nabla_{\theta_c}J(\theta_c) = \sum \limits_{t=1} \limits^{T} \E_{p(a_{1:T};\theta_{c})}[\nabla_{\theta_c} \log p(a_t|a_{(t-1):1};\theta_c)R]
    \end{equation*}
    \item An empirical approximation of the above quantity is \\
      \begin{equation*}
        \frac{1}{m} \sum \limits_{k=1} \limits^{m} \sum \limits_{t=1} \limits^{T} \nabla_{\theta_c} \log p(a_t|a_{(t-1):1};\theta_c)R_k
      \end{equation*}
      \begin{itemize}
        \item $m$ is the number of different architectures that the controller samples in one batch
        \item $T$ is the number of hyperparameters our controller has to predict to design a neural network architecture
        \item $R_k$ is the validation accuracy of the k-th neural network
      \end{itemize}
   \end{itemize}
           \hspace{8cm} \tiny{Credit : Zoph,Le}

\end{frame}

\begin{frame} {Neural Architecture Search}
  \begin{figure}
    \centering
      \scalebox{0.8}{\includegraphics{plots/neu_arch_search_three.png}}
  \end{figure}
          \hspace{8cm} \tiny{Credit : Zoph,Le}

\end{frame}
% \begin{vbframe} %frame with breaks and verbatim
% 
% \end{vbframe}

\endlecture