<<setup-child, include = FALSE>>=
library(knitr)
set_parent("../style/preamble.Rnw")
knitr::opts_chunk$set(cache=TRUE)
@

<<size = "scriptsize", include=FALSE>>=
source("code/functions.R")
@

\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-nn}

\lecturechapter{3}{Optimization}
\lecture{Deeplearning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{How learning differs from Pure optimization}
  \begin{itemize}
    \item In machine learning we usually act \textbf{indirectly}.
    \item Technically, we would like to minimize the expected generalization error (or risk):
      $$\riskt = \E_{(x,y)\sim p_{data}} [\Lxyt]$$
    with $p_{data}$ being the true underlying distribution.
    \lz
      \begin{itemize}
        \item If we knew $p_{data}$, the minimization of the risk would be an optimization task!
        \item However, when we only have a set of training samples, we deal with a machine learning problem.
      \end{itemize}
  \end{itemize}
\framebreak
  \begin{itemize}
    \item An alternative without directly assuming something about $p_{data}$ is to approximate $\riskt$ based on the training data, by means of the empirical risk:
      $$\risket = \frac{1}{n} \sumin \Lxyit$$
    \item So rather than optimizing the risk directly, we optimize the empirical risk, and hope that the risk decreases as well.
    \item The empirical risk minimization is prone to overfitting as models with high capacity can simply memorize the training set.
    \item Thus, we have to tweak our optimization such that the quantity that we actually optimize is even more different from the quantity that we truly want to optimize (in reality we obviously optimize $\Oreg$, but to keep things easy we spare that).
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{Recall: Gradient descent (from Chapter 1)}
%   \begin{itemize}
%     \item Let $\fx$ be an arbitrary, differentiable, unrestricted target function, which we want to minimize.
%       \begin{itemize}
%         \item We can calculate the gradient $g = \nabla \fx$, which always points in the direction of the \textbf{steepest ascent}.
%         \item Thus $-g = -\nabla \fx$ points in the direction of the \textbf{steepest descent}!
%       \end{itemize}
%     \item Standing at a point $x_k$ during minimization, we can improve this point by doing the following step:
% $$f(x_{k+1}) = f(x_k) - \alpha \nabla f(x_k)$$
%  \enquote{Walking down the hill, towards the valley.}
%     \item $\alpha$ determines the length of the step and is called \textbf{step size} or in terms of neural networks \textbf{learning rate}.
%   \end{itemize}
% \end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Recall: Gradient descent (Chapter 1)}
  \begin{center}
    $f(x_1, x_2) = -\sin(x_1) \cdot \frac{1}{2\pi} \exp\left( (x_2 - \pi / 2)^2 \right)$
<<echo=FALSE, fig.width=8, fig.height=4>>=
require("colorspace")
require("ggplot2")
foo = function(x, y) {
  -1 * sin(x) * dnorm(y, mean = pi / 2, sd = 0.8)
}

x = y = seq(0, pi, length = 50)
z = outer(x, y, foo)
p = c(list(list(.1, 3)), optim0(.1, 3, FUN = foo, maximum = FALSE))

sd_plot(phi = 25, theta = 20, xlab = "x1", ylab = "x2")
@
  \end{center}
  \hspace{2cm} "Walking down the hill, towards the valley."
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Stochastic gradient descent}
  \begin{itemize}
    \item Optimization algorithms that use the entire training set are called \textbf{batch} or \textbf{deterministic}.
      \begin{itemize}
        \item All training samples are processed in one huge step.
        \item Computationally very costly, particularly in deeplearning!
      \end{itemize}
    \item Optimization algorithms that use only a single example at a time are called \textbf{stochastic} or \textbf{online}.
    \item Most optimization algorithms applied in deeplearning are somewhere in between!
      \begin{itemize}
        \item Usually more than one, but less than all training samples are used at each iteration. Traditionally called \textbf{minibatch}, nowadays \textbf{stochastic} methods.
      \end{itemize}
    \item One \textbf{epoch} means one pass of the full training set.
    \item Since we divide the training set into minibatches, each epoch goes through the whole training set. Each iteration goes through one minibatch.
    \item SGD and its modifications are the most used optimization algorithms for machine learning in general and for deep learning in particular.
  \end{itemize}
  \begin{algorithm}[H]
  \footnotesize
    \caption{Basic SGD parameter update at training iteration k}
    \begin{algorithmic}[1]
    \State \textbf{require} learning rate $\alpha$ \strut
    \State \textbf{require} initial parameter $\theta$ \strut
      \While{stopping criterion not met}
        \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Sample a minibatch of $m$ examples from the training set $\{\tilde{x}^{(1)},\dots,\tilde{x}^{(m)}\}$ \strut}
        \State Compute gradient estimate: $\hat{g} \leftarrow \frac{1}{m} \nabla_\theta \sum_{i} \Lxym$
        \State Apply update: $\theta \leftarrow \theta - \alpha \hat{g}$
      \EndWhile
    \end{algorithmic}
  \end{algorithm}
  \begin{itemize}
    \item Thus, what SGD basically does is computing an unbiased estimate of the gradient by taking the average gradients of a minibatch to update the parameter $\theta$.
  \end{itemize}
\framebreak
\begin{itemize}
  \item For the mnist example of chapter one, we used SGD
  \begin{itemize} 
    \item with a minibatch of size 100
    \item and trained for 10 epochs.
  \end{itemize}
  <<batchsize, size = "footnotesize", cache = TRUE, eval = FALSE, echo = TRUE>>=

model = mx.model.FeedForward.create(
    symbol = softmax, 
    X = train.x, y = train.y,
    optimizer = "sgd",
    array.batch.size = 100L,
    num.round = 10L
)
@
  \item Consequently we feed our algorithm successively with 100 training samples before updating the weights.
  \item An epoch means that the model gets to see the whole training set one time.
  \item Thus, one epoch involves $\frac{\text{training samples}}{\text{batch size}}$ gradient updates.
  \item We repeat this procedure for 10 times.
\end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Batch size as a hyperparameter}

<<batchSizeBenchmark, size = "small", cache = TRUE, eval = FALSE, echo = TRUE>>=
data = mx.symbol.Variable("data")
fc1 = mx.symbol.FullyConnected(data, num_hidden = 128)
act1 = mx.symbol.Activation(fc1, act_type = "relu")
fc2 = mx.symbol.FullyConnected(act1, num_hidden = 64)
act2 = mx.symbol.Activation(fc2, act_type = "relu")
fc3 = mx.symbol.FullyConnected(act2, num_hidden = 32)
act3 = mx.symbol.Activation(fc3, ,act_type = "relu")
fc4 = mx.symbol.FullyConnected(act3, num_hidden = 10)
softmax = mx.symbol.SoftmaxOutput(fc4)
@

%\begin{itemize}
  %\item 
  Let us train this architecture with six different values for the
    $$\text{batch size} \in (16, 17, 20, 100, 128, 1000)$$
  on mnist.
%\end{itemize}

\framebreak

\begin{figure}
  \centering
  \includegraphics[width=11cm]{plots/batchSizeBenchmarkTrain.png}
\end{figure}

\vspace{-0.5cm}

\begin{minipage}{\linewidth}
\centering
  \begin{center}
    \captionof{table}{Different batch sizes and training time in seconds}
      \begin{tabular}{l | c c c c c c}
      Batch size & 16 & 17 & 20 & 100 & 128 & 1000 \\
      \hline 
      Training time & 44.42 & 44.84 & 38.84 &  8.91 & 7.67 & 2.59
      \end{tabular}
  \end{center}
\end{minipage}

 \framebreak
 
\begin{figure}
  \centering
  \includegraphics[width=11cm]{plots/batchSizeBenchmarkTest.png}
\end{figure}

\vspace{-0.5cm}

\begin{minipage}{\linewidth}
\centering
  \begin{center}
    \captionof{table}{Different batch sizes and training time in seconds}
      \begin{tabular}{l | c c c c c c}
      Batch size & 16 & 17 & 20 & 100 & 128 & 1000 \\
      \hline 
      Training time & 44.42 & 44.84 & 38.84 &  8.91 & 7.67 & 2.59
      \end{tabular}
  \end{center}
\end{minipage}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Flat Regions}
  \begin{itemize}
    \item In optimization we look for areas with zero gradient.
    \item A variant of zero gradient are flat regions such as saddle points.
    \item For the error surface $f$ of a neural network, the expected ratio of the number of saddle points to local minima typically grows exponentially with n 
    $$f: \mathbb{R}^n \rightarrow \mathbb{R}$$ 
    In other words: deeper networks exhibit alot more saddle points than local minima.
    \item The Hessian at a local minimum has only positive eigenvalues. At a saddle point it is a mixture of positive and negative eigenvalues.
    \item Why is that?
\framebreak
    \item Imagine the sign of each eigenvalue is generated by coin flipping:
    \begin{itemize}
      \item In a single dimension, it is easy to obtain a local minimum (e.g. \enquote{head} means positive eigenvalue).
      \item In an $n$-dimensional space, it is exponentially unlikely that all $n$ coin tosses will be head.
    \end{itemize}
    \item A property of many random functions is that eigenvalues of the Hessian become more likely to be positive in regions of lower cost.
    \item For the coin flipping example, this means we are more likely to have heads $n$ times if we are at a critical point with low cost.
    \item That means in particular that local minima are much more likely to have low cost than high cost and critical points with high cost are far more likely to be saddle points.
    \item See Dauphin et al. (2014) for a more detailed investigation.
\framebreak
    \item \enquote{Saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum} (Dauphin et al. (2014)).
  \begin{figure}
    \centering
    \includegraphics[width=10.5cm]{plots/cost.png}
  \end{figure}
  \end{itemize}
\framebreak
\framebreak 
  \begin{center}
    $f(x_1, x_2) = x_1^2 - x_2^2$
  \end{center}
  \begin{figure}
    \centering
    \includegraphics[width=4cm]{plots/saddlepoint.png}
  \end{figure} 
  \begin{itemize}
    \item Along $x_1$, the function curves upwards (eigenvector of the Hessian with positive eigenvalue). Along $x_2$, the function curves downwards (eigenvector of the Hessian with negative eigenvalue).
  \end{itemize}
\framebreak 
  \begin{itemize}
    \item So how do saddle points impair optimization?
    \item First-order algorithms that use only gradient information \textbf{might} get stuck in saddle points.
    \item Second-order algorithms experience even greater problems when dealing with saddle points. Newtons method for example actively searches for a region with zero gradient. That might be another reason why second-order methods have not succeeded in replacing gradient descent for neural network training. 
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{

\frametitle{Example: saddle point with gradient descent}

  \center
  \only<1>{\includegraphics[width=9cm]{plots/opt1.png}}%
  \only<2>{\includegraphics[width=9cm]{plots/opt2.png}}%
  \only<3>{\includegraphics[width=9cm]{plots/opt3.png}}%
  \only<4>{\includegraphics[width=9cm]{plots/opt10.png}}%
  
  \begin{itemize}

    \only<1>{\item[] Red dot: starting location}
    \only<2>{\item[] First step..}
    \only<3>{\item[] ..second step..}
    \only<4>{\item[] ..tenth step got stuck and can't escape the saddle point!}
    
  \end{itemize}

}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Difficult vs easy loss surfaces}
  \begin{figure}
  \centering
    \includegraphics[width=12cm]{plots/difficult_vs_easy.png}
    \caption{Left: difficult loss surface, right: easy loss surface (Hao Li et al. (2017))}ad
  \end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% the code below produces the same results but looks alot worse since 
% knitr is very stupid and makes plots jumping around

%\begin{vbframe}{Example: saddle point}
%   \begin{center}
%     $f(x_1, x_2) = x_1^2 - x_2^2$
% <<opt1, size = "small", echo=FALSE, fig.width=6, fig.height=3.5, eval = TRUE>>=
% foo = function(x, y){
%   x^2 - y^2
% }
% x = y = seq(-1.6, 1.6, length = 30)
% z = outer(x, y, foo)
% p = c(list(list(1, 0)), optim0(.2, 0, FUN = foo, maximum = FALSE, maxit = 1))
% #p = c(list(list(-100, 100)))
% sd_plot(phi = 40, theta = 320, xlab = "x1", ylab = "x2")
% @
%     Starting point
%   \end{center}
% \framebreak
%   \begin{center}
%     $f(x_1, x_2) = x_1^2 - x_2^2$
% <<opt2, size = "small", echo=FALSE, fig.width=6, fig.height=3.5, eval = TRUE>>=
% foo = function(x, y){
%   x^2 - y^2
% }
% x = y = seq(-1.6, 1.6, length = 30)
% z = outer(x, y, foo)
% p = c(list(list(1, 0)), optim0(.2, 0, FUN = foo, maximum = FALSE, maxit = 2))
% #p = c(list(list(-100, 100)))
% sd_plot(phi = 40, theta = 320, xlab = "x1", ylab = "x2")
% @
%     First step
%   \end{center}
% \framebreak
%   \begin{center}
%     $f(x_1, x_2) = x_1^2 - x_2^2$  
% <<opt3, echo=FALSE, fig.width=8, fig.height=4>>=
% foo = function(x, y){
%   x^2 - y^2
% }
% x = y = seq(-1.6, 1.6, length = 30)
% z = outer(x, y, foo)
% p = c(list(list(1, 0)), optim0(.2, 0, FUN = foo, maximum = FALSE, maxit = 3))
% #p = c(list(list(-100, 100)))
% sd_plot(phi = 40, theta = 320, xlab = "x1", ylab = "x2")
% @
%     Second step
%   \end{center}
% \framebreak
%   \begin{center}
%     $f(x_1, x_2) = x_1^2 - x_2^2$
% <<opt4, echo=FALSE, fig.width=8, fig.height=4>>=
% foo = function(x, y){
%   x^2 - y^2
% }
% x = y = seq(-1.6, 1.6, length = 30)
% z = outer(x, y, foo)
% p = c(list(list(1, 0)), optim0(.2, 0, FUN = foo, maximum = FALSE, maxit = 10))
% #p = c(list(list(-100, 100)))
% sd_plot(phi = 40, theta = 320, xlab = "x1", ylab = "x2")
% @
%     Tenth step
%   \end{center}
%\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Cliffs and exploding gradients}
  \begin{itemize}
    \item As a result from the multiplication of several parameters, the objective function for highly nonlinear deep neural networks (i.e. recurrent nets, chapter 5) often contain sharp nonlinearities.
      \begin{itemize}
        \item That may result in very high derivatives in some places.
        \item As the parameters get close to such a cliff regions, a gradient descent update can catapult the parameters very far.
        \item Such an occurrence can lead to losing most of the optimization work that had been done.
      \end{itemize}
    \item However, serious consequences can be easily avoided using a technique called \textbf{gradient
clipping}.
    \item The gradient does not specify the optimal step size, but only the optimal direction
within an infinitesimal region.
\framebreak 
    \item Gradient clipping simply caps the step size to be small enough that it is less likely to go outside the region where the gradient indicates the direction of steepest descent.
    \item We simply \enquote{prune} the norm of the gradient at some threshold $h$:
    $$\text{if  } ||\nabla \theta|| > \text h: \nabla \theta \leftarrow \frac{h}{||\nabla \theta||} \nabla \theta $$
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Example: cliffs and exploding gradients}
  \begin{figure}
  \centering
    \includegraphics[width=8cm]{plots/cliff2.png}
    \caption{\enquote{The objective function for highly nonlinear deep neural networks or for
recurrent neural networks often contains sharp nonlinearities in parameter space resulting
from the multiplication of several parameters. These nonlinearities give rise to very
high derivatives in some places. When the parameters get close to such a cliff region, a
gradient descent update can catapult the parameters very far, possibly losing most of the
optimization work that had been done} (Goodfellow et al. (2016)).}
  \end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{Local minima}
%   \begin{itemize}
%     \item Convex optimization problems can be reduced to \enquote{finding a local minimum}.
%       \begin{itemize}
%         \item Any local minimum is a global minimum!
%       \end{itemize}
%     \item In neural networks we have to deal with non-convex problems.
%       \begin{itemize}
%         \item Generally, we have many saddle points!
%         \item Nearly any deep network is guaranteed to have an extremely large amount of saddle points.
%       \end{itemize}
%     \item In practice, nearly all difficulty with neural network optimization arises from local minima.
%       \begin{itemize}
%         \item To test if stuck in a local minina one could plot the norm of the gradient over time.
%         \item If it does not shrink to insignificant size, the problem is neither local minima nor any other kind of critical point.
%       \end{itemize}
%   \end{itemize}
% \end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Example: multimodal function}
Potential snippet from a loss surface of a deep neural network with many local minima:
<<echo=FALSE, fig.height = 4>>=
library(smoof)
library(ggplot2)
library(plot3D)
foo = function(x, y) {
  (x - y)^2 + exp((1 - sin(x))^2) * cos(y) + exp((1 - cos(y))^2) * sin(x)
}
x = y = seq(-10, 10, length = 50)
z = outer(x, y, foo)
p = c(list(list(-100, 100)))
sd_plot(phi = 40, theta = 185, xlab = "x1", ylab = "x2")
@
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Momentum}
  \begin{itemize}
    \item Designed to accelerate learning, especially when facing high curvature, small but consistent or noisy gradients.
    \item Momentum accumulates an exponentially decaying moving average of past gradients:
      \begin{eqnarray*} 
        \nu &\leftarrow& \varphi \nu - \alpha \underbrace{\nabla_\theta \Big(\frac{1}{m} \sum_{i} \Lxym \Big)}_{g(\theta)} \\
        \theta &\leftarrow& \theta + \nu
      \end{eqnarray*}
    \item We introduce a new hyperparameter $\varphi \in [0, 1)$, determining how quickly the contribution of previous gradients decay.
    \item $\nu$ is called \enquote{velocity} and derives from a physical analogy describing how particles move through a parameter space (Newton's law of motion).
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Momentum}
  \begin{itemize}
    \item So far the step size was simply the gradient $g$ multiplied by the learning rate $\alpha$.
    \item Now, the step size depends on how \textbf{large} and how \textbf{aligned} a sequence of gradients are.
      \begin{itemize}
        \item The step size grows when many succesive gradients point in the same direction.
      \end{itemize}
    \item Common values for $\varphi$ are $0.5, 0.9$ and even $0.99$.
    \item We can think of momentum as the fraction $\frac{1}{1-\varphi}$
      \begin{itemize}
        \item Then, $\varphi = 0.9$ corresponds to multiplying the maximum speed by 10 relative to
the gradient descent algorithm. 
      \end{itemize}
    \item Generally, the larger $\varphi$ is relative to the learning rate $\alpha$, the more previous gradients affect the current direction.
    \item A very good website with an in-depth analysis of momentum: \url{https://distill.pub/2017/momentum/}
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}
Example:
  \footnotesize 
  \begin{eqnarray*}
    \nu_{1} &\leftarrow& \varphi \nu_0 - \alpha g(\theta_0) \\
    \theta_{1} &\leftarrow& \theta_0 + \varphi \nu_0 - \alpha g(\theta_0) \\
    \\
    \nu_{2} &\leftarrow& \varphi \nu_1 - \alpha g(\theta_1) \\
            &=& \varphi (\varphi \nu_0 - \alpha g(\theta_0)) - \alpha g(\theta_1) \\
    \theta_{2} &\leftarrow& \theta_1 + \varphi (\varphi \nu_0 - \alpha g(\theta_0)) - \alpha g(\theta_1) \\
    \\
    \nu_{3} &\leftarrow& \varphi \nu_2 - \alpha g(\theta_2) \\
            &=& \varphi (\varphi (\varphi \nu_0 - \alpha g(\theta_0)) - \alpha g(\theta_1)) - \alpha g(\theta_2) \\
    \theta_{3} &\leftarrow& \theta_{2} + \varphi (\varphi (\varphi \nu_0 - \alpha g(\theta_0)) - \alpha g(\theta_1)) - \alpha g(\theta_2) \\
            &=& \theta_2 + \varphi^3\nu_0 - \varphi^2\alpha g(\theta_0) - \varphi \alpha g(\theta_1) - \alpha g(\theta_2) \\
            &=& \theta_{2} - \alpha(\varphi^2g(\theta_{0}) + \varphi^1g(\theta_{1}) + \varphi^0g(\theta_{2})) + \varphi^3 \nu_0 \\
    \\
\theta_{k+1} &=& \theta_{k} - \alpha \displaystyle\sum_{j = 0}^{k} \varphi^j g(\theta_j) + \varphi^{k+1}\nu_0
  \end{eqnarray*}
\framebreak

Suppose momentum always observes the same gradient $g(\theta)$:
  \footnotesize 
  \begin{eqnarray*}
    \theta_{k+1} &=& \theta_{k} - \alpha \displaystyle\sum_{j = 0}^{k} \varphi^j g(\theta_j) + \varphi^{k+1}\nu_0 \\
                 &=& \theta_{k} - \alpha g(\theta) \displaystyle\sum_{j = 0}^{k} \varphi^j + \varphi^{k+1}\nu_0 \\
                 &=& \theta_{k} - \alpha g(\theta) \frac{1 - \varphi^{k+1}}{1 - \varphi} + \varphi^k \nu_0 \\
                 \lim_{k \to \infty} &=& \theta_k - \alpha g(\theta) \frac{1}{1 - \varphi}
  \end{eqnarray*}

Thus, momentum will accelerate in the direction of $-g(\theta)$ until reaching terminal velocity with step size: 
    $$-\alpha g(\theta)(1 + \varphi + \varphi^2 + \varphi^3 + ...) = -\alpha g(\theta) \frac{1}{1 - \varphi}$$
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{

\frametitle{Momentum: physical explanation}

  \center
  \only<1>{\includegraphics[height = 7.5 cm, width = 7.5 cm]{plots/momentum_physics_0.png}}%
  \only<2>{\includegraphics[height = 7.5 cm, width = 7.5 cm]{plots/momentum_physics_1.png}}%
  \only<3>{\includegraphics[height = 7.5 cm, width = 7.5 cm]{plots/momentum_physics_2.png}}%
  \only<4>{\includegraphics[height = 7.5 cm, width = 7.5 cm]{plots/momentum_physics_3.png}}%
  \only<5>{\includegraphics[height = 7.5 cm, width = 7.5 cm]{plots/momentum_physics_4.png}}%
  \only<6>{\includegraphics[height = 7.5 cm, width = 7.5 cm]{plots/momentum_physics_5.png}}%

}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Gradient descent vs momentum}
  \begin{figure}
  \centering
    \includegraphics[height = 6 cm, width = 10 cm]{plots/gd_vs_momentum.png}
    \caption{Gradient descent (left) needs many steps \textbf{and} gets stuck in a local minimum Momentum (right) finds the global minimum}
  \end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{SGD with momentum}
  \begin{algorithm}[H]
    \caption{Stochastic gradient descent with momentum}
    \begin{algorithmic}[1]
    \State \textbf{require} learning rate $\alpha$ and momentum $\varphi$ \strut
    \State \textbf{require} initial parameter $\theta$ and initial velocity $\nu$ \strut
      \While{stopping criterion not met}
        \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Sample a minibatch of $m$ examples from the training set $\{\tilde{x}^{(1)},\dots,\tilde{x}^{(m)}\}$ \strut}
        \State Compute gradient estimate: $\hat{g} \leftarrow + \frac{1}{m} \nabla_\theta \sum_{i} \Lxym$
        \State Compute velocity update: $\nu \leftarrow \varphi \nu - \alpha \hat{g}$
        \State Apply update: $\theta \leftarrow \theta + \nu$
      \EndWhile
    \end{algorithmic}
  \end{algorithm}
\framebreak
  \begin{figure}
  \centering
    \includegraphics[height = 6 cm, width = 10 cm]{plots/momentum.png}
    \caption{The contour lines show a quadratic loss function with a poorly conditioned Hessian matrix. The two curves show how standard gradient descent (black) and momentum (red) learn when dealing with ravines.}
  \end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Momentum in practice}
  \begin{itemize}
    \item Lets try out different values of momentum (with SGD) on the MNIST data.
    \item We apply the same architecture we've used a dozen of times already (note that we used $\varphi = 0.9$ in all computations so far, i.e. in chapter 1 and 2)!
  \end{itemize}
<<mxnet1, size = "small", cache = TRUE, eval = FALSE, echo = TRUE>>=
fc1 = mx.symbol.FullyConnected(data, num_hidden = 512)
act1 = mx.symbol.Activation(fc1, activation = "relu")
fc2 = mx.symbol.FullyConnected(act1, num_hidden = 512)
act2 = mx.symbol.Activation(fc2, activation = "relu")
fc3 = mx.symbol.FullyConnected(act2, num_hidden = 512)
act3 = mx.symbol.Activation(fc3, activation = "relu")
fc4 = mx.symbol.FullyConnected(act3, num_hidden = 10)
softmax = mx.symbol.SoftmaxOutput(fc4, name = "sm")
@
\framebreak
  \begin{figure}
  \centering
    \includegraphics[width=12cm]{plots/momentum_train.png}
  \end{figure}
\framebreak
  \begin{figure}
  \centering
    \includegraphics[width=12cm]{plots/momentum_test.png}
  \end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Nestorov momentum}
  \begin{itemize}
    \item Momentum aims to solve poor conditioning of the hessian but also variance in the stochastic gradient.
    \item Nesterov momentum modifies the algorithm such that the gradient is evaluated after the current velocity is applied:
      \begin{eqnarray*} 
        \nu &\leftarrow& \varphi \nu - \alpha \nabla_\theta \Big[\frac{1}{m} \sum_{i} \Lmomentum \Big] \\
        \theta &\leftarrow& \theta + \nu
      \end{eqnarray*}
    \item We can interpret Nesterov momentum as an attempt to add a correction factor to the basic method.
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Stochastic gradient descent}
  \begin{itemize}
    \item We would like to force convergence until reaching a local minimum.
    \item A decisive parameter of SGD is the learning rate $\alpha$.
    \item Applying SGD, we have to decrease the learning rate over time, thus $\alpha_k$ (learning rate at training iteration k).
      \begin{itemize}
        \item The estimator $\hat{g}$ is computed of small batches.
        \item Random sampling $m$ training samples introduces noise, that does not vanish even if we find a minimum.
      \end{itemize}
    \item In practice, a common strategy is to decay the learning rate linearly over time until iteration $\tau$:
    \begin{eqnarray*}
      \alpha_k &=& (1 - \epsilon)\alpha_0 + \epsilon\alpha_\tau \text{ \ \ \ \ \ \ \ \ \ \ \ \ with }  \epsilon = \frac{k}{\tau} \text{ and } \alpha_{k+1} \text{ const.} \\
               &=& k(-\frac{\alpha_0 +  \alpha_\tau}{\tau}) + \alpha_0
    \end{eqnarray*}
  \end{itemize}
\framebreak
\scriptsize Example for $\tau = 4$:
\begin{minipage}{\linewidth}
\centering
  \begin{center}
        {\footnotesize
        \begin{tabular}{l | c | c}
        iteration &  $\epsilon$ & $\alpha_k$ \\ \hline
        \multicolumn{1}{c|}{$1$}       
        & $0.25$  
        & $(1-\frac{1}{4})\alpha_0 + \frac{1}{4}\alpha_\tau = \frac{3}{4}\alpha_0 + \frac{1}{4}\alpha_\tau$\\
        
        \multicolumn{1}{c|}{$2$}       
        & $0.5$   
        & $\frac{2}{4}\alpha_0 + \frac{2}{4}\alpha_\tau$ \\
        
        \multicolumn{1}{c|}{$3$}       
        & $0.75$  
        & $\frac{1}{4}\alpha_0 + \frac{3}{4}\alpha_\tau$ \\
        
        \multicolumn{1}{c|}{$4$}       
        & $1$     
        & 0 + $\alpha_\tau$ \\
        
        \multicolumn{1}{c|}{$...$}       
        &      
        & $\alpha_\tau$ \\
        
        \multicolumn{1}{c|}{$k+1$}     
        & 
        & $\alpha_\tau $
        \end{tabular}
        }
  \end{center}
\end{minipage}
  \begin{figure}
  \centering
    \includegraphics[width=8cm]{plots/weight_decay.png}
  \end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Adaptive learning rates}
  \begin{itemize}
    \item Learning rates are reliably one of the hyperparameters that is the most difficult to set because it has a significant impact on the models performance.
    \item Naturally, it might make sense to use a different learning rate for each parameter, and automaticaly adapt them throughout the training process.
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Adagrad}
  \begin{itemize}
    \item Adagrad adapts the learning rate to the parameters.
    \item In fact, adagrad scales learning rates inversely proportional to the square root of the sum of all of their historical squared values.
      \begin{itemize}
        \item Parameters with large partial derivatives of the loss obtain a rapid decrease in their learning rate.
        \item Parameters with small partial derivatives on the other hand obtain a relatively small decrease in their learning rate.
      \end{itemize}
    \item For that reason, Adagrad might be well suited when dealing with sparse data. 
    \item Goodfellow et al. (2016) say that the accumulation of squared gradients can result in a premature and overly decrease in the learning rate.
  \end{itemize}
\framebreak
  \begin{algorithm}[H]
    \small
    \caption{Adagrad}
    \begin{algorithmic}[1]
    \scriptsize 
    \State \textbf{require} Global learning rate $\alpha$ \strut
    \State \textbf{require} initial parameter $\theta$ \strut
    \State \textbf{require} Small constant $\beta$, perhaps $10^{-7}$, for numerical stability \strut
    \State \textbf{Initialize} gradient accumulation variable $r = 0$
       \While{stopping criterion not met}
         \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Sample a minibatch of $m$ examples from the training set $\{\tilde{x}^{(1)},\dots,\tilde{x}^{(m)}\}$ \strut}
         \State Compute gradient estimate: $\hat{g} \leftarrow \frac{1}{m} \nabla_\theta \sum_{i} \Lxym$
         \State Accumulate squared gradient $r \leftarrow r + g \odot  g$
         \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Compute update: $\nabla \theta = - \frac{\alpha}{\beta + \sqrt(r)} \odot g$ (division and square root applied element-wise) \strut}
         \State Apply update: $\theta \leftarrow \theta + \nabla\theta$
       \EndWhile
    \end{algorithmic}
  \end{algorithm}
  \begin{itemize}
    \item \enquote{$\odot$} is called Hadamard or element-wise product.
    \item Example:
    \vspace{0.2cm}
    \item[] $A =
            \begin{bmatrix}
              1 & 2 \\
              3 & 4
            \end{bmatrix}, \ 
            B =
            \begin{bmatrix}
              5 & 6 \\
              7 & 8
            \end{bmatrix}, \ \text{ then } A \odot B =
            \begin{bmatrix}
              1 \cdot 5 & 2 \cdot 6 \\
              3 \cdot 7 & 4 \cdot 8
            \end{bmatrix}$
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{RMSProp}
  \begin{itemize}
    \item RMSprop is a modification of Adagrad.
    \item It's intention is to resolve Adagrad's radically diminishing learning rates.
    \item The gradient accumulation is replaced by an exponentially weighted moving average.
    \item Theoretically, that leads to performance gains in non-convex scenarios.
    \item Empirically, RMSProp is a very effective optimization algorithm. Particularly, it is employed routinely by deep learning practitioners (Goodfellow et al. (2016)).
  \end{itemize}
\framebreak
  \begin{algorithm}[H]
    \small
    \caption{RMSProp}
    \begin{algorithmic}[1]
    \State \textbf{require} Global learning rate $\alpha$ and decay rate $\rho$ \strut
    \State \textbf{require} initial parameter $\theta$ \strut
    \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{\textbf{require} Small constant $\beta$, perhaps $10^{-6}$, to stabilize division by small numbers \strut}
    \State Initialize gradient accumulation variable $r = 0$
      \While{stopping criterion not met}
        \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Sample a minibatch of $m$ examples from the training set $\{\tilde{x}^{(1)},\dots,\tilde{x}^{(m)}\}$ \strut}
        \State Compute gradient estimate: $\hat{g} \leftarrow \frac{1}{m} \nabla_\theta \sum_{i} \Lxym$
        \State Accumulate squared gradient $r \leftarrow \rho r + (1 - \rho) g \odot  g$
        \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Compute update: $\nabla\theta = - \frac{\alpha}{\beta + \sqrt(r)} \odot g$ \strut}
        \State Apply update: $\theta \leftarrow \theta + \nabla\theta$
      \EndWhile
    \end{algorithmic}
  \end{algorithm}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Adam}
  \begin{itemize}
    \item Adaptive Moment Estimationis another method that computes adaptive learning rates for each parameter.
    \item Adam uses the first and the second moments of the gradients.
      \begin{itemize}
        \item Adam keeps an exponentially decaying average of past gradients (first moment)
        \item Like RMSProp it stores an exponentially decaying average of past squared gradient (second moment)
        \item Thus, it can be seen as a combination of RMSProp and momentum.
      \end{itemize}
    \item Basically Adam uses the combined averages of previous gradients at different moments to give it more \enquote{persuasive power} to adaptively update the parameters.
  \end{itemize}
\framebreak
\begin{algorithm}[H]
  \scriptsize 
  \caption{Adam}
  \begin{algorithmic}[1]
  \State \textbf{require:} Step size $\mu$ (suggested default: 0.001) \strut
  \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{\textbf{require:} Exponential decay rates for moment estimates, $\rho_1$ and $\rho_2$ in $[0,1)$ (Suggested defaults: 0.9 and 0.999 respectively)} \strut
  \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{\textbf{require:} Small constant $\beta$ (Suggested default $10^{-8}$) \strut}
  \State \textbf{require:} Initial parameters $\theta$ 
  \State Initialize 1st and 2nd moment variables $s = 0, r = 0$
  \State Initialize time step $t = 0$
    \While{stopping criterion not met}
      \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Sample a minibatch of $m$ examples from the training set $\{\tilde{x}^{(1)},\dots,\tilde{x}^{(m)}\}$ \strut}
      \State Compute gradient estimate: $\hat{g} \leftarrow \frac{1}{m} \nabla_\theta \sum_{i} \Lxym$
      \State $t \leftarrow t + 1$
      \State Update biased first moment estimate: $s \leftarrow \rho_1 s + (1 - \rho_1) g$
      \State Update biased second moment estimate: $r \leftarrow \rho_2 r + (1 - \rho_2) g \odot g$
      \State Correct bias in first moment: $\hat{s} \leftarrow \frac{s}{1-\rho_1^t}$
      \State Correct bias in second moment: $\hat{r} \leftarrow \frac{r}{1-\rho_2^t}$
      \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Compute update: $\nabla\theta = - \mu \frac{\hat{s}}{\sqrt{\hat{r}} + \beta}$ \strut}
      \State Apply update: $\theta \leftarrow \theta + \nabla\theta$
    \EndWhile
  \end{algorithmic}
\end{algorithm}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{SGD vs. Adaptive Learning Rates}
  \begin{itemize}
    \item Let us apply our standard architecture on the mnist problem and try our different optimizers.
    \item These are:
<<mxnet2, size = "scriptsize", cache = TRUE, eval = FALSE, echo = TRUE>>=
mx.opt.sgd(wd = 0.001, learning.rate = 0.03, 
  momentum = 0.9)

mx.opt.adagrad(wd = 0.001, learning.rate = 0.05, 
  epsilon = 1e-08)

mx.opt.rmsprop(wd =  0.001, learning.rate = 0.002, 
  gamma1 = 0.95, gamma2 = 0.9)

mx.opt.adam(wd = 0.001, learning.rate = 0.001,
  beta1 = 0.9, beta2 = 0.999, epsilon = 1e-08)
@
  \end{itemize}
\framebreak
  \begin{figure}
  \centering
    \includegraphics[width=12cm]{plots/optimizerTrain.png}
  \end{figure}
\framebreak
  \begin{figure}
  \centering
    \includegraphics[width=12cm]{plots/optimizerTest.png}
  \end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{Batch normalization}
% \textbf{Motivation:}
% 
% As shown earlier, neural networks learn a nonlinear transformation of the input space such that in the last layer(s), a simple classification is sufficient to seperate our data well.
% To do so, especially in deep networks, we need to coordinate updates between the layers.
% Batch normalization forces the model to learn a nonlinear transformation in a layer by removing changes in mean and standard deviation from the layers output.
% 
% \framebreak
%   \begin{itemize}
%     \item Batch Normalization is no algorithm, but rather a technique to improve optimization in certain situations.
%     \item It is an extra component that can be placed between each layer of the neural network.
%     \item That component takes the activation of the antecedent layer and normalizes it before sending it to the next \enquote{actual layer}.
%     $$ H' = \frac{H-\mu}{\sigma}$$
%     with $H$ being the activated minibatch of the previous layer, $\mu$ the mean and $\sigma$ the standard deviation of each unit.
%   \end{itemize}
% \framebreak
%   \begin{itemize}
%     \item For our final benchmark in this chapter we compute two models to predict the mnist data.
%     \item One will extend our basic architecture such that we add batch normalization to all hidden layers.
%     \item We use SGD as optimizer with $momentum = 0.9$, $learning.rate = 0.03$ and $wd = 0.001$.
%     \item []
% <<mxnet3, size = "scriptsize", cache = TRUE, eval = FALSE, echo = TRUE>>=
% fc1 = mx.symbol.FullyConnected(data, num_hidden = 512)
% act1 = mx.symbol.Activation(fc1, act_type = "relu")
% bn1 = mx.symbol.BatchNorm(act1)
% fc2 = mx.symbol.FullyConnected(bn1, num_hidden = 512)
% act2 = mx.symbol.Activation(fc2, act_type = "relu")
% bn2 = mx.symbol.BatchNorm(act2)
% fc3 = mx.symbol.FullyConnected(bn2, num_hidden = 512)
% act3 = mx.symbol.Activation(fc3, act_type = "relu")
% bn3 = mx.symbol.BatchNorm(act3)
% fc4 = mx.symbol.FullyConnected(bn3, num_hidden = 10)
% softmax = mx.symbol.SoftmaxOutput(fc4, name = "sm")
% @
%   \end{itemize}
% \framebreak
%   \begin{figure}
%   \centering
%     \includegraphics[width=12cm]{plots/bnTrain.png}
%   \end{figure}
% \framebreak
%   \begin{figure}
%   \centering
%     \includegraphics[width=12cm]{plots/bnTest.png}
%   \end{figure}
% \end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame} {Batch Normalization}
  \begin{itemize}
    \vspace{3mm}
    \item BatchNorm is an extremely popular technique that improves the training speed and stability of deep neural nets
    \vspace{3mm}
    \item It is an extra component that can be placed between each layer of the neural network
    \vspace{3mm}
    \item It works by changing the "distribution" of activations at each hidden layer of the network
    \vspace{3mm}
    \item We know that is sometimes beneficial to normalize the inputs to a learning algorithm by shifting and scaling all the features so that they have 0 mean and unit variance.
    \vspace{3mm}
    \item Analogously, BatchNorm applies a similar transformation to the activations of the hidden layers(with a couple of additional tricks)
  \end{itemize}
\end{frame}

\begin{frame} {Batch Normalization}
  \begin{itemize}
    \item For a hidden layer with $m$ neurons ($z_1, \dots, z_m$), BatchNorm is applied to each $z_i$ by considering the activations of $z_i$ \textbf{over a given minibatch} of inputs
    \item Let $z_i^j$ denote the activation of $z_i$ for input $x^j$ in the minibatch(of size N)
    \item The mean and variance of the activations are
      \begin{equation*}
          \begin{array}{l}
            \mu_i = \frac{1}{N} \sum \limits_j \limits^N z_i^j \\
            \sigma^2_i = \frac{1}{N} \sum \limits_j \limits^N (z_i^j - \mu_i)^2
          \end{array}
      \end{equation*}
    \item Each $z_i^j$ is then normalized 
        \begin{equation*}
          \tilde z_i^j = \frac {z_i^j - \mu_i}{\sqrt{\sigma^2_i + \epsilon}}
        \end{equation*}
        where a small constant, $\epsilon$, is added for numerical stability
  \end{itemize}
\end{frame}

\begin{frame} {Batch Normalization}
  \begin{itemize}
    \item It may not be desirable to normalize the activations in such a rigid way because potentially useful information can be lost in the process
    \item Therefore, we commonly let the training algorithm decide the "right amount" of normalization by allowing it to re-shift and re-scale $\tilde z_i^j$ to arrive at the batch normalized activation $\hat z_i^j$
        \begin{equation*}
          \hat z_i^j = \gamma_i \tilde z_i^j + \beta_i
        \end{equation*}
    \item $\gamma_i$ and $\beta_i$ are learnable parameters that are also tweaked by backpropogation
    \item $\hat z_i^j$ then becomes the input to the next layer
    \item Note : the algorithm is free to scale and shift each $\tilde z_i^j$ back to its original (unnormalized) value
  \end{itemize}
\end{frame}

\begin{frame} {Batch Normalization : Illustration}
  \begin{itemize}
    \item Recall: $z_i = \sigma(W_i^T x)$
    \item So far, we've applied batch-norm to the activation $z_i$. It is possible(and more common) to apply batch norm to $W_i^T x$ \textit{before} passing it to the non-linearity $\sigma$.
  \end{itemize}
    \begin{figure}
    \centering
      \scalebox{0.32}{\includegraphics{plots/batchy.png}}
      \caption{\footnotesize FC = Fully Connected layer. BatchNorm is applied \textit{before} the nonlinear activation function.}
  \end{figure}
\end {frame}


\begin{frame} {Batch Normalization}
  \begin{itemize}
    \item The key impact of BatchNorm on the training process is this : it reparametrizes the underlying optimization problem to \textbf{make its landscape significantly more smooth}
    \item One aspect of this is that the loss changes at a smaller rate and the magnitudes of the gradients are also smaller(see Santurkar et.al 2018)
  \end{itemize}
  \begin{figure}
    \centering
      \scalebox{0.6}{\includegraphics{plots/bn_effect.png}}
  \end{figure}
\end{frame}

\begin{vbframe} {Batch Normalization}
\begin{itemize}
    \item For our final benchmark in this chapter we compute two models to predict the mnist data.
    \item One will extend our basic architecture such that we add batch normalization to all hidden layers.
    \item We use SGD as optimizer with $momentum = 0.9$, $learning.rate = 0.03$ and $wd = 0.001$.
    \item []
<<mxnet3, size = "scriptsize", cache = TRUE, eval = FALSE, echo = TRUE>>=
fc1 = mx.symbol.FullyConnected(data, num_hidden = 512)
act1 = mx.symbol.Activation(fc1, act_type = "relu")
bn1 = mx.symbol.BatchNorm(act1)
fc2 = mx.symbol.FullyConnected(bn1, num_hidden = 512)
act2 = mx.symbol.Activation(fc2, act_type = "relu")
bn2 = mx.symbol.BatchNorm(act2)
fc3 = mx.symbol.FullyConnected(bn2, num_hidden = 512)
act3 = mx.symbol.Activation(fc3, act_type = "relu")
bn3 = mx.symbol.BatchNorm(act3)
fc4 = mx.symbol.FullyConnected(bn3, num_hidden = 10)
softmax = mx.symbol.SoftmaxOutput(fc4, name = "sm")
@
  \end{itemize}
\framebreak
  \begin{figure}
  \centering
    \includegraphics[width=12cm]{plots/bnTrain.png}
  \end{figure}
\framebreak
  \begin{figure}
  \centering
    \includegraphics[width=12cm]{plots/bnTest.png}
  \end{figure}

\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Second order optimization}
  \begin{itemize}
    \item Second order optimization methods include information about the curvature but require computing the hessian matrix.
      \begin{itemize}
        \item Very expensive for large networks!
        \item Think of the network we applied in the regularization chapter:
      \end{itemize}
    \begin{figure}
      \centering
        \includegraphics[width=8cm]{plots/mxnet_regulariation_1.png}
    \end{figure}
    \item This model has a huge amount of parameters:
    \begin{eqnarray*}
      &=& \underbrace{784 \cdot 512}_{\text{input to 1st layer}} + \underbrace{512^2}_{\text{1st to 2nd layer}} + \underbrace{512^2}_{\text{2nd to 3rd layer}} + \underbrace{512 \cdot 10}_{\text{3rd to output}} \\
      &=& 3.285.504
    \end{eqnarray*}
  \end{itemize}
\framebreak
  \begin{itemize}
    \item A first order method would need $3.285.504$ partial derivatives to compute the gradients.
    \item Second order methods on the other hand require $$3.285.504^2 = 10.794.536.534.016$$ partial derivatives!
    \item For small problems/networks one might consider the BFGS (Broyden-Fletcher-Goldfarb-Shanno) algorithm, which computes an approximation of the hessian (still expensive!).
      \begin{itemize}
        \item Very difficult to implement, alot of programming know-how necessary to create an efficient implementation.
      \end{itemize}
    \item For huge networks even gradient descent becomes too expensive.
      \begin{itemize}
        \item Solution: parameters are instead grouped into mini-batches (stochastic gradient descent)
      \end{itemize}
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {Training deep nets}
  \begin{itemize}
    \small{\item There are a few important subtleties about neural network training to bear in mind.
    \item \underline{\#1} : In practice, training may not arrive at a critical point of any kind.
        \begin{figure}
    \centering
      \scalebox{0.65}{\includegraphics{plots/no_critical.png}}
      \tiny{\\Source : Goodfellow}
      \caption{\footnotesize Left : A plot of gradient norms(one norm per epoch, sampled at random) while training a CNN. The solid curve shows a running average of all gradient norms. Right: The classification error rate for the CNN on a validation set }
  \end{figure}
  In the figure above, the gradient norms \textbf{increase} over time. Therefore, the training process is not converging to a critical point even though the validation error reaches a very low level.}
  \end{itemize}
\end{frame}

\begin{frame} {Training deep nets}
  \begin{itemize}
    \small{\item \underline{\#2} : The loss function can lack a global minimum point(where the gradient norm is 0) and the training loss may instead only asymptotically approach some value.
    \vspace{2mm}
    \item For example, consider a classifier $p(y|\mathbf{x})$, where $y$ is a discrete set of three possible labels and the probabilities are computed using softmax.
    \vspace{2mm}
    \item For a single training example $\mathbf{x'}$, if the correct output is $p(y|\mathbf{x'}) = [1,0,0]$, softmax can only asymptotically approach this value.\\
      \begin{align*}
 \text{Input(to softmax)} &\rightarrow \text{Output} \\
 [7.5,6,6]  &\rightarrow [\textcolor{red}{0.69143845}, 0.15428077, 0.15428077] \\
 [7.5,-1,-1] &\rightarrow [\textcolor{red}{0.99700214}, 0.00149893, 0.00149893] \\
 [7.5,-14,-14] &\rightarrow [\textcolor{red}{9.999e-01}, 4.599e-10, 4.599e-10]
\end{align*}
  \vspace{2mm}
\item Therefore, if the loss function is the negative log likelihood $-\log p(y|\mathbf{x})$, the training loss can become arbitrarily close to 0 but can never actually reach it.
  }\end{itemize}
\end{frame}

\begin{frame} {Training deep nets}
  \begin{itemize}
  \small{
    \item \underline{\#3} : If the training algorithm makes only \textit{locally} optimal moves(as in gradient descent), it may move away from regions of \textit{much} lower cost.
      \begin{figure}
    \centering
      \scalebox{0.65}{\includegraphics{plots/local_hill.png}}
      \tiny{\\Source : Goodfellow}
    \end{figure}
    \item In the figure above, initiliazing the parameter on the "wrong" side of the hill will result in suboptimal performance.
    \vspace{2mm}
    \item In higher dimensions, however, it may be possible for gradient descent to go around the hill but such a trajectory might be very long and result in excessive training time.}
  \end{itemize}
\end{frame}









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}
\frametitle{References}
\footnotesize{
\begin{thebibliography}{99}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Ian Goodfellow et al., 2016]{1} Ian Goodfellow, Yoshua Bengio and Aaron Courville (2016)
\newblock Deep Learning
\newblock \emph{\url{http://www.deeplearningbook.org/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Yann Dauphin et al., 2014]{2} Yann Dauphin, Razvan Pascanu, {\c{C}}aglar G{\"{u}}l{\c{c}}ehre, Kyunghyun Cho, Surya Ganguli, Yoshua Bengio (2014)
\newblock Identifying and attacking the saddle point problem in high-dimensional non-convex optimization
\newblock \emph{\url{https://arxiv.org/abs/1406.2572}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Hao Li et al., 2017]{2} Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, Tom Goldstein (2017)
\newblock Visualizing the Loss Landscape of Neural Nets
\newblock \emph{\url{https://arxiv.org/abs/1712.09913}}


\end{thebibliography}
}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlecture











