<<setup-child, include = FALSE>>=
library(knitr)
set_parent("../style/preamble.Rnw")
knitr::opts_chunk$set(cache=TRUE)
@

<<size = "scriptsize", include=FALSE>>=
source("code/functions.R")

@

\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-nn}

\lecturechapter{4}{Optimization II}
\lecture{Deeplearning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame} {Backpropagation}
%    Recall, in the XOR example, we computed :
%    $$\frac{d \Lxy}{d W_{11}} = 
%        \frac{d z_{1,in}}{d W_{11}} \cdot \frac{d z_{1,out}}{d z_{1,in}} \cdot \frac{d f_{in}}{d z_{1,out}} \cdot \frac{d f_{out}}{d f_{in}} \cdot \frac{d \Lxy}{d f_{out}}$$
%  \begin{figure}
%    \centering
%      \includegraphics[width=9cm]{plots/backprop_gg0.png}
%      \caption{All five terms of our chain rule}
%  \end{figure}
%\end{frame}
%\begin{frame} {Backpropagation}
%    % \item Recall, in the XOR example, we computed
%    % $$\frac{d \Lxy}{d W_{11}} = 
%    %     \Bigg( \frac{d z_{1,in}}{d W_{11}} \Bigg( \frac{d z_{1,out}}{d z_{1,in}} \Bigg( \frac{d f_{in}}{d z_{1,out}} \Bigg( \frac{d f_{out}}{d f_{in}} \cdot \frac{d \Lxy}{d f_{out}}\Bigg)\Bigg)\Bigg)\Bigg)$$
%      Next, let's compute :
%      $$\frac{d \Lxy}{d W_{21}} = 
%        \Bigg( \frac{d z_{1,in}}{d W_{21}} \Bigg( \frac{d z_{1,out}}{d z_{1,in}} \Bigg( \frac{d f_{in}}{d z_{1,out}} \Bigg( \frac{d f_{out}}{d f_{in}} \cdot \frac{d \Lxy}{d f_{out}}\Bigg)\Bigg)\Bigg)\Bigg)$$
%        \begin{figure}
%    \centering
%      \includegraphics[width=9cm]{plots/backprop_gg.png}
%      \caption{All five terms of our chain rule}
%  \end{figure}
%\end{frame}
%\begin{frame} {Backpropagation}
%  \begin{itemize}
%    \item Examining the two expressions :
%    $$\frac{d \Lxy}{d W_{11}} = 
%        \Bigg( \frac{d z_{1,in}}{d W_{11}} \textcolor{red}{\Bigg( \frac{d z_{1,out}}{d z_{1,in}} \Bigg( \frac{d f_{in}}{d z_{1,out}} \Bigg( \frac{d f_{out}}{d f_{in}} \cdot \frac{d \Lxy}{d f_{out}}\Bigg)\Bigg)\Bigg)}\Bigg)$$
%      $$\frac{d \Lxy}{d W_{21}} = 
%        \Bigg( \frac{d z_{1,in}}{d W_{21}} \textcolor{red}{\Bigg( \frac{d z_{1,out}}{d z_{1,in}} \Bigg( \frac{d f_{in}}{d z_{1,out}} \Bigg( \frac{d f_{out}}{d f_{in}} \cdot \frac{d \Lxy}{d f_{out}}\Bigg)\Bigg)\Bigg)}\Bigg)$$
%    \item We see that there is significant overlap / redundancy in the two expressions. A huge chunk of the second expression has already been computed while computing the first one.
%    \item \textbf{Main idea} : Simply cache these subexpressions instead of recomputing them each time.
%    
%  \end{itemize}
%\end{frame}
%\begin{frame} {Backpropagation}
%  \begin{itemize}
%    \item Let
%      $$\delta = 
%        \textcolor{red}{\Bigg( \frac{d z_{1,out}}{d z_{1,in}} \Bigg( \frac{d f_{in}}{d z_{1,out}} \Bigg( \frac{d f_{out}}{d f_{in}} \cdot \frac{d \Lxy}{d f_{out}}\Bigg)\Bigg)\Bigg)}$$
%        be cached.
%    \item The two expressions now become,
%    $$\frac{d \Lxy}{d W_{11}} = 
%         \frac{d z_{1,in}}{d W_{11}} \cdot \delta$$ 
%      $$\frac{d \Lxy}{d W_{21}} = 
%        \frac{d z_{1,in}}{d W_{21}} \cdot \delta$$
%        where $\delta$ is simply "plugged in".
%    \item As you can imagine, caching subexpressions in this way and plugging in where needed can result in \textbf{massive} gains in efficiency for deep and "wide" neural networks. 
%    \item In fact, this simple algorithm, which was first applied to neural networks way back in 1985, is \textit{still} the biggest breakthrough in deep learning.
%  \end{itemize}
%\end{frame}
%\begin{frame} {Backpropagation}
%  \begin{itemize}
%    \item On the other hand, if we had done a \textit{forward} computation of the derivatives:
%    $$\frac{d \Lxy}{d W_{11}} = \Bigg(\Bigg(\Bigg(\Bigg( \frac{d z_{1,in}}{d W_{11}} \frac{d z_{1,out}}{d z_{1,in}} \Bigg)  \frac{d f_{in}}{d z_{1,out}} \Bigg) \frac{d f_{out}}{d f_{in}} \Bigg) \frac{d \Lxy}{d f_{out}}\Bigg)$$
%      $$\frac{d \Lxy}{d W_{21}} = 
%        \Bigg(\Bigg(\Bigg(\Bigg( \frac{d z_{1,in}}{d W_{21}} \frac{d z_{1,out}}{d z_{1,in}} \Bigg)  \frac{d f_{in}}{d z_{1,out}} \Bigg) \frac{d f_{out}}{d f_{in}} \Bigg) \frac{d \Lxy}{d f_{out}}\Bigg)$$
%      there would \textit{be} no common subexpressions to cache. We would have to compute the \text{entire} expression for each and every weight!
%    \item This would make the computations too inefficient to make gradient descent tractable for large neural networks.
%  \end{itemize}
%\end{frame}
%
%\begin{frame} {Backpropagation}
%  \begin{itemize}
%    \item \small{Let's now derive a general formulation of backpropagation.}
%    \item \small{The neurons in layers $i-1$, $i$ and $i+1$ are indexed by $j$, $k$ and $m$, respectively.
%    \item The output layer will be referred to as layer O.}
%   \begin{figure}
%    \centering
%      \scalebox{1}{\includegraphics{plots/backnet.png}}
%      \tiny{\\Credit:Erik HallstrÃ¶m}
%    \end{figure}
%  \end{itemize}
%\end{frame}
%
%\begin{frame} {Backpropagation}
%  \begin{itemize}
%    \item The derivative of the loss $L$ w.r.t weight  $W_{j,k}^{(i)}$ of neuron $k$ in layer $i$ is,
%    \begin{eqnarray*}
%    \frac{d L}{d W_{j,k}^{(i)}} &=& \frac{d L}{d z_{k,in}^{(i)}} \frac{d z_{k,in}^{(i)}}{d W_{j,k}^{(i)}} = \frac{d L}{d z_{k,out}^{(i)}} \frac{d z_{k,out}^{(i)}}{d z_{k,in}^{(i)}} \frac{d z_{k,in}^{(i)}}{d W_{j,k}^{(i)}} \\
%    &=& \sum_m \Bigg( \frac{d L}{d z_{m,in}^{(i+1)}} \frac{d z_{m,in}^{(i+1)}}{d z_{k,out}^{(i)}} \Bigg) \frac{d z_{k,out}^{(i)}}{d z_{k,in}^{(i)}} \frac{d z_{k,in}^{(i)}}{d W_{j,k}^{(i)}}
%    \end{eqnarray*}
%    \item Note : The sum in the expression above is over all the neurons in layer $i+1$. This is simply an application of the chain rule.
%  \end{itemize}
%\end{frame}
%\begin{frame} {Backpropagation}
%  \begin{itemize}
%    \item We have, 
%      \begin{eqnarray*}
%        \frac{d L}{d W_{j,k}^{(i)}} &=& \sum_m \Bigg( \frac{d L}{d z_{m,in}^{(i+1)}} \frac{d z_{m,in}^{(i+1)}}{d z_{k,out}^{(i)}} \Bigg) \frac{d z_{k,out}^{(i)}}{d z_{k,in}^{(i)}} \frac{d z_{k,in}^{(i)}}{d W_{j,k}^{(i)}}
%      \end{eqnarray*}
%      \item Here,
%        \begin{itemize}
%          \item $z_{k,out}^{(i)} = \sigma(z_{k,in}^{(i)})$ 
%          \item $z_{k,in}^{(i)} = \sum_j W_{j,k}^{(i)}z_{j,out}^{(i-1)}  + b_k^{(i)}$
%          \item $z_{m,in}^{(i+1)} = \sum_k W_{k,m}^{(i+1)}z_{k,out}^{(i)} + b_m^{(i+1)}$
%        \end{itemize}
%      \item Therefore,
%      $$\frac{d L}{d W_{j,k}^{(i)}} = \sum_m \Bigg( \frac{d L}{d z_{m,in}^{(i+1)}} W_{k,m}^{(i+1)} \Bigg) \sigma'(z_{k,in}^{(i)}) z_{j,out}^{(i-1)}$$
%    \end{itemize}
%\end{frame}
%\begin{frame} {Backpropagation}
%  \begin{itemize}
%    \item The "$\delta$", or \textit{error signal},  for a neuron $k$ in layer $i$ will now represent how much the loss $L$ changes when the input to the neuron (that is, the weighted sum $z_{k,in}^{(i)}$) changes:
%    $$ \delta_k^{(i)} = \frac{d L}{d z_{k,in}^{(i)}}$$
%    \item Using the results from the previous slide, this can be rewritten as,
%        $$ \delta_k^{(i)} = \sum_m \Bigg( \frac{d L}{d z_{m,in}^{(i+1)}} W_{k,m}^{(i+1)} \Bigg) \sigma'(z_{k,in}^{(i)})$$ which, in turn, can be rewritten as,
%        $$ \delta_k^{(i)} = \sum_m \Bigg( \delta_m^{(i+1)} W_{k,m}^{(i+1)} \Bigg) \sigma'(z_{k,in}^{(i)})$$
%    \item Therefore, we now have a \textbf{recursive definition} for the error signal of a neuron in layer $i$ in terms of the error signals of the neurons in layer $i+1$ and, by extension, layers \{i+2, i+3 \ldots , O\}!
%  \end{itemize}
%\end{frame}
%\begin{frame} {Backpropagation}
%  \begin{itemize}
%    \item Given the error signal $\delta_k^{(i)}$ of neuron $k$ in layer $i$, the derivative of loss $L$ w.r.t to the weight $W_{j,k}$ is simply :
%        $$
%           \frac{d L}{d W_{j,k}^{(i)}} = \frac{d L}{d z_{k,in}^{(i)}} \frac{d z_{k,in}^{(i)}}{d W_{j,k}^{(i)}} 
%           = \delta_k^{(i)} z_{j,out}^{(i-1)} $$
%        because $z_{k,in}^{(i)} = \sum_j W_{j,k}^{(i)}z_{j,out}^{(i-1)}  + b_k^{(i)}$
%    \item Similarly, the derivative of loss $L$ w.r.t bias $b_k^{(i)}$ is :
%      $$ \frac{d L}{d b_k^{(i)}} = \frac{d L}{d z_{k,in}^{(i)}} \frac{d z_{k,in}^{(i)}}{d b_k^{(i)}} = \delta_k^{(i)}$$
%  \end{itemize}
%\end{frame}
%\begin{frame} {Backpropagation}
%  \begin{itemize}
%    \item We've seen how to compute the error signals for individual neurons. It can be shown that the error signal $\delta^{i}$ for an entire layer $i$ can be computed as follows ($\odot$ = element-wise product):
%      \begin{itemize}
%        \item $\delta^{(O)} = \nabla_{f_{out}}L \odot \tau'(f_{in})$
%        \item $\delta^{(i)} = W^{(i+1)}\delta^{(i+1)} \odot \sigma'(z_{in}^{(i)})$
%      \end{itemize}
%    \item As we've seen earlier, the error signal for a given layer $i$ depends recursively on the error signals of \textit{later} layers \{i+1, i+2, \ldots , O\}.
%    \item Therefore, backpropagation works by computing and storing the error signals \textbf{backwards}. That is, starting at the output layer and ending at the first hidden layer. This way, the error signals of later layers \textit{propagate backwards} to the earlier layers.
%    \item The derivative of the loss $L$ w.r.t a given weight is computed efficiently by plugging in the cached error signals thereby avoiding expensive and redundant computations. 
%  \end{itemize}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Challanges in NN optimization}

\begin{frame} {Effects of curvature}
The curvature affects the outcome of the gradient step\dots
\begin{figure}
\begin{center}
	\includegraphics[width=.75\textwidth]{plots/curvature.png}
\end{center}
\caption{Goodfellow \emph{et al.}, (2016), ch. 4}
\end{figure}
\end{frame}

\begin{vbframe} {Excursion: second derivative and curvature}
\begin{itemize}
\item We can think of the second derivative as measuring curvature.
\item For example for $f:\,\R\to\R$
\begin{align*}
f'(x)=0 \text{ and } f''(x)>0 &\quad\Rightarrow\quad \text{minimum at $x$} \\
f'(x)=0 \text{ and } f''(x)<0 &\quad\Rightarrow\quad \text{maximum at $x$}
\end{align*}
\item For $f:\,\R^n\to\R$ the Hessian matrix $H$
with $H_{ij} = \frac{\partial^2}{\partial x_i \partial x_j} f(x)$
can be seen as the second derivative.
\item The second derivative in a certain direction give by a unit vector $d$
is given by $d^T \!H\,d$.
\item Since $H$ is real and symmetric (why?), eigendecomposition yields
$H = V \diag(\lambda) V^{-1}$
with $V$ and $\lambda$ collecting eigenvectors and eigenvalues, respectively.
\end{itemize}

\framebreak

\begin{itemize}
\item With $i$th eigenvector $v^{(i)}$ we get ${v^{(i)}}^T \!H\, v^{(i)} = \lambda_i$.
\item Therefore the min/max eigenvalue gives the min/max second derivative.
\item Furthermore
\begin{align*}
\lambda_i > 0 \quad\forall i &\quad\Rightarrow\quad \text{minimum at $x$} \\
\lambda_i < 0 \quad\forall i &\quad\Rightarrow\quad \text{maximum at $x$}
\end{align*}
\end{itemize}

\end{vbframe}


\begin{frame} {Effects of curvature}
\begin{itemize}
\item Second-order Taylor approximation (with gradient $g$) around current point $\theta^0$
\begin{equation*}
L(\theta) = L(\theta^0) + (\theta-\theta^0)^T g + \frac{1}{2}(\theta-\theta^0)^T \!H\, (\theta-\theta^0)
\end{equation*}
\item SGD with learning rate $\alpha$ yields new parameters $\theta^0-\alpha g$ and new loss value
$$
L(\theta^0-\alpha g) = L(\theta^0) - \alpha g^Tg + \frac{1}{2}	\alpha^2 g^T\!H\,g  \,.
$$
\item When $g^T H g$ is to large, we might get $L(\theta^0-\alpha g) > L(\theta^0)$.
\item If $g^T H g$ is positive, optimal step size is
$$
\alpha^* = \frac{g^T g}{g^T \!H\, g} \ge \frac{1}{\lambda_{\text{max}}}  \,.
$$
\end{itemize}
\end{frame}

\begin{frame} {Ill-conditioning}
\begin{itemize}
\item If second derivatives differ a lot from each other,
SGD performs poorly.
\item $\alpha$ needs to be small enough for large curvature directions,
resulting in low progress in others.
\end{itemize}

\begin{figure}
\begin{center}
	\includegraphics[width=.5\textwidth]{plots/illconditioning.png}
\end{center}
\caption{Goodfellow \emph{et al.}, (2016), ch. 4}
\end{figure}
\end{frame}


\begin{vbframe} {Local minima}
Weight space symmetry:
\begin{itemize}
\item If we swap incoming weight vectors for neuron $i$ and $j$
and do the same for the outcoming weights,
modelled function stays unchanged.
\begin{itemize}
\item[$\Rightarrow$] with $n$ hidden units and one hidden layer
there are $n!$ networks with the same empirical risk
\end{itemize}
\item If we multiply incoming weights of a ReLU neuron with $\beta$
and outcoming with $1/\beta$ the modeled function stays unchanged.
\begin{itemize}
\item[$\Rightarrow$] The empirical risk of a NN can have uncountable many
minima with equivalent empirical risk.
\end{itemize}
\end{itemize}

\framebreak

\begin{itemize}
\item In practice only local minima with a high value compared to the global minima
are problematic.
\begin{figure}
\begin{center}
\includegraphics[width=.5\textwidth]{plots/minima.png}
\end{center}
\caption{Goodfellow \emph{et al.}, (2016), ch. 4}
\end{figure}
\item Success in applications shows that local minima are often not a problem.
\item Experts suspect that most local minima have low empirical risk.
\item Simple test: norm of gradient should get close to zero.
\end{itemize}




\framebreak

 But
  \begin{itemize}
   \small{%\item There are a few important subtleties about neural network training to bear in mind.
    \item \underline{\#1} : In practice, training may not arrive at a critical point of any kind.
        \begin{figure}
    \centering
      \scalebox{0.62}{\includegraphics{plots/no_critical.png}}
      \tiny{\\Source : Goodfellow}
      \caption{\footnotesize Left : A plot of gradient norms (one norm per epoch, sampled at random) while training a CNN. The solid curve shows a running average of all gradient norms. Right: The classification error rate for the CNN on a validation set }
  \end{figure}
  In the figure above, the gradient norms \textbf{increase} over time. Therefore, the training process is not converging to a critical point even though the validation error reaches a very low level.}
  \end{itemize}

\framebreak

  \begin{itemize}
    \small{\item \underline{\#2} : The loss function can lack a global minimum point (where the gradient norm is 0) and the training loss may instead only asymptotically approach some value.
    \item For example, consider a classifier $p(y|\mathbf{x})$, where $y$ is a discrete set of three possible labels and the probabilities are computed using softmax.
    \item For a single training example $\mathbf{x'}$, if the correct output is $p(y|\mathbf{x'}) = [1,0,0]$, softmax can only asymptotically approach this value.\\
      \begin{align*}
 \text{Input (to softmax)} &\rightarrow \text{Output} \\
 [7.5,6,6]  &\rightarrow    [\textcolor{red}{0.69143845}, 0.15428077, 0.15428077] \\
 [7.5,-1,-1] &\rightarrow   [\textcolor{red}{0.99700214}, 0.00149893, 0.00149893] \\
 [7.5,-14,-14] &\rightarrow [\textcolor{red}{0.99999999}, 4.599e-10, 4.599e-10]
\end{align*}
\item Therefore, if the loss function is the negative log likelihood $-\log p(y|\mathbf{x})$, the training loss can become arbitrarily close to 0 but can never actually reach it.
  }\end{itemize}
\end{vbframe}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Difficult vs easy loss surfaces}
  \begin{figure}
  \centering
    \includegraphics[width=12cm]{plots/difficult_vs_easy.png}
    \caption{Left: difficult loss surface, right: easy loss surface (Hao Li et al. (2017))}
  \end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Example: multimodal function}
Potential snippet from a loss surface of a deep neural network with many local minima:
<<echo=FALSE, fig.height = 4>>=
library(smoof)
library(ggplot2)
library(plot3D)
foo = function(x, y) {
  (x - y)^2 + exp((1 - sin(x))^2) * cos(y) + exp((1 - cos(y))^2) * sin(x)
}
x = y = seq(-10, 10, length = 50)
z = outer(x, y, foo)
p = c(list(list(-100, 100)))
sd_plot(phi = 40, theta = 185, xlab = "x1", ylab = "x2")
@
\end{vbframe}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Flat Regions}
  \begin{itemize}
    \item In optimization we look for areas with zero gradient.
    \item A variant of zero gradient are flat regions such as saddle points.
    \item For the error surface $f$ of a neural network, the expected ratio of the number of saddle points to local minima typically grows exponentially with n 
    $$f: \mathbb{R}^n \rightarrow \mathbb{R}$$ 
    In other words: deeper networks exhibit alot more saddle points than local minima.
    \item The Hessian at a local minimum has only positive eigenvalues. At a saddle point it is a mixture of positive and negative eigenvalues.
    \item Why is that?
\framebreak
    \item Imagine the sign of each eigenvalue is generated by coin flipping:
    \begin{itemize}
      \item In a single dimension, it is easy to obtain a local minimum (e.g. \enquote{head} means positive eigenvalue).
      \item In an $n$-dimensional space, it is exponentially unlikely that all $n$ coin tosses will be head.
    \end{itemize}
    \item A property of many random functions is that eigenvalues of the Hessian become more likely to be positive in regions of lower cost.
    \item For the coin flipping example, this means we are more likely to have heads $n$ times if we are at a critical point with low cost.
    \item That means in particular that local minima are much more likely to have low cost than high cost and critical points with high cost are far more likely to be saddle points.
    \item See Dauphin et al. (2014) for a more detailed investigation.
\framebreak
    \item \enquote{Saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum} (Dauphin et al. (2014)).
  \begin{figure}
    \centering
    \includegraphics[width=10.5cm]{plots/cost.png}
  \end{figure}
  \end{itemize}
\framebreak
\framebreak 
  \begin{center}
    $f(x_1, x_2) = x_1^2 - x_2^2$
  \end{center}
  \begin{figure}
    \centering
    \includegraphics[width=4cm]{plots/saddlepoint.png}
  \end{figure} 
  \begin{itemize}
    \item Along $x_1$, the function curves upwards (eigenvector of the Hessian with positive eigenvalue). Along $x_2$, the function curves downwards (eigenvector of the Hessian with negative eigenvalue).
  \end{itemize}
\framebreak 
  \begin{itemize}
    \item So how do saddle points impair optimization?
    \item First-order algorithms that use only gradient information \textbf{might} get stuck in saddle points.
    \item Second-order algorithms experience even greater problems when dealing with saddle points. Newtons method for example actively searches for a region with zero gradient. That might be another reason why second-order methods have not succeeded in replacing gradient descent for neural network training. 
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{

\frametitle{Example: saddle point with gradient descent}

  \center
  \only<1>{\includegraphics[width=9cm]{plots/opt1.png}}%
  \only<2>{\includegraphics[width=9cm]{plots/opt2.png}}%
  \only<3>{\includegraphics[width=9cm]{plots/opt3.png}}%
  \only<4>{\includegraphics[width=9cm]{plots/opt10.png}}%
  
  \begin{itemize}

    \only<1>{\item[] Red dot: starting location}
    \only<2>{\item[] First step..}
    \only<3>{\item[] ..second step..}
    \only<4>{\item[] ..tenth step got stuck and can't escape the saddle point!}
    
  \end{itemize}

}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Cliffs and exploding gradients}
  \begin{itemize}
    \item As a result from the multiplication of several parameters, the objective function for highly nonlinear deep neural networks (i.e. recurrent nets, chapter 5) often contain sharp nonlinearities.
      \begin{itemize}
        \item That may result in very high derivatives in some places.
        \item As the parameters get close to such a cliff regions, a gradient descent update can catapult the parameters very far.
        \item Such an occurrence can lead to losing most of the optimization work that had been done.
      \end{itemize}
    \item However, serious consequences can be easily avoided using a technique called \textbf{gradient
clipping}.
    \item The gradient does not specify the optimal step size, but only the optimal direction
within an infinitesimal region.
\framebreak 
    \item Gradient clipping simply caps the step size to be small enough that it is less likely to go outside the region where the gradient indicates the direction of steepest descent.
    \item We simply \enquote{prune} the norm of the gradient at some threshold $h$:
    $$\text{if  } ||\nabla \theta|| > \text h: \nabla \theta \leftarrow \frac{h}{||\nabla \theta||} \nabla \theta $$
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Example: cliffs and exploding gradients}
  \begin{figure}
  \centering
    \includegraphics[width=8cm]{plots/cliff2.png}
    \caption{\enquote{The objective function for highly nonlinear deep neural networks or for
recurrent neural networks often contains sharp nonlinearities in parameter space resulting
from the multiplication of several parameters. These nonlinearities give rise to very
high derivatives in some places. When the parameters get close to such a cliff region, a
gradient descent update can catapult the parameters very far, possibly losing most of the
optimization work that had been done} (Goodfellow et al. (2016)).}
  \end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe} {Poor correspondence between local and global structure}
  \begin{itemize}
  \small{
    \item If the training algorithm makes only \textit{locally} optimal moves (as in gradient descent), it may move away from regions of \textit{much} lower cost.
      \begin{figure}
    \centering
      \scalebox{0.65}{\includegraphics{plots/local_hill.png}}
      \tiny{\\Source : Goodfellow}
    \end{figure}
    \item In the figure above, initiliazing the parameter on the "wrong" side of the hill will result in suboptimal performance.}
  \end{itemize}
  
  \framebreak

\begin{itemize}
    \item In higher dimensions, however, it may be possible for gradient descent to go around the hill but such a trajectory might be very long and result in excessive training time.
\end{itemize}
    
	\begin{figure}
	\begin{center}
	\includegraphics[width=.5\textwidth]{plots/mountain.png}
	\end{center}
	\caption{Goodfellow \emph{et al.}, (2016), ch. 8}
	\end{figure}

\end{vbframe}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Basic learning algorithms}

\begin{vbframe}{Momentum}
  \begin{itemize}
    \item Designed to accelerate learning, especially when facing high curvature, small but consistent or noisy gradients.
    \item Momentum accumulates an exponentially decaying moving average of past gradients:
      \begin{eqnarray*} 
        \nu &\leftarrow& \varphi \nu - \alpha \underbrace{\nabla_\theta \Big(\frac{1}{m} \sum_{i} \Lxym \Big)}_{g(\theta)} \\
        \theta &\leftarrow& \theta + \nu
      \end{eqnarray*}
    \item We introduce a new hyperparameter $\varphi \in [0, 1)$, determining how quickly the contribution of previous gradients decay.
    \item $\nu$ is called \enquote{velocity} and derives from a physical analogy describing how particles move through a parameter space (Newton's law of motion).
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Momentum}
  \begin{itemize}
    \item So far the step size was simply the gradient $g$ multiplied by the learning rate $\alpha$.
    \item Now, the step size depends on how \textbf{large} and how \textbf{aligned} a sequence of gradients are.
      \begin{itemize}
        \item The step size grows when many succesive gradients point in the same direction.
      \end{itemize}
    \item Common values for $\varphi$ are $0.5, 0.9$ and even $0.99$.
    \item We can think of momentum as the fraction $\frac{1}{1-\varphi}$
      \begin{itemize}
        \item Then, $\varphi = 0.9$ corresponds to multiplying the maximum speed by 10 relative to
the gradient descent algorithm. 
      \end{itemize}
    \item Generally, the larger $\varphi$ is relative to the learning rate $\alpha$, the more previous gradients affect the current direction.
    \item A very good website with an in-depth analysis of momentum: \url{https://distill.pub/2017/momentum/}
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}
Example:
  \footnotesize 
  \begin{eqnarray*}
    \nu_{1} &\leftarrow& \varphi \nu_0 - \alpha g(\theta^{[0]}) \\
    \theta^{[1]} &\leftarrow& \theta^{[0]} + \varphi \nu_0 - \alpha g(\theta^{[0]}) \\
    \\
    \nu_{2} &\leftarrow& \varphi \nu_1 - \alpha g(\theta^{[1]}) \\
            &=& \varphi (\varphi \nu_0 - \alpha g(\theta^{[0]})) - \alpha g(\theta^{[1]}) \\
    \theta^{[2]} &\leftarrow& \theta^{[1]} + \varphi (\varphi \nu_0 - \alpha g(\theta^{[0]})) - \alpha g(\theta^{[1]}) \\
    \\
    \nu_{3} &\leftarrow& \varphi \nu_2 - \alpha g(\theta^{[2]}) \\
            &=& \varphi (\varphi (\varphi \nu_0 - \alpha g(\theta^{[0]})) - \alpha g(\theta^{[1]})) - \alpha g(\theta^{[2]}) \\
    \theta^{[3]} &\leftarrow& \theta^{[2]} + \varphi (\varphi (\varphi \nu_0 - \alpha g(\theta^{[0]})) - \alpha g(\theta^{[1]})) - \alpha g(\theta^{[2]}) \\
            &=& \theta^{[2]} + \varphi^3\nu_0 - \varphi^2\alpha g(\theta^{[0]}) - \varphi \alpha g(\theta^{[1]}) - \alpha g(\theta^{[2]}) \\
            &=& \theta^{[2]} - \alpha(\varphi^2g(\theta^{[0]}) + \varphi^1g(\theta^{[1]}) + \varphi^0g(\theta^{[2]})) + \varphi^3 \nu_0 \\
    \\
\theta^{[t+1]} &=& \theta^{[t]} - \alpha \displaystyle\sum_{j = 0}^{t} \varphi^j g(\theta^{[t - j]}) + \varphi^{t+1}\nu_0
  \end{eqnarray*}
%\framebreak
Suppose momentum always observes the same gradient $g(\theta)$:
  \footnotesize 
  \begin{eqnarray*}
    \theta^{[t+1]} &=& \theta^{[t]} - \alpha \displaystyle\sum_{j = 0}^{t} \varphi^j g(\theta^{[j]}) + \varphi^{t+1}\nu_0 \\
                 &=& \theta^{[t]} - \alpha g(\theta) \displaystyle\sum_{j = 0}^{t} \varphi^j + \varphi^{t+1}\nu_0 \\
                 &=& \theta^{[t]} - \alpha g(\theta) \frac{1 - \varphi^{t+1}}{1 - \varphi} + \varphi^t \nu_0 \\
                 \lim_{t \to \infty} &=& \theta^{[t]} - \alpha g(\theta) \frac{1}{1 - \varphi}
  \end{eqnarray*}

Thus, momentum will accelerate in the direction of $-g(\theta)$ until reaching terminal velocity with step size: 
    $$-\alpha g(\theta)(1 + \varphi + \varphi^2 + \varphi^3 + ...) = -\alpha g(\theta) \frac{1}{1 - \varphi}$$
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame} {Momentum : Illustration}
     The vector $\nu_{3}$ (for $ \nu_0 = 0$) :  \begin{eqnarray*} \nu_{3} &=& \varphi (\varphi (\varphi \nu_0 - \alpha g(\theta^{[0]})) - \alpha g(\theta^{[1]})) - \alpha g(\theta^{[2]}) \\ &=& - \varphi^2 (\alpha g(\theta^{[0]})) - \varphi (\alpha g(\theta^{[1]})) - \alpha g(\theta^{[2]}) \end{eqnarray*}
  \begin{figure}
    \centering
      \scalebox{0.90}{\includegraphics{plots/moment_vis.png}}
      \caption{\footnotesize{If consecutive (negative) gradients point mostly in the same direction, the velocity "builds up". On the other hand, if consecutive (negative) gradients point in very different directions, the velocity "dies down".}}
    \end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\frame{

\frametitle{Momentum: physical explanation}

  \center
  \only<1>{\includegraphics[height = 7.5 cm, width = 7.5 cm]{plots/momentum_physics_0.png}}%
  \only<2>{\includegraphics[height = 7.5 cm, width = 7.5 cm]{plots/momentum_physics_1.png}}%
  \only<3>{\includegraphics[height = 7.5 cm, width = 7.5 cm]{plots/momentum_physics_2.png}}%
  \only<4>{\includegraphics[height = 7.5 cm, width = 7.5 cm]{plots/momentum_physics_3.png}}%
  \only<5>{\includegraphics[height = 7.5 cm, width = 7.5 cm]{plots/momentum_physics_4.png}}%
  \only<6>{\includegraphics[height = 7.5 cm, width = 7.5 cm]{plots/momentum_physics_5.png}}%

}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Gradient descent vs momentum}
  \begin{figure}
  \centering
    \includegraphics[height = 6 cm, width = 10 cm]{plots/gd_vs_momentum.png}
    \caption{Gradient descent (left) needs many steps \textbf{and} gets stuck in a local minimum Momentum (right) finds the global minimum}
  \end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{SGD with momentum}
  \begin{algorithm}[H]
    \caption{Stochastic gradient descent with momentum}
    \begin{algorithmic}[1]
    \State \textbf{require} learning rate $\alpha$ and momentum $\varphi$ \strut
    \State \textbf{require} initial parameter $\theta$ and initial velocity $\nu$ \strut
      \While{stopping criterion not met}
        \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Sample a minibatch of $m$ examples from the training set $\{\tilde{x}^{(1)},\dots,\tilde{x}^{(m)}\}$ \strut}
        \State Compute gradient estimate: $\hat{g} \leftarrow + \frac{1}{m} \nabla_\theta \sum_{i} \Lxym$
        \State Compute velocity update: $\nu \leftarrow \varphi \nu - \alpha \hat{g}$
        \State Apply update: $\theta \leftarrow \theta + \nu$
      \EndWhile
    \end{algorithmic}
  \end{algorithm}
\framebreak
  \begin{figure}
  \centering
    \includegraphics[height = 6 cm, width = 10 cm]{plots/momentum.png}
    \caption{The contour lines show a quadratic loss function with a poorly conditioned Hessian matrix. The two curves show how standard gradient descent (black) and momentum (red) learn when dealing with ravines.}
  \end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Momentum in practice}
  \begin{itemize}
    \item Lets try out different values of momentum (with SGD) on the MNIST data.
    \item We apply the same architecture we've used a dozen of times already (note that we used $\varphi = 0.9$ in all computations so far, i.e. in chapter 1 and 2)!
  \end{itemize}
<<mxnet1, size = "small", cache = TRUE, eval = FALSE, echo = TRUE>>=
fc1 = mx.symbol.FullyConnected(data, num_hidden = 512)
act1 = mx.symbol.Activation(fc1, activation = "relu")
fc2 = mx.symbol.FullyConnected(act1, num_hidden = 512)
act2 = mx.symbol.Activation(fc2, activation = "relu")
fc3 = mx.symbol.FullyConnected(act2, num_hidden = 512)
act3 = mx.symbol.Activation(fc3, activation = "relu")
fc4 = mx.symbol.FullyConnected(act3, num_hidden = 10)
softmax = mx.symbol.SoftmaxOutput(fc4, name = "sm")
@
\framebreak
  \begin{figure}
  \centering
    \includegraphics[width=12cm]{plots/momentum_train.png}
  \end{figure}
\framebreak
  \begin{figure}
  \centering
    \includegraphics[width=12cm]{plots/momentum_test.png}
  \end{figure}
\end{vbframe}
\begin{vbframe}{Nestorov momentum}
  \begin{itemize}
    \item Momentum aims to solve poor conditioning of the hessian but also variance in the stochastic gradient.
    \item Nesterov momentum modifies the algorithm such that the gradient is evaluated after the current velocity is applied:
      \begin{eqnarray*} 
        \nu &\leftarrow& \varphi \nu - \alpha \nabla_\theta \Big[\frac{1}{m} \sum_{i} \Lmomentum \Big] \\
        \theta &\leftarrow& \theta + \nu
      \end{eqnarray*}
    \item We can interpret Nesterov momentum as an attempt to add a correction factor to the basic method.
  \end{itemize}
\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Learning rates}

\begin{frame} {Learning Rate}
  \begin{itemize}
    \item The learning rate is a very important hyperparameter.
    \item To systematically find a good learning rate, we can start at a very low learning-rate and gradually increase it (linearly or exponentially) after each mini-batch.
    \item We can then then plot the learning rate and the training loss for each batch.
    \item A good learning rate is one that results in a steep decline in the loss.
    \begin{figure}
    \centering
      \scalebox{0.7}{\includegraphics{plots/lr_jh.png}}
      \tiny{\\credit:jeremyjordan}
    \end{figure}
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% the code below produces the same results but looks alot worse since 
% knitr is very stupid and makes plots jumping around

%\begin{vbframe}{Example: saddle point}
%   \begin{center}
%     $f(x_1, x_2) = x_1^2 - x_2^2$
% <<opt1, size = "small", echo=FALSE, fig.width=6, fig.height=3.5, eval = TRUE>>=
% foo = function(x, y){
%   x^2 - y^2
% }
% x = y = seq(-1.6, 1.6, length = 30)
% z = outer(x, y, foo)
% p = c(list(list(1, 0)), optim0(.2, 0, FUN = foo, maximum = FALSE, maxit = 1))
% #p = c(list(list(-100, 100)))
% sd_plot(phi = 40, theta = 320, xlab = "x1", ylab = "x2")
% @
%     Starting point
%   \end{center}
% \framebreak
%   \begin{center}
%     $f(x_1, x_2) = x_1^2 - x_2^2$
% <<opt2, size = "small", echo=FALSE, fig.width=6, fig.height=3.5, eval = TRUE>>=
% foo = function(x, y){
%   x^2 - y^2
% }
% x = y = seq(-1.6, 1.6, length = 30)
% z = outer(x, y, foo)
% p = c(list(list(1, 0)), optim0(.2, 0, FUN = foo, maximum = FALSE, maxit = 2))
% #p = c(list(list(-100, 100)))
% sd_plot(phi = 40, theta = 320, xlab = "x1", ylab = "x2")
% @
%     First step
%   \end{center}
% \framebreak
%   \begin{center}
%     $f(x_1, x_2) = x_1^2 - x_2^2$  
% <<opt3, echo=FALSE, fig.width=8, fig.height=4>>=
% foo = function(x, y){
%   x^2 - y^2
% }
% x = y = seq(-1.6, 1.6, length = 30)
% z = outer(x, y, foo)
% p = c(list(list(1, 0)), optim0(.2, 0, FUN = foo, maximum = FALSE, maxit = 3))
% #p = c(list(list(-100, 100)))
% sd_plot(phi = 40, theta = 320, xlab = "x1", ylab = "x2")
% @
%     Second step
%   \end{center}
% \framebreak
%   \begin{center}
%     $f(x_1, x_2) = x_1^2 - x_2^2$
% <<opt4, echo=FALSE, fig.width=8, fig.height=4>>=
% foo = function(x, y){
%   x^2 - y^2
% }
% x = y = seq(-1.6, 1.6, length = 30)
% z = outer(x, y, foo)
% p = c(list(list(1, 0)), optim0(.2, 0, FUN = foo, maximum = FALSE, maxit = 10))
% #p = c(list(list(-100, 100)))
% sd_plot(phi = 40, theta = 320, xlab = "x1", ylab = "x2")
% @
%     Tenth step
%   \end{center}
%\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{Local minima}
%   \begin{itemize}
%     \item Convex optimization problems can be reduced to \enquote{finding a local minimum}.
%       \begin{itemize}
%         \item Any local minimum is a global minimum!
%       \end{itemize}
%     \item In neural networks we have to deal with non-convex problems.
%       \begin{itemize}
%         \item Generally, we have many saddle points!
%         \item Nearly any deep network is guaranteed to have an extremely large amount of saddle points.
%       \end{itemize}
%     \item In practice, nearly all difficulty with neural network optimization arises from local minima.
%       \begin{itemize}
%         \item To test if stuck in a local minina one could plot the norm of the gradient over time.
%         \item If it does not shrink to insignificant size, the problem is neither local minima nor any other kind of critical point.
%       \end{itemize}
%   \end{itemize}
% \end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Learning Rate Schedule}
  \begin{itemize}
    \item We would like to force convergence until reaching a local minimum.
    \item Applying SGD, we have to decrease the learning rate over time, thus $\alpha^{[t]}$ (learning rate at training iteration $t$).
      \begin{itemize}
        \item The estimator $\hat{g}$ is computed of small batches.
        \item Random sampling $m$ training samples introduces noise, that does not vanish even if we find a minimum.
      \end{itemize}
    \item In practice, a common strategy is to decay the learning rate linearly over time until iteration $\tau$:
    
    
%    \begin{eqnarray*}
%      \alpha^{[t]} &=& (1 - \epsilon)\alpha^{[0]} + \epsilon\alpha^{[\tau]} \text{ \ \ \ \ \ \ \ \ \ \ \ \ with }  \epsilon = \frac{t}{\tau} \text{ and } \alpha^{[t+1]} \text{ const.} \\
%               &=& t(-\frac{\alpha^{[0]} +  \alpha^{[\tau]}}{\tau}) + \alpha^{[0]}
%    \end{eqnarray*}

    \begin{align*}
      \alpha^{[t]} &= (1 - \frac{t}{\tau})\alpha^{[0]} + \frac{t}{\tau}\alpha^{[\tau]}
                    = t(-\frac{\alpha^{[0]} +  \alpha^{[\tau]}}{\tau}) + \alpha^{[0]}
      \quad\text{for $t\le\tau$}\\
      \alpha^{[t]} &= \alpha^{[\tau]} \quad\text{for $t>\tau$}
    \end{align*}
    
    
    
  \end{itemize}
\framebreak
\scriptsize Example for $\tau = 4$:
\begin{minipage}{\linewidth}
\centering
  \begin{center}
        {\footnotesize
        \begin{tabular}{l | c | c}
        iteration &  $\epsilon$ & $\alpha^{[t]}$ \\ \hline
        \multicolumn{1}{c|}{$1$}       
        & $0.25$  
        & $(1-\frac{1}{4})\alpha^{[0]} + \frac{1}{4}\alpha^{[\tau]} = \frac{3}{4}\alpha^{[0]} + \frac{1}{4}\alpha^{[\tau]}$\\
        
        \multicolumn{1}{c|}{$2$}       
        & $0.5$   
        & $\frac{2}{4}\alpha^{[0]} + \frac{2}{4}\alpha^{[\tau]}$ \\
        
        \multicolumn{1}{c|}{$3$}       
        & $0.75$  
        & $\frac{1}{4}\alpha^{[0]} + \frac{3}{4}\alpha^{[\tau]}$ \\
        
        \multicolumn{1}{c|}{$4$}       
        & $1$     
        & 0 + $\alpha^{[\tau]}$ \\
        
        \multicolumn{1}{c|}{$...$}       
        &      
        & $\alpha^{[\tau]}$ \\
        
        \multicolumn{1}{c|}{$t+1$}     
        & 
        & $\alpha^{[\tau]} $
        \end{tabular}
        }
  \end{center}
\end{minipage}
  \begin{figure}
  \centering
    \scalebox{0.95}{\includegraphics[width=8cm]{plots/weight_decayy.png}}
  \end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {Cyclical Learning Rates}
  \begin{itemize}
    \item Another option is to have a learning rate that periodically varies between two values according to some cyclic function.
    \item Therefore, if training doesn't improve the loss anymore (possibly due to saddle points), increasing the learning rate makes it possible to rapidly traverse such regions.
    \item Recall, saddle points are far more likely than local minima in deep nets.
    \item Each cycle has a fixed length in terms of the number of iterations. 
  \end{itemize}
\end{frame}
\begin{frame} {Cyclical Learning Rates}
  \begin{itemize}
    \item One such cyclical function is the "triangular" function.
    \begin{figure}
      \centering
        \scalebox{1}{\includegraphics{plots/cyc_triangular.png}}
        \tiny{\\credit:Hafidz Zulkifli}
    \end{figure}
    \item In the right image, the range is cut in half after each cycle.
  \end{itemize}
\end{frame}
\begin{frame} {Cyclical Learning Rates}
  \begin{itemize}
    \item Yet another option is to abruptly "restart" the learning rate after a fixed number of iterations. 
    \item Loshchilov et al. (2016) proposed  "cosine annealing" (between restarts).
    \begin{figure}
      \centering
        \scalebox{0.6}{\includegraphics{plots/cyc_cosine.png}}
        \tiny{\\credit:Hafidz Zulkifli}
    \end{figure}
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Algorithms with adaptive learning rates}

\begin{vbframe}{Adaptive learning rates}
  \begin{itemize}
    \item Learning rates are reliably one of the hyperparameters that is the most difficult to set because it has a significant impact on the models performance.
    \item Naturally, it might make sense to use a different learning rate for each parameter, and automaticaly adapt them throughout the training process.
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Adagrad}
  \begin{itemize}
    \item Adagrad adapts the learning rate to the parameters.
    \item In fact, adagrad scales learning rates inversely proportional to the square root of the sum of all of their historical squared values.
      \begin{itemize}
        \item Parameters with large partial derivatives of the loss obtain a rapid decrease in their learning rate.
        \item Parameters with small partial derivatives on the other hand obtain a relatively small decrease in their learning rate.
      \end{itemize}
    \item For that reason, Adagrad might be well suited when dealing with sparse data. 
    \item Goodfellow et al. (2016) say that the accumulation of squared gradients can result in a premature and overly decrease in the learning rate.
  \end{itemize}
\framebreak
  \begin{algorithm}[H]
    \small
    \caption{Adagrad}
    \begin{algorithmic}[1]
    \scriptsize 
    \State \textbf{require} Global learning rate $\alpha$ \strut
    \State \textbf{require} initial parameter $\theta$ \strut
    \State \textbf{require} Small constant $\beta$, perhaps $10^{-7}$, for numerical stability \strut
    \State \textbf{Initialize} gradient accumulation variable $r = 0$
       \While{stopping criterion not met}
         \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Sample a minibatch of $m$ examples from the training set $\{\tilde{x}^{(1)},\dots,\tilde{x}^{(m)}\}$ \strut}
         \State Compute gradient estimate: $\hat{g} \leftarrow \frac{1}{m} \nabla_\theta \sum_{i} \Lxym$
         \State Accumulate squared gradient $r \leftarrow r + g \odot  g$
         \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Compute update: $\nabla \theta = - \frac{\alpha}{\beta + \sqrt(r)} \odot g$ (division and square root applied element-wise) \strut}
         \State Apply update: $\theta \leftarrow \theta + \nabla\theta$
       \EndWhile
    \end{algorithmic}
  \end{algorithm}
  \begin{itemize}
    \item \enquote{$\odot$} is called Hadamard or element-wise product.
    \item Example:
    \vspace{0.2cm}
    \item[] $A =
            \begin{bmatrix}
              1 & 2 \\
              3 & 4
            \end{bmatrix}, \ 
            B =
            \begin{bmatrix}
              5 & 6 \\
              7 & 8
            \end{bmatrix}, \ \text{ then } A \odot B =
            \begin{bmatrix}
              1 \cdot 5 & 2 \cdot 6 \\
              3 \cdot 7 & 4 \cdot 8
            \end{bmatrix}$
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{RMSProp}
  \begin{itemize}
    \item RMSprop is a modification of Adagrad.
    \item It's intention is to resolve Adagrad's radically diminishing learning rates.
    \item The gradient accumulation is replaced by an exponentially weighted moving average.
    \item Theoretically, that leads to performance gains in non-convex scenarios.
    \item Empirically, RMSProp is a very effective optimization algorithm. Particularly, it is employed routinely by deep learning practitioners (Goodfellow et al. (2016)).
  \end{itemize}
\framebreak
  \begin{algorithm}[H]
    \small
    \caption{RMSProp}
    \begin{algorithmic}[1]
    \State \textbf{require} Global learning rate $\alpha$ and decay rate $\rho$ \strut
    \State \textbf{require} initial parameter $\theta$ \strut
    \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{\textbf{require} Small constant $\beta$, perhaps $10^{-6}$, to stabilize division by small numbers \strut}
    \State Initialize gradient accumulation variable $r = 0$
      \While{stopping criterion not met}
        \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Sample a minibatch of $m$ examples from the training set $\{\tilde{x}^{(1)},\dots,\tilde{x}^{(m)}\}$ \strut}
        \State Compute gradient estimate: $\hat{g} \leftarrow \frac{1}{m} \nabla_\theta \sum_{i} \Lxym$
        \State Accumulate squared gradient $r \leftarrow \rho r + (1 - \rho) g \odot  g$
        \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Compute update: $\nabla\theta = - \frac{\alpha}{\beta + \sqrt(r)} \odot g$ \strut}
        \State Apply update: $\theta \leftarrow \theta + \nabla\theta$
      \EndWhile
    \end{algorithmic}
  \end{algorithm}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Adam}
  \begin{itemize}
    \item Adaptive Moment Estimationis another method that computes adaptive learning rates for each parameter.
    \item Adam uses the first and the second moments of the gradients.
      \begin{itemize}
        \item Adam keeps an exponentially decaying average of past gradients (first moment)
        \item Like RMSProp it stores an exponentially decaying average of past squared gradient (second moment)
        \item Thus, it can be seen as a combination of RMSProp and momentum.
      \end{itemize}
    \item Basically Adam uses the combined averages of previous gradients at different moments to give it more \enquote{persuasive power} to adaptively update the parameters.
  \end{itemize}
\framebreak
\begin{algorithm}[H]
  \scriptsize 
  \caption{Adam}
  \begin{algorithmic}[1]
  \State \textbf{require:} Step size $\alpha$ (suggested default: 0.001) \strut
  \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{\textbf{require:} Exponential decay rates for moment estimates, $\rho_1$ and $\rho_2$ in $[0,1)$ (Suggested defaults: 0.9 and 0.999 respectively)} \strut
  \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{\textbf{require:} Small constant $\beta$ (Suggested default $10^{-8}$) \strut}
  \State \textbf{require:} Initial parameters $\theta$ 
  \State Initialize 1st and 2nd moment variables $s = 0, r = 0$
  \State Initialize time step $t = 0$
    \While{stopping criterion not met}
      \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Sample a minibatch of $m$ examples from the training set $\{\tilde{x}^{(1)},\dots,\tilde{x}^{(m)}\}$ \strut}
      \State Compute gradient estimate: $\hat{g} \leftarrow \frac{1}{m} \nabla_\theta \sum_{i} \Lxym$
      \State $t \leftarrow t + 1$
      \State Update biased first moment estimate: $s \leftarrow \rho_1 s + (1 - \rho_1) g$
      \State Update biased second moment estimate: $r \leftarrow \rho_2 r + (1 - \rho_2) g \odot g$
      \State Correct bias in first moment: $\hat{s} \leftarrow \frac{s}{1-\rho_1^t}$
      \State Correct bias in second moment: $\hat{r} \leftarrow \frac{r}{1-\rho_2^t}$
      \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Compute update: $\nabla\theta = - \alpha \frac{\hat{s}}{\sqrt{\hat{r}} + \beta}$ \strut}
      \State Apply update: $\theta \leftarrow \theta + \nabla\theta$
    \EndWhile
  \end{algorithmic}
\end{algorithm}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{SGD vs. Adaptive Learning Rates}
  \begin{itemize}
    \item Let us apply our standard architecture on the mnist problem and try our different optimizers.
    \item These are:
<<mxnet2, size = "scriptsize", cache = TRUE, eval = FALSE, echo = TRUE>>=
mx.opt.sgd(wd = 0.001, learning.rate = 0.03, 
  momentum = 0.9)

mx.opt.adagrad(wd = 0.001, learning.rate = 0.05, 
  epsilon = 1e-08)

mx.opt.rmsprop(wd =  0.001, learning.rate = 0.002, 
  gamma1 = 0.95, gamma2 = 0.9)

mx.opt.adam(wd = 0.001, learning.rate = 0.001,
  beta1 = 0.9, beta2 = 0.999, epsilon = 1e-08)
@
  \end{itemize}
\framebreak
  \begin{figure}
  \centering
    \includegraphics[width=12cm]{plots/optimizerTrain.png}
  \end{figure}
\framebreak
  \begin{figure}
  \centering
    \includegraphics[width=12cm]{plots/optimizerTest.png}
  \end{figure}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{Batch normalization}
% \textbf{Motivation:}
% 
% As shown earlier, neural networks learn a nonlinear transformation of the input space such that in the last layer(s), a simple classification is sufficient to seperate our data well.
% To do so, especially in deep networks, we need to coordinate updates between the layers.
% Batch normalization forces the model to learn a nonlinear transformation in a layer by removing changes in mean and standard deviation from the layers output.
% 
% \framebreak
%   \begin{itemize}
%     \item Batch Normalization is no algorithm, but rather a technique to improve optimization in certain situations.
%     \item It is an extra component that can be placed between each layer of the neural network.
%     \item That component takes the activation of the antecedent layer and normalizes it before sending it to the next \enquote{actual layer}.
%     $$ H' = \frac{H-\mu}{\sigma}$$
%     with $H$ being the activated minibatch of the previous layer, $\mu$ the mean and $\sigma$ the standard deviation of each unit.
%   \end{itemize}
% \framebreak
%   \begin{itemize}
%     \item For our final benchmark in this chapter we compute two models to predict the mnist data.
%     \item One will extend our basic architecture such that we add batch normalization to all hidden layers.
%     \item We use SGD as optimizer with $momentum = 0.9$, $learning.rate = 0.03$ and $wd = 0.001$.
%     \item []
% <<mxnet3, size = "scriptsize", cache = TRUE, eval = FALSE, echo = TRUE>>=
% fc1 = mx.symbol.FullyConnected(data, num_hidden = 512)
% act1 = mx.symbol.Activation(fc1, act_type = "relu")
% bn1 = mx.symbol.BatchNorm(act1)
% fc2 = mx.symbol.FullyConnected(bn1, num_hidden = 512)
% act2 = mx.symbol.Activation(fc2, act_type = "relu")
% bn2 = mx.symbol.BatchNorm(act2)
% fc3 = mx.symbol.FullyConnected(bn2, num_hidden = 512)
% act3 = mx.symbol.Activation(fc3, act_type = "relu")
% bn3 = mx.symbol.BatchNorm(act3)
% fc4 = mx.symbol.FullyConnected(bn3, num_hidden = 10)
% softmax = mx.symbol.SoftmaxOutput(fc4, name = "sm")
% @
%   \end{itemize}
% \framebreak
%   \begin{figure}
%   \centering
%     \includegraphics[width=12cm]{plots/bnTrain.png}
%   \end{figure}
% \framebreak
%   \begin{figure}
%   \centering
%     \includegraphics[width=12cm]{plots/bnTest.png}
%   \end{figure}
% \end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Batch normalization}

\begin{frame} {Batch Normalization}
  \begin{itemize}
    \item BatchNorm is an extremely popular technique that improves the training speed and stability of deep neural nets.
    \item It is an extra component that can be placed between each layer of the neural network.
    \item It works by changing the "distribution" of activations at each hidden layer of the network.
    \item We know that it is sometimes beneficial to normalize the inputs to a learning algorithm by shifting and scaling all the features so that they have 0 mean and unit variance.
    \item Analogously, BatchNorm applies a similar transformation to the activations of the hidden layers (with a couple of additional tricks).
  \end{itemize}
\end{frame}

\begin{frame} {Batch Normalization}
  \begin{itemize}
    \item For a hidden layer with $m$ neurons ($z_1, \dots, z_m$), BatchNorm is applied to each $z_j$ by considering the activations of $z_j$ \textbf{over a given minibatch} of inputs.
    \item Let $z_j^{(i)}$ denote the activation of $z_j$ for input $x^{(i)}$ in the minibatch (of size N).
    \item The mean and variance of the activations are
      \begin{equation*}
          \begin{array}{l}
            \mu_j = \frac{1}{N} \sum \limits_i \limits^N z_j^{(i)} \\
            \sigma^2_j = \frac{1}{N} \sum \limits_i \limits^N (z_j^{(i)} - \mu_j)^2
          \end{array}
      \end{equation*}
    \item Each $z_j^{(i)}$ is then normalized 
        \begin{equation*}
          \tilde z_j^{(i)} = \frac {z_j^{(i)} - \mu_j}{\sqrt{\sigma^2_j + \epsilon}}
        \end{equation*}
        where a small constant, $\epsilon$, is added for numerical stability.
  \end{itemize}
\end{frame}

\begin{frame} {Batch Normalization}
  \begin{itemize}
    \item It may not be desirable to normalize the activations in such a rigid way because potentially useful information can be lost in the process.
    \item Therefore, we commonly let the training algorithm decide the "right amount" of normalization by allowing it to re-shift and re-scale $\tilde z_j^{(i)}$ to arrive at the batch normalized activation $\hat z_j^{(i)}$ :
        \begin{equation*}
          \hat z_j^{(i)} = \gamma_j \tilde z_j^{(i)} + \beta_j
        \end{equation*}
    \item $\gamma_j$ and $\beta_j$ are learnable parameters that are also tweaked by backpropogation.
    \item $\hat z_j^{(i)}$ then becomes the input to the next layer.
    \item Note : the algorithm is free to scale and shift each $\tilde z_j^{(i)}$ back to its original (unnormalized) value.
  \end{itemize}
\end{frame}

\begin{frame} {Batch Normalization : Illustration}
  \begin{itemize}
    \item Recall: $z_j = \sigma(W_j^T x + b_j)$
    \item So far, we've applied batch-norm to the activation $z_j$. It is possible (and more common) to apply batch norm to ($W_j^Tx + b_j$) \textit{before} passing it to the non-linearity $\sigma$.
  \end{itemize}
    \begin{figure}
    \centering
      \scalebox{0.32}{\includegraphics{plots/batchy.png}}
      \caption{\footnotesize FC = Fully Connected layer. BatchNorm is applied \textit{before} the nonlinear activation function.}
  \end{figure}
\end {frame}


\begin{frame} {Batch Normalization}
  \begin{itemize}
    \item The key impact of BatchNorm on the training process is this : it reparametrizes the underlying optimization problem to \textbf{make its landscape significantly more smooth}.
    \item One aspect of this is that the loss changes at a smaller rate and the magnitudes of the gradients are also smaller (see Santurkar et al. 2018).
  \end{itemize}
  \begin{figure}
    \centering
      \scalebox{0.6}{\includegraphics{plots/bn_effect.png}}
  \end{figure}
\end{frame}

\begin{frame} {Batch Normalization : Inference}
  \begin{itemize}
    \item Once the network has been trained, how can we generate a prediction for a single input (either at test time or in production)?
    \item One option is to feed the entire training set to the (trained) network and compute the means and standard deviations.
    \item More commonly, during training, an exponentially weighted running average of each of these statistics over the minibatches is maintained.
    \item The learned $\gamma$ and $\beta$ parameters are then used (in conjunction with the running averages) to generate the output.
  \end{itemize}
\end{frame}


\begin{vbframe} {Batch Normalization}
\begin{itemize}
    \item For our final benchmark in this chapter we compute two models to predict the mnist data.
    \item One will extend our basic architecture such that we add batch normalization to all hidden layers.
    \item We use SGD as optimizer with $momentum = 0.9$, $learning.rate = 0.03$ and $wd = 0.001$.
    \item []
<<mxnet3, size = "scriptsize", cache = TRUE, eval = FALSE, echo = TRUE>>=
fc1 = mx.symbol.FullyConnected(data, num_hidden = 512)
act1 = mx.symbol.Activation(fc1, act_type = "relu")
bn1 = mx.symbol.BatchNorm(act1)
fc2 = mx.symbol.FullyConnected(bn1, num_hidden = 512)
act2 = mx.symbol.Activation(fc2, act_type = "relu")
bn2 = mx.symbol.BatchNorm(act2)
fc3 = mx.symbol.FullyConnected(bn2, num_hidden = 512)
act3 = mx.symbol.Activation(fc3, act_type = "relu")
bn3 = mx.symbol.BatchNorm(act3)
fc4 = mx.symbol.FullyConnected(bn3, num_hidden = 10)
softmax = mx.symbol.SoftmaxOutput(fc4, name = "sm")
@
  \end{itemize}
\framebreak
  \begin{figure}
  \centering
    \includegraphics[width=12cm]{plots/bnTrain.png}
  \end{figure}
\framebreak
  \begin{figure}
  \centering
    \includegraphics[width=12cm]{plots/bnTest.png}
  \end{figure}

\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{vbframe}{Second order optimization}
%   \begin{itemize}
%     \item Second order optimization methods include information about the curvature but require computing the hessian matrix.
%       \begin{itemize}
%         \item Very expensive for large networks!
%         \item Think of the network we applied in the regularization chapter:
%       \end{itemize}
%     \begin{figure}
%       \centering
%         \includegraphics[width=8cm]{plots/mxnet_regulariation_1.png}
%     \end{figure}
%     \item This model has a huge amount of parameters:
%     \begin{eqnarray*}
%       &=& \underbrace{784 \cdot 512}_{\text{input to 1st layer}} + \underbrace{512^2}_{\text{1st to 2nd layer}} + \underbrace{512^2}_{\text{2nd to 3rd layer}} + \underbrace{512 \cdot 10}_{\text{3rd to output}} \\
%       &=& 3.285.504
%     \end{eqnarray*}
%   \end{itemize}
% \framebreak
%   \begin{itemize}
%     \item A first order method would need $3.285.504$ partial derivatives to compute the gradients.
%     \item Second order methods on the other hand require $$3.285.504^2 = 10.794.536.534.016$$ partial derivatives!
%     \item For small problems/networks one might consider the BFGS (Broyden-Fletcher-Goldfarb-Shanno) algorithm, which computes an approximation of the hessian (still expensive!).
%       \begin{itemize}
%         \item Very difficult to implement, alot of programming know-how necessary to create an efficient implementation.
%       \end{itemize}
%     \item For huge networks even gradient descent becomes too expensive.
%       \begin{itemize}
%         \item Solution: parameters are instead grouped into mini-batches (stochastic gradient descent)
%       \end{itemize}
%   \end{itemize}
% \end{vbframe}



\section{Hardware and Software for Deep Learning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame} {Hardware for Deep Learning}

  \begin{itemize}
    \item Deep neural networks require special hardware to be trained efficiently.
    \item The training is done using \textbf{G}raphics \textbf{P}rocessing \textbf{U}nits (GPUs) and a special programming language called CUDA.
    \item Training on standard CPUs takes a very long time and is infeasible for anything but toy examples.
  \end{itemize}
\begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/cpu_gpu.png}}
      \caption{\textit{Left:} Each CPU can do 2-8 parallel computations. \textit{Right:} A single GPU can do thousands of simple parallel computations.}
  \end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame} {Software for Deep Learning}
  \begin{itemize}
    \vspace{15mm}
    \item CUDA is a very \textit{low level} programming language and thus writing code for deep learning requires a lot of work.
    \vspace{5mm}
    \item Deep learning (software) frameworks, like Tensorflow and Pytorch, abstract CUDA and provide additional functionality.
  \end{itemize}
\end{frame}
\begin{frame} {Software for Deep Learning}
  \begin{wrapfigure}{R}{0.3\textwidth}
    \centering
      \scalebox{0.3}{\includegraphics{plots/tflow.png}}
  \end{wrapfigure}
\textbf{Tensorflow}
\begin{itemize}
  \item Currently the most popular framework
  \item Supported by Google
  \item Python, R, C++ and Javascript APIs
  \item Distributed training on GPUs and TPUs
  \item Tools for visualizing neural nets, running them efficiently on phones and embedded devices.
\end{itemize}

\begin{wrapfigure}{R}{0.3\textwidth}
    \centering
      \scalebox{0.3}{\includegraphics{plots/keras.png}}
  \end{wrapfigure}
\textbf{Keras}
  \begin{itemize}
    \item Intuitive, high-level wrapper for rapid prototyping
    \item Capable of running on Tensorflow, CNTK or Theano backends
    \item Python and R APIs
  \end{itemize}
\end{frame}

\begin{frame} {Software for Deep Learning}
  \begin{wrapfigure}{R}{0.3\textwidth}
    \centering
      \scalebox{0.3}{\includegraphics{plots/pytorch.png}}
  \end{wrapfigure}

\textbf{Pytorch}
  \begin{itemize}
    \item Extremely popular framework that is suitable for both research and production
    \item Supported by Facebook
    \item Python and C++ APIs
    \item Distributed training on GPUs
  \end{itemize}

  \begin{wrapfigure}{R}{0.3\textwidth}
    \centering
      \scalebox{0.3}{\includegraphics{plots/mxnet.png}}
  \end{wrapfigure}
  \vspace{3mm}
  \textbf{MXNet}
    \begin{itemize}
      \item Open-source deep learning framework written in C++ and cuda (used by Amazon for their Amazon Web Services) 
      \item Scalable, allowing fast model training
      \item Supports flexible model programming and multiple languages (C++, Python, Julia, Matlab, JavaScript, Go, \textbf{R}, Scala, Perl)
    \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}
\frametitle{References}
\footnotesize{
\begin{thebibliography}{99}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Ian Goodfellow et al., 2016]{1} Ian Goodfellow, Yoshua Bengio and Aaron Courville (2016)
\newblock Deep Learning
\newblock \emph{\url{http://www.deeplearningbook.org/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Yann Dauphin et al., 2014]{2} Yann Dauphin, Razvan Pascanu, {\c{C}}aglar G{\"{u}}l{\c{c}}ehre, Kyunghyun Cho, Surya Ganguli, Yoshua Bengio (2014)
\newblock Identifying and attacking the saddle point problem in high-dimensional non-convex optimization
\newblock \emph{\url{https://arxiv.org/abs/1406.2572}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Hao Li et al., 2017]{2} Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, Tom Goldstein (2017)
\newblock Visualizing the Loss Landscape of Neural Nets
\newblock \emph{\url{https://arxiv.org/abs/1712.09913}}

\bibitem[Shibani Santurkar et al., 2018]{2} Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, Aleksander Madry
\newblock How Does Batch Normalization Help Optimization?
\newblock \emph{\url{https://arxiv.org/abs/1805.11604}}





\end{thebibliography}
}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlecture











