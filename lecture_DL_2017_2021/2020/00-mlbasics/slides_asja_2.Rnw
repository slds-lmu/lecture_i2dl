%Blank Lecture
%This file is a child of preamble.Rnw in the style folder
%if you want to add stuff to the preamble go there to make
%your changes available to all childs


<<setup-child, include = FALSE>>=
library(knitr)
set_parent("../style/preamble.Rnw")
@



\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\lecturechapter{1}{Introduction to Machine Learning II}
\lecture{Deep Learning}

\begin{frame}{Recall from last week}
\begin{itemize}
\item The supervised learning scenario is based on labeld training data, a model and a learning algorithm
\item The learning algorithm can be be decomposed into hypothesis space, evaluation, and optimization
\item Evaluation is based on a loss function and  the (empirical) risk
\item Optimization corresponds to empirical risk minimization
\item Linear/logistic regression are examples of regression/classification models 
\item Squared error/cross entropy are examples of loss functions for regression/classification 
\item Likelihood maximization is the standard principle for learning the parameters of probabilitic models (i.e. the loss is the negative log-likelihood)
\item Principles can be generalized from linear models to arbitraty score functions (like neural networks)
\end{itemize}
\end{frame}




\section{Performance Measures}


\begin{frame}{Performance Measures}

\begin{centering}
\includegraphics[width=9cm,page=1]{plots/make_classif_regr_error_intro_plot.pdf}
\end{centering}

To compare the performance of two ML models on a single observation or a
new data set, we would ideally like to boil them down to a
\textbf{single} numeric measure of the performance.

\end{frame}

\begin{frame}{Performance Measures vs Loss Functions}


Performance measures are related to losses, but are not the same

\begin{itemize}

\item A performance measure is simply applied at the end, to a fitted model and some 
  test data under consideration. A loss function is optimized by a learning algorithm

\item A performance measure is usually dictated by our prediction problem. There are no technical 
  restrictions on it, except that it needs to be computable and appropriate%, as it is onlymeasured on a model and data set, 
  to evaluate a model on a data set.
  A loss function on the other hand needs to be \enquote{handable} by the optimizer (e.g. %certain degree of smoothness is usually required
  needs to be differentiabe)

\item Sometimes, performance mearures arer called \enquote{outer losses},
  and \enquote{innner loss} is used for the loss minimized by the learner

\item Sometime, one can make the inner loss match the outer loss, so the learning algorithm 
  optimizes exactly the metric we are interested in. But often we use approximations for the
  inner loss, e.g., for efficiency

\end{itemize}

\end{frame}


\begin{frame}{Performance Measures -Regression:MSE}

The \textbf{Mean Squared Error} compares the mean of the squared
distances between the target variable \(y\) and the predicted target
\(\yh\). \[
MSE = \frac{1}{n} \sumin (\yi - \yih)^2 \in [0;\infty]
\]

Single observations with a large prediction error heavily influence the
\textbf{MSE}, as they enter quadratically

\scriptsize

<<echo=FALSE, out.width="0.7\\textwidth", fig.width = 7, fig.height = 3>>=
source("code/plot_loss_measures.R")

set.seed(31415)

x = 1:5
y = 2 + 0.5 * x + rnorm(length(x), 0, 1.5)
data = data.frame(x = x, y = y)
model = lm(y ~ x)

plotModQuadraticLoss(data = data, model = model, pt_idx = c(1,4))
@

\normalsize 

\end{frame}

\begin{frame}{Performance Measures-Regression:MAE}

A more robust (but not neccessarily better) way to compute a performance
measure is the \textbf{Mean Absolute Error}:

\[
MAE = \frac{1}{n} \sumin |\yi - \yih| \in [0;\infty]
\]

The absolute error is more robust. On the other hand, the absolute error
is not differentiable at \(0\).

\scriptsize

<<echo=FALSE, out.width="0.7\\textwidth", fig.width = 7, fig.height = 3>>=
plotModAbsoluteLoss(data, model = model, pt_idx = c(1,4))
@

\normalsize 

\end{frame}

%\begin{frame}{Regression: \(R^2\)}
%
%Another often used measure is \(R^2\). It measures the \emph{fraction of
%variance explained} by the model. \[
%R^2 = 1 - \frac{\sumin (\yi - \yih)^2}{\sumin (\yi - \bar{y})^2} = \frac{SSE_{Model}}{SSE_{Intercept}}
%\]
%
%\begin{itemize}
%\item
%  If \(R^2\) is measured on the training data: \(R^2 \in [0;1]\).
%\item
%  If \(R^2\) is measured on held out data: \(R^2 \in [-\infty;1]\).
%\end{itemize}
%
%\end{frame}

%\begin{frame}{Regression: Defining a custom loss}
%
%Assume a use case, where the target variable can have a wide range of
%values across different orders of magnitude. A possible solution would
%be to use a loss functions that allows for better model evaluation and
%comparison. The \textbf{Mean Squared Logarithmic Absolute Error} is not
%strongly influenced by large values due to the logarithm. \[
%\frac{1}{n} \sumin (\log(|\yi - \yih| + 1))^2
%\]
%
%\scriptsize
%
%<<echo=FALSE, out.width="0.7\\textwidth", fig.width = 7, fig.height = 3>>=
%plotModAbsLogQuadLoss(data, model = model, pt_idx = c(1,4))
%@
%
%\normalsize 
%
%\end{frame}

\begin{frame}{Performance Measures - Classification: confusion matrix}

The confusion matrix presents an intuitive %metric used for establishing the correctness and 
way to access the accuracy of a  classification model %It is used for classification problems where the output can be of two or more types of classes.
\newline
\begin{centering}
\includegraphics[width=11cm,page=3]{plots/confusion_matrix_measures.pdf}
\end{centering}

\end{frame}

\begin{frame}{Performance Measures - Classification: Accuracy / Error}

The confusion matrix is not a performance measure as such, but most
performance metrics for binary classification are based on it
\newline
%\begin{centering}
\hspace{2cm}\includegraphics[width=11cm,page=4]{plots/confusion_matrix_measures.pdf}
%\end{centering}

Classification Error = 1 - Accuracy

\end{frame}

\begin{frame}{Performance Measures -Classification: Thresholding}

Thresholding flexibly converts measured probabilities to labels

\begin{centering}
\includegraphics[height=8cm, width=14cm,page=2]{plots/confusion_matrix_measures.pdf}
\end{centering}

\end{frame}

\begin{frame}{Performance Measures-Classification: Overview}

\small

\begin{table}[]
\begin{tabular}{ll}
\hline
\textbf{Classification}& \textbf{Explanation}  \\ \hline
Accuracy              &  Fraction of correct classifications                             \\ \hline
Balanced Accuracy     &  Fraction of correct classifications in each class               \\ \hline
Recall                &  Fraction of positives a classifier captures                     \\ \hline
Precision             &  Fraction of positives in instances predicted as positive        \\ \hline 
F1-Score              &  Tradeoff between precision and recall                           \\ \hline
AUC                   &  Measures calibration of predicted probabilities                 \\ \hline
Brier Score           &  Squared difference between predicted probability                \\ 
                      & and true label                                                   \\ \hline
Logloss               &  Emphasizes errors for predicted probabilities close             \\ 
                      & to 0 and 1                                                       \\ \hline
\end{tabular}
\end{table}

\normalsize

\end{frame}

\section{Generalization}



\begin{frame}{Training Error}

The \textbf{training error} %(also called apparent error or resubstitution error) 
is estimated by the average error over the training set
\(\Dtrain\):

\includegraphics[width=10cm,page=4]{plots/train_error.png}

 It is usually a very unreliable and overly optimistic
  estimator of future performance (e.g. training 1-NN is always zero).


\end{frame}

%\begin{frame}{Training Error}
%
%\begin{itemize}
%\item
%  The training error is usually a very unreliable and overly optimistic
%  estimator of future performance.
%\item
%  Goodness-of-fit measures like \(R^2\), likelihood, AIC, BIC, deviance
%  are all based on the training error.
%\end{itemize}
%
%\end{frame}

\begin{frame}{Example: Polynomial Regression}

Assume we have a single, univariate \(x\) and that \(y\) can be
approximated by a \(d\)th-order polynomial

\[ \fxt = \theta_0 + \theta_1 x + \cdots + \theta_d x^d = \sum_{j = 0}^{d} \theta_j x^j\text{.} \]

\(\theta_j\) are the coefficients and \(d\) is called the degree.

This is still linear regression / a linear model, and for a fixed \(d\),
the \(\theta\) can easily be obtained through the normal equations.

\end{frame}

\begin{frame}{Example: Polynomial Regression}

True relationship is
\(f(x) = 0.5 + 0.4 \cdot \sin (2 \pi x) + \epsilon\)

\scriptsize

<<echo=FALSE, out.width="\\textwidth", fig.width = 8, fig.height = 5>>=
source("code/plot_train_test.R")

.h = function(x) 0.5 + 0.4 * sin(2 * pi * x)
h = function(x) .h(x) + rnorm(length(x), mean = 0, sd = 0.05)

set.seed(1234)
x.all = seq(0, 1, length = 26L)
ind = seq(1, length(x.all), by = 2)

mydf = data.frame(x = x.all, y = h(x.all))


ggTrainTestPlot(data = mydf, truth.fun = .h, truth.min = 0, truth.max = 1, test.plot = FALSE,
  test.ind = ind)[["plot"]] + ylim(0, 1)
@
\normalsize 

\end{frame}

\begin{frame}{Example: Polynomial Regression}

Models of different \textbf{complexity}, i.e., of different orders of the
polynomial, are fitted. How should we choose \(d\)?

\scriptsize

<<echo=FALSE, out.width="\\textwidth", fig.width = 8, fig.height = 5>>=
source("code/plot_train_test.R")
out = ggTrainTestPlot(data = mydf, truth.fun = .h, truth.min = 0, truth.max = 1, test.plot = FALSE,
  test.ind = ind, degree = c(1, 5, 9))
out[["plot"]] + ylim(0, 1)
@
\normalsize 

\end{frame}

\begin{frame}{Example: Polynomial regression}

The performance of the models on the training data can be measured by
calculating the training error according to the mean squared
error (MSE) criterion:

\[ \frac{1}{n} \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2\text{.} \]

d = 1: 0.03, \qquad d = 5: 0.002, \qquad d = 9: 0.001 \\

\vfill

Observation: Simply using the training error for model evaluation seems to be a bad idea!

\end{frame}

%\begin{frame}{Overfitting}


%
% \begin{vbframe}{Risk minimizing functions}
%
% Overview of binary classification losses and the corresponding risk minimizing functions:
%
% \lz
%
% \begin{tabular}{p{2.5cm}|p{3.5cm}|p{4.5cm}}
%   loss name & loss formula  & minimizing function \\
%   \hline
%   0-1 & $[y \neq \hx]$ & $\hxh = \footnotesize \begin{cases} 1 \quad \text{ if } \pix > 1/2 \\ -1 \quad \pix < 1/2  \end{cases}$ \\
%   Hinge & $\max\{0, 1 - \yf\}$ & $\fh(x) =  \footnotesize \begin{cases} 1 \quad \text{ if } \pix > 1/2 \\ -1 \quad \pix < 1/2  \end{cases}$ \\
%   Logistic & $\ln(1+\exp(-y\fx))$ & $\fh(x) =  \ln \biggl(\frac{\pix}{1-\pix}\biggr)$ \\
%   Cross entropy & $-y\ln(\pix) \newline -(1-y)\ln(1-\pix)$ & \\
%   & & \\
%   Exponential & $\exp(-y\fx)$ &
%
% \end{tabular}
%
% \end{vbframe}

% \section{Selected methods for regression and classification}



%
%\begin{frame}{Training Error Problems}
%
%\begin{itemize}
%\item
%  Extend any ML training in the following way: On top of normal fitting,
%  we also store \(\Dtrain\). During prediction, we first check whether
%  \(x\) is already stored in this set. If so, we replicate its label.
%  The training error of such an (unreasonable) procedure will always be
%  zero.
%\item
%  The training error of 1-NN is always zero.
%\item
%  The training error of an interpolating spline in regression is always
%  zero.
%\item
%  For models of severely restricted capacity, and given enough data, the
%  training error might provide reliable information. E.g. consider a
%  linear model in 5d, with 10.000 training points. But: What happens if
%  we have less data? And \(p\) becomes larger? Can you precisely define
%  where the training error becomes unreliable?
%\end{itemize}
%
%\end{frame}

\begin{frame}{Test Error and Hold-Out Splitting}

%Goal: We want to make good predictions for new data! So thhe 

%To reliably assess a model, we need to define
Problem: How to estimate the generalization error, i.e. the prediction error for  \enquote{new unseen data}?

%\begin{itemize}
%\item
%  how to simulate the scenario of \enquote{new unseen data} and
%\item
%  how to estimate the generalization error, i.e. the prediction error for  \enquote{new unseen data}
%\end{itemize}

Solution: Hold-out splitting and evaluation

\begin{itemize}
\item
  Straightforward idea: %to
   measure performance %, let's simulate how our model will be perform 
   on a new set of data not used during training
%\item
 % So, to evaluate a given model: predict and evaluate only on data not
  %used during training.
\item
  For a given set \(\D\), we have to preserve some data for testing that
  we cannot use for training. We need to define a training set
  \(\Dtrain\) and a test set \(\Dtest\), e.g.~by randomly partitioning
  the original dataset \(\D\)
\end{itemize}

\end{frame}

\begin{frame}{Test Error and Hold-Out Splitting}

\includegraphics[width=11cm]{plots/test_error.png}

\end{frame}

\begin{frame}{Example: Polynomial Regression}

\scriptsize

<<echo=FALSE, out.width="0.9\\textwidth", fig.width = 9, fig.height = 6>>=
source("code/plot_train_test.R")
ggTrainTestPlot(data = mydf, truth.fun = .h, truth.min = 0, truth.max = 1, test.plot = TRUE,
  test.ind = ind)[["plot"]] + ylim(0, 1)
@

\normalsize 

\end{frame}

\begin{frame}{Example: Polynomial Regression}

\scriptsize

<<echo=FALSE, out.width="0.9\\textwidth", fig.width = 9, fig.height = 6>>=
source("code/plot_train_test.R")
ggTrainTestPlot(data = mydf, truth.fun = .h, truth.min = 0, truth.max = 1, test.plot = TRUE,
  test.ind = ind, degree = c(1, 5, 9))[["plot"]] + ylim(0, 1)
@

\normalsize 

\end{frame}

\begin{frame}{Test Error and Hold-Out Splitting}

\scriptsize

<<echo=FALSE, out.width="0.9\\textwidth", fig.height=6>>=
source("code/plot_train_test.R")
degrees = 1:9

errors = ggTrainTestPlot(data = mydf, truth.fun = .h, truth.min = 0, truth.max = 1, test.plot = TRUE,
  test.ind = ind, degree = degrees)[["train.test"]]

par(mar = c(4, 4, 1, 1))
#par(mar = c(4, 4, 0, 0) + 0.1)
plot(1, type = "n", xlim = c(1, 10), ylim = c(0, 0.1),
  ylab = "MSE", xlab = "degree of polynomial")
lines(degrees, sapply(errors, function(x) x["train"]), type = "b")
lines(degrees, sapply(errors, function(x) x["test"]), type = "b", col = "gray")

legend("topright", c("training error", "test error"), lty = 1L, col = c("black", "gray"))
text(3.75, 0.05, "High Bias,\nLow Variance", bg = "white")
arrows(4.75, 0.05, 2.75, 0.05, code = 2L, lty = 2L, length = 0.1)

text(6.5, 0.05, "Low Bias,\nHigh Variance", bg = "white")
arrows(7.5, 0.05, 5.5, 0.05, code = 1, lty = 2, length = 0.1)
@

\normalsize  Test error is best for \(d=3\)

\end{frame}


\begin{frame}{Training vs.~Test Error}

The training error

\begin{itemize}
\item
  is an over-optimistic, biased 
  estimator as the performance is
  measured on the same data the %learned prediction function
  %\(\fhDtrain(x)\) 
 model  was trained on
\item
  decreases with smaller training set size as it is easier for the model
  to learn the underlying structure in the training set perfectly
\item
  decreases with increasing model complexity as the model is able to
  learn more complex structures
\end{itemize}

The test error

\begin{itemize}
\item
  will typically decrease when the training set increases as the model
  generalizes better with more data
\item
  will have higher variance with decreasing test set size
\item
  will have higher variance with increasing model complexity
\end{itemize}

\end{frame}

%\begin{frame}{Training vs.~Test Error}
%
%The test error
%
%\begin{itemize}
%\item
%  will typically decrease when the training set increases as the model
%  generalizes better with more data.
%\item
%  will have higher variance with decreasing test set size.
%\item
%  will have higher variance with increasing model complexity.
%\end{itemize}
%
%\end{frame}

\begin{frame}{Resampling}

\begin{itemize}
\item
 If $\D$ is not huge, we need to construct a performance estimator % throug\emph{resampling},  
 that uses the data more efficiently
\item
  \textbf{Resampling}: % variants operate similar: The 
  split the data set% is split
  repeatedly into training and tests sets, and %we later 
  aggregate
  (e.g.~average) the results
\item
  The usual trick is to make training sets larger (to keep the
  %pessimistic +
  bias small), and to handle the variance introduced by
  smaller test sets through many repetitions and averaging of results.
\end{itemize}

\end{frame}

\begin{frame}{Resampling}

\begin{itemize}
\item
  Aim: estimate the actual model performance. %of learning algorithm.
\item
  Uses the data more efficiently than a simple train-test split.
\item
  Repeatedly split in train and test, then aggregate results.
\end{itemize}

\begin{center}
\includegraphics[width=7cm]{plots/ml_abstraction-crop.pdf}
\end{center}

\end{frame}

\begin{frame}{Cross-Validation}

\begin{itemize}
\item
  Split the data into \(k\) roughly equally-sized partitions
\item
  Use each part once as test set and joint \(k-1\) others as training set
\item
  Obtain \(k\) test errors and average
\end{itemize}

Example: 3-fold cross-validation:

\begin{center}
\includegraphics[width=7cm]{plots/crossvalidation.png}
\end{center}

\end{frame}


\begin{frame}{Cross-Validation}

\begin{itemize}
\item
  \(k = n\) is known as \textbf{leave-one-out (LOO)} cross-validation or
  jackknife.
\item
  The performance estimates for each fold are NOT independent, because
  of the structured overlap of the training sets. 
  %Hence, the variance of
  %the estimator increases again for very large \(k\) (close to LOO),
  %when training sets nearly completely overlap.
\item LOO is  a nearly unbiased estimator for the performance after training on the full data set, but has high variance.
\item
  Repeated \(k\)-fold CV (multiple random partitions) can improve error
  estimation for small sample size.
\end{itemize}

\end{frame}


\begin{frame}{Summary: Training/evaluating a learning algorithm with limited data}

\begin{itemize}
\item
  If data is limited, one wants to %fit a model on 
  use all available data for training
\item
  Problem: we need to estimate how well %this model 
  the learning algorithm performs in the future,
  but no data %now 
  is left to reliably do this
  %But we really need this.
\item
  Idea: %In order to approximate this, we %do the next best thing. We  estimate
  approximate how well the learner works when it sees nearly \(n\) points from the
  same data distribution.
\item
  Use holdout splittung/ CV/resampling to produce performance estimates.
  %estimate exactly this number. 
 % A
  %\enquote{pessimistic bias} %refers to
  When using much less data in
  fitting than \(n\), we \enquote{hurt} the learner unfairly (pessimistic bias)
  \item
  %Strictly speaking, resampling only produces one number, the
  %performance estimator. 
  %Often It does 
  This is done to estimate the performace, not to produce models, parameters, etc.
  These are intermediate results and discarded
\item
  The model and final parameters are obtained by training
  on the complete dataset. 
%\item
%  This is a bit weird and complicated, but we have to live with this.
\end{itemize}

\end{frame}

%\begin{frame}{Summary of concepts}
%
%\begin{itemize}
%\item
%  Strictly speaking, resampling only produces one number, the
%  performance estimator. It does NOT produce models, paramaters, etc.
%  These are intermediate results and discarded.
%\item
%  The model and parameters are obtained when we fit the learner finally
%  on the complete data.
%\item
%  This is a bit weird and complicated, but we have to live with this.
%\end{itemize}
%
%\end{frame}

%\begin{frame}{Comments}
%
%\begin{itemize}
%\item
%  5CV or 10CV have become standard, 10-times repeated 10CV for small
%  sample sizes, but THINK about whether that makes sense in your
%  application.
%\item
%  Do not use Hold-Out, CV with few iterations, or subsampling with a low
%  subsampling rate for small samples, since this can cause the estimator
%  to be extremely biased, with large variance.
%\item
%  A \(\D\) with \(|\D| = 100.000\) can have small sample size properties
%  if one class has only 100 observations \ldots
%\end{itemize}
%
%\end{frame}


\section{Overfitting and Regularization}



\begin{frame}{Overfitting to training data}

\begin{itemize}
\item
  Learner finds a pattern in the data that is not actually true in the
  real world, it \textbf{overfits} the data
\item
  Every powerful learner can \enquote{hallucinate} patterns
\item
  Happens when you have too many hypotheses and not enough data to tell
  them apart
\item
  The more data, the more \enquote{bad} hypotheses are eliminated
\item
  If the hypothesis space is not constrained, there may never be enough
  data
\item
  There is often a parameter that allows you to constrain
  (\textbf{regularize}) the learner
\end{itemize}

\end{frame}

\begin{frame}{Overfitting to training data}

\begin{center}
\includegraphics[width=12cm]{plots/overfitting.png}
\end{center}



%\begin{columns}[T,onlytextwidth]
%\column{0.5\textwidth}
%Overfitting learner \\
%\vspace{0.5cm}
%
% \scriptsize
%
%
%
%<<echo=FALSE, results='hide', out.width='\\textwidth'>>=
%library(mlbench)
%library(BBmisc)
%data = as.data.frame(mlbench.2dnormals(n = 200, cl = 3))
%data$classes = mapValues(data$classes, "3", "1")
%task = makeClassifTask(data = data, target = "classes")
%learner = makeLearner("classif.ksvm")
%plotLearnerPrediction(learner, task, kernel = "rbfdot", C = 1, sigma = 100, pointsize = 4)
%@
%
%
% \normalsize 
%Better training set performance (seen examples)
%
%\column{0.5\textwidth}
%Non-overfitting learner \\
%\vspace{0.5cm}
%
% \scriptsize
%
%<<echo=FALSE, results='hide', out.width='\\textwidth'>>=
%plotLearnerPrediction(learner, task, kernel = "rbfdot", C = 1, sigma = 1, pointsize = 4)
%@
%
%
% \normalsize 
%Better test set performance (unseen examples)
%\end{columns}

\end{frame}

\begin{frame}{Overfitting and Noise}

\begin{itemize}
\item
  Overfitting is seriously exacerbated by \emph{noise} (errors in the
  training data)
\item
  An unconstrained learner will start to model that noise
\item
  It can also arise when relevant features are missing in the data
\item
  In general it's better to make some mistakes on training data
  (\enquote{ignore some observations}) than trying to get all correct
%\item
%  To avoid overfitting:  
%  \begin{itemize}
%    \item
%      Some learners can do \enquote{early stopping} before perfectly fitting
%      (i.e., overfitting) the training data
%    \item
%      In general, prefer simpler hypotheses altogether when they work well
%      (e.g.~regularization, pruning)
 % \end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Triple Trade-Off}

When learning a model from data, there is a
trade-off between three factors:

\begin{itemize}
\item
  complexity of the hypothesis space % we fit to the training data
  %capacity 
\item
  amount of training data (in terms of both instances and
  informative features)
\item
  generalization error on new examples
\end{itemize}

\vfill

If the %capacity of the %learning algorithm is large enough to a)
complexity of the 
hypothesis space is large enough to 
approximate the data generating process, %and b) exploit the information contained in the data, 
the generalization error will decrease as the
amount of training data increases. 



\end{frame}

\begin{frame}{Trade-Off Between Generalization Error and Complexity}

%Apparent error (on the training data) and actual error (prediction error
%on new data) evolve in the opposite direction with increasing
%complexity:

For a fixed ammount of training data, the generalization error (i.e. actual error) decreases
first and then starts to increase (overfitting) as the complexity of the
hypothesis space \(H\) increases.

\scriptsize

<<fig.height=3, fig.width=8, echo=FALSE>>=
par(mar = c(2.1, 2.1, 0, 0))
x <- seq(0, 1, length.out = 20)
y1 <- c(1 - x[1:4], 1.5 - 4 * x[5:6], 0.5 - 0.8 * x[7:10], 0.15 - 0.1 * x[11:20])
X <- seq(0, 1, length.out = 1000)
Y1 <- predict(smooth.spline(x, y1, df = 10), X)$y
plot(X, Y1, type = "l", axes = FALSE, xlab = "Complexity", ylab = "Error")
mtext("Complexity", side = 1, line = 1)
mtext("Error", side = 2, line = 1)
Y2 <- 0.5 * X
lines(X, Y1 + Y2)
abline(v = 0.42, lty = 2)
text(0.4, 0.93, "Underfitting", pos = 2)
text(0.44, 0.93, "Overfitting", pos = 4)
arrows(0.4, 0.98, 0.2, 0.98, length = 0.1)
arrows(0.44, 0.98, 0.64, 0.98, length = 0.1)
box()
text(0.85, 0.13, "Apparent error")
text(0.85, 0.55, "Actual error")
@
\normalsize  \vspace{-0.2cm} \(\Rightarrow\) %Optimization regarding themodel complexity is desirable: 
Goal: Find the "right" amount of complexity for
the given amount of data where generalization error becomes minimal.

\end{frame}


\begin{vbframe}{How to avoid Overfitting}

Why can \textbf{overfitting} happen? And how to avoid it?

\begin{itemize}
\def\labelenumi{\arabic{enumi}.}
\item
  Not enough data \(\to\) collect \textbf{more data}
\item
  Data is noisy \(\to\) collect \textbf{better data} (reduce noise) \item
  Aggressive loss optimization \(\to\) \textbf{optimize less}
\item
  Models are too complex \(\to\) use \textbf{less complex models}
\end{itemize}

Mechanisms to avoid overfitting are often reffered to as \textbf{regularization}.

\end{vbframe}

%\begin{vbframe}{Regularization}
\begin{vbframe}{Overfitting: Example}

Assume we want to predict the daily maximum \textbf{ozone level} in LA
given a data set containing \(50\) observations.

\vfill

We use (a subset of) the \texttt{Ozone} data set from the \texttt{mlbench} package:
It contains \(12\) features describing time conditions (e.g.,
weekday, month), the weather (e.g.~temperature at different weather
stations, humidity, wind speed) or geographic variables (e.g.~the
pressure gradient).



\framebreak

We fit a linear regression model using \emph{all} of the features

\[
f(x | \theta) = \theta^Tx = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_{12} x_{12}.
\]

We evaluate the performance with \(10\) times \(10\)-fold CV.

\vfill

While our model fits the training data almost perfectly (left), it
generalizes poorly to new test data (right). We \textbf{overfitted}.


<<echo=FALSE, fig.height=2, warning=FALSE>>=
load("data/ozone_example.RData")

dfp =df_incdata[nobs == 50, ]

p = ggplot(data = dfp, aes(x = 0, y = value, colour = variable))
p = p + geom_boxplot() + labs(colour = " ")
p = p + scale_colour_discrete(labels = c("Train error", "Test error"))
p = p + xlab(" ") + ylab("Mean Squared Error")
p = p + ylim(c(0, 400)) + theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
p
@

\end{vbframe}


\begin{vbframe}{How to avoid Overfitting}


\textbf{Approach 1: collect more data}

Example: We explore our results for increased data set size by \(10\) times
\(10\)-fold CV. The fit worsens slightly, but the test error decreases.

\vfill


<<echo=FALSE, fig.height=3, warning=FALSE>>=
library(data.table)

dfp = setDT(df_incdata)[, .(mean.mse = mean(value)), by = c("nobs", "variable")]

p = ggplot(data = dfp, aes(x = nobs, y = mean.mse, colour = variable))
p = p + geom_line() + ylim(c(0, 500)) + labs(colour = " ")
p = p + scale_colour_discrete(labels = c("Train error", "Test error"))
p = p + xlab("Size of data set") + ylab("MSE")
p
@

\vfill

Good idea, but often not feasible in practice.

\framebreak

\textbf{Approach 2}: \textbf{early stopping}

When performing iterative optimization, display model performance on an hold out \\\textbf{validation set} (final performance still needs to evaluated on a test set!).

\begin{center}
\includegraphics[width=7cm]{plots/early_stopping.png}
\end{center}

\framebreak

\textbf{Approach 3}: reduce \textbf{complexity}

Example: 
To model the ozone level:
\begin{itemize}
\item we try the simplest model we can think of: we dont use any features, but always predict the empirical mean (i.e. intercept only model/ featureless predictor)

\[
f(x | \theta) = \theta_0 = \frac{1}{n}\sumin \yi
\]

\item 
We then increase the complexity of the model step-by-step by adding one
feature at a time.

\end{itemize} 


\framebreak

We can control the complexity of the model by including/excluding
features. We can try out all feature combinations and investigate the
model fit.

\vfill


<<echo=FALSE, fig.height=3, warning=FALSE, message = FALSE>>=
load("data/ozone_example.RData")

p = ggplot(data = df_incfeatures, aes(x = type, y = mean.mse, colour = variable))
p = p + geom_line() + labs(colour = " ")
p = p + scale_colour_discrete(labels = c("Train error", "Test error"))
p = p + xlab("Number of features") + ylab("Mean Squared Error")
p = p + ylim(c(0, 200))
p = p + scale_x_continuous(breaks = 0:12)
p
@

\vfill

Note: For simplicity, we added the features in one specific order - but there are $2^{12} = 4096$
potential feature combinations.

\end{vbframe}

\begin{vbframe}{Regularization: Complexity vs.~Fit}

We have have contradictory goals

\begin{itemize}

\item
  \textbf{maximizing the fit} (minimize the train loss)
\item
  \textbf{minimizing the complexity} of the model.
\end{itemize}

We need to find the \enquote{sweet spot}.

\begin{center}
\begin{figure}
\includegraphics[width=0.6\textwidth]{plots/complexity-vs-fit.png}
\end{figure}
\end{center}

\framebreak

%Until now, we know only one regcan either add a feature completely or not at all.
We saw that we can control model complexity by adding/removing features. 

\vfill 

Instead of controlling the complexity in a discrete way (e.g. by specifying
the features), we might prefer to control the complexity
\textbf{on a continuum} from simple to complex.

\vfill

\begin{center}
\begin{figure}
\includegraphics[width=0.6\textwidth]{plots/complexity-vs-fit-continuous.png}
\end{figure}
\end{center}

\end{vbframe}

\begin{frame}{Regularized Linear Regression}

Instead of pure \textbf{empirical risk minimization}, we add a penalty
for complex (read: large) parameters \(\theta_j\):

\[
\riskrt = \sumin \left(\yi - \theta^T \xi\right)^2 + \lambda \cdot \underbrace{J(\theta)}_{\text{penalty}}
\]

\begin{itemize}
\item The parameter \(\lambda\) is called \textbf{regularization (hyper)parameter}, it controls the complexity of the learned model  %control parameter, it controls the regularization strength. 
\item If \(\lambda = 0\), \(\riskrt\) reduces to simple empirical risk minimization
\end{itemize}
´

\end{frame}

\begin{frame}{Ridge Regression}

If we choose \(J(\theta)\) to be the \(L_2\)-penalty\(^{(*)}\), the
model is called \textbf{ridge regression}.
It tries to \emph{shrink} the regression coefficients \(\theta\) towards
\(0\). We minimize:
\vspace{-0.3cm}
\[
\mathcal{R}_{ridge}(\theta) = \sumin (\yi - \theta^T \xi)^2 + \lambda ||\theta||_2^2
\]

\pause

$^{(*)}$ The $L_2$ norm or Euclidean norm is $||\theta||_2^2 = \theta_1^2 + \theta_2^2 + ... + \theta_p^2$
\vspace{-0.3cm}
\begin{figure}
\center
\includegraphics[width=0.3\textwidth]{plots/regularizedRegression.png}
\caption{Goodfellow et all, Deep Learning, p.~229}
\end{figure}


%\framebreak

%Equivalent formulation as constrained optimization problem:
%
%\vspace{-0.8cm}
%
%\begin{eqnarray*}
%&\min_{\theta}& \sum_{i=1}^n (\yi - \theta^T \xi)^2 \\
%&\text{subject to: }& ||\theta||_2^2  \leq \lambda \\
%\end{eqnarray*}
%
%\begin{figure}
%\center
%\includegraphics[width=0.3\textwidth]{plots/ridge.png}\\
%\end{figure}
%
\end{frame}

\begin{vbframe}{Lasso Regression}

Another shrinkage method is the so-called \textbf{lasso regression},
which uses an \(L_1\) penalty\(^{(*)}\) on \(\theta\). We minimize:
\vspace{-0.1cm}
\[
\mathcal{R}_{lasso}(\theta) = \sum_{i=1}^n (\yi - \theta^T \xi)^2 + \lambda ||\theta||_1
\]
Note that optimization becomes harder now, because the problem is not
differentiable anymore.
$^{(*)}$ The $L_1$ norm is $||\theta||_1 = |\theta_1| + |\theta_2| + ... + |\theta_p|$
\vspace{-0.1cm}
\begin{figure}
\center
\includegraphics[width=0.3\textwidth]{plots/lasso.png}\\
\end{figure}

%\framebreak
%
%Equivalent formulation as constrained optimization problem:
%
%\vspace{-0.8cm}
%
%\begin{eqnarray*}
%&\min_{\theta}& \sum_{i=1}^n (\yi - \theta^T \xi)^2 \\
%&\text{subject to: }& ||\theta||_1  \leq \lambda \\
%\end{eqnarray*}
%
%\begin{figure}
%\center
%\includegraphics[width=0.3\textwidth]{plots/lasso.png}\\
%\end{figure}

\end{vbframe}

\begin{frame}{Ridge vs.~Lasso Regression}

\textbf{Sparsity of the solution}:

\begin{itemize}

\item
  Lasso regression ensures a \textbf{sparse} solution (it selects
  features)
\item
  Ridge regression includes all features
\end{itemize}

\textbf{High-dimensional feature spaces} (\(p \gg n\)):

\begin{itemize}

\item
  Lasso selects at most \(n\) features (even if more features are
  associated with the target)
\item
  Ridge regression performs well in high-dimensional feature spaces
\end{itemize}

\textbf{Correlated features}:

\begin{itemize}

\item
  If features are highly correlated, lasso selects only one of these and
  ignores the others
\end{itemize}

Observation: Adding a regularization term to the loss has an high influence on the final model.

\end{frame}

\begin{vbframe}{Example: Polynomial Ridge Regression}

Again, let us consider a \(d\)th-order polynomial

\[ f(x) = \theta_0 + \theta_1 x + \cdots + \theta_d x^d = \sum_{j = 0}^{d} \theta_j x^j\text{} \]

True relationship is \(f(x) = 5 + 2x +10x^2 - 2x^3 + \epsilon\)

We saw. that a linear model  with \(d = 10\) tends to overfit:

\begin{figure}
\center
\includegraphics[width=0.9\textwidth]{plots/ex_linearRegression.png}\\
\end{figure}

%<<echo=FALSE, fig.height=3, warning=FALSE>>=
%source("code/ridge_polynomial_reg.R")
%
%set.seed(314259)
%f = function (x) {
%  return (5 + 2 * x + 10 * x^2 - 2 * x^3)
%}
%
%x = runif(40, -2, 5)
%y = f(x) + rnorm(length(x), 0, 10)
%
%x.true = seq(-2, 5, length.out = 400)
%y.true = f(x.true)
%df = data.frame(x = x.true, y = y.true)
%
%lambda.vec = 0
%
%plotRidge(x, y, lambda.vec, baseTrafo, degree = 10) +
%  geom_line(data = df, aes(x = x, y = y), color = "red", size = 1) +
%  xlab("x") + ylab("f(x)") + ggtitle("Predicting Using Linear Regression")
%@

\framebreak

Using ridge regression with suitable \textbf{regularization parameter}  $\lambda$ leads to better approximations of the data generating process.

\vfill

\begin{figure}
\center
\includegraphics[width=0.9\textwidth]{plots/ex_ridgeRegression.png}\\
\end{figure}


%<<echo=FALSE, fig.height=3, warning=FALSE>>=
%source("code/ridge_polynomial_reg.R")
%
%f = function (x) {
%  return (5 + 2 * x + 10 * x^2 - 2 * x^3)
%}
%
%set.seed(314259)
%x = runif(40, -2, 5)
%y = f(x) + rnorm(length(x), 0, 10)
%
%x.true = seq(-2, 5, length.out = 400)
%y.true = f(x.true)
%df = data.frame(x = x.true, y = y.true)
%
%lambda.vec = c(0, 10, 100)
%
%plotRidge(x, y, lambda.vec, baseTrafo, degree = 10) +
%  geom_line(data = df, aes(x = x, y = y), color = "red", size = 1) +
%  xlab("x") + ylab("f(x)") + ggtitle("Predicting Using Ridge Regression")
%@

\end{vbframe}


\begin{frame}{Example: Regularized Logistic Regression}

We can also add a regularizer to the risk of logistic regression

\begin{align*}
\riskrt &= \risket + \lambda \cdot J(\theta) \\
&= %\sumin \mathsf{log}\left(1 + \mathsf{exp}\left(-2\yi f\left(\left.\xi\right| \theta\right)\right)\right) 
\prod_{i=1}^n  \tau(\theta^T x)^{y^{(i)}} [1 -  \tau(\theta^T x)]^{1 - y^{(i)}}
+ \lambda \cdot J(\theta)
\end{align*}

%The other parts of the logistic regression remains exactly the same
%except for the fitting algorithm to find \(\hat{\theta}\).

\end{frame}

\begin{frame}{Example: Regularized Logistic Regression}

We fit a logistic regression model using polynomial features for \(x_1\)
and \(x_2\) with maximum degree of \(7\). We add an L2 penalty. We
see

\begin{itemize}

\item
  \(\lambda = 0\): the unregularized model seems to overfit
\item
  \(\lambda = 0.0001\): regularization helps to learn the underlying
  mechanism
\item
  \(\lambda = 1\) displays the real data generating process very well
\end{itemize}

%\scriptsize
%
%<<echo=FALSE, fig.height=3, fig.width=10, out.height="3cm", out.width="10cm", cache=FALSE, warning=FALSE>>=
%source("code/regularized_log_reg.R")
%@
%
%\normalsize 
\begin{figure}
\includegraphics[width=0.9\textwidth]{plots/regularizedLogReg.png}
\end{figure}

\end{frame}

\begin{frame}{Regularization}

We now generalize the concept of regularization:

\begin{itemize}

\item
  Instead of linear nodel, any model \(f\) could be considered
\item
  Instead of the MSE, any loss function \(L\) can be considered
\item
  Instead of the \(L_1\) or \(L_2\) penalty %on \(\theta\), we add apenalty for complex models \(J(\theta)\) 
  and \textbf{complexity penalty} or
  \textbf{regularizer} \(J(\theta)\) can be considers 
\end{itemize}

\[
\riskrt = \risket + \lambda \cdot J(\theta)
\]

\end{frame}

\begin{frame}{Regularization}

\begin{center}
\begin{figure}
\includegraphics[width=0.6\textwidth]{plots/biasvariance_scheme.png}
\caption{\footnotesize{Hastie, The Elements of Statistical Learning, 2009}}
\end{figure}
\end{center}

\end{frame}

\begin{frame}{Regularized Risk Minimization}

We now have extreme flexibility to make appropriate choices in
\(\riskr\): \[
\riskrt = \sumin \Lxyit + \lambda \cdot J(\theta)
\]

\begin{itemize}

\item
  the \textbf{model structure} for \(f\), affects how features influence
  \(y\)
\item
  the \textbf{loss function} that measures how error should be treated
\item
  \textbf{regularization} \(J(\theta)\) that encodes our inductive bias
  and preference for certain simpler models
\end{itemize}

\end{frame}

%\begin{frame}{Reg. Risk Min.: Weight decay}
%
%Let's optimize the L2-regularized risk of a model $\fxt$
%
%\[
%\min_{\theta} \riskrt = \risket + \lambda ||\theta||^2
%\]
%
%by gradient descent.
%
%\[
%\nabla \riskrt = \nabla \risket + 2 \lambda \theta
%\]
%
%and iteratively update $\theta$ by step size \(\alpha\) times the
%negative gradient
%
%\[
%\theta^{(new)} = \theta^{(old)} - \alpha \left(\nabla \risket + \lambda \theta^{(old)}\right) =
%\theta^{(old)} (1 - \alpha \lambda) - \alpha \nabla \risket
%\]
%
%The term \(\lambda \theta^{(old)}\) causes the parameter
%(\textbf{weight}) to \textbf{decay} in proportion to its size. This is a
%very well-known technique in neural networks - simply L2-regularization
%in disguise.
%
%\end{frame}



\begin{frame}{Further Comments}


\begin{itemize}
\item Note that very often we do not include $\theta_0$ in the penalty term $J(\theta)$ (but this can be implementation-dependent)
\item These methods are not equivariant under scaling of the inputs, so one usually standardizes the features
\item The regularization parameter \(\lambda\) is a hyperparameter that needs
to be chosen carefully. One way  to tune it  is cross-validation
\end{itemize}

\end{frame}

\begin{vbframe}{Regularized Risk Minimization and Bayes}

We have already created a link between maximum likelihood estimation and empirical risk minimization.
%Now we will generalize this for
How about regularized risk minimization?

\lz

Assume we have a parametrized distribution $p(x | \theta)$ for our data and a prior $p(\theta)$ over our
parameter space, all in the Bayesian framework.

From Bayes theorem follows:

$$
p(\theta | x) = \frac{p(x | \theta) p(\theta) }{p(x)} \propto p(x | \theta) p(\theta)
$$

\framebreak

The \emph{maximum a-posterio} (MAP)  estimator of $\theta$ is  defined as the minimizer of
\vspace{-0.2cm}
$$
- \sumin \log p(\xi | \theta) - \log p(\theta).
$$

Again, we identify the loss $\Lxyt$ with $-\log(p(x | \theta))$. If $p(\theta)$ is constant, so we have a
  uniform, non-informative prior, we again arrive at empirical risk minimization.

\lz

If not, we can identify $-log(p(\theta))$, our prior, with our regularizer $J(\theta)$.
If we introduce a further control constant $\lambda$ in front of the prior,
we arrive at regularized risk minimization.

\lz

Example: Rigde regression with linear model $\fxt$
\begin{itemize}
\item L2 loss = normal distribution of error terms
\item L2 regularizer = Gaussian prior on $\theta$
\end{itemize}


\end{vbframe}


%\begin{vbframe}{Curse of Dimensionality}
%
%We investigate how the linear regression model behaves in high dimensional
%spaces.
%
%\vfill
%
%We take the Boston Housing data set, where the value of houses in the
%area around Boston is predicted based on \(14\) features describing the
%region (e.g., crime rate, status of the population, etc. ).
%
%\vfill
%
%We train a linear model on the data. We then add \(100, 200, 300, ...\)
%noise variables (containing no information at all) and look at the
%performance of a linear model trained on this modified data (\(10\)
%times repeated \(10\)-fold CV).
%
%\framebreak
%
%We compare the performance of a linear regression model to that of a regression tree.
%
%<<echo=FALSE, fig.height=3.5>>=
%library(reshape2)
%bres = readRDS("data/cod_lm_rpart.rds")
%bres = melt(bres, id.vars = c("task.id", "learner.id", "d"))
%
%p = ggplot(data = bres[bres$d != 500, ], aes(x = d, y = value, colour = learner.id))
%p = p + geom_line()
%p = p + xlab("Number of noise variables") + ylab("Mean Squared Error") + labs(colour = "Learner")
%p = p + ylim(c(0, 300))
%p
%@
%
%\(\to\) The unregularized linear model struggles with the added noise features,
%while our tree seems to nicely filter them out
%
%\framebreak
%
%Many basic ML methods suffer from the curse of dimensionality. A
%large part of ML is concerned with dealing with this problem and finding
%ways around it.
%
%\vfill
%
%Possible approaches are:
%
%\begin{itemize}
%
%\item
%  Increasing the space coverage by gathering more observations (not
%  always viable in practice!)
%\item
%  Reducing the number of dimensions before training (e.g.~PCA or
%  feature selection)
%\item
%  Regularization
%\end{itemize}
%
%\end{vbframe}

\begin{frame}{Summary}
\begin{itemize}
\item A performance measure is applied to evaluate a trained model
\item We saw different measures for different tasks, sometimes they match the loss
\item Not the training error, the generalization performance counts
\item One can estimate it by evaluating the model on a hold-out data set (i.e. a test data set) or via cross validation
\item  Overfitting: model may over-adapt to the training data
\item Overfitting can be avoided by using more training data, early stopping, restricting the hypothesis class or other forms of regularization
\item In reguralized risk minimization a complexity penalty is added to the loss function, e.g. in ridge and lasso regression
\item From a Bayesian perspective, the reguralizer can be identified as a prior

\end{itemize}
\end{frame}

\endlecture
