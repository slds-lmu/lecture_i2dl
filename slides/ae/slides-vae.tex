\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-nn}

\title{Deep Learning}

\date{}

\begin{document}
\newcommand{\titlefigure}{plots/frontpage.png}

\newcommand{\learninggoals}{
  \item introduction and intution of VAE
  \item VAE-parameter fitting
  \item reparametrization trick
}

\lecturechapter{Variational Autoencoder (VAE)}
\lecture{I2DL}


\begin{frame}
\frametitle{Variational Autoencoder (VAE): Intution}

Independently proposed by:
\small{
\begin{itemize}
\item Kingma and Welling, \emph{Auto-Encoding Variational Bayes}, ICLR 2014
\item Rezende, Mohamed and Wierstra, \emph{Stochastic back-propagation and variational inference in deep latent Gaussian models.} ICML 2014
\end{itemize}}

\vspace{1mm}

Conventional AEs compute a deterministic feature vector that describes the attributes of the input in latent space:

                \begin{figure}
                \centering
                \includegraphics[width=7cm]{plots/ae_intution.png}
                \vspace{-8pt}
                \caption{\tiny{source: https://www.jeremyjordan.me/variational-autoencoders/}}
                \end{figure}
    
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Variational Autoencoder (VAE)}

                \begin{figure}
                \centering
                \includegraphics[width=9cm]{plots/vae_motivation.png}
                \vspace{-6pt}
                \caption{\tiny{source: https://www.jeremyjordan.me/variational-autoencoders/}}
                \end{figure}
                \vspace{-10pt}

\begin{itemize}
\item Key difference in variational autoencoders are:
  \begin{itemize}
    \item Uses a variational approach to learn the latent representation
    \item Allows to describe observation in latent space in probablistic manner.
  \end{itemize}     
\end{itemize}  

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Variational Autoencoder (VAE): Intution}

                \begin{figure}
                \centering
                \includegraphics[width=7cm]{plots/vae_intution.png}
                \vspace{-8pt}
                \caption{\tiny{source: https://www.jeremyjordan.me/variational-autoencoders/}}
                \end{figure}


\begin{itemize}
\item Describe each latent attribute as probability distribution
\item Allows to model uncertainty in input data
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{VAE: Statistical motivation}

\begin{itemize}
\item Suppose the hidden variable $z$ which generates an observation $x$
\pause
\item By training variational autoencoder, we determin the distribution $z$ and would like to compute $p(z|x)$. \\
\centering{$p\left( {z|x} \right) = \frac{{p\left( {x|z} \right)p\left( z \right)}}{{p\left( x \right)}}$}\\
\vspace{5pt}
\pause
\item \raggedright{However, computing $p(x)$ is difficult since it is an intractable distribution!} \\
\centering{$p\left( x \right) = \int {p\left( {x|z} \right)p\left( z \right)dz}$}\\
\vspace{5pt}
\pause
\item \raggedright{Instead we can apply variational inference} \\
\pause
\item \raggedright{Let's approximate $p(z|x)$ by tractable distribution $q(z|x)$ which is very similar to $p(z|x)$.} \\

\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5

\begin{frame}
\frametitle{VAE: Statistical motivation}

\begin{itemize}
\item \raggedright{KL divergence measures of difference between two probability distributions. Thus, if we wanted to ensure that $q(z|x)$ was similar to $p(z|x)$, we could minimize the KL divergence between the two distributions:} \\
\centering{$\min KL\left( {q\left( {z|x} \right)||p\left( {z|x} \right)} \right)$} \\
\pause
\raggedright{by maximizing the following:}\\
\pause
\centering{$\max {E_{q\left( {z|x} \right)}}\log p\left( {x|z} \right) - KL\left( {q\left( {z|x} \right)||p\left( z \right)} \right)$} \\
\vspace{5pt}
\item \raggedright{The first term represents the reconstruction likelihood and the second term ensures that our learned distribution $q$ is similar to the true prior distribution $p$.}
\item which forces $q(z|x)$ to be similar to true prior distribution $p(z)$

\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{VAE: Statistical motivation}

\begin{itemize}
\item $p(z)$ often assumed to be Guassian distribution \\ $\rightarrow$ determining $q(z|x)$ boils down to estimating $\mu$ and $\sigma$.
\item Use neural network to estimate $q(z|x)$ and $p(z|x)$
\end{itemize}


                \begin{figure}
                \centering
                \includegraphics[width=9cm]{plots/vae.png}
                \vspace{-6pt}
                \caption{\tiny{source: https://www.jeremyjordan.me/variational-autoencoders/}}
                \end{figure}
                \vspace{-10pt}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{VAE Training}

                \begin{figure}
                \centering
                \includegraphics[width=7cm]{plots/vae_training.png}
                \vspace{-6pt}
                \caption{\tiny{source: https://www.jeremyjordan.me/variational-autoencoders/}}
                \end{figure}
  
  
\begin{itemize}
\item Loss function: $ L(\theta, \phi; x, z) = E_{q_{\phi} ({z|x})} \log p_{\theta} ({x|z}) - KL ({q_{\phi} ({z|x})|| p_{\theta} (z)})$
\item Problem: network contains \textbf{sampling} operator $\rightarrow$ we can not backpropagate through!
\end{itemize}              

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Reparametrization Trick}

\begin{itemize}
\item We can not backpropagate through random sampling- what now?
\pause
\item "Push" random sampling out of backpropagation path by reparametrization
\end{itemize}  

                \begin{figure}
                \centering
                \includegraphics[width=11cm]{plots/vae_reparam.png}
                \end{figure}
                
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Visualization of latent space}

\vspace{3mm}

                \begin{figure}
                \centering
                \includegraphics[width=11cm]{plots/vae_visualization.png}
                \end{figure}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Variational autoencoders as a generative model}

\begin{itemize}
\item New data can be generated by sampling from distributions in the latent space $\rightarrow$ reconstructed by decoder
\pause
\item Diagonal prior enforces independent latent variables $\rightarrow$ can encode different factors of variations
\pause
\item Examples of generated samples:

\end{itemize}  
\end{frame}

\frame{
    \frametitle{Latent Variables learned by a VAE}
    
    \begin{figure}
    \centering
    \scalebox{0.4}{\includegraphics{plots/LatentVariables-FF.png}}
    \caption{\footnotesize Images generated by a VAE are superimposed on the latent vectors that generated them. In the two-dimensional latent space of the VAE, the first dimension encodes the position of the face and the second dimension encodes the expression. Therefore, starting at any point in the latent space, if we move along either axis, the corresponding property will change in the generated image.    (Goodfellow et al.,  2016)}
    \end{figure}
    
}

\frame{
    \frametitle{Latent Variables learned by a VAE}
    
    \begin{figure}
    \centering
    \scalebox{0.5}{\includegraphics{plots/vae_mnist_n.png}}
    \caption{\footnotesize Images generated by a VAE are superimposed on the latent vectors that generated them. The two-dimensional latent space of the VAE captures much of the variation present in MNIST. Different regions in the latent space correspond to different digits in the generated images. (Goodfellow et al.,  2016)}
    \end{figure}
    
}



\frame{
    \frametitle{Samples from a vanilla VAE}
    
    \begin{figure}
      \centering
      \scalebox{0.9}{\includegraphics{plots/vae_faces.png}}
      \tiny{\\ Credit : Wojciech Mormul}
      \caption{\footnotesize Samples generated by a VAE that was trained on images of people's faces.}
    \end{figure}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Variational autoencoders: Summary}

\begin{itemize}
  \item Probabilistic models $\rightarrow$ allow to generate data
  \item Intractable density $\rightarrow$ optimises variational lower bound instead
  \item Trained by back propagation by using reparametrization
  \pause
  \item Pros:
    \begin{itemize}
      \item Principled approach to generative models
      \item Latent space reparametrization can be useful for other tasks
    \end{itemize}  
  \pause
  \item Cons:
    \begin{itemize}
      \item Only maximizes lower bound of likelihood
      \item Samples in standard models often of lower equality compared to GANs
    \end{itemize} 
  \item Active area of research!
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}
\frametitle{References}
\footnotesize{
\begin{thebibliography}{99}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Jeremy, 2018]{4} Jeremy Jordan (2018)
\newblock Variational Autoencoders
\newblock \emph{\url{https://www.jeremyjordan.me/variational-autoencoders/}}

\bibitem[Weng, 2014]{5} Lilian Weng (2018)
\newblock From Autoencoder to Beta-VAE
\newblock \emph{\url{https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{thebibliography}
}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}
\frametitle{Appendix: Loss Function ELBO }

\begin{itemize}
\item In this section, we provide more details on loss function; 
\item In VAE, instead of mapping the input into a fixed vector, we want to map it into a distribution. Let's label this distribution as $p_{\thetab}$, parameterized by ${\thetab}$. The relationship between the data input $x$ and the latent encoding vector $z$ can be fully defined by:

  \begin{itemize}
    \item Prior $p_{\thetab}(\mathbf{z})$
    \item Liklihood $p_{\thetab}( \xv|\mathbf{z})$
    \item Posterior $p_{\thetab}(\mathbf{z} | \xv)$
  \end{itemize}
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}
\frametitle{VAE-Parameter Fitting}
\vspace{2mm}

Assuming that we know the real parameter ${\theta}^*$ for this distribution. 

To generate a sample from a real data point $x^{(i)}$:

\begin{itemize}
\item sample $x^{(i)}$ from a prior distribution $p_{{\theta}^*} (z)$
\item a value $x^{(i)}$ is generated from a conditional distribution $p_{{\theta}^*} (x|z=z^{(i)})$ 
\end{itemize}

\vspace{1mm}
The optimal parameter ${\theta}^*$ is the one that maximizes the probability of generating real data samples:

\vspace{2mm}
\centering{
${\theta}^* = \argmax_{\theta} \prod_{i=1}^{n}  p_{\thetab} (x^{(i)})$
}
\vspace{2mm}

we use the log probabilities to convert the product on right-hand side to a sum:

\vspace{2mm}
\centering{
${\theta}^* = \argmax_{\theta} \sum_{i=1}^{n} log p_{\thetab} (x^{(i)})$
}

\end{frame}  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%55

\begin{frame}
\frametitle{VAE-Parameter Fitting}
\vspace{2mm}

Now let's update the equation to better demonstrate the data generation process so as to involve the encoding vector:

\vspace{2mm}
\centering{
$ p_{\thetab} (\xv^{(i)}) = \int p_{\thetab} (x^{(i)} | z)  p_{\thetab} (z) dz$ 
}

\vspace{2mm}

Unfortunately it is not easy to compute $ p_{\thetab} (\xv^{(i)}) $ in this way, as it is very expensive to check all the possible values of $z$ and sum them up.
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{VAE-Parameter Fitting: Variational Lower Bound}

%Since $\log p(\xv_s)$ is intractable, optimization is often based on
%\textbf{variational lower bound} (or \textbf{Evidence Lower BOund (ELBO)})
%\textbf{Evidence Lower BOund (ELBO)}
%of log-likelihood for training example $\xv_s$
    \begin{itemize}
\item $\log p_{\thetab}(\xv)$ is intractable.
\item But we can compute a \textbf{variational lower bound}:
%\textbf{Evidence Lower BOund (ELBO)}
\end{itemize}
%\pause
\only<1>{
    \begin{align*}
    \log p_{\thetab}(\xv) &= \log\int p_{\thetab}(\xv,\mathbf{z}) d\mathbf{z} =  \log \int  q_{\varphi}(\mathbf{z}|\xv) \frac{p_{\thetab}(\xv,\mathbf{z})}{q_{\varphi}(\mathbf{z}|\xv)} d\mathbf{z} \\
    &\color{white}{\geq  \int  q(\vec h|\xv_s)  \log \frac{p(\xv_s,\vec h)}{q(\vec h|\xv_s)} d\vec h }\\
    &\color{white}{\geq \int  q(\vec h|\xv_s)  \big( \log p(\xv_s|\vec h) + \log p(\vec h) - \log q(\vec h|\xv_s) \big)d\vec h} \\
    & \color{white}{mathbb{E}_{q(\vec h|\xv_s)}\big[ \log p(\xv_s|\vec h) \big] - KL\big[ q(\vec h|\xv_s) ||  p(\vec h) \big] }
    \end{align*}}
\only<2>{
    \begin{align*}
    \log p_{\thetab}(\xv) &= \log\int p_{\thetab}(\xv,\mathbf{z}) d\mathbf{z} =  \log \int  q_{\varphi}(\mathbf{z}|\xv) \frac{p_{\thetab}(\xv,\mathbf{z})}{q_{\varphi}(\mathbf{z}|\xv)} d\mathbf{z} \\
    &\color{black}{\geq  \int  q_{\varphi}(\mathbf{z}|\xv)  \log \frac{p_{\thetab}(\xv,\mathbf{z})}{q_{\varphi}(\mathbf{z}|\xv)} d\mathbf{z} }\\
    &\color{white}{= \int  q(\vec h|\xv_s)  \big( \log p(\xv_s|\vec h) + \log p(\vec h) - \log q(\vec h|\xv_s) \big)d\vec h} \\
    & \color{white}{mathbb{E}_{q(\vec h|\xv_s)}\big[ \log p(\xv_s|\vec h) \big] - KL\big[ q(\vec h|\xv_s) ||  p(\vec h) \big] }
    \end{align*}
    \begin{block}{Jensen's inequality}
    Let $f$ be a concave function  and $\mathbf{x}$ an integrable random variable. Then it holds:
        $f(\E{[\mathbf{x}]})  \geq \E{[f(\mathbf{x})]}$.
    \end{block}
    
    
}
\only<3>{
    \begin{align*}
    \log p_{\thetab}(\xv) &= \log\int p_{\thetab}(\xv,\mathbf{z}) d\mathbf{z} =  \log \int  q_{\varphi}(\mathbf{z}|\xv) \frac{p_{\thetab}(\xv,\mathbf{z})}{q_{\varphi}(\mathbf{z}|\xv)} d\mathbf{z} \\
    &\color{black}{\geq  \int  q_{\varphi}(\mathbf{z}|\xv)  \log \frac{p_{\thetab}(\xv,\mathbf{z})}{q_{\varphi}(\mathbf{z}|\xv)} d\mathbf{z} }\\
    %&\color{black}{= \int  q_{\varphi}(\mathbf{z}|\xv)  \big( \log p_{\thetab}(\xv|\mathbf{z}) + \log p_{\thetab}(\mathbf{z}) - \log q_{\varphi}(\mathbf{z}|\xv) \big)d\mathbf{z}} \\
    &\color{black}{= \int  q_{\varphi}(\mathbf{z}|\xv)  \big( \log p_{\thetab}(\xv|\mathbf{z}) + \log \frac{p_{\thetab}(\mathbf{z})}{ q_{\varphi}(\mathbf{z}|\xv)} \big)d\mathbf{z}} \\
    & \color{white}{mathbb{E}_{q(\vec h|\xv_s)}\big[ \log p(\xv_s|\vec h) \big] - KL\big[ q(\vec h|\xv_s) ||  p(\vec h) \big] }
    \end{align*}
    %Todo: Box mit Logarithmus-Regeln einbauen!
}
\only<4>{
    \begin{align*}
    \log p_{\thetab}(\xv) &= \log\int p_{\thetab}(\xv,\mathbf{z}) d\mathbf{z} =  \log \int  q_{\varphi}(\mathbf{z}|\xv) \frac{p_{\thetab}(\xv,\mathbf{z})}{q_{\varphi}(\mathbf{z}|\xv)} d\mathbf{z} \\
    &\color{black}{\geq  \int  q_{\varphi}(\mathbf{z}|\xv)  \log \frac{p_{\thetab}(\xv,\mathbf{z})}{q_{\varphi}(\mathbf{z}|\xv)} d\mathbf{z} }\\
    &\color{black}{= \int  q_{\varphi}(\mathbf{z}|\xv)  \big( \log p_{\thetab}(\xv|\mathbf{z}) + \log \frac{p_{\thetab}(\mathbf{z})}{ q_{\varphi}(\mathbf{z}|\xv)} \big)d\mathbf{z}} \\
    & \color{black}{=\mathbb{E}_{q_{\varphi}(\mathbf{z}|\xv)}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big] - KL\big[ q_{\varphi}(\mathbf{z}|\xv) ||  p_{\thetab}(\mathbf{z}) \big] }
    \end{align*}}
\only<5>{
    \begin{align*}
    \log p_{\thetab}(\xv) &= \log\int p_{\thetab}(\xv,\mathbf{z}) d\mathbf{z} =  \log \int  q_{\varphi}(\mathbf{z}|\xv) \frac{p_{\thetab}(\xv,\mathbf{z})}{q_{\varphi}(\mathbf{z}|\xv)} d\mathbf{z} \\
    &\color{black}{\geq  \int  q_{\varphi}(\mathbf{z}|\xv)  \log \frac{p_{\thetab}(\xv,\mathbf{z})}{q_{\varphi}(\mathbf{z}|\xv)} d\mathbf{z} }\\
    &\color{black}{= \int  q_{\varphi}(\mathbf{z}|\xv)  \big( \log p_{\thetab}(\xv|\mathbf{z}) + \log \frac{p_{\thetab}(\mathbf{z})}{ q_{\varphi}(\mathbf{z}|\xv)} \big)d\mathbf{z}} \\
    & \color{black}{=\underbrace{\mathbb{E}_{q_{\varphi}(\mathbf{z}|\xv)}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big] - KL\big[ q_{\varphi}(\mathbf{z}|\xv) ||  p_{\thetab}(\mathbf{z}) \big] }_{:=ELBO(\thetab, \varphi, \xv)} }
    \end{align*}}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5

\begin{frame}
\frametitle{VAE-Parameter Fitting: Variational Lower Bound}

\only<1>{
    \begin{equation*}
    ELBO(\thetab, \varphi, \xv) = \mathbb{E}_{q_{\varphi}(\mathbf{z}|\xv)}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big] - KL\big[ q_{\varphi}(\mathbf{z}|\xv) ||  p_{\thetab}(\mathbf{z}) \big]
    \end{equation*}
    %\begin{itemize}
    %\item \color{white}{First term resembles reconstruction loss}
    %\item \color{white}{Second  term penalizes encoder for deviating from prior}
    %\end{itemize}
}
\only<2>{
    \begin{equation*}
    ELBO(\thetab, \varphi, \xv) = \mathbb{E}_{q_{\varphi}(\mathbf{z}|\xv)}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big] - KL\big[ q_{\varphi}(\mathbf{z}|\xv) ||  p_{\thetab}(\mathbf{z}) \big]
    \end{equation*}
    \begin{itemize}
    \item Also known as \textbf{Evidence Lower BOund (ELBO)}.
    %\item \color{black}{First term resembles reconstruction loss}
    %\item \color{white}{Second  term penalizes encoder for deviating from prior}
    \end{itemize}
}
\only<3>{
    \begin{equation*}
    ELBO(\thetab, \varphi, \xv) = \color{red}{\mathbb{E}_{q_{\varphi}(\mathbf{z}|\xv)}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big]} \color{black}{- KL\big[ q_{\varphi}(\mathbf{z}|\xv) ||  p_{\thetab}(\mathbf{z}) \big]}
    \end{equation*}
    \begin{itemize}
    \item Also known as \textbf{Evidence Lower BOund (ELBO)}.
    \item \color{black}{First term resembles reconstruction loss}.
    %\item \color{white}{Second  term penalizes encoder for deviating from prior}
    \end{itemize}
}
\only<4>{
    \begin{equation*}
    ELBO(\thetab, \varphi, \xv) = \color{black}{\mathbb{E}_{q_{\varphi}(\mathbf{z}|\xv)}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big]} \color{red}{- KL\big[ q_{\varphi}(\mathbf{z}|\xv) ||  p_{\thetab}(\mathbf{z}) \big]}
    \end{equation*}
    \begin{itemize}
    \item Also known as \textbf{Evidence Lower BOund (ELBO)}.
    \item \color{black}{First term resembles reconstruction loss}.
    \item \color{black}{Second  term penalizes encoder for deviating from prior}.
    \end{itemize}
}
\only<5>{
    \begin{equation*}
    ELBO(\thetab, \varphi, \xv) = \mathbb{E}_{q_{\varphi}(\mathbf{z}|\xv)}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big] - KL\big[ q_{\varphi}(\mathbf{z}|\xv) ||  p_{\thetab}(\mathbf{z}) \big]
    \end{equation*}
    \begin{itemize}
    \item Also known as \textbf{Evidence Lower BOund (ELBO)}.
    \item \color{black}{First term resembles reconstruction loss.}
    \color{black}{ \item Second  term penalizes encoder for deviating from prior.}
    \item It can be shown that
    %$\log p(\xv_s) = ELBO(q(\vec h|\xv_s)) + KL\big[ q(\vec h|\xv_s) ||  p(\vec h|\xv ) \big]$
        $ELBO(\thetab, \varphi, \xv) = \log p_{\thetab}(\xv) -  KL\big[ q_{\varphi}(\mathbf{z}|\xv) ||  p_{\thetab}(\mathbf{z}|\xv ) \big]$\\
    $\Rightarrow$ by maximizing the ELBO we maximize $p_{\thetab}(\xv)$ and minimize $KL\big[ q_{\varphi}(\mathbf{z}|\xv) ||  p_{\thetab}(\mathbf{z}|\xv ) \big]$.
    \end{itemize}
}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5

\frame{
    \frametitle{VAE-Model Definition}
    
    %Idea: Use a NN to learn a mapping from some latent variable $\mathbf{z}$ to a complicated distribution on $\xv$.
    
    %$$p(\xv) = \int p(\xv, \mathbf{z}) d\mathbf{z} = \int p(\xv| \vec %z) p_{\thetab}(\mathbf{z}) d\mathbf{z}$$
            %where $p_{\thetab}(\mathbf{z})$ is a some simple distribution and $ p(\xv| \mathbf{z})=g(\mathbf{z})$
            %$$p(\xv, \mathbf{z}) = p(\xv| \mathbf{z}) p_{\thetab}(\mathbf{z})$$
            
            Idea:
            \begin{itemize}
        \item Set $p_{\thetab}(\mathbf{z})$ to some simple distribution. % e.g. $\mathcal{N}(1,0)$.
        \item Parametrize inference model and generative model with  neural networks $f(\xv, \varphi)$ and $g(\mathbf{z}, \thetab)$.
        \end{itemize}
        
        
        \begin{figure}
        \centering
        \includegraphics[width=5cm]{plots/VAE-inferenceModel.png}
        \includegraphics[width=5cm]{plots/VAE-generativeModel2.png}
        \end{figure}
        
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5

\frame{
    \frametitle{VAE-Model Definition}
    
    %Idea: Use a NN to learn a mapping from some latent variable $\mathbf{z}$ to a complicated distribution on $\xv$.
    
    %$$p(\xv) = \int p(\xv, \mathbf{z}) d\mathbf{z} = \int p(\xv| \vec %z) p_{\thetab}(\mathbf{z}) d\mathbf{z}$$
            %where $p_{\thetab}(\mathbf{z})$ is a some simple distribution and $ p(\xv| \mathbf{z})=g(\mathbf{z})$
            %$$p(\xv, \mathbf{z}) = p(\xv| \mathbf{z}) p_{\thetab}(\mathbf{z})$$
            
            
            \only<1>{
                %\vspace{-0.8cm}
                Usually:
                    \begin{itemize}
                \item $f(\xv, \varphi)= \left( \bm{\mu}_{\mathbf{z}}(\xv), \bm{\sigma}_{\mathbf{z}}(\xv) \right)$ and $q_{\varphi}(\mathbf{z}| \xv) = \mathcal{N}(\mathbf{z}; \bm{\mu}_{\mathbf{z}}(\xv), \bm{\sigma}^2_{\mathbf{z}}(\xv))$
                    %\item \color{white}{$p_{\thetab}(\mathbf{z})= \mathcal{N}(\mathbf{z}; 1,0)$}
                %\item \color{white}{$g(\mathbf{z})= \left( \mu_{\xv}(\mathbf{z}), \sigma_{\xv}(\mathbf{z}) \right)$ and $p_{\thetab}(\mathbf{z}| \xv) = \mathcal{N}(\mathbf{z}; \bm{\mu}_{\xv}(\mathbf{z}), \bm{\sigma}_{\xv}(\mathbf{z}))$}
                \end{itemize}
                
                \vspace{1.3cm}
                \begin{figure}
                \hspace{-6cm}\includegraphics[width=5cm]{plots/VAE-inferenceModelGaussian.png}
                %\includegraphics[width=6cm]{VAE-generativeModelGaussian.png}
                \end{figure}
            }
        
        \only<2>{
            Usually:
                \begin{itemize}
            \item $f(\xv, \varphi)= \left( \bm{\mu}_{\mathbf{z}}(\xv), \bm{\sigma}_{\mathbf{z}}(\xv) \right)$ and $q_{\varphi}(\mathbf{z}| \xv) = \mathcal{N}(\mathbf{z}; \bm{\mu}_{\mathbf{z}}(\xv), \bm{\sigma}^2_{\mathbf{z}}(\xv))$
                \item $p_{\thetab}(\mathbf{z})= \mathcal{N}(\mathbf{z}; 0,1)$
                    \pause
                \item $g(\mathbf{z}, \thetab)= \left( \bm{\mu}_{\xv}(\mathbf{z}), \bm{\sigma}_{\xv}(\mathbf{z}) \right)$ and $p_{\thetab}(\xv| \mathbf{z}) = \mathcal{N}(\xv; \bm{\mu}_{\xv}(\mathbf{z}), \bm{\sigma}^2_{\xv}(\mathbf{z}))$
                    \end{itemize}
                
                
                \begin{figure}
                \centering
                \includegraphics[width=5cm]{plots/VAE-inferenceModelGaussian.png}
                \includegraphics[width=6cm]{plots/VAE-generativeModelGaussian.png}
                \end{figure}
        }
        
}


\begin{frame}
\frametitle{VAE-Parameter Fitting: reparameterization Trick}
\begin{itemize}
\item Goal: Learn parameters $\varphi$ and $\thetab$ by maximizing %the ELBO (lower bound of $\log p_{\thetab}(\xv)$) by gradient ascent.
\begin{equation*}
ELBO(\thetab, \varphi, \xv) = \mathbb{E}_{q_{\varphi}(\mathbf{z}|\xv)}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big] - KL\big[ q_{\varphi}(\mathbf{z}|\xv) ||  p_{\thetab}(\mathbf{z}) \big]
\end{equation*}
based on gradient ascent.
\item Idea: Approximate first term
\begin{align*}
\mathbb{E}_{q_{\varphi}(\mathbf{z}|\xv)}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big] &= \mathbb{E}_{\mathbf{z} \sim \mathcal{N}(\mathbf{z}; \bm{\mu}_{\mathbf{z}}(\xv), \bm{\sigma}_{\mathbf{z}}(\xv))}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big]\\
&\approx \frac{1}{L} \sum_{l=1}^{L} \log p_{\thetab}(\xv|\mathbf{z}^{(l)})
\end{align*}
\item  Problem:
    %Given $\mathbf{z} \sim q_{\varphi}(\mathbf{z}| \xv)$,
Given this average,
how should one take derivatives
%(a function of) $\mathbf{z}$
    %$ q_{\varphi}(\mathbf{z}| \xv)$
    w.r.t.~$\varphi$?
    %\item Solution:
    % For $ q_{\varphi}(\vec h| \xv) =\mathcal{N}(\bm{\mu}, \bm{\sigma})$, $\varphi=(\bm{\mu}, \bm{\sigma})$ \\ reparametrize: $\alert{\vec h= \bm{\mu} + \bm{\sigma} \odot \bm{\epsilon}}$, with $\alert{\bm{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{1})}$
    %\item and let $(\bm{\mu}, \bm{\sigma})=f(\xv)$ be a function given by a neural network.
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{VAE-Parameter Fitting: reparameterization Trick}

\begin{block}{Recall: Linear transformation of a normal random variable}
\begin{center}
Let $\bm{\epsilon}$ be standard normally distributed, i.e. $\bm{\epsilon} \sim\mathcal{N}(\bm{\epsilon}; \mathbf{0}, \mathbf{1})$. Then for $\mathbf{z} = \bm{\epsilon} \cdot \bm{\sigma} + \bm{\mu} $ it holds: $\mathbf{z}\sim \mathcal{N}(\mathbf{z}; \bm{\mu}, \bm{\sigma}^2)$
    \end{center}
\end{block}
\only<2>{
    Back to our problem:
        \begin{align*}
    \mathbb{E}_{q_{\varphi}(\mathbf{z}|\xv)}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big] &= \mathbb{E}_{\mathbf{z} \sim \mathcal{N}(\mathbf{z}; \bm{\mu}_{\mathbf{z}}(\xv), \bm{\sigma}_{\mathbf{z}}(\xv))}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big]\\
    &\approx \frac{1}{L} \sum_{l=1}^{L} \log p_{\thetab}(\xv|\mathbf{z}^{(l)})
    \end{align*}
    Solution: Define \color{red}{$\mathbf{z} = \bm{\epsilon} \cdot \bm{\sigma}_{\mathbf{z}}(\xv )+ \bm{\mu}_{\mathbf{z}}(\xv ) $} \color{black}{ where} \color{red}{$\bm{\epsilon} \sim\mathcal{N}(\bm{\epsilon}; \mathbf{0}, \mathbf{1})$. }
}
\only<3>{
    Back to our problem:
        \begin{align*}
    \mathbb{E}_{q_{\varphi}(\mathbf{z}|\xv)}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big] &= \mathbb{E}_{\color{red}{\bm{\epsilon} \sim\mathcal{N}(\bm{\epsilon}; \mathbf{0}, \mathbf{1})}}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big]\\
    &\approx \frac{1}{L} \sum_{l=1}^{L} \log p_{\thetab}(\xv|\mathbf{z}^{(l)})
    \end{align*}
    Solution: Define \color{red}{$\mathbf{z} = \bm{\epsilon} \cdot \bm{\sigma}_{\mathbf{z}}(\xv )+ \bm{\mu}_{\mathbf{z}}(\xv ) $} \color{black}{ where} \color{red}{$\bm{\epsilon} \sim\mathcal{N}(\bm{\epsilon}; \mathbf{0}, \mathbf{1})$. }
}
\only<4>{
    Back to our problem:
        \begin{align*}
    \mathbb{E}_{q_{\varphi}(\mathbf{z}|\xv)}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big] &= \mathbb{E}_{\color{red}{\bm{\epsilon} \sim\mathcal{N}(\bm{\epsilon}; \mathbf{0}, \mathbf{1})}}\big[ \log p_{\thetab}(\xv|\color{red}{\mathbf{z} = \bm{\epsilon} \cdot \bm{\sigma}_{\mathbf{z}}(\xv )+ \bm{\mu}_{\mathbf{z}}(\xv ) }\color{black}{) \big]}\\
        &\approx \frac{1}{L} \sum_{l=1}^{L} \log p_{\thetab}(\xv|\mathbf{z}^{(l)})
        \end{align*}
        Solution: Define \color{red}{$\mathbf{z} = \bm{\epsilon} \cdot \bm{\sigma}_{\mathbf{z}}(\xv )+ \bm{\mu}_{\mathbf{z}}(\xv ) $}\color{black}{ where} \color{red}{$\bm{\epsilon} \sim\mathcal{N}(\bm{\epsilon}; \mathbf{0}, \mathbf{1})$. }
}
\only<5>{
    Back to our problem:
        \begin{align*}
    \mathbb{E}_{q_{\varphi}(\mathbf{z}|\xv)}\big[ \log p_{\thetab}(\xv|\mathbf{z}) \big] &= \mathbb{E}_{\color{red}{\bm{\epsilon} \sim\mathcal{N}(\bm{\epsilon}; \mathbf{0}, \mathbf{1})}}\big[ \log p_{\thetab}(\xv|\color{red}{\mathbf{z} = \bm{\epsilon} \cdot \bm{\sigma}_{\mathbf{z}}(\xv )+ \bm{\mu}_{\mathbf{z}}(\xv ) }\color{black}{) \big]}\\
        &\approx \frac{1}{L} \sum_{l=1}^{L} \log p_{\thetab}(\xv|\color{red}{\mathbf{z}^{(l)} = \bm{\epsilon}^{(l)} \cdot \bm{\sigma}_{\mathbf{z}}(\xv )+ \bm{\mu}_{\mathbf{z}}(\xv ) } )
        \end{align*}
        Solution: Define \color{red}{$\mathbf{z} = \bm{\epsilon} \cdot \bm{\sigma}_{\mathbf{z}}(\xv )+ \bm{\mu}_{\mathbf{z}}(\xv ) $}\color{black}{ where} \color{red}{$\bm{\epsilon} \sim\mathcal{N}(\bm{\epsilon}; \mathbf{0}, \mathbf{1})$. }
}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlecture
\end{document}

