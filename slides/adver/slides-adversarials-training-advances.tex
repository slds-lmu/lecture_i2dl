\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-nn}

\title{Deep Learning}

\date{}

\begin{document}
\newcommand{\titlefigure}{plots/fgsm_mod.png}
%modify picture
\newcommand{\learninggoals}{
  \item Advanced adversarial training 
  \item Projected gradient descent
  \item Fast gradient sign method
  %\item Principal component analysis
}

\lecturechapter{Adversarial Training Advances}
\lecture{I2DL}


\newcommand{\Dsubtrain}{\mathcal{D}_{\text{subtrain}}}
\newcommand{\Dval}{\mathcal{D}_{\text{val}}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Neural Networks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



 \begin{vbframe}{Projected Gradient Descent}
    \begin{itemize}
       \item In contrast to logistic regression, neural networks can have a bumpier, 
       non-convex loss surface.     
       \item As a consequence, Danskin's theorem does not longer hold and the inner optimization problem must be solved approximately. 
       \item One approximation method is projected gradient descent (PGD)$^1$.
        \item Let $\fh$ be the pretrained model, $\xv$ the input to the model, $y$ the target and $L\left(y, \fh(\xv | \thetab) \right)$ the loss function used to train the network.
        \item In each gradient descent step, the basic PGD algorithm updates $\deltab$ as: 
        \begin{equation*}
            \deltab^{[t+1]} = \mathcal{P}\left(\deltab^{[t]} + \alpha \nabla_{\deltab} L\left(y, \fh(\xv + \deltab^{[t]}| \thetab) \right) \right)
        \end{equation*}
        where $\mathcal{P}$ denotes the projection onto the ball of interest. \\
        \lz
        {\scriptsize $^1$ Technically speaking, it is gradient \textit{ascent} since we are maximizing a function, but for the sake of generality, we just refer to the process here as gradient descent.}


        %%%%%%%%%%%%%% Original %%%%%%%%%%%%%%%%%%%5
        % \item In essence, we move with a step size $\alpha$ \textbf{in the direction} of the gradient of the loss with respect to $\deltab$. Then we clip the corresponding values of $\deltab$ back to the chosen perturbation set.  
        % \item In case of $\Delta$ being $\mathcal{B}^{\infty}_{\epsilon}$ the projection is
        % \begin{equation*}
        %     \mathcal{P}_{\Delta}(\deltab) = clip(\deltab, [-\epsilon, \epsilon])
        % \end{equation*}
        % \item One gradient step is then defined as 
        % \begin{equation*}
        %     \deltab^{[t+1]} = clip\left(\delta^{[t]}+ \alpha \nabla_{\deltab} L\left(y, \fh(\xv | \thetab) \right), [-\epsilon, \epsilon]\right)
        % \end{equation*}
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        
        
        
        \item In essence, $\deltab^{[t+1]}$ is obtained by moving from $\deltab^{[t]}$ (with a step size $\alpha$) \textbf{in the direction} of the gradient of the loss with respect to $\deltab$ (evaluated at $\deltab^{[t]}$) and then projecting back to $\Delta$.
        \item In case where the feasible set is $\mathcal{B}^{\infty}_{\epsilon}$, the projection of an arbitrary vector $\hidz$ is simply:
        \begin{equation*}
            \mathcal{P}(\hidz) = clip(\hidz, [-\epsilon, \epsilon])
        \end{equation*}
        \item Therefore, when $\Delta$ is $\mathcal{B}^{\infty}_{\epsilon}$, one gradient step is then defined as: 
        \begin{equation*}
            \deltab^{[t+1]} = clip\left(\deltab^{[t]}+ \alpha \nabla_{\deltab} L\left(y, \fh(\xv + \deltab^{[t]}| \thetab) \right), [-\epsilon, \epsilon]\right)
        \end{equation*}
        \end{itemize}
        \vspace{-0.5cm}
          \begin{figure}
    \centering
      %\captionsetup{font=footnotesize,labelfont=footnotesize, labelfont = bf}
      \scalebox{0.85}{\includegraphics{plots/pgd_mod.png}}
      %\tiny{\\Credit: Kolter and Madry}
      \caption{\small{\textit{Left}: $\deltab^{[t]}+ \alpha \nabla_{\deltab} L(y, \fh(\xv + \deltab^{[t]}| \thetab))$ is projected back to the perturbation set $\Delta$ using $\mathcal{P}$. Here, $\Delta$ is $\mathcal{B}^{2}_{\epsilon}$.
      \textit{Right}: Multiple steps of PGD are shown in case of $\Delta = \mathcal{B}^{\infty}_{\epsilon}$; the projection must be only executed in the last step (Kolter \& Madry, 2019). (Note: A variant of PGD known as \textit{normalized} PGD is shown here. This is why each step has the same size. See Kolter et al. (2019) for details.)}}
  \end{figure}
    
\end{vbframe}
   
     \begin{vbframe} {Fast Gradient Sign Method}
   \begin{itemize}
   \item Fast Gradient Sign Method (FGSM) is a special case of PGD when $\Delta$ = $\mathcal{B}^{\infty}_{\epsilon}$ and $\alpha \rightarrow \infty$ . 
   \item As before, the projection of an arbitrary vector $\hidz$ onto $\mathcal{B}^{\infty}_{\epsilon}$ is $\mathcal{P}(\hidz) = clip(\hidz, [-\epsilon, \epsilon])$. 
   \item As $\alpha \rightarrow \infty$, the elements of $\deltab$ are either set to $-\epsilon$ or $\epsilon$ depending on the sign of the corresponding component of the gradient.
   \item Thus, the FGSM algorithm computes $\deltab$ as
   \small
    \begin{equation*}
      \deltab = \epsilon sign\left(\nabla_{\xv} L\left(y, \fh(\xv | \thetab) \right) \right)
    \end{equation*}
    \normalsize
    \item Note that for FGSM we only conduct one calculation and not multiple gradient descent steps.
    \item Furthermore, because we usually initialize $\deltab^{[0]}$ as $0$     
    \small
    $$\nabla_{\deltab} L\left(y, \fh(\xv + \deltab | \thetab) \right)  = \nabla_{\xv} L\left(y, \fh(\xv | \thetab) \right)$$
    \normalsize
  \begin{figure}
    \centering
      \captionsetup{font=footnotesize,labelfont=footnotesize, labelfont = bf}
      \scalebox{0.32}{\includegraphics{plots/fgsm_mod.png}}
      %\tiny{\\Credit: Kolter and Madry}
      \caption{$\deltab$ is obtained by setting each element of $\nabla_{\xv} L\left(y, \fh(\xv | \thetab) \right)$ to $-\epsilon$ or $\epsilon$ depending on its sign. Note that this slightly changes the direction of the step that is taken (Kolter \& Madry, 2019).}
  \end{figure}
        \item Implicitly, the (element-wise) $sign$ function is simply a way to constrain the size of the "step" that we take in the direction of the gradient. It is basically equal to a projection of the step back on $\Delta$ (which is $\mathcal{B}^{\infty}_{\epsilon}$).
        \end{itemize}
      \framebreak
        \begin{itemize}
           \item Note: The optimal attack against the linear binary classifier we saw in the last section was also FGSM!
        %\item FGSM assumes that the linear approximation of the function given by its gradient at the point $\xv$ is a reasonably good approximation for the function over the entire region $|\deltab|_{\infty} \le \epsilon$. 
            \end{itemize}
  \begin{figure}
    \centering
      \scalebox{1}{\includegraphics{plots/fgsm_panda.png}}
      %\tiny{\\Credit: Goodfellow}
      \caption{\small{By adding an imperceptibly small vector whose elements are equal to the sign of the elements of the gradient of the cost function with respect to the input, GoogLeNet's classification of the image was changed from 'panda' to 'gibbon'. In this example, the $\epsilon$ is 0.007 (Goodfellow, 2017).}}
  \end{figure}
\end{vbframe}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{References}
\begin{vbframe}
\frametitle{References}
\footnotesize{
\begin{thebibliography}{99}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Kolter and Madry (2019)]{1} Zico Kolter and Aleksander Madry (2019)
\newblock Adversarial Robustness - Theory and Practice
\newblock \emph{\url{https://adversarial-ml-tutorial.org/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Goodfellow et al., 2016]{1} Ian Goodfellow, Yoshua Bengio and Aaron Courville (2016)
\newblock Deep Learning
\newblock \emph{\url{http://www.deeplearningbook.org/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Goodfellow, 2017]{1} Ian Goodfellow (2017)
\newblock Lecture 16 | Adversarial Examples and Adversarial Training
\newblock \emph{\url{https://www.youtube.com/watch?v=CIfsB_EYsVI}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Goodfellow et al., 2017]{1}  Ian GoodfellowNicolas PapernotSandy HuangRocky DuanPieter AbbeelJack Clark (2017)
\newblock Attacking Machine Learning with Adversarial Examples
\newblock \emph{\url{https://openai.com/blog/adversarial-example-research/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \bibitem[Hastie et al., 2009]{2} Trevoe Hastie, Robert Tibshirani and Jerome Friedman (2009)
% \newblock The Elements of Statistical Learning
% \newblock \emph{\url{https://statweb.stanford.edu/\%7Etibs/ElemStatLearn/}}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Nguyen et al., 2015]{1} Anh Nguyen, Jason Yosinski and Jeff Clune (2015)
\newblock Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images
\newblock \emph{\url{https://arxiv.org/abs/1412.1897}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Krizhevsky et al., 2012]{1} Alex Krizhevsky, Ilya Sutskever and Geoffrey E. Hinto (2012)
\newblock ImageNet Classification with Deep Convolutional Neural Networks. NIPS. 
\newblock \emph{\url{https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \bibitem[Hinton et al., 2012]{3} Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky Ilya Sutskever and Ruslan Salakhutdinov (2012)
% \newblock Improving neural networks by preventing co-adaptation of feature detectors
% \newblock \emph{\url{http://arxiv.org/abs/1207.0580}}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Maxence Prevost (2018)]{1} Maxence Prevost (2018)
\newblock Adversarial ResNet50 
\newblock \emph{\url{http://arxiv.org/abs/1207.0580}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Sharif et al. (2016)]{1} Mahmood Sharif, Sruti  Bhagavatula and Lujo Bauer (2016)
\newblock Accessorize to a Crime: Real and Stealthy Attacks onState-of-the-Art Face Recognition. Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security.
\newblock \emph{\url{https://dl.acm.org/doi/10.1145/2976749.2978392}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Goodfellow et al., 2014]{5} Goodfellow, Shlens (2014)
\newblock Explaining and Harnessing Adversarial Examples
\newblock \emph{\url{https://github.com/maxpv/maxpv.github.io/blob/master/notebooks/Adversarial_ResNet50.ipynb}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Papernot et al., 2016]{5} Papernot , McDaniel, Goodfellow, Jha, Celik, Swamy (2016)
\newblock Practical Black-Box Attacks against Machine Learning
\newblock \emph{\url{https://arxiv.org/abs/1602.02697}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Athalye et al., 2017]{5} Athalye , Engstrom, Ilyas, Kwok (2017)
\newblock Synthesizing Robust Adversarial Examples
\newblock \emph{\url{https://arxiv.org/abs/1707.07397}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Brown et al., 2018]{1} Tom B. Brown and Catherine Olsson, Research Engineers, Google Brain Team (2018)
\newblock 
Introducing the Unrestricted Adversarial Examples Challenge 
\newblock \emph{\url{https://ai.googleblog.com/2018/09/introducing-unrestricted-adversarial.html}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{thebibliography}
}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\endlecture
\end{document}
