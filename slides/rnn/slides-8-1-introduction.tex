

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-nn}

\newcommand{\titlefigure}{figure/rnn_sample_z5.png}
%modify picture
\newcommand{\learninggoals}{
  \item Motivation
  \item Introduction
  \item Computational Graph
}

\title{Deep Learning}
\date{}



\begin{document}

\lecturechapter{Recurrent Neural Networks - Introduction}
\lecture{I2DL}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {Motivation for RNNs}
  \begin{itemize}
    \item The two major types of neural network architectures that we've seen so far are fully-connected networks and Convolutional Neural Networks (CNNs).
    \item In both cases, the input layers have a fixed size and, therefore, these networks can (typically) only handle fixed-length inputs.
    \item The primary reason for this is that it is not possible to vary the size of the input layer without also varying the number of learnable parameters/weights in the network.
    \item In many cases, we would like to feed \textbf{variable length inputs} to the network.
    \item Common examples of this are \textbf{sequence data} such as time-series, audio and text.
    \item Therefore, we need a new class of neural network architectures that are able to handle such variable length inputs: \textbf{Recurrent Neural Networks (RNNs)}.
  \end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \frame{
 
% \frametitle{RNNs - What for?}
%   So far we encountered two types of data: tabular data and image data.
%   Suppose we would like to process sequential inputs, such as
%   \begin{itemize}
%     \item Text data (for text recognition, machine translation, sentiment classification)
%     \item Audio signal analysis (music generation, speech recognition)
%     \item Time series analysis (to predict the stock market,DNA sequence analysis).
%   \end{itemize}
%   Can we do that with a convnet?
%   \begin{itemize}
%     \item[]
%   \end{itemize}
%   \begin{centering}
%   \begin{minipage}{0.42\textwidth}
%     \begin{figure}
%         \only<1-2>{\includegraphics[width=4.5cm]{plots/neuralnet2.png}}
%         \caption{A dense architecture. \textcolor{white}{bla bla bla blabla blabla blabla blabla blabla bla}}
%     \end{figure}
%   \end{minipage}\hfill
%   \begin{minipage}{0.57\textwidth}
%   \vspace{-0.2cm}
%     \begin{itemize}
%       \only<1>{\item[] \textcolor{white}{bla}} % stupid trick to get rid of compiling error
%       \only<2>{\item[] Hardly, the major drawbacks of these models are:} %The major drawbacks of these models for sequential data are:}
%       \only<2>{\begin{itemize}
%         \only<2>{\item \textbf{a fixed input length}. \\ The length of sequential inputs can vary!}
%         \only<2>{\item \textbf{all the examples are independently and identically distributed}. \\ For sequential inputs, there are short and long term temporal dependencies!}
%       \end{itemize}} 
%     \end{itemize}  
%   \end{minipage}
%   \end{centering}
% }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section{RNNs -- The basic idea}

\begin{frame} {RNNs - Introduction}
  \begin{itemize}
    \item \small{Suppose we have some text data and our task is to analyse the \textit{sentiment} in the text.
    \item For example, given an input sentence, such as "This is good news.", the network has to classify it as either 'positive' or 'negative'.
    \item We would like to train a simple neural network (such as the one below) to perform the task.}
  \end{itemize}
  \begin{figure}
      \centering
      \scalebox{0.6}{\includegraphics{figure/ffwd.png}}
      \caption{\footnotesize{Two equivalent visualizations of a dense net with a single hidden layer.}}
  \end{figure}
\end{frame}

\begin{frame} {RNNs - Introduction}
  \begin{itemize}
    \item Because sentences can be of varying lengths, we need to modify the dense net architecture to handle such a scenario.
    \item One approach is to draw inspiration from the way a human reads a sentence; that is, one word at a time.
    \item An important cognitive mechanism that makes this possible is "\textbf{short-term memory}".
    \item As we read a sentence from beginning to end, we retain some information about the words that we have already read and use this information to understand the meaning of the entire sentence.
    \item Therefore, in order to feed the words in a sentence sequentially to a neural network, we need to give it the ability to retain some information about past inputs.
    %\item \texbf{We need to track long-term dependencies.}
  \end{itemize}
\end{frame}
 
\begin{frame} {RNNs - Introduction}
  \begin{itemize}
   \item %It's important to note that 
    When words in a sentence are fed to the network one at a time, the inputs are no longer independent. For example, it is much more likely that the word "good" is followed by "morning" rather than "plastic". \textbf{We need to model this (long-term) dependence.} 
    \item %Even though we've decided to feed a single word at a time, 
    Each word must still be encoded as a fixed-length vector because the size of the input layer will remain fixed.
    \item Here, for the sake of the visualization, each word is represented as a 'one-hot coded' vector of length 5. (<eos> = 'end of sequence')
    \begin{figure}
      \centering
      \scalebox{0.55}{\includegraphics{figure/one_hot_1.png}}
  \end{figure}
    (The standard approach is to use word embeddings (more on this later)).
  \end{itemize}
\end{frame}

\begin{frame} {RNNs - Introduction}
  \begin{itemize}
    \item Our goal is to feed the words to the network sequentially in discrete time-steps.
    \item A regular dense neural network with a single hidden layer only has two sets of weights: 'input-to-hidden' weights $\bm{W}$ and 'hidden-to- output' weights $\bm{U}$.
  \end{itemize}
  \begin{figure}
      \centering
      \scalebox{0.7}{\includegraphics{figure/ffwd1.png}}
  \end{figure}
\end{frame}

\begin{frame} {RNNs - Introduction}
  \begin{itemize}
    \item \small{In order to enable the network to retain information about past inputs, we introduce an \textbf{additional set of weights} $\bm{V}$, from the hidden neurons at time-step $t$ to the hidden neurons at time-step $t+1$.
    \item Having this additional set of weights makes the activations of the hidden layer depend on \textbf{both} the current input and the activations for the \textit{previous} input.}
  \end{itemize}
  \begin{figure}
      \centering
      \scalebox{0.80}{\includegraphics{figure/hidrohid.png}}
      \caption{\footnotesize Input-to-hidden weights $\bm{W}$ and \textbf{hidden-to-hidden} weights $\bm{V}$. The hidden-to-output weights $\bm{U}$ are not shown in the figure.}
  \end{figure}
\end{frame}

\begin{frame} {RNNs - Introduction}
  \begin{itemize}
    \item With this additional set of hidden-to-hidden weights $\bm{V}$, the network is now a Recurrent Neural Network (RNN).
    \item In a regular feed-forward network, the activations of the hidden layer are only computed using the input-hidden weights $\bm{W}$ (and bias $\bm{b}$).
    $$\bm{z}= \sigma(\bm{W}^\top \xv + \bm{b})$$
    \item In an RNN, the activations of the hidden layer (at time-step $t$) are computed using \textit{both} the input-to-hidden weights $W$ and the hidden-to-hidden weights $\bm{V}$.
    $$\bm{z}^{[t]} = \sigma(\mathbf{\textcolor{red}{\bm{V}^\top\bm{z}^{[t-1]}}} + \bm{W}^\top \bm{x}^{[t]} + \bm{b})$$
    \item The vector $\bm{z}^{[t]}$ represents the short-term memory of the RNN because it is a function of the current input $\bm{x}^{[t]}$ and the activations $\bm{z}^{[t-1]}$ of the previous time-step.
    \item Therefore, by recurrence, it contains a "summary" of \textit{all} previous inputs. 
  \end{itemize}
\end{frame}




\begin{frame} {Application example - Sentiment Analysis}
  \begin{itemize}
    \item At $t = 0$, we feed the word "This" to the network and obtain $\bm{z}^{[0]}$.
    \item $\bm{z}^{[0]} = \sigma(\bm{W}^\top \bm{x}^{[0]} + \bm{b})$
  \end{itemize}
  \begin{figure}
      \centering
      \scalebox{0.55}{\includegraphics{figure/rnn_sample_z0.png}}
  \end{figure}
  Because this is the very first input, there is no past state (or, equivalently, the state is initialized to 0).
\end{frame}

\begin{frame} {Application example - Sentiment Analysis}
  \begin{itemize}
    \item At $t = 1$, we feed the second word to the network to obtain $\bm{z}^{[1]}$.
    \item $\bm{z}^{[1]} = \sigma(\bm{V}^\top\textcolor{red}{\bm{z}^{[0]}} + \bm{W}^\top \xv^{[1]} + \bm{b})$
  \end{itemize}
  \begin{figure}
      \centering
      \scalebox{0.55}{\includegraphics{figure/rnn_sample_z1.png}}
  \end{figure}
\end{frame}

\begin{frame} {Application example - Sentiment Analysis}
  \begin{itemize}
    \item At $t = 2$, we feed the next word in the sentence.
    \item $\bm{z}^{[2]} = \sigma(\bm{V}^\top\textcolor{red}{\bm{z}^{[1]}} + \bm{W}^\top \xv^{[2]} + \bm{b})$
  \end{itemize}
  \begin{figure}
      \centering
      \scalebox{0.55}{\includegraphics{figure/rnn_sample_z2.png}}
  \end{figure}
\end{frame}

\begin{frame} {Application example - Sentiment Analysis}
  \begin{itemize}
    \item At $t = 3$, we feed the next word ("news") in the sentence.
    \item $\bm{z}^{[3]} = \sigma(\bm{V}^\top\textcolor{red}{\bm{z}^{[2]}} + \bm{W}^\top \xv^{[3]} + \bm{b})$
  \end{itemize}
  \begin{figure}
      \centering
      \scalebox{0.65}{\includegraphics{figure/rnn_sample_z3.png}}
  \end{figure}
\end{frame}

\begin{frame} {Application example - Sentiment Analysis}
  \begin{itemize}
    \item Once the entire input sequence has been processed, the prediction of the network can be generated by feeding the activations of the final time-step to the output neuron(s).
    \item $f = \sigma (\bm{U}^\top \bm{z}^{[4]} + {c})$, where $c$ is the bias of the output neuron.
  \end{itemize}
  \begin{figure}
      \centering
      \scalebox{0.65}{\includegraphics{figure/rnn_sample_z5.png}}
  \end{figure}
\end{frame}

\begin{frame} {Parameter Sharing}
  \begin{itemize}
    \item This way, the network can process the sentence one word at a time and the length of the network can vary based on the length of the sequence.
    \item It is important to note that no matter how long the input sequence is, the matrices $\bm{W}$ and $\bm{V}$ are the same in every time-step. This is another example of \textbf{parameter sharing}.
    \item Therefore, the number of weights in the network is independent of the length of the input sequence.
  \end{itemize}
\end{frame}

\begin{frame} {Sequence Modeling - Design Criteria}

To model sequence we need to:


  \begin{itemize}
    \item Handle variable-length sequence,
    \item Track long-term dependencies,
    \item Maintain information about order,
    \item Share parameter across the sequence.
  \end{itemize}

  \begin{figure}
      \centering
      \scalebox{0.2}{\includegraphics{figure/vanilla_rnn.png}}
      \caption{\footnotesize {Vanilla RNN}}
  \end{figure}

\end{frame}

\begin{frame} {RNNs - Use Case specific architectures}

  \small{RNNs are very versatile. They can be applied to a wide range of tasks.
  
  \begin{figure}
      \centering
      \scalebox{0.9}{\includegraphics{figure/one_to_one.png}}
      \caption{\footnotesize {RNNs can be used in tasks that involve multiple inputs and/or multiple outputs. }}
  \end{figure}
  Examples:}
  \begin{itemize}
    \item \small{Sequence-to-One: Sentiment analysis, document classification.
    \item One-to-Sequence: Image captioning.
    \item Sequence-to-Sequence: Language modelling, machine translation, time-series prediction.}
  \end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section{RNNs - Computational Graph}

\frame{
\frametitle{RNNs - Computational Graph}
  \center
  \begin{figure}%
    \only<1>{\includegraphics[width=7cm]{figure/nonrecurrent.png}}%
%   \only<2-3>{\includegraphics[width=7cm]{plots/rnn_comp2.png}}%
    \only<2>{\includegraphics[width=1.5cm]{figure/recurrent_single.png}}%
    \only<3-4>{\includegraphics[width=7cm]{figure/recurrent_unfolded.png}}%
    \only<5>{\includegraphics[width=1.5cm]{figure/rnn_zt_figure.png}}%
  \end{figure}%
  \vspace{-0.2cm}
  \begin{itemize}
    \only<1>{\item On the left is the computational graph for the dense net on the right. A loss function $L$ measures how far each output
    $f$ is from the corresponding training target $y$. }
    % \only<2-3>{\item In order to derive RNNs we have to extend our notation.}
    % \only<3>{\begin{itemize}
    %   \only<3>{\item So far, we mapped some inputs $x$ to outputs $f$:}
    %   \only<3>{\item[] $f = \tau(c + U^T z) = \tau(c + U^T \sigma(b + W^T x))$}
    %   \only<3>{\item[] ..with $W$ and $U$ being weight matrices.}
    % \end{itemize}}
    \only<2>{\item A helpful way to think of an RNN is as multiple copies of the same network, each passing a message to a successor.}
    \only<2>{\item RNNs are networks with loops, allowing information to persist.}
    \only<3>{\item Things might become more clear if we unfold the architecture.}
    \only<3>{\item We call $\bm{z}^{[t]}$ the \textit{state} of the system at time $t$.}
    \only<3>{\item Recall, the state contains information about the whole past sequence.}
    \only<4>{\item We went from 
      \begin{eqnarray*} 
        f &=& \tau(c + \bm{U}^\top \sigma(\bm{b} + \bm{W}^\top \bm{x})) \text{ for the dense net, to } \\
        f^{[t]} &=& \tau(c + \bm{U}^\top \sigma(\bm{b} + \bm{V}^\top \bm{z}^{[t-1]} + \bm{W}^\top \xv^{[t]})) \text{ for the RNN. }
      \end{eqnarray*}}
    \only<5>{\item Potential computational graph for time-step $t$:}
    \only<5>{\item[] $$f^{[t]} = \tau(c + \bm{U}^\top \sigma(\bm{b} + \bm{V}^\top \bm{z}^{[t-1]} + \bm{W}^\top \xv^{[t]})) $$ }
  \end{itemize}
}


\frame{
\frametitle{RNNs - Computational Graph with recurrent output-hidden connections}

Recurrent connections do not need to map from hidden to hidden neurons!

\center
\begin{figure}
\includegraphics[width=7cm]{figure/recurrent_unfolded_fromoutput.png} 
     \tiny{\\ An RNN whose only recurrence is the feedback connection from the output to the hidden layer. At each time stept, the input is $x_t$, the hidden layer activations are $z^{(t)}$, the outputs are $f^{(t)}$, the targets are $y^{(t)}$, and the loss is $L^{(t)}$. (Left) Circuitdiagram. (Right) Unfolded computational graph. The RNN in this figure is trained to put a specific output value into $f$, and $f$ is the only information it is allowed to send to the future. There are no direct connections from $z$ going forward. The previous $z$ is connected to the present only indirectly, via the predictions it was used to produce.}
\end{figure}


}

\frame{
\frametitle{RNNs - Computational Graph for seq to one mapping}

RNNs do not need to produce an output at each time step. Often only one output is produced after processing the whole sequence. 

\center
\begin{figure}
\includegraphics[width=5cm]{figure/rnn_last_output.png} 
     \tiny{\\Time-unfolded recurrent neural network with a single output at the end of the sequence. Such a network can be used to summarize a sequence and produce a fixed size representation used as input for further processing. There might be a target right at the end (as depicted here), or the gradient on the output $f^{(T)}$ can be obtained by backpropagating from further downstream modules.}
\end{figure}

}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}
\frametitle{References}
\footnotesize{
\begin{thebibliography}{99}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Ian Goodfellow et al., 2016]{1} Ian Goodfellow, Yoshua Bengio and Aaron Courville (2016)
\newblock Deep Learning
\newblock \emph{\url{http://www.deeplearningbook.org/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Andrej Karpathy., 2015]{1} Andrej Karpathy (2015)
\newblock The Unreasonable Effectiveness of Recurrent Neural Networks
\newblock \emph{\url{http://karpathy.github.io/2015/05/21/rnn-effectiveness/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{thebibliography}
}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlecture
\end{document}