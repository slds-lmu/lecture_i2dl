\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-nn}

\newcommand{\titlefigure}{figure/regu_pic.png}
\newcommand{\learninggoals}{
  \item Regularized cost functions
  \item Norm penalties
  \item Weight decay
  \item Equivalence with constrained optimization
}

\title{Deep Learning}
\date{}

\begin{document}

\lecturechapter{Basic Regularization}
\lecture{I2DL}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {Regularization}
\begin{itemize}
\item Any technique that is designed to reduce the test error possibly at the expense of increased training error can be considered a form of regularization.
\item Regularization is important in DL because NNs can have extremely high capacity (millions of parameters) and are thus prone to overfitting.
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Revision: Regularized Risk Minimization}
\begin{itemize}
\item The goal of regularized risk minimization is to penalize the complexity of the model to minimize the chances of overfitting.
\item By adding a parameter norm penalty term \(J(\thetav)\) to the empirical risk $\risket$ we obtain a regularized cost function:
$$\riskrt = \risket + \lambda \text{\(J(\thetav)\)}$$
with hyperparamater $\lambda \in [0, \infty)$, that weights the penalty term, relative to the unconstrained objective function $\risket$.
\item Therefore, instead of pure \textbf{empirical risk minimization}, we add a penalty
for complex (read: large) parameters \(\thetav\).
\item Declaring $\lambda = 0$ obviously results in no penalization.
\item We can choose between different parameter norm penalties \(J(\thetav)\).
\item In general, we do not penalize the bias.
\end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{L2-regularization / Weight decay}
Let us optimize the L2-regularized risk of a model $\fxt$
\[\min_{\thetav} \riskrt = \min_{\thetav} \risket + \frac{\lambda}{2} \|\thetav\|^2_2\]
by gradient descent. The gradient is
\[\nabla \riskrt = \nabla \risket + \lambda \thetav.\]
We iteratively update $\thetav$ by step size \(\alpha\) times the
negative gradient
\[\thetav^{[\text{t+1}]} = \thetav^{[\text{t}]} - \alpha \left(\nabla \risket + \lambda \thetav^{[\text{t}]}\right) =
\thetav^{[\text{t}]} (1 - \alpha \lambda) - \alpha \nabla \risket\]
$\to$ The term \(\lambda \thetav^{[t]}\) causes the parameter
(\textbf{weight}) to \textbf{decay} in proportion to its size (which gives rise to the name). 
\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Equivalence to Constrained Optimization}
Norm penalties can be interpreted as imposing a constraint on the weights. One can show that 
 $$\argmin_{\theta} \risket + \lambda \text{\(J(\thetav)\)}$$
 is equvilalent to
 \begin{align*}
 & \argmin_{\thetav}  \risket \\
  &\text{subject to \;\;\;}  J(\thetav) \leq k
 \end{align*}
  for some value $k$ that depends on $\lambda$ the nature of 
 $\risket$ (Goodfellow et al., 2016)
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Example: Weight decay}
\begin{minipage}{0.45\textwidth}
\begin{itemize}
\item We fit the huge neural network on the right side on a smaller fraction of MNIST (5000 train and 1000 test observations)
\item Weight decay: $\lambda \in (10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}, 0)$
\end{itemize}
\end{minipage}
\begin{minipage}{0.45\textwidth}
\begin{figure}
\centering
\includegraphics[width= 8.7cm, height=7.7cm]{figure/mxnet_graph_decay.png}
\end{figure}
\end{minipage}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
\centering
\scalebox{1}{\includegraphics{figure_man/weight_decay1.pdf}}
\\
A high weight decay of $10^{-2}$ leads to a high error on the training data.
\end{figure}

\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
\centering
\scalebox{1}{\includegraphics{figure_man/weight_decay2.pdf}}
\\
Second strongest weight decay leads to the best result on the test data.
\end{figure}
\end{vbframe}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Tensorflow Playground}
\begin{figure}
\centering
\scalebox{0.8}{\includegraphics{figure/tensorflow_playground.png}}
  \caption{\url{https://playground.tensorflow.org/}, (Carter)}
  \end{figure}
\end{frame}

\begin{frame}{Tensorflow Playground - Exercise}
  \begin{figure}
    \centering
      \scalebox{0.8}{\includegraphics{figure/tensorflow_exercise.png}}
    \caption{\url{https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/playground-exercise-examining-l2-regularization} (Google)}
  \end{figure}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}
\frametitle{References}
\footnotesize{
\begin{thebibliography}{99}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[(Goodfellow, 2016)]{1} 
Goodfellow, I., Bengio, Y., \& Courville, A. (2016). \textit{Deep Learning}. MIT Press. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[(MathWorks)]{2} 
MathWorks. (n.d.). \textit{Regularization}. MATLAB \& Simulink. \url{https://de.mathworks.com/discovery/regularization.html}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[(Carter)]{3} 
Carter, D. S. and S. (n.d.). \textit{Tensorflow - Neural Network Playground. A Neural Network Playground}. \url{https://playground.tensorflow.org} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[(Google)]{4} Google. (n.d.). \textit{Regularization for Simplicity: Playground Exercise (L2 Regularization)}. Google. \url{https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/playground-exercise-examining-l2-regularization?hl=de} 
\end{thebibliography}
}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlecture
\end{document}