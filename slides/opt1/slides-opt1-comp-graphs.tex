
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-nn}

\newcommand{\titlefigure}{figure/backprop_gg_new.png}
\newcommand{\learninggoals}{
  \item Chain rule of calculus
  \item Computational graphs
}

\newenvironment{rightbrace}
  {\left.\begin{aligned}}
  {\end{aligned}\right\rbrace}

\title{Deep Learning}
\date{}

\begin{document}

\lecturechapter{Chain Rule and Computational Graphs}
\lecture{I2DL}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{frame}
%\frametitle{Lecture outline}
%\tableofcontents
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}{Local Minima/Convexity}
% \end{frame}

% \section{Chain Rule and Computational Graphs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Chain rule of calculus}
  % \begin{itemize}
  %   \item The chain rule can be used to compute derivatives of the composition of two or more functions.
  %   \item Let $x \in \R^m$, $y \in \R^n$, \\
  %         $g: \R^m \to \R^n$ and $f: \R^n \to \R$. \\
  %   \item If $y = g(x)$ and $z = f(y)$, the chain rule yields $$\frac{\partial z}{\partial x_i} = \sum_j \frac{\partial z}{\partial y_j} \frac{\partial y_j}{\partial x_i}$$
  %         or in vector notation $$\nabla_x z = \Big(\frac{\partial y}{\partial x}\Big)^\top \nabla_y z,$$
  %         where $\frac{\partial y}{\partial x}$ is the $n \times m$ jacobian matrix of $g$.
  % \end{itemize}
  \begin{itemize}
    \item The chain rule can be used to compute derivatives of the composition of two or more functions.
    \item Let $\xv \in \R^m$, $\mathbf{y} \in \R^n$, \\
          $g: \R^m \to \R^n$ and $f: \R^n \to \R$. \\
    \item If $\mathbf{y} = g(\xv)$ and $z = f(\mathbf{y})$, the chain rule yields: $$\frac{\partial z}{\partial x_i} = \sum_j \frac{\partial z}{\partial y_j} \cdot \frac{\partial y_j}{\partial x_i}$$
          or, in vector notation: $$\nabla_{\xv} z = \Big(\frac{\partial \mathbf{y}}{\partial \xv}\Big)^\top \nabla_{\mathbf{y}} z,$$
          where $\frac{\partial \mathbf{y}}{\partial \xv}$ is the ($n \times m$) Jacobian matrix of $g$.
  \end{itemize}
\end{vbframe}  

\begin{vbframe}{Computational graphs}
  \begin{minipage}{0.5\textwidth}
    \begin{itemize}
      \item CGs are nested expresssions, visualized as graphs.
      \item Each node is a variable, either an input or derived.
      \item Derived variables are functions applied to other variables.
    \end{itemize}
  \end{minipage}\hfill
  \begin{minipage}{0.5\textwidth}
    \begin{figure}
      \centering
        \includegraphics[width=6cm]{figure/compgraph1.png}
        \tiny{\\source : Goodfellow et al. (2016)}
        \caption{The computational graph for the expression $H = \sigma(XW + B)$ with activation function $\sigma(\cdot)$.}
    \end{figure}
  \end{minipage}  
\end{vbframe}

\begin{vbframe}{Chain rule of calculus: Example 1}
  \begin{minipage}{0.5\textwidth}
    \begin{itemize}
      \item Suppose we have the following computational graph.
      \item To compute the derivative of $\frac{\partial z}{\partial w}$ %$\frac{\partial z}{\partial w}$ 
      we need to recursively apply the chain rule. That is:
      \begin{eqnarray*}
        \frac{\partial z}{\partial w} &=& \frac{\partial z}{\partial y} \cdot \frac{\partial y}{\partial x} \cdot \frac{\partial x}{\partial w} \\
                                  &=& f'_3(y) \cdot f'_2(x) \cdot f'_1(w) \\
                                  &=& f'_3(f_2(f_1(w))) \cdot f'_2(f_1(w)) \cdot f'_1(w)
      \end{eqnarray*}
    \end{itemize}
  \end{minipage}\hfill
  \begin{minipage}{0.32\textwidth}
    \begin{figure}
      \centering
        \includegraphics[width=1cm]{figure/compgraph2.png}
        \begin{footnotesize}
        \tiny{\\source : Goodfellow et al. (2016)}
        \caption{A computational graph, such that $x = f_1(w),$ $y = f_2(x)$ and $z = f_3(y)$.}
        \end{footnotesize}
    \end{figure}
  \end{minipage}
% \framebreak
%   \begin{figure}
%     \centering
%       \includegraphics[width=4.5cm]{figure/compgraph3.png}
%       \caption{Applying the chain rule to the example yields us a computational graph with a symbolic description of the
% derivatives.}
%   \end{figure}  
\end{vbframe}

\begin{frame}{Chain rule of calculus: Example 2}

   \begin{figure}
    \centering
      \scalebox{0.27}{\includegraphics{figure/chain_tree.png}}
  \end{figure}
  
To compute $\nabla_\xv z$, we apply the chain rule % to the computational graph above,
  \begin{itemize}
    \item $\frac {\partial z}{\partial x_1} = \sum_j \frac{\partial z}{\partial y_j} \frac{\partial y_j}{\partial x_1} = \frac {\partial z}{\partial y_1} \frac {\partial y_1}{\partial x_1} + \frac {\partial z}{\partial y_2} \frac {\partial y_2}{\partial x_1}$
    \item $\frac {\partial z}{\partial x_2} = \sum_j \frac{\partial z}{\partial y_j} \frac{\partial y_j}{\partial x_2} = \frac {\partial z}{\partial y_1} \frac {\partial y_1}{\partial x_2} + \frac {\partial z}{\partial y_2} \frac {\partial y_2}{\partial x_2}$
  \end{itemize}
  \vspace{2mm}
    Therefore, the gradient of $z$ w.r.t $\xv$ is
    \begin{itemize}
      \item  $\nabla_\xv z = \begin{bmatrix}
               \frac {\partial z}{\partial x_1} \\
               \frac {\partial z}{\partial x_2} \\
             \end{bmatrix} = \underbrace{\begin{bmatrix} \frac{\partial y_1}{\partial x_1}&\frac {\partial y_2}{\partial x_1}\\
                                             \frac {\partial y_1}{\partial x_2}&\frac {\partial y_2}{\partial x_2}\\
             \end{bmatrix}}_{\textcolor{red}{(\frac{\partial \mathbf{y}}{\partial \xv})^\top}} \underbrace{\begin{bmatrix} \frac {\partial z}{\partial y_1} \\
            \frac {\partial z}{\partial y_2} \\ \end{bmatrix}}_{\textcolor{red}{\nabla_{\mathbf{y}} z}} = \Big(\frac{\partial \mathbf{y}}{\partial \xv}\Big)^\top \nabla_{\mathbf{y}} z $
  \end{itemize}
\end{frame}



\begin{frame} {Computational Graph: Neural Net}
  \begin{figure}
      \centering
        \scalebox{0.75}{\includegraphics{figure/neo_comp.png}}
        \caption{A neural network can be seen as a computational graph. $\phi$ is the weighted sum and $\sigma$ and $\tau$ are the activations. \\
        Note: In contrast to the top figure, the arrows in the computational graph below merely indicate \textbf{dependence}, not weights.}
    \end{figure}
\end{frame}
% \begin{frame}{Chain rule of calculus}
% \begin{figure}
%     \centering
%       \scalebox{0.4}{\includegraphics{figure/chain_tree.png}}
%   \end{figure}
%   
%     $\nabla_x z = \begin{bmatrix}
%            \frac {\partial z}{\partial x_1} \\
%            \frac {\partial z}{\partial x_2} \\
%          \end{bmatrix} = \begin{bmatrix}
%            \frac {\partial z}{\partial x_1} \\
%            \frac {\partial z}{\partial x_2} \\
%          \end{bmatrix}$
% \end{frame}

% ------------------------------------------------------------------------------

\begin{vbframe}{Differentiation with graphs}

\small
Computational graphs are much more than a means of visualization -- they make 
complex computations feasible in an \textbf{automated} manner.
\vfill

Let's use a graph to calculate derivatives step by step for a very simple 
example. We compute an expression $e$ from 
two inputs $a$ and $b$:
$$e = (a + b) \cdot (b + 1)$$

\vfill

\begin{minipage}[c]{0.55\textwidth}
  \raggedright \small
  \begin{itemize}
    \item We can translate this into a graph.
    \item Inputs and elementary 
    operations \\($+$, $\cdot$, \dots) are assigned to \textbf{nodes}.
    \item Each node's output is propagated to the subsequent nodes:
    \begin{equation*}
      \begin{rightbrace}
          c &= a + b ~~ \\
          d &= b + 1
      \end{rightbrace}
      ~~ e = c \cdot d
    \end{equation*}
  \end{itemize}
\end{minipage}%
\begin{minipage}{0.05\textwidth}
\phantom{foo}
\end{minipage}%
\begin{minipage}[c]{0.4\textwidth}
  \includegraphics[width=\textwidth]{figure_man/compgraph_example_1}
  
  \tiny{\url{https://colah.github.io/posts/2015-08-Backprop/}}
\end{minipage}

\begin{itemize} 
  \item For $a = 2$ and $b = 1$ the graph evaluates to $e = 6$.
\end{itemize}

\framebreak
\normalsize

\begin{itemize} 
  \item Just like forward evaluation, we can employ graphs to go 
  \textbf{backwards}, i.e., take \textbf{partial derivatives}
  w.r.t. to inputs.
  \item Since graphs are nothing but compositions of elementary operations 
  with trivial derivatives, we can do this via \textbf{chain rule}.
  \item Two simple rules hold: we multiply derivatives along edges and sum 
  over all paths connecting an input to the output.
  \item However, we might quickly run into a \textbf{combinatorial explosion}:
  
  \includegraphics[width=0.8\textwidth]{figure_man/compgraph_example_3}
  
  \framebreak 
  
  \item The trick is to exploit the commutativity of the chain rule.
  \begin{itemize} 
    % \small
    \item We can also go from right to left without changing the result.
    \item Taking a closer look, we see that the different partial derivatives 
    share multiplicative components.
    \item in 
    this \textbf{reverse-mode differentiation}.
  \end{itemize}
\end{itemize}


\framebreak

\begin{itemize}
  \footnotesize
  \item Recall that we need first-order derivatives for gradient descent.
  \item We use the chain rule again to compute the respective impacts of $a$ and 
  $b$ on $e$:
\end{itemize}

\vspace{0.3cm}

\begin{minipage}{0.45\textwidth}
  \begin{itemize}
    \scriptsize
    \item $\nabla_a e = \underbrace{\textcolor{blue}{\pd{e}{c}}}_{d = 2} \cdot 
    \underbrace{\pd{c}{a}}_{1} = 2$
    \item $\nabla_b e = \underbrace{\pd{e}{d}}_{c = 3} \cdot 
    \underbrace{\pd{d}{b}}_{1} + \underbrace{\textcolor{blue}{\pd{e}{c}}}_{d = 2} 
    \cdot \underbrace{\pd{c}{b}}_{1} = 5$
  \end{itemize}
\end{minipage}%
\begin{minipage}{0.05\textwidth}
\phantom{foo}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\includegraphics[width=\textwidth]{figure_man/compgraph_example_2}
\end{minipage}

\vspace{0.3cm}

\begin{itemize} 
  \footnotesize
  \item The same calculations we have done with pen and paper on the left side 
  can be outsourced to computational graphs.
  \item By assembling modules of basic computations (think of elementary 
  operations such as sums, logarithms etc.), graphs are able to 
  calculate derivatives for \textbf{arbitrarily complex} function compositions.
  \item Also note how the chain rule exploits the \textcolor{blue}{reuse} of
  components, so they need to be computed only once.
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame} {Simple Example?}
%   \begin{figure}
%     \centering
%       \scalebox{1}{\includegraphics{figure/simex.png}}
%   \end{figure}
% \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section{References}

%\begin{vbframe}
%\frametitle{References}
%\footnotesize{
%\begin{thebibliography}{99}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\bibitem[Ian Goodfellow et al., 2016]{1} Ian Goodfellow, Yoshua Bengio and Aaron Courville (2016)
%\newblock Deep Learning
%\newblock \emph{\url{http://www.deeplearningbook.org/}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\end{thebibliography}
%}
%\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlecture
\end{document}
