\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-nn}

\begin{document}
\newcommand{\titlefigure}{figure/cifar10_eg.png}
%modify picture
\newcommand{\learninggoals}{
  \item Application of CNNs in Visual Recognition
}

\lecturechapter{CNN Applications}
\lecture{I2DL}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Application - Image Classification}
    \begin{itemize}
        \item One use case of CNNs is image classification.
        \item There exists a broad variety of network architectures for image classification such as the LeNet, AlexNet, InceptionNet and ResNet which will be discussed later in detail.
        \item All these architectures rely on a set of sequences of convolutional layers and aim to learn the mapping from an image to a probability score over a set of classes.
    \end{itemize}
\framebreak
    \begin{figure}
        \centering
        \includegraphics[width=7cm]{figure/recognition.png}
        \caption{Image classification with Cifar-10: famous benchmark dataset with 60000 images and 10 classes (Krizhevsky, 2009). There is also a much more difficult version with 60000 images and 100 classes.}
    \end{figure}
\framebreak
    \begin{figure}
        \centering
        \includegraphics[width=4cm]{figure/cifar_frog.png}
        \caption{One example of the Cifar-10 data: A highly pixelated, coloured image of a frog with dimension [3, 32, 32] (Krizhevsky, 2009). }
    \end{figure}
\framebreak
    \begin{figure}
        \centering
          \scalebox{1}{\includegraphics{figure/cifar10_eg.png}}
        \caption{An example of a CNN architecture for classification on the Cifar-10 dataset (FC = Fully Connected) (based on Krizhevsky, 2009).}
    \end{figure}  
\end{vbframe}

\begin{frame} {CNN vs a Fully Connected net on Cifar-10}
  \begin{figure}
        \centering
          \scalebox{0.75}{\includegraphics{figure/cnn_vs_dense_1.png}}
        \caption{Performance of a CNN and a fully connected neural net ("Dense") on Cifar-10. Both networks have roughly the same number of layers. They were trained using the same learning rate, weight decay and dropout rate. The CNN performs better as it has the right inductive bias for this task (based on Krizhevsky, 2009).}
    \end{figure} 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Application - Beyond Image Classification}
There are many visual recognition problems that are related to image classification, such as:

\begin{itemize}
    \item object detection
    \item image captioning
    \item semantic segmentation
    \item visual question answering
    \item visual instruction navigation
    \item scene graph generation
\end{itemize}
\end{vbframe}


\begin{vbframe}{Application - Image Colorization}
    \begin{itemize}
        \item Basic idea (introduced by Zhang et al., 2016):
        \begin{itemize}
            \item Train the net on pairs of grayscale and colored images.
            \item Force it to make a prediction on the color-value \textbf{for each pixel} in the grayscale input image.
            \item Combine the grayscale-input with the color-output to yield a colorized image.
        \end{itemize}
        \item Very comprehensive material on the method is provided on the author's website. \href{http://richzhang.github.io/colorization/}{\beamergotobutton{Click here}}
            \end{itemize}
            \framebreak
            \begin{figure}
            \centering
            \includegraphics[width=11cm]{figure/fish_lab.png}
            \caption{The CNN learns the mapping from grayscale (L) to color (ab) for each pixel in the image. The L and ab maps are then concatenated to yield the colorized image. The authors use the LAB color space for the image representation (Zhang et al., 2016).}
            \end{figure}
            \framebreak
            \begin{figure}
            \centering
            \includegraphics[width=4cm]{figure/lab.png}
            \caption{\small The colour space (ab) is quantized in a total of 313 bins. This allows to treat the color prediction as a classification problem where each pixel is assigned a probability distribution over the 313 bins and the one with the highest softmax score is taken as predicted color value. The bin is then mapped back to the corresponding numeric (a,b) values. The network is optimized using a multinomial cross-entropy loss over the 313 quantized (a,b) bins (Zhang et al., 2016).}
            \end{figure}
            \framebreak
            \begin{figure}
            \centering
            \includegraphics[width=11.5cm]{figure/color_architecture.png}
            \caption{The architecture consists of stacked CNN layers which are upsampled towards the end of the net. It makes use of \textit{dilated convolutions} and \textit{upsampling layers} which will be explained later. The output is a tensor of dimension [64, 64, 313] that stores the 313 probabilities for each element of the final, downsampled 64x64 feature maps (Zhang et al., 2016).} 
            \end{figure}
            \framebreak
            \begin{figure}
            \centering
            \includegraphics[width=11.5cm]{figure/color_architecture.png}
            \caption{This block is then upsampled to a dimension of 224x224 and the predicted color bins are mapped back to the (a,b) values yielding a depth of 2. Finally, the L and the ab maps are concatenated to yield a colored image (Zhang et al., 2016).} 
            \end{figure}
            \end{vbframe}
            %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                \begin{vbframe}{Application - Object localization}
            \begin{itemize}
            \item Until now, we used CNNs for \textit{single}-class classification of images - \textbf{which object is on the image?}
            \item Now we extend this framework - \textbf{is there an object in the image and if yes, where and which?}
            \end{itemize}
            \begin{figure}
            \centering
            \includegraphics[width=4cm]{figure/localize_cat.png}
            \caption{Classify and detect the location of the cat (Golovko et al., 2019).}
            \end{figure}
            \framebreak
            \begin{itemize}
            % source1:  https://medium.com/machine-learning-bites/deeplearning-series-objection-detection-and-localization-yolo-algorithm-r-cnn-71d4dfd07d5f
            % source2: https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/object_localization_and_detection.html
            \item Bounding boxes can be defined by the location of the left lower corner as well as the height and width of the box: [$b_x$, $b_y$, $b_h$, $b_w$].
            \item We now combine three tasks (detection, classification and localization) in one architecture.
            \item This can be done by adjusting the label output of the net.
            \item Imagine a task with three classes (cat, car, frog).
            \item In standard classification we would have: 
                $$
                \text{label vector}
            \begin{bmatrix}
            c_{cat}\\
            c_{car} \\
            c_{frog}
            \end{bmatrix}
            \text{and softmax output}
            \begin{bmatrix}
            P(y = cat| X, \theta)\\
            P(y = car| X, \theta)\\
            P(y = frog| X, \theta)
            \end{bmatrix}
            $$
                \end{itemize}
            \framebreak
            \begin{itemize}
            \item We include the information, if there is a object as well as the bounding box parametrization in the label vector.
            \item This gives us the following label vector:
                $$
                \begin{bmatrix}
            b_x\\
            b_y \\
            b_h \\
            b_w.\\
            c_o \\
            c_{cat} \\
            c_{car} \\
            c_{frog} \\
            \end{bmatrix} =             
                \begin{bmatrix}
            \text{x coordinate box}\\
            \text{y coordinate box} \\
            \text{height box} \\
            \text{width box}.\\
            \text{presence of object, binary} \\
            \text{class cat, one-hot} \\
            \text{class car, one-hot}\\
            \text{class frog, one-hot} \\
            \end{bmatrix}
            $$
                \end{itemize}
            \framebreak
            \begin{figure}
            \centering
            \includegraphics[width=8cm]{figure/naive_localization.png}
            \caption{General view of the neural network model used to solve the problem of detecting objects in an image (Golovko et al., 2019)}
            \end{figure}
            \begin{itemize}
            \item Naive approach: use a CNN with two heads, one for the class classification and one for the bounding box regression.
            \item But: What happens, if there are two cats in the image?
                \item Different approaches: "Region-based" CNNs (R-CNN, Fast R-CNN and Faster R-CNN) and "single-shot" CNNs (SSD and YOLO).
            \end{itemize}

            \end{vbframe}
         
\begin{vbframe}{Semantic Segmentation}

The goal of semantic image segmentation is to label each pixel of an image with a corresponding class of what is being represented. 

\begin{figure}
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{figure/unet.png}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{figure/semantic_segmentation.png}
\end{minipage}
\caption{U-net architecture (Ronneberger et al., 2015)}
\end{figure}

\end{vbframe}

\begin{vbframe}{Image Captioning}

The goal of image captioning is to convert a given input image into a natural language description.

\begin{figure}
\centering
\begin{minipage}{.65\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{figure/caption_generation.png}
\end{minipage}%
\begin{minipage}{.35\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{figure/example_sample_generation.png}
\end{minipage}
\caption{"A group of people riding on top of an elephant”: Caption generation with augmented visual attention by Biswas et al., 2020.}
\end{figure}

\end{vbframe}


\begin{vbframe}{Visual Question Answering}

Visual Question Answering is a research area about building a computer system to answer questions presented in an image and a natural language.

    \begin{figure}
        \centering
        \includegraphics[width=8cm]{figure/VQA.png}
        \caption{Combining CNN/RNN for VQA (Agrawal et al., 2017)}
    \end{figure}


\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}
\frametitle{References}
\footnotesize{
\begin{thebibliography}{99}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[(Krizhevsky, 2009)]{1} Krizhevsky, A. (2009). Learning Multiple Layers of Features from Tiny Images.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[(Zhang et al., 2016)]{2} Zhang, R., Isola, P., \& Efros, A. A. (2016). Colorful Image Colorization. In B. Leibe, J. Matas, N. Sebe, \& M. Welling (Eds.), \textit{Computer Vision -- ECCV 2016} (pp. 649–666). Cham: Springer International Publishing.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[(Golovko et al., 2019)]{3} Golovko, V. A., Kroshchanka, A., Mikhno, E., Komar, M., Sachenko, A., Bezobrazov, S. V., \& Shylinska, I. (2019). Deep Convolutional Neural Network for Recognizing the Images of Text Documents. \textit{Modern Machine Learning Technologies}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[(Ronneberger et al., 2015)]{4} Ronneberger, O., Fischer, P., \& Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[(Biswas et al., 2020)]{5}Biswas, S., Parikh, D., et al. (2020). \textit{Caption generation with augmented visual attention. Journal of Deep Learning Research.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[(Agrawal et al., 2017)]{7} Anand, N. (2015). Agrawal, A., Lu, J., Antol, S., Mitchell, M., Zitnick, C., Parikh, D., \& Batra, D. (05 2017). VQA: Visual Question Answering: www.visualqa.org. International Journal of Computer Vision, 123. \url{doi:10.1007/s11263-016-0966-6}

\end{thebibliography}
}
\end{vbframe}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\endlecture
\end{document}
