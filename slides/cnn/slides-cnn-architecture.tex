% style/preamble throws error, hence preamble4tex is kept here
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-nn}

\newcommand{\titlefigure}{figure/alexnet.png}
\newcommand{\learninggoals}{
  \item Architecture
}

\title{Deep Learning}
\date{}

\begin{document}

\lecturechapter{CNN: Architecture}
\lecture{I2DL}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Different Perspectives of CNNs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{CNNs - Perspective I}
\center
\includegraphics[width=9cm]{figure/cnn_scheme.png}
\begin{itemize}
\item Schematic architecture of a CNN.
\item The input tensor is convolved by different filters yielding different feature maps (colored) in subsequent layers.
\item A dense layer connects the final feature maps with the softmax-activated output neurons.
\end{itemize}
\end{vbframe}

\begin{vbframe}{CNNs - Perspective II}
\center
\includegraphics[width=11cm]{figure/cnn_flat.png}
\begin{itemize}
\item Flat view of a CNN architecture for a classification problem.
\item The architecture consists of 2 CNN layers, each followed by max-pooling, then flattened and connected with the final output neurons via a FC layer.
\end{itemize}
\end{vbframe}

\begin{vbframe}{CNNs - Perspective III}
\center
\includegraphics[width=8cm]{figure/3dviz_fcn.png}
\begin{itemize}
% \item Awesome interactive visualization by \cite{29} (\href{http://scs.ryerson.ca/~aharley/vis/}{click here})
\item Interactive visualization (by Adam Harley)  \href{http://scs.ryerson.ca/~aharley/vis/}{\beamergotobutton{Click here}}.
\item Vanilla 2-layer FC net on MNIST data for input digit $3$.
\item Each neuron in layer 1 is connected to each of the input neurons.
\end{itemize}
\framebreak
\center
\includegraphics[width=8cm]{figure/3dviz_cnn_front.png}
\begin{itemize}
\item Front view on 2-layer CNN with Pooling and final dense layer on MNIST data for input digit $3$.
\item Each neuron in the second CNN layer is connected to a patch of neurons from each of the previous feature maps via the convolutional kernel.
\end{itemize}
\framebreak
\center
\includegraphics[width=7.5cm]{figure/3dviz_cnn_bottom.png}
\begin{itemize}
\item Bottom view on 2-layer CNN with Pooling and final dense layer on MNIST data for input digit $3$.
\item Each neuron in the second CNN layer is connected to a patch of neurons from each of the previous feature maps via the convolutional kernel.
\end{itemize}
\end{vbframe}



\frame{
    \frametitle{CNNs - Architecture}
    
    \center
    %\only<1>{\includegraphics[width=11cm]{figure/cnn_scheme.png}}%
        \only<1>{\includegraphics[width=11cm]{figure/cnn1}}%
    \only<2>{\includegraphics[width=11cm]{figure/cnn2}}%
    \only<3>{\includegraphics[width=11cm]{figure/cnn3}}%
    \only<4>{\includegraphics[width=11cm]{figure/cnn4}}%
    \only<5>{\includegraphics[width=11cm]{figure/cnn5}}%
    \only<6>{\includegraphics[width=11cm]{figure/cnn6}}%
    \only<7>{\includegraphics[width=11cm]{figure/cnn7}}%
    \only<8>{\includegraphics[width=11cm]{figure/cnn8}}%
    \only<9>{\includegraphics[width=11cm]{figure/cnn9}}%
    \only<10>{\includegraphics[width=11cm]{figure/cnn9}}%
    \only<11>{\includegraphics[width=11cm]{figure/cnn11}}%
    \only<12>{\includegraphics[width=11cm]{figure/cnn11}}%
    \only<13>{\includegraphics[width=11cm]{figure/cnn12}}%
    
    \begin{itemize}
    %\only<1>{\item Schematic architecture of a CNN}
    \only<1>{\item Suppose we have the following input tensor with dimensions $10 \times 10 \times 3$.}
    \only<2>{\item We use a filter of size $2$.}
    \only<3>{\item Applying it to the first spatial location, yields one scalar value.}
    \only<4>{\item The second spatial location yields another one..}
    \only<5>{\item ...and another one...}
    \only<6>{\item ...and another one...}
    \only<7>{\item Finally we obtain an output which is called feature map.}
    \only<8>{\item We initialize another filter to obtain a second feature map.}
    \only<9>{\item All feature maps yield us a \enquote{new image} with dim $h \times w \times N$.}
    \only<10>{\item We actually append them to a new tensor with depth = \# filters.}
    \only<11>{\item All feature map entries will then be activated (e.g. via ReLU), just like the neurons of a standard feedforward net.}
    \only<12>{\item One may use pooling operations to downsample the dimensions of the feature maps.}
    \only<12>{\item Pooling is applied on each feature map independently: the latter blue block is the pooled version of the previous blue feature map.}
    \only<13>{\item Many of these layers can be placed successively, to extract even more complex features.}
    \only<13>{\item The feature maps are fed into each other sequentially. For instance, each filter from the second Conv Layer gets all previous feature maps from the first Conv Layer as an input. Each filter from the first layer extracts information from the input image tensor.}
    \only<13>{\item The feature maps of the final Conv Layer are flattened (into a vector) and fed into a dense layer which, in turn, is followed by more dense layers and finally, the output layer.}
    \end{itemize}
    }
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlecture
\end{document}
