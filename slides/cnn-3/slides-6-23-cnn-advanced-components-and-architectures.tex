
\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-nn}

\newcommand{\titlefigure}{plots/architectures/lenet.png}
%modify picture
\newcommand{\learninggoals}{
  \item LeNet
  \item AlexNet
  \item VGG
  \item Network in Network
}

\title{Deep Learning}
\date{}



\begin{document}

\lecturechapter{Modern Architectures - I}
\lecture{I2DL}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{From LeNet to AlexNet}

\section{LeNet}

\begin{vbframe}{LeNet Architecture}
  \begin{itemize}
    \item Pioneering work on CNNs by Yann Lecun in 1998. 
    \item Applied on the MNIST dataset for automated handwritten digit recognition.
    \item Consists of convolutional, "subsampling" and dense layers.
    \item Complexity and depth of the net was mainly restricted by limited computational power back in the days.
  \end{itemize}
  \begin{figure}
  \centering
    \includegraphics[width=9cm]{plots/architectures/lenet.png}
    \caption{LeNet architecture: two conv layers with subsampling, followed by dense layers and a 'Gaussian connections' layer.}
  \end{figure}
  \framebreak
  \begin{itemize}
    \item A neuron in a subsampling layer looks at a $2 \times 2$ region of a feature map, sums the four values, multiplies it by a trainable coefficient, adds a trainable bias and then applies a sigmoid activation.
    \item A stride of 2 ensures that the size of the feature map reduces by about a half.
    \item The 'Gaussian connections' layer has a neuron for each possible class. 
    \item The output of each neuron in this layer is the (squared) Euclidean distance between the activations from the previous layer and the weights of the neuron.
  \end{itemize}   
\end{vbframe}

\section{AlexNet}
\begin{vbframe}{AlexNet}
  \begin{itemize}
    \item AlexNet, which employed an 8-layer CNN, won the ImageNet Large Scale Visual Recognition (LSVR) Challenge 2012 by a phenomenally large margin.
    \item The network trained in parallel on two small GPUs, using two streams of convolutions which are partly interconnected.
    \item The architectures of AlexNet and LeNet are very similar, but there are also significant differences: 
       \begin{itemize}
          \item First, AlexNet is deeper than the comparatively small LeNet5. AlexNet consists of eight layers: five convolutional layers, two fully-connected hidden layers, and one fully-connected output layer. 
          \item Second, AlexNet used the ReLU instead of the sigmoid as its activation function. 
       \end{itemize}
  \end{itemize}
    \begin{figure}
        \centering
        \includegraphics[width=4.5cm]{plots/moderncnn/alexnet.png}
        \caption{From LeNet (left) to AlexNet (right).}
    \end{figure}
\end{vbframe}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{VGG}

%\subsection{VGG Blocks}
\begin{vbframe}{VGG Blocks}
  \begin{itemize}
    \item The block composed of convolutions with $3 \times 3$  kernels with padding of 1 (keeping height and width) and $2 \times 2$  max pooling with stride of 2 (halving the resolution after each block).
    \item The use of blocks leads to very compact representations of the network definition. 
    \item It allows for efficient design of complex networks.
  \end{itemize}
  
    \begin{figure}
  \centering
    \includegraphics[width=3cm]{plots/moderncnn/vggblock.png}
    \tiny{\\ credit : D2DL}
    \caption{VGG block.}
  \end{figure}
  
\end{vbframe}


%\subsection{VGG Network}

\begin{vbframe}{VGG Network}
  \begin{itemize}
    \item Architecture introduced by Simonyan and Zisserman, 2014 as \enquote{Very Deep Convolutional Network}.
    \item A deeper variant of the AlexNet.
    \item Basic idea is to have small filters and Deeper networks
    \item Mainly uses many cnn layers with a small kernel size $3 \times 3$.
    \item Stack of three $3 \times 3$ cnn (stride 1) layers has same effective receptive field as one $7 \times 7$ conv layer.
    \item Performed very well in the ImageNet Challenge in 2014.
    \item Exists in a small version (VGG16) with a total of 16 layers (13 cnn layers and 3 fc layers) and 5 VGG blocks while a larger version (VGG19) with 19 layers (16 cnn layers and 3 fc layers) and 6 VGG blocks.
  \end{itemize}
\framebreak
  \begin{figure}
  \centering
    \includegraphics[width=7cm]{plots/moderncnn/vgg.png}
    \tiny{\\ credit : D2DL}
    \caption{From AlexNet to VGG that is designed from building blocks.}
  \end{figure}

\end{vbframe}

\section{Network in Network (NiN)}

%\subsection{NiN Blocks}

\begin{vbframe}{NiN Blocks}
  \begin{itemize}
    \item The idea behind NiN is to apply a fully-connected layer at each pixel location (for each height and width). If we tie the weights across each spatial location, we could think of this as a $1 \times 1$ convolutional layer.
    \item The NiN block consists of one convolutional layer followed by two $1 \times 1$  convolutional layers that act as per-pixel fully-connected layers with ReLU activations.
    \item The convolution window shape of the first layer is typically set by the user. The subsequent window shapes are fixed to $1 \times 1$.
  \end{itemize}
  \begin{figure}
  \centering
    \includegraphics[width=2cm]{plots/moderncnn/ninblock.png}
    \tiny{\\ credit : D2DL}
    \caption{NiN block.}
  \end{figure}

\end{vbframe}



\begin{vbframe}{Network in Network (NiN)}
  \begin{itemize}
    \item NiN uses blocks consisting of a convolutional layer and multiple $1 \times 1$ convolutional layers. This can be used within the convolutional stack to allow for more per-pixel nonlinearity.
    \item NiN removes the fully-connected layers and replaces them with global average pooling (i.e., summing over all locations) after reducing the number of channels to the desired number of outputs (e.g., 10 for Fashion-MNIST).
    \item Removing the fully-connected layers reduces overfitting. NiN has dramatically fewer parameters.
    \item The NiN design influenced many subsequent CNN designs.
  \end{itemize}
\framebreak
  \begin{figure}
  \centering
    \includegraphics[width=7cm]{plots/moderncnn/nin.png}
    \tiny{\\ credit : D2DL}
    \caption{Comparing architectures of VGG and NiN, and their blocks.}
  \end{figure}

\end{vbframe}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}
\frametitle{References}
\footnotesize{
\begin{thebibliography}{99}

% \bibitem[Ronneberger et al., 2015]{12} Olaf Ronneberger, Philipp Fischer, Thomas Brox (2015)
% \newblock U-Net: Convolutional Networks for Biomedical Image Segmentation
% \newblock \emph{\url{http://arxiv.org/abs/1505.04597}}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Zhou et. al , 2016]{25} B. Zhou, Khosla, A., Labedriza, A., Oliva, A. and A. Torralba (2016)
\newblock Deconvolution and Checkerboard Artifacts
\newblock \emph{\url{http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Szegedy et. al , 2014]{26} Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke and Andrew Rabinovich (2014)
\newblock Going deeper with convolutions
\newblock \emph{\url{https://arxiv.org/abs/1409.4842}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \bibitem[Jie Hu et. al , 2014]{27} Jie Hu, Shen, Li and Gang Sun (2017)
% \newblock Squeeze-and-Excitation Networks
% \newblock \emph{\url{https://arxiv.org/abs/1709.01507}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \bibitem[Szegedy Christian et. al , 2015]{28} Christian Szegedy, Vanhoucke, Vincent, Sergey, Ioffe, Shlens, Jonathan and Wojna Zbigniew (2015)
% \newblock Rethinking the Inception Architecture for Computer Vision
% \newblock \emph{\url{https://arxiv.org/abs/1512.00567}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[He et. al , 2015]{29} Kaiming He, Zhang, Xiangyu, Ren, Shaoqing, and Jian Sun (2015)
\newblock Deep Residual Learning for Image Recognition
\newblock \emph{\url{https://arxiv.org/abs/1512.03385}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[Zhou et. al, 2016]{32} Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva and Antonio Torralba (2016)
\newblock Learning Deep Features for Discriminative Localization
\newblock \emph{\url{http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\end{thebibliography}
}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\endlecture
\end{document}
