%This file is a child of preamble.Rnw in the style folder
%if you want to add stuff to the preamble go there to make
%your changes available to all childs

<<setup-child, include = FALSE>>=
library(knitr)
set_parent("../style/preamble.Rnw")
library(randomForest)
@

\lecturechapter{0}{Add on material: Extremely randomized trees}
\lecture{Fortgeschrittene Computerintensive Methoden}

\begin{vbframe}{Extremely randomized trees}
  \begin{itemize}
    \item Alias: Extra-Trees
		\item Tree-based ensemble method for classification or regression
		\item Randomized attribute and cut-point choice
		\item Strength of randomization can be controlled by attribute choice
		\item \textbf{Totally} randomized trees: both attribute and cut-point are chosen fully at random
    \item Goal
			\begin{itemize}
			\item Improvement of prediction accuracy
			\item Reduction of the bias/variance
			\item Finding an efficient computation method
			\end{itemize}
		\item For an Extra-Trees ensemble $M$ Extra-Trees are build on the complete training data set $\D$
		\item Final prediction through aggregation (majority vote/average)
	\end{itemize}

\framebreak

The Extra-Trees splitting algorithm comprises three functions:
\begin{itemize}
\item Split\_a\_node
\item Pick\_a\_random\_split
\item Stop\_split
\end{itemize}
\begin{algorithm}[H]
  \small
  \setstretch{1.15}
  \caption*{Split\_a\_node}
  \begin{algorithmic}[1]
  \State \textbf{Input:} The local training subset $\Np$ corresponding to the node we want to split
  \State \textbf{Output:} A split or nothing
  \State If \textbf{Stop\_split}$(\Np)$ is TRUE then return nothing
  \State Otherwise select $K$ attributes $\{x_1, \dots, x_k\}$ among all non-constant candidate attributes
  \State Draw $K$ splits $\{t_1, \dots, t_k\}$ where
  $t_j =$ \textbf{Pick\_a\_random\_split}$(\Np, \xj), \forall j = 1, \dots, K$
  \State Return a split $t_*$ such that $Score = max_ {j = 1, \dots K} Score(t_j, \Np)$
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \small
  \setstretch{1.15}
  \caption*{Pick\_a\_random\_split}
  \begin{algorithmic}[1]
  \State \textbf{Input: } A subset $\Np$ and an attribute $\xj$
  \State \textbf{Output: } A split
  \State If attribute is numerical:
    \begin{itemize}
    \item Compute the maximal and the minimal value of $\xj$ in $\Np$
    \item Draw a cut-point $t_c$ uniformly in $[x_{min}^{\Np},x_{max}^{\Np}]$
    \end{itemize}
  \State If attribute is categorical:
    \begin{itemize}
    \item Let $\mathcal{Q}$ be the set of all values of attribute $\xj$
    \item Randomly draw a proper non empty subset $\mathcal{Q}_{1}$ of values of $\xj$ and a corresponding complementary set $\mathcal{Q}_{2}$
    \end{itemize}
    \end{algorithmic}
\end{algorithm}
\end{vbframe}

\begin{algorithm}[H]
  \small
  \setstretch{1.15}
  \caption*{Stop\_split}
  \begin{algorithmic}[1]
  \State \textbf{Input:} A subset $\Np$
  \State \textbf{Output:} A Boolean
    \begin{itemize}
    \item $|\Np|$ < $n_\text{min}$,
			\item all candidate attributes are constant in $\Np$ or
			\item the output variable is constant in $\Np$
    \end{itemize}
  \end{algorithmic}
\end{algorithm}

\begin{vbframe}{Extra-Trees vs. Random Forests}
 \begin{itemize}
 \item Commonalities
    \begin{itemize}
    \item Prediction by averaging or majority vote
    \item Goal of both methods: Reduction of variance and decorrelation of trees
    \end{itemize}
 \item Different approaches
    \begin{itemize}
    \item Random forest:
      \begin{itemize}
      \item Bagging
      \item Random choice of attributes at each node
      \end{itemize}
    \item Extra-Trees:
      \begin{itemize}
      \item Trees are build on the whole dataset $\D$
      \item Random choice of splits at each node
      \end{itemize}
    \end{itemize}
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Parameters of Extra-Trees}
Extra-Trees has three parameters which are important for prediction accuracy:
\begin{itemize}
		 \item[$K$:] Number of attributes randomly selected at each node.\\ Determines the strength of randomization.
		\item[$n_{min}$:] Minimal sample size for splitting a node.\\ Determines strength of averaging output noise.
		\item[$M$:] Number of trees in an ensemble.\\ Determines strength of variance reduction.
		\end{itemize}
\end{vbframe}

\begin{vbframe}{How to choose the right strength of randomization?}
\begin{itemize}
	\item $K \in  [1,...,n]$
	\item The smaller $K$, the greater the randomization
	\item Extreme cases:\\
		\begin{itemize}
		\item $K = 1$: Totally randomized tree
		\item $K = n$: No randomization regarding the choice of the attribute
		\end{itemize}
	\item Choice of $K$ depends on:
		\begin{itemize}
		\item Kind of problem (regression or classification)
		\item Structure of data
		\end{itemize}
\end{itemize}

\framebreak

Some rules of thumb:
\begin{itemize}
\item Classification problem:
  \begin{itemize}
	\item Symmetric attributes, invariant with respect to permutations of the attributes: $K = 1$ (= total randomization)
	\item High proportion of irrelevant variables: choose higher $K$\\
	(= less randomization)
	\item Attributes of variable importance: default-value ($K=\sqrt{n}$)
	\end{itemize}
\item Regression problem:
  \begin{itemize}
  \item The higher $K$, the better prediction accuracy.
  \end{itemize}
\end{itemize}
\end{vbframe}

\begin{vbframe}{How to choose the right smoothing strength?}
\begin{itemize}
\item The greater $n_{min}$, the...
	\begin{itemize}
	\item smaller the tree.
	\item higher the bias.
	\item smaller the variance.
	\end{itemize}
\item Optimal $n_{min}$ depends on the level of output noise:
The greater the output noise, the greater $n_{min}$ needs to be chosen.
\item Default values ($n_{min} = 2$ for classifcation, $n_{min} = 5$	for regression) appear to be robust choices in a broad range of typical conditions.
\end{itemize}
\end{vbframe}

\begin{vbframe}{How to choose the right averaging strength?}
\begin{itemize}
\item The greater  $M$, the lesser the error rate, the better prediction accuracy.
\item Choice of $M$ is a compromise between computational requirements and accuracy
\end{itemize}
\end{vbframe}

\begin{vbframe}{Extra-Trees in practice}
\begin{itemize}
\item \code{extraTrees()} from package \pkg{extraTrees}
\item Extra-Trees were compared to CART, tree bagging, local random subspace and random forest w.r.t. prediction accuracy and computational efficiency.
\item Extra-Trees is as accurate or more accurate than the other methods.
However, no general recommendation can be given since prediction performance depends on the data.
\item Extra-Trees show competitive results in computational efficiency.
\end{itemize}
\end{vbframe}
