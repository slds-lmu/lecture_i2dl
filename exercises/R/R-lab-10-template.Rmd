---
title: "Deep Learning Lab 9 | Summer Term 2025"
author: "Emanuel Sommer, Prof. Dr. David RÃ¼gamer"
output: pdf_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE, echo = TRUE)
```

## Exercise 1

In this exercise, we are going to revise the sentiment classifier for IMDB reviews we developed in a previous lab. Earlier, we encoded each review as a single "bag-of-words" vector which had one element for each word in our dictionary set to one if that word was found in the review, zero otherwise. This allowed us to use a simple fully-connected neural network but, on the flip side, we lost all information contained in the ordering and of the words and possible multiple repetitions. Recurrent neural networks, however, are able to process reviews directly. Let's see how!

The first step is to load the data. For brevity, we only use the 10000 most common words and consider reviews shorter than 251 words, but if you can use a GPU then feel free to use all reviews and all words!

```{r}
library(keras3)
imdb <- dataset_imdb(num_words = 10000, maxlen=250)
c(c(x_train_imdb, y_train_imdb), c(x_test_imdb, y_test_imdb)) %<-% imdb
```


Each review is a vector of numbers, each corresponding to a different word:

```{r}
x_train_imdb[[5]]
```

Even though RNNs can process sequences of arbitrary length, all sequences in the same batch must be of the same length, while sequences in different batches can have different length. In this case, however, we pad all sequences to the same length as this makes for much simpler code. Keras provides a function to do so for you called `pad_sequences` (read the documentation!).

```{r}
x_train_imdb = (
  #!hwbegin TODO pad the training reviews to the same length we used above
  pad_sequences(x_train_imdb, 250)
  #!hwend
)

x_test_imdb =(
  #!hwbegin TODO pad the testing reviews to the same length we used above
  pad_sequences(x_test_imdb, 250)
  #!hwend
)
```

Next, we define our sequential model. The first layer is an _embedding_ layer that associates a vector of numbers to each word in the vocabulary. These numbers are updated during training just like all other weights in the network. Crucially, thanks to this embedding layer we do not have to one-hot-encode the reviews but we can use the word indices directly, making the process much more efficient.

Note the parameter `mask_zero`: this indicates that zeros in the input sequences are used for padding (verify that this is the case!). Internally, this is used by the RNN to ignore padding tokens, preventing them from contributing to the gradients (read more in the user guide, [link](https://keras.io/guides/understanding_masking_and_padding/)!).

```{r}
model <- keras_model_sequential() |>
  layer_embedding(input_dim = 10001, output_dim = 64, mask_zero = TRUE) |>
  layer_lstm(units = 32) |>
  layer_dense(1, activation = "sigmoid")

model |> compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

# the summary will only display the shapes after being built.
# summary(model)
```

```{r}
hist = model |> fit(
  x_train_imdb,
  y_train_imdb,
  batch_size = 32,
  epochs = 6,
  verbose = 0,
  validation_data = list(x_test_imdb, y_test_imdb)
)

plot(hist)
```

The model seems to be learning more easily than the simple baseline we created time ago, which had an accuracy of 85-88% on the test data. Let it train for longer and tune the architecture above to reach as high accuracy as possible! (note that evaluating on the same data that you used for early stopping is cheating).

## Exercise 2

In this exercise, we are going to build a model that is able to sum two numbers, each given as a sequence of images of handwritten digits. The network will first use a convolutional encoder to transform each digit into a feature vector. These feature vectors will then be processed by a LSTM that will produce as output each digit of the sum.

In doing this, we will learn how to use Keras's functional API to create models with more than one inputs, as well as how to apply the same model independently to each item of a sequence.

### Dataset
We are now going to create a synthetic dataset using images from MNIST.

```{r}
mnist = dataset_mnist()

x_train = mnist$train$x / 255
y_train = to_categorical(mnist$train$y)
dim(x_train) <- c(nrow(x_train), 28, 28)
```

The first function we need is used to encode all digits of a number into a one-hot representation. From here on, we use `max_len` to indicate the maximum number of digits in a number. If a number has fewer digits we will pad it with zeros.

```{r}
encode_number_to_onehot <- function(num, max_len) {
  num_str <- sprintf(paste("%0", max_len, "d", sep=""), num)

  encoded <- array(0, dim=c(max_len, 10))
  for(i in 1:max_len) {
    n <- as.integer(substring(num_str, i, i))
    encoded[i,n+1] <- 1
  }

  encoded
}

encode_number_to_onehot(195, 4)
```
We now write a function to extract from MNIST the images of each digit in a given number.

```{r}
encode_number_to_images <- function(num, max_len) {
  images <- array(0, dim=c(max_len, 28, 28, 1))
  num_str <- sprintf(paste("%0", max_len, "d", sep=""), num)
  for(i in 1:max_len) {
    n <- as.integer(substring(num_str, i, i))
    digit_idx <- (1:nrow(x_train))[y_train[,n+1] == 1]
    img_idx <- sample(digit_idx, 1)
    images[i,,,1] <- x_train[img_idx,,]
  }

  images
}
```

Let's now create a synthetic dataset with 25,000 random pairs of numbers and their sum.

```{r}
make_dataset <- function(n_samples, max_len) {
  x1 <- array(0, dim=c(n_samples, max_len, 28, 28, 1))
  x2 <- array(0, dim=c(n_samples, max_len, 28, 28, 1))
  yy <- array(0, dim=c(n_samples, max_len, 10))

  for(i in 1:n_samples) {
    # ensure the sum always has at most max_len digits
    n1 <- sample.int(10**max_len / 2 - 1, 1)
    n2 <- sample.int(10**max_len / 2 - 1, 1)

    #!hwbegin TODO encode n1 and n2 to images and save them into x1 and x2
    x1[i,,,,] <- encode_number_to_images(n1, max_len)
    x2[i,,,,] <- encode_number_to_images(n2, max_len)
    #!hwend

    #!hwbegin TODO encode the sum of n1 and n2 as one-hot and save into yy
    yy[i,,] <- encode_number_to_onehot(n1 + n2, max_len)
    #!hwend
  }

  list(x1=x1, x2=x2, y=yy)
}

max_len = 3
train_set <- make_dataset(25000, max_len)
```

### The model

Let's now see how to create the model in Keras.

This network will have two inputs, one for each number. The numbers have three digits, each of which is an image of size 28 x 28 x 1.
To use the functional API, we need to manually create the input layers and specify the proper input shape (as we always did):

```{r}
in_shape = c(
  #!hwbegin TODO determine the input shape
  max_len, 28, 28, 1
  #!hwend
)

first_number_input <- keras_input(shape = in_shape)
second_number_input <- keras_input(shape = in_shape)
```

The network will use the same convolutional encoder for all digits in both numbers. Let us first define this encoder as its own model, a normal CNN:

```{r}
digit_encoder <- keras_model_sequential() |>
  #!hwbegin TODO add some convolutional and pooling layers as you see fit
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu") |>
  layer_max_pooling_2d(pool_size = c(2, 2)) |>
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") |>
  layer_max_pooling_2d(pool_size = c(2, 2)) |>
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") |>
  #!hwend
  layer_global_average_pooling_2d()
```

This CNN will transform each digit from a tensor of shape (28, 28, 1) to a vector of size, for example, 128. In order to apply this encoder to all digits of a number, we loop over the digits and apply the encoder to each one:

```{r}
first_number_encoded <- list()
for(i in 1:max_len) {
  # Notice that we use pythonic indexing here, so we need to subtract 1 from i
  #!hwbegin TODO apply the encoder to the i-th digit of the first number
  first_number_encoded[[i]] <- digit_encoder(first_number_input[, i - 1])
  #!hwend
}
second_number_encoded <- list()
for(i in 1:max_len) {
  #!hwbegin TODO apply the encoder to the i-th digit of the second number
  second_number_encoded[[i]] <- digit_encoder(second_number_input[, i - 1])
  #!hwend
}
```

The input of this sequence of transformations has shape (3, 28, 28, 1) while its output has shape (3, 64). This is because each (28, 28, 1) slice is processed by the CNN into a vector with 64 elements.

After we apply the CNN to both numbers, we need to "merge" the two sequence of vectors. There are several options here, here we choose to concatenate the two digit vectors in each time-step to produce a single vector of size 128:

```{r}
encoded_numbers <- layer_concatenate(
  #!hwbegin TODO concatenat and reshape appropriately
  layer_concatenate(inputs = first_number_encoded) |> layer_reshape(target_shape = c(3, 64)),
  layer_concatenate(inputs = second_number_encoded) |> layer_reshape(target_shape = c(3, 64)),
  axis = 3
  #!hwend
)
```

This will result in a tensor of shape (3, 128). Let's feed this into a bidirectional LSTM, followed by a dense layer with to perform the final classification for each digit of the result. In order to do so, you may find the `bidirectional` layer useful. Also be mindful of returning all hidden states from the LSTM, not only the last one. Refer again to the documentation.

```{r}
model_output <- encoded_numbers |>
  #!hwbegin create the final classifier
  layer_dropout(0.5) |>
  bidirectional(layer_lstm(units = 64, return_sequences = TRUE)) |>
  layer_dense(10, "softmax")
  #!hwend
```

We now have all components of the model. Let's then create and compile it:

```{r}
model <- keras_model(
  inputs = list(first_number_input, second_number_input),
  outputs = model_output
)

model |> compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics=c("accuracy")
)
```
### Training and validation

Finally, let's train this model on the synthetic dataset we created earlier. Since the model has two inputs, we must pass a `list` with two elements to `fit`:

```{r}
hist <- model |> fit(
  list(train_set$x1, train_set$x2),
  train_set$y,
  #!hwbegin TODO insert appropriate parameters for fitting
  validation_split = 0.2,
  epochs = 20,
  batch_size = 32,
  verbose = 0 # try out 1 for nice progress bars + visuals
  #!hwend
)

plot(hist)
```

It is amazing what we achieved with such a small (for the standard of deep learning) model and dataset! If you are curious look at some difficult examples to get a feeling where the model still struggles.


## Exercise 3

In this exercise, we are going to focus on the concept of self-attention. Generally speaking, self-attention allows the model to capture dependencies within a single sequence, weighting the importance of the individual sequence elements/token relative to each other. To do so the classical self-attention mechanism consists of three trainable weight matrices $W_q^{d \times d_q}$, $W_k^{d \times d_k}$, $W_v^{d \times d_v}$ with $d_k = d_q$. In addition, we usually also have an input matrix $X^{N \times d}$ where each row represents one token/element of this input. This can e.g. be an embedding vector per token but also a vector representation obtained from other network components. 

To actually understand the mechanism behind self-attention we want to calculate one iteration of the self-attention procedure. Let us assume we have the input sentence: Alice visits Bob. The corresponding embedding vectors per token are $x^{(1)} = (2, 1, 0)$, $x^{(2)} = (0, 0, 1)$, $x^{(3)} = (0, 2, 0)$. The weight matrices are 
\begin{equation}W_q = \left(\begin{matrix} 0 & 2  \\ 1 & 0 \\ 3 & 1 \end{matrix}\right), W_k = \left(\begin{matrix} 1 & 0  \\ 0 & 1 \\ 1 & 3 \end{matrix}\right), W_v = \left(\begin{matrix} 4 & 2  \\ 6 & 5 \\ 3 & 2\end{matrix}\right) \end{equation}

1. Compute $Q = XW_q, K= XW_k$ and $V= XW_v$. 
2. Compute the attention weights $A = \text{softmax}(\frac{QK^T}{\sqrt(d_k)})$
    1. What is the dimension of $A$? 
    2. What does the multiplication of $Q$ and $K$ actually mean? How can we interpret the values of A? 
3. Compute the attention outputs $\text{Attention}(K, Q, V) = AV$
    1. What are the dimensions of $\text{Attention}(K, Q, V)$? 
    2. How can we interpret this operation?

<!--#!solutionbegin-->

1.  
    \begin{align} Q &= X W_q = \left(\begin{matrix} 2 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 2 & 0 \end{matrix}\right)
    \left(\begin{matrix} 0 & 2 \\ 1 & 0 \\ 3 & 1 \end{matrix}\right)
    = \left(\begin{matrix} 1.0 & 4.0 \\ 3.0 & 1.0 \\ 2.0 & 0.0 \end{matrix}\right) \\
    K &= X W_k = \left(\begin{matrix} 2 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 2 & 0 \end{matrix}\right)
    \left(\begin{matrix} 1 & 0 \\ 0 & 1 \\ 1 & 3 \end{matrix}\right)
    = \left(\begin{matrix} 2.0 & 1.0 \\ 1.0 & 3.0 \\ 0.0 & 2.0 \end{matrix}\right) \\
    V &= X W_v = \left(\begin{matrix} 2 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 2 & 0 \end{matrix}\right)
    \left(\begin{matrix} 4 & 2 \\ 6 & 5 \\ 3 & 2 \end{matrix}\right)
    = \left(\begin{matrix} 14.0 & 9.0 \\ 3.0 & 2.0 \\ 12.0 & 10.0 \end{matrix}\right) \end{align}

2. 
    \begin{equation} A= \text{softmax} \left( \frac{QK^T}{\sqrt{2}} \right) =
    \left(\begin{matrix}
    0.007 & 0.965 & 0.028 \\ 
    0.657 & 0.324 & 0.019 \\ 
    0.768 & 0.187 & 0.045 
    \end{matrix}\right) \end{equation}

    1. $N \times N$
    2.  - Vector-wise similarity/closeness between the elements in Q and K. 
        - Intuitively: Ideally, answers the question: Per word (embedding) q, how much does an respective element from k relate to q for the given context. How much does k help us to understand what q is about? 
        - This is also why we project into the different spaces. If $Q=K$ exactly (i.e., no learned projection), then the dot product $QK^T$ would usually produce mostly high values along the diagonal and smaller values else where. Softmax on this would then result in an almost one-hot attention matrix (where each token attends mostly to itself). Applying this to $V$ would then mean each word is weighted mostly by itself, so the attention mechanism would not change the representation in a meaningful way.
3. 
    \begin{equation} \text{Attention}(K, Q, V) = A V =
    \left(\begin{matrix}
    3.33 & 2.27 \\ 
    10.40 & 6.75 \\ 
    11.86 & 7.74 
    \end{matrix}\right) \end{equation}

    1. $N \times d_v$ 
    2. The attention mechanism computes a weighted sum of the value vectors, where the attention weights determine the contribution of each token. Thus, each output representation incorporates the most relevant contextual information provided by the other tokens based on their importance in the given context.


<!--#!solutionend-->


One problem of the traditional self-attention mechanism is the memory consumption of $\mathcal{O}(n^2)$, due to storing the full $N \times N$ matrix. An approach that mitigates this bottleneck is FlashAttention. The core idea of FlashAttention is to divide the matrices $Q, K, V$ into blocks along the first dimension and perform the softmax operation block by block, s.t. you iterativley update the softmax-values of the previous blocks, with the values obtained for the current block. For example, letâs consider a specific query row $q_i \in \mathbb{R}^{1 \times d_k}$ from $Q$, and a blockwise decomposition of the key matrix $K$ into two blocks, $K^{(1)} \in \mathbb{R}^{b_1 \times d_k}$ and $K^{(2)} \in \mathbb{R}^{b_2 \times d_k}$, corresponding to the first and second chunks of the key matrix along the sequence dimension. The corresponding query-key dot products yield the two row vectors $\mathbf{a}_i^{(1)} = q_i {K^{(1)}}^\top$ and $\mathbf{a}_i^{(2)} = q_i {K^{(2)}}^\top$. In the end, we want to compute the softmax over the fully concatenated vector $\mathbf{a}_i = \left(\mathbf{a}_i^{(1)}, \mathbf{a}_i^{(2)}\right)$.

FlashAttention uses softmax tiling, which for our vector $\mathbf{a_i}$ consists of the following steps:

1. Construct the softmax-formulation for each $\mathbf{a}_i^{(1)}, \mathbf{a}_i^{(2)}$ independently. In addition, we compute the respective maximum per vector so $m_1 = max(\mathbf{a}_i^{(1)})$ and $m_2 = max(\mathbf{a}_i^{(2)})$ and subtract that from the exponents in the respective softmax formulation:

\begin{equation} 
\text{softmax}(\mathbf{a}_i^{(1)}) = \frac{\mathbf{f(a_i^{(1)})}}{l_1}
\end{equation}

\begin{equation} 
\text{softmax}(\mathbf{a}_i^{(2)}) = \frac{\mathbf{f(a_i^{(2)})}}{l_2}
\end{equation}

with $\mathbf{f(a_i^{(t)})} = \left( e^{a_{i,1}^{(t)} - m_t} \quad \dots \quad e^{a_{i,b_t}^{(t)} - m_t}\right)$ and $l_t = \sum_j e^{a_{i,j}^{(t)} - m_t}$

2. Compute the global maximum across both vectors, so $m = max(m_1, m_2)$ and construct the normalizers $e^{m_1 - m}$, $e^{m_2 - m}$. Apply them to the softmax: 
\begin{equation} 
\text{softmax}(\mathbf{a}_i^{(1)}) = \frac{\mathbf{f(a_i^{(1)})} \cdot e^{m_1 - m}}{l_1 \cdot e^{m_1 - m}}
\end{equation}

\begin{equation} 
\text{softmax}(\mathbf{a}_i^{(2)}) = \frac{\mathbf{f(a_i^{(2)})} \cdot e^{m_2 - m}}{l_2 \cdot e^{m_2 - m}}
\end{equation}

3. Construct the softmax for $\mathbf{a}_i$ via additivley combining the two individual softmax's: 
\begin{equation} 
\text{softmax}(\mathbf{a}_i) = \frac{\mathbf{f(a_i)}}{l}
\end{equation}

    with $\mathbf{f(a_i)} = \left (\mathbf{f(a_i^{(1)})} \cdot e^{m_1 - m}, \mathbf{f(a_i^{(2)})} \cdot e^{m_2 - m} \right)$ and $l = l_1 \cdot e^{m_1 - m} + l_2 \cdot e^{m_2 - m}$


Now, show that the formulation in 3. actually is equal to the vanilla softmax formulation directly computed for the complete $\mathbf{a_i}$
\begin{equation} 
\text{softmax}(\mathbf{a}_i) = \frac{\left(e^{a_{i,1}} \quad \dots \quad e^{a_{i,b}}\right)}{\sum_j e^{a_{i,j}}}
\end{equation}

<!--#!solutionbegin-->

Since 
\begin{align*}
\mathbf{f}(\mathbf{a}_i^{(1)})_j \cdot e^{m_1 - m} 
&= \left( e^{a_{i,j}^{(1)} - m_1} \cdot e^{m_1 - m} \right) 
= \left( e^{a_{i,j}^{(1)} - m} \right) \\
\mathbf{f}(\mathbf{a}_i^{(2)})_j \cdot e^{m_2 - m} 
&= \left( e^{a_{i,j}^{(2)} - m_2} \cdot e^{m_2 - m} \right) 
= \left( e^{a_{i,j}^{(2)} - m} \right)
\end{align*}

So:
\begin{align*}
\mathbf{f}(\mathbf{a}_i) &= \left( e^{a_{i,1} - m}, \dots, e^{a_{i,b_1 + b_2} - m} \right) \\
l &= \sum_{j=1}^{b_1 + b_2} e^{a_{i,j} - m}
\end{align*}

Multiplying numerator and denominator by $e^{m}$, we obtain the final expression as:
\begin{align*}
\text{softmax}(\mathbf{a}_i) = \frac{ \left( e^{a_{i,1}}, \dots, e^{a_{i,b_1 + b_2}} \right) }{ \sum_{j=1}^{b_1 + b_2} e^{a_{i,j}} }
\end{align*}

which is exactly the vanilla softmax.


Finally, let's recap why this method is great for reducing the memory consumption and for increasing computational efficiency. The block-wise (and thus intermediate) attention scores never need to be fully stored. It is enough to store only the per-block partial results and running statistics (like the max and sum for softmax). In addition, computation within each block is fully parallelizable across queries and across the elements inside the block. Synchronization is only needed when combining the results across blocks, to correctly update the softmax normalization and the final output.

<!--#!solutionend-->


## Exercise 4

Now, in the last exercise we want to apply the self attention concept to our model from exercise 1. Reusing both the data preprocessing functions and the training loop, we only have to update the model:
We want to use 4 attention heads, each with a dimension of 64. The input to the attention layer is the output of the embedding layer. The output of the attention layer is then averaged over the time dimension and passed to a dense layer with a single output.

```{r}
# Define the input layer
input <- layer_input(shape = list(250))

# Embedding layer
embedding <- input |>
  layer_embedding(input_dim = c(10001), output_dim = 64, mask_zero = TRUE)

# Multi-head attention layer
#!hwbegin TODO add the multi-head attention layer.
attention <- layer_multi_head_attention(
  num_heads = 4,
  key_dim = 64
)(query = embedding, key = embedding, value = embedding)
#!hwend

#!hwbegin TODO add the global average pooling layer
pooled <- attention |>
  layer_global_average_pooling_1d()
#!hwend

# Dense output layer
output <- pooled |>
  layer_dense(units = 1, activation = "sigmoid")

# Create the model
model <- keras_model(inputs = input, outputs = output)

model |> compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

# the summary will only display the shapes after being built.
# summary(model)
```

```{r}
hist = model |> fit(
  x_train_imdb,
  y_train_imdb,
  batch_size = 32,
  epochs = 6,
  verbose = 0,
  validation_data = list(x_test_imdb, y_test_imdb)
)

plot(hist)
```