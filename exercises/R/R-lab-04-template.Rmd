---
title: "Lab 4"
output: pdf_document
papersize: a4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE, echo = TRUE)
```

**Lecture**: Deep Learning (Prof. Dr. David RÃ¼gamer, Emanuel Sommer)

Welcome to the fourth lab. In this lab, we will derive the backpropagation equations, code the training procedure, and test it on our beloved dataset with five points.

## Exercise 1
Consider a neural network with $L$ layers and a loss function $\mathcal{L}(\textbf{y},\textbf{z}^{(L)}_{\cdot,out})$. Call the output of the $i$-th unit of the $\ell$-th layer $\textbf{z}^{(\ell)}_{i,out}=\sigma^{(\ell)}(\textbf{z}^{(\ell)}_{i,in})$ with $\textbf{z}^{(\ell)}_{i,in}=\sum_j\textbf{W}^{(\ell)}_{ji}\textbf{z}^{(\ell-1)}_{j,out}+\textbf{b}^{(\ell)}_{i}$ its pre-activation output. Finally, consider $\delta^{(\ell)}_i=\partial\mathcal{L}(\textbf{y},\textbf{z}^{(L)}_{\cdot,out})/\partial\mathbf{z}^{(\ell)}_{i,in}$ the gradient of the loss with respect to the pre-activation outputs of layer $\ell$.

Derive the back-propagation algorithm for a network with arbitrary architecture. You might find the results of the previous lab a useful reference, as well as chapter 5 of the book _Mathematics for Machine Learning_ (https://mml-book.github.io).

\begin{enumerate}
\item Show that
\begin{align}
\delta^{(L)}_i &= \frac{\partial\mathcal{L}(\textbf{y},\textbf{z}^{(L)}_{\cdot,out})}{\partial\textbf{z}^{(L)}_{i,out}}
\cdot{\sigma^\prime}^{(L)}(\textbf{z}^{(L)}_{i,in})
\\
\label{eq:dldwji}
\frac{\partial \mathcal{L}(\textbf{y},\textbf{z}^{(L)}_{\cdot,out})}{\partial \textbf{W}^{(\ell)}_{ji}}&=\delta^{(\ell)}_i\cdot\textbf{z}^{(\ell-1)}_{j,out} \\
\frac{\partial \mathcal{L}(\textbf{y},\textbf{z}^{(L)}_{\cdot,out})}{\partial \textbf{b}^{(\ell)}_{i}}&=\delta^{(\ell)}_i \\
\label{eq:deltas}
\delta^{(\ell-1)}_i&=\left(\sum_k\delta^{(\ell)}_k\cdot\textbf{W}^{(\ell)}_{ik}\right)\cdot{\sigma^\prime}^{(\ell-1)}(\textbf{z}^{(\ell-1)}_{i,in})
\end{align}

\item Use vectorized operations (i.e., operations with vectors and matrices) to compute the gradients with respect to a single sample.

\item $[$Optional$]$ Extend the vectorized operations to handle data in batches, and show that:
\begin{align}
\Delta^{(L)}&=\nabla_{\textbf{Z}^{(L)}_{out}}\mathcal{L}(\textbf{Y},\textbf{Z}^{(L)}_{out})\odot{\sigma^\prime}^{(L)}(\textbf{Z}^{(L)}_{in}) \\
\nabla_{\textbf{W}^{(\ell)}}\mathcal{L}(\textbf{Y},\textbf{Z}^{(\ell)}_{out})&={\textbf{Z}^{(\ell-1)}_{out}}^T \cdot\Delta^{(\ell)} \\
\nabla_{\textbf{b}^{(\ell)}}\mathcal{L}(\textbf{Y},\textbf{Z}^{(L)}_{out})&=\sum_i {\Delta^{(\ell)}_i}^T \\
\Delta^{(\ell-1)}&=\Delta^{(\ell)}{\textbf{W}^{(\ell)}}^T\odot{\sigma^\prime}^{(\ell-1)}(\textbf{Z}^{(\ell-1)}_{in})
\end{align}

where $\Delta^{(\ell)}$, $\textbf{Y}$ and $\textbf{Z}^{(\ell)}_{out}$ are matrices whose $i$-th row contain the respective vectors $\delta$, $\textbf{y}$ and $\textbf{z}^{(\ell)}_{\cdot,out}$ for the $i$-th sample in the batch, and $\odot$ is the element-wise product.
\end{enumerate}


<!--#!solutionbegin-->
### Solution

#### Question 1

By applying the chain rule, we have, for the last layer:

\begin{align}
\delta^{(L)}_i
&=\frac{\partial\mathcal{L}(\textbf{y},\textbf{z}^{(L)}_{\cdot,out})}{\partial\textbf{z}^{(L)}_{i,in}}
=\frac{\partial\mathcal{L}(\textbf{y},\textbf{z}^{(L)}_{\cdot,out})}{\partial\textbf{z}^{(L)}_{i,out}}
  \cdot\frac{\partial\textbf{z}^{(L)}_{i,out}}{\partial\textbf{z}^{(L)}_{i,in}}
=
\underbrace{
  \frac{\partial\mathcal{L}(\textbf{y},\textbf{z}^{(L)}_{\cdot,out})}{\partial\textbf{z}^{(L)}_{i,out}}
}_{\text{Loss-dependent}}\cdot{\sigma^\prime}^{(L)}(\textbf{z}^{(L)}_{i,in})
\end{align}

Where the first term depends on the loss function. Using the chain rule again, the derivatives of the weights of a generic layer $\ell$ are:

\begin{align}
\frac{
  \partial \mathcal{L}(\textbf{y},\textbf{z}^{(L)}_{\cdot,out})
}{
  \partial \textbf{W}^{(\ell)}_{ji}
}
&=\frac{
  \partial \mathcal{L}(\textbf{y},\textbf{z}^{(L)}_{\cdot,out})
}{
  \partial \textbf{z}^{(\ell)}_{i,in}
}\cdot\frac{
  \partial \textbf{z}^{(\ell)}_{i,in}
}{
  \partial \textbf{W}^{(\ell)}_{ji}
} \\
&=\delta^{(\ell)}_i\cdot\frac{
  \partial
}{
  \partial \textbf{W}^{(\ell)}_{ji}
}\underbrace{\left(
  \sum_k\textbf{W}^{(\ell)}_{ki}\textbf{z}^{(\ell-1)}_{k,out}+\textbf{b}^{(\ell)}_{i}
\right)}_{\textbf{z}^{(\ell)}_{i,in}} \\
&=\delta^{(\ell)}_i\cdot\textbf{z}^{(\ell-1)}_{j,out}
\end{align}

And, as for the bias:

\begin{align}
\frac{
  \partial \mathcal{L}(\textbf{y},\textbf{z}^{(L)}_{\cdot,out})
}{
    \partial \textbf{b}^{(\ell)}_{i}
}
&=\frac{
  \partial \mathcal{L}(\textbf{y},\textbf{z}^{(L)}_{\cdot,out})
}{
  \partial \textbf{z}^{(\ell)}_{i,in}
}\cdot\frac{
  \partial \textbf{z}^{(\ell)}_{i,in}
}{
  \partial \textbf{b}^{(\ell)}_{i}
} \\
&=\delta^{(\ell)}_i\cdot\frac{
  \partial
}{
  \partial \textbf{b}^{(\ell)}_{i}
}\underbrace{\left(
  \sum_k\textbf{W}^{(\ell)}_{ki}\textbf{z}^{(\ell-1)}_{k,out}+\textbf{b}^{(\ell)}_{i}
\right)}_{\textbf{z}^{(\ell)}_{i,in}} \\
&=\delta^{(\ell)}_i
\end{align}

Finally, the deltas of the previous layer are:

\begin{align}
\delta^{(\ell-1)}_i
&=\frac{
  \partial\mathcal{L}(\textbf{y},\textbf{z}^{(L)}_{\cdot,out})
}{
  \partial\mathbf{z}^{(\ell-1)}_{i,in}
} \\
&=\frac{
  \partial\mathcal{L}(\textbf{y},\textbf{z}^{(L)}_{\cdot,out})
}{
  \partial\mathbf{z}^{(\ell-1)}_{i,out}
}
\cdot\frac{
  \partial\mathbf{z}^{(\ell-1)}_{i,out}
}{
  \partial\mathbf{z}^{(\ell-1)}_{i,in}
} \\
&=\left(
  \sum_k\frac{
    \partial\mathcal{L}(\textbf{y},\textbf{z}^{(L)}_{\cdot,out})
  }{
    \partial\mathbf{z}^{(\ell)}_{k,in}
  }\cdot\frac{
    \partial\mathbf{z}^{(\ell)}_{k,in}
  }{
    \partial\mathbf{z}^{(\ell-1)}_{i,out}
  }
\right)
\cdot{\sigma^\prime}^{(\ell-1)}(\textbf{z}^{(\ell-1)}_{i,in}) \\
&=\left(
  \sum_k
  \delta^{(\ell)}_k
  \cdot\frac{
    \partial
  }{
    \partial\mathbf{z}^{(\ell-1)}_{i,out}
  }\left(
    \sum_l\textbf{W}^{(\ell)}_{lk}\textbf{z}^{(\ell-1)}_{l,out}+\textbf{b}^{(\ell)}_{k}
  \right)
\right)\cdot{\sigma^\prime}^{(\ell-1)}(\textbf{z}^{(\ell-1)}_{i,in}) \\
&=\left(
  \sum_k\delta^{(\ell)}_k\cdot\textbf{W}^{(\ell)}_{ik}
\right)\cdot{\sigma^\prime}^{(\ell-1)}(\textbf{z}^{(\ell-1)}_{i,in})
\end{align}

#### Question 2

The trick to find vectorized formulas is to see how to compute the previous equations all at the same time via matrix multiplication. Always check the dimensionality of the matrices and vectors involved, to make sure the shape of the result matches what it should be.

For the last deltas, each neuron is treated independently from the others, therefore an element-wise multiplication between the two vectors does the job:

\begin{equation}
\label{eq:lastdelta}
\begin{aligned}
\delta^{(L)}
&=\nabla_{\textbf{z}^{(L)}_{\cdot,out}}\mathcal{L}(\textbf{y},\textbf{z}^{(L)}_{\cdot,out})
\odot{\sigma^\prime}^{(L)}(\textbf{z}^{(L)}_{\cdot,in}) \\
(N^{(L)}\times 1) &= (N^{(L)}\times 1) \odot (N^{(L)}\times 1)
\end{aligned}
\end{equation}

Where the second row indicates the dimensionality, rows times columns, of the elements involved.

To compute the gradients for the weights in Eq. \ref{eq:dldwji}, we multiply every activation of the previous layer by every delta of the current layer, resulting into a matrix which contains all combinations of $\textbf{z}^{(\ell)}_{i,out}$ times $\delta^{(\ell)}_j$. This is computed as an "outer product":

\begin{equation}
\label{eq:gradw}
\begin{aligned}
\nabla_{\textbf{W}^{(\ell)}}\mathcal{L}(\textbf{y},\textbf{z}^{(L)}_{\cdot,out})&=\textbf{z}^{(\ell-1)}_{\cdot,out}\cdot{\delta^{(\ell)}}^T \\
(N^{(\ell-1)}\times N^{(\ell)})&=(N^{(\ell-1)}\times 1)\cdot (N^{(\ell)}\times 1)^T
\end{aligned}
\end{equation}

The gradient for the biases is easy to compute:

\begin{equation}
\nabla_{\textbf{b}^{(\ell)}}\mathcal{L}(\textbf{y},\textbf{z}^{(L)}_{\cdot,out})=\delta^\ell
\end{equation}

Finally, the deltas for the previous layer:

\begin{equation}
\begin{aligned}
\label{eq:deltasvec}
\delta^{(\ell-1)}&=\left(\textbf{W}^{(\ell)}\cdot\delta^{(\ell)}\right)
\odot{\sigma^\prime}^{(\ell-1)}(\textbf{z}^{(\ell-1)}_{\cdot,in}) \\
(N^{(\ell-1)}\times 1)&=\left(
(N^{(\ell-1)}\times N^{(\ell)})\cdot(N^{(\ell)}\times 1)
\right)\odot(N^{(\ell-1)}\times 1)
\end{aligned}
\end{equation}

Which follows because the sum in Eq. \ref{eq:deltas} is the dot-product of the $i$-th row of $\textbf{W}^{(\ell)}$ with $\delta^{(\ell)}$. Doing this separately for each row results in the matrix-vector multiplication $\textbf{W}^{(\ell)}\cdot\delta^{(\ell)}$.

#### Question 3 [Optional]

We now extend these formulas to handle batched data. Vectors become matrices where each row contains the vector for the corresponding sample in the batch:

 - The sample labels become a matrix $\textbf{Y}$, with $\textbf{Y}_{ij}$ the label for the $j$-th output of the $i$-th sample;
 - The hidden activations become $\textbf{Z}^{(\ell)}_{out}$, with $\textbf{Z}^{(\ell)}_{ij,out}$ the activation of the $j$-th unit in the $\ell$-th layer for the $i$-th sample;
 - The deltas become a matrix $\Delta^{(\ell)}$, where row $i$ contains $\delta^{(\ell)}$ for the $i$-th example in the batch.

Remember, the first thing you should do to understand these formulas is to think at the dimensionality of the vectors and matrices involved and make sure they match.

The delta for the output layer is:

\begin{equation}
\Delta^{(L)}
=\nabla_{\textbf{Z}^{(L)}_{out}}\mathcal{L}(\textbf{Y},\textbf{Z}^{(L)}_{out})
\odot{\sigma^\prime}^{(L)}(\textbf{Z}^{(L)}_{in})
\end{equation}

which looks the same as Eq. \ref{eq:lastdelta} above, except that now we are using matrices instead of vectors. But the operation is the same: element-wise multiplication.

The gradient with respect to $\textbf{W}^{(\ell)}$ is a bit more involved to compute, as it includes a three-dimensional tensor: the first dimension is for the samples, the second dimension is for the neurons of the $(\ell-1)$-th layer, and the third dimension for the neurons of the $\ell$-th layer. In other words, we are taking the gradients in Eq. \ref{eq:gradw}, which are matrices, for each sample, and "stacking" them one on top of each other to get a "cube" of gradients. The element indexed by $i,j,k$ is the derivative of the loss of the $i$-th sample in the batch with respect to $\textbf{W}^{(\ell)}_{jk}$.

\begin{equation}
\left(\nabla_{\textbf{W}^{(\ell)}}\mathcal{L}(\textbf{Y},\textbf{Z}^{(L)}_{out})\right)_{ijk}
=\left(
  \frac{\partial \mathcal{L}(\textbf{Y},\textbf{Z}^{(L)}_{out})}{\partial \textbf{W}^{(\ell)}_{jk}}
\right)_i
=\textbf{Z}^{(\ell-1)}_{ij,out}\cdot\Delta^{(\ell)}_{ik}
\end{equation}

To find the gradient of the weights with respect to the whole batch, we need to average this on the first dimension (the samples in the batch) to get the gradient:

\begin{equation}
\frac{\partial\mathcal{L}(\textbf{Y},\textbf{Z}^{(L)}_{out})}{\partial \textbf{W}^{(\ell)}_{jk}}
=\sum_i
\textbf{Z}^{(\ell-1)}_{ij,out}
\cdot\Delta^{(\ell)}_{ik}
\end{equation}

If you look closely, you should realize that this is just a matrix product. Let's use a simpler notation to make it clear:

\begin{equation}
A_{jk}=\sum_i B_{ij}\cdot C_{ik}=\sum_i {\left(B^T\right)}_{ji}\cdot C_{ik}
\end{equation}

Therefore, after much pain:

\begin{equation}
\nabla_{\textbf{W}^{(\ell)}}\mathcal{L}(\textbf{Y},\textbf{Z}^{(\ell)}_{out})
={\textbf{Z}^{(\ell-1)}_{out}}^T
\cdot\Delta^{(\ell)}
\end{equation}

The biases are straightforward, we just have to sum over the deltas of each sample:

\begin{equation}
\nabla_{\textbf{b}^{(\ell)}}\mathcal{L}(\textbf{Y},\textbf{Z}^{(L)}_{out})=\sum_i {\Delta^{(\ell)}_i}^T
\end{equation}

Finally, the deltas of the previous layer. From Eq. \ref{eq:deltas}, each element is:

\begin{equation}
\Delta^{(\ell-1)}_{ij}=\left(
  \sum_k\Delta^{(\ell)}_{ik}\cdot\textbf{W}^{(\ell)}_{jk}
\right)\cdot{\sigma^\prime}^{(\ell-1)}(\textbf{Z}^{(\ell-1)}_{ij,in})
\end{equation}

The sum is again a matrix product, therefore:

\begin{equation}
\Delta^{(\ell-1)}=\Delta^{(\ell)}{\textbf{W}^{(\ell)}}^T\odot{\sigma^\prime}^{(\ell-1)}(\textbf{Z}^{(\ell-1)}_{in})
\end{equation}

<!--#!solutionend-->

## Exercise 2

In this exercise, we will code the backpropagation algorithm and apply it to our five-points dataset.

First, let's define a function to quickly create a neural network with layers of given size. It will use tanh activation in the hidden layers and sigmoid for the output layer. Although we will use it for classification, we use the mean squared error loss for a change.

```{r}
# activations, losses, and their gradient
sigmoid = function(x) { 1 / (1 + exp(-x)) }
sigmoid_derivative = function(x) { sigmoid(x) * (1 - sigmoid(x)) }
tanh_derivative = function(x) { 1 - tanh(x)^2 }
mse = function(ytrue, ypred) { mean((ytrue - ypred)^2) }
mse_derivative = function(ytrue, ypred) { 2 * (ypred - ytrue) / length(ytrue) }


nnet_new = function(layer_sizes) {
  # all information about the network is stored in a list
  nnet = list(
    weights = list(),
    biases = list(),
    activations = list(),
    activations_derivatives = list(),
    loss = mse,
    loss_derivative = mse_derivative
  )

  # create random weight matrices and bias vectors
  last_size = layer_sizes[1]  # the first element is the number of inputs
  for(l in 2:length(layer_sizes)) {
    this_size = layer_sizes[l]

    # weights are initialize using the the famous "Glorot" initialization
    b = sqrt(6 / (this_size + last_size))
    nnet$weights[[l - 1]] = matrix(
      runif(last_size * this_size, -b, b), ncol = this_size
    )

    # biases are initialized to zero
    nnet$biases[[l - 1]] = rep(0, this_size)

    # set the activation
    nnet$activations[[l - 1]] = tanh
    nnet$activations_derivatives[[l - 1]] = tanh_derivative

    last_size = this_size
  }

  # change the output activation to sigmoid
  nnet$activations[[length(nnet$activations)]] = sigmoid
  nnet$activations_derivatives[[length(nnet$activations)]] = sigmoid_derivative

  nnet
}

nnet = nnet_new(c(2, 5, 3, 1))
```

Let us now write the forward pass:

```{r}
nnet_predict = function(nnet, data_x) {
  # data_x is a matrix with samples on rows
  #!hwbegin TODO perform the forward pass and return the predictions
  n_layers = length(nnet$weights)
  zout = data_x
  for(l in 1:n_layers) {
    zin = t(t(zout %*% nnet$weights[[l]]) + nnet$biases[[l]])
    zout = nnet$activations[[l]](zin)
  }
  zout
  #!hwend
}
```

As in the previous labs, let us visualize the output for a randomly initialized network:

```{r}
library(scales)
library(ggplot2)

grid = as.matrix(expand.grid(x1 = seq(-2, 2, 1 / 25), x2 = seq(-2, 2, 1 / 25)))
plot_grid = function(predictions) {
  # plots the predicted value for each point on the grid;
  # the predictions should have one column and
  # the same number of rows (10,201) as the data
  df = cbind(as.data.frame(grid), y = predictions)
  ggplot() +
    geom_tile(aes(x = x1, y = x2, fill = y, color = y), df) +
    scale_color_gradient2(low = muted("blue", 70), mid = "white",
                         high = muted("red", 70), limits = c(0, 1),
                         midpoint = 0.5) +
    scale_fill_gradient2(low = muted("blue", 70), mid = "white",
                        high = muted("red", 70), limits = c(0, 1),
                        midpoint = 0.5) +
    geom_point(aes(x=c(0, 1, 0, -1, 0), y=c(0, 0, -1, 0, 1))) +
    labs(x = latex2exp::TeX('$x_1$'), y = latex2exp::TeX('$x_2$')) +
    theme_minimal()
}

# run this a few times to see what different random networks predict
nnet = nnet_new(c(2, 5, 3, 1))
plot_grid(nnet_predict(nnet, grid))
```

Now, we code backpropagation to compute the gradients. Use the vectorized formulas in Equations 5-8 to make your code much faster.

```{r}
nnet_gradients = function(nnet, x, y) {
  # x is a matrix with samples on rows
  # y is a vector with the labels

  n_layers = length(nnet$weights)
  activations = list(x)
  pre_activations = list(x)
  #!hwbegin TODO perform the forward pass, storing all (pre-)activations
  for(l in 1:n_layers) {
    zin = t(t(activations[[l]] %*% nnet$weights[[l]]) + nnet$biases[[l]])
    zout = nnet$activations[[l]](zin)
    pre_activations[[l + 1]] = zin
    activations[[l + 1]] = zout
  }
  #!hwend
  loss = nnet$loss(y, activations[[length(activations)]])

  weights_gradients = list()
  biases_gradients = list()
  #!hwbegin TODO compute the gradients
  # Eq. 5
  deltas = nnet$loss_derivative(
    y, activations[[length(activations)]]
  ) * nnet$activations_derivatives[[n_layers]](pre_activations[[length(activations)]])
  for(l in n_layers:1) {
    weights_gradients[[l]] = t(activations[[l]]) %*% deltas  # Eq. 6
    biases_gradients[[l]] = colSums(deltas)  # Eq. 7

    if(l > 1) {
      # Eq. 8
      deltas = deltas %*% t(
        nnet$weights[[l]]
      ) * nnet$activations_derivatives[[l - 1]](pre_activations[[l]])
    }
  }
  #!hwend

  # make sure the gradients have the correct size
  for(l in 1:n_layers) {
    stopifnot(dim(nnet$weights[[l]]) == dim(weights_gradients[[l]]))
    stopifnot(length(nnet$biases[[l]]) == length(biases_gradients[[l]]))
  }

  # return gradients as a list
  list(
    loss = loss,
    weights_gradients = weights_gradients,
    biases_gradients = biases_gradients
  )
}

data_x = matrix(c(
  0, 1, 0, -1, 0,
  0, 0, -1, 0, 1
), ncol = 2)
data_y = c(1, 0, 0, 0, 0)
nnet_gradients(nnet, data_x, data_y)
```

We now need to implement gradient descent:

```{r}
nnet_gradient_descent_step = function(nnet, gradients, learning_rate) {
  #!hwbegin TODO perform one step of gradient descent,\n# modifying the parameters of the network
  for(l in 1:length(nnet$weights)) {
    nnet$weights[[l]] = (
      nnet$weights[[l]] - learning_rate * gradients$weights_gradients[[l]]
    )

    nnet$biases[[l]] = (
      nnet$biases[[l]] - learning_rate * gradients$biases_gradients[[l]]
    )
  }
  #!hwend
  nnet  # return the modified parameters
}


nnet_train = function(nnet, x, y, n_epochs, learning_rate) {
  losses = list()

  #!hwbegin TODO iterate over the dataset for the given number of epochs and\n# modify the weights at each epoch.\n# use all the data to compute the gradients
  for(e in 1:n_epochs) {
    gradients = nnet_gradients(nnet, x, y)
    nnet = nnet_gradient_descent_step(nnet, gradients, learning_rate)
    losses[[length(losses) + 1]] = gradients$loss
  }
  #!hwend

  list(
    losses = unlist(losses),
    nnet = nnet
  )
}
```

Finally, let us train the network on the small dataset:

```{r}
data_x = matrix(c(
  0, 1, 0, -1, 0,
  0, 0, -1, 0, 1
), ncol = 2)
data_y = c(1, 0, 0, 0, 0)

nnet = nnet_new(c(2, 5, 3, 1))
result = nnet_train(nnet, data_x, data_y, 2500, 0.25)
nnet_predict(result$nnet, data_x)
```
By plotting the loss after each parameter update, we can be sure that the network converged:

```{r}
ggplot(data.frame(loss = result$losses), aes(x = 1:length(loss), y = loss)) +
  geom_line() +
  labs(x = "Epoch", y = "Loss") +
  theme_minimal()
```
And the decision boundary of the network is:

```{r}
plot_grid(nnet_predict(result$nnet, grid))
```

Try to train a few randomly initialized network to discover different decision boundaries. Try to modify the learning rate and see how it affects the convergence speed. Finally, try different ways to initialize the weights and note how the trainability of the network is affected.





