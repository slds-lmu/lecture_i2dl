% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  a4paper,
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Lab 7},
  pdfauthor={Emilio Dorigatti},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{bbold}
\usepackage{tikz}
\usepackage[utf8]{inputenc}
\usetikzlibrary{snakes,arrows,shapes}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Lab 7}
\author{Emilio Dorigatti}
\date{2020-12-18}

\begin{document}
\maketitle

Welcome to the seventh lab, which is focused on convolutions and
convolutional neural networks. The first exercise shows how to train
CNNs with Keras, while the second exercise is about implementing
convolutions for black-and-white images. The third exercise is about
computing the gradients of the convolution operator.

\hypertarget{exercise-1}{%
\subsection{Exercise 1}\label{exercise-1}}

In this exercise, we will learn how to build CNNs in Keras to classify
images.

CNNs are a special type of neural network inspired by the structure of
the visual cortex in animals. They can be applied to a wide range of
tasks such as image recognition, time-series analysis, sentence
classification, etc. Two key features that differentiate CNNs from fully
connected nets are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Local connections: Each neuron in a convolutional layer is only
  connected to a subset of the neurons in the previous layer.
\item
  Shared weights: Each convolutional layer consists of multiple filters
  and each filter consists of multiple neurons. All the neurons in a
  given filter share the same weights but each of these neurons is
  connected to a different subset of the neurons in the previous layer.
\end{enumerate}

CNNs consistently outperform all other models in machine vision tasks
such as image recognition, object detection, etc.

\hypertarget{classifying-hand-written-digits}{%
\subsubsection{Classifying hand-written
digits}\label{classifying-hand-written-digits}}

We will be working with is the MNIST dataset (included in Keras). It
consists of 28x28 pixels, grayscale images of hand-written digits and
their associated labels (0 to 9). The training set contains 60,000
images and the test set contains 10,000. Let's load the data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(keras)}

\NormalTok{mnist }\OtherTok{=} \FunctionTok{dataset\_mnist}\NormalTok{()}

\CommentTok{\#train\_images and train\_labels form the training set}
\NormalTok{train\_images }\OtherTok{=}\NormalTok{ mnist}\SpecialCharTok{$}\NormalTok{train}\SpecialCharTok{$}\NormalTok{x}
\NormalTok{train\_labels }\OtherTok{=}\NormalTok{ mnist}\SpecialCharTok{$}\NormalTok{train}\SpecialCharTok{$}\NormalTok{y}

\CommentTok{\#test\_images and test\_labels from the test set}
\NormalTok{test\_images }\OtherTok{=}\NormalTok{ mnist}\SpecialCharTok{$}\NormalTok{test}\SpecialCharTok{$}\NormalTok{x}
\NormalTok{test\_labels }\OtherTok{=}\NormalTok{ mnist}\SpecialCharTok{$}\NormalTok{test}\SpecialCharTok{$}\NormalTok{y}
\end{Highlighting}
\end{Shaded}

The images are encoded as 3D arrays of integers from 0 to 255, and the
labels are 1D arrays of integers from 0 to 9.

\hypertarget{build-the-network}{%
\subsubsection{Build the network}\label{build-the-network}}

A CNN typically consists of a series of convolutional and pooling layers
followed by a few fully-connected layers. The convolutional layers
detect important visual patterns in the input, and the fully-connected
layers then classify the input based on the activations in the final
convolutional/pooling layer. Each convolutional layer consists of
multiple filters. When the CNN is trained, each filter in a layer
specializes in identifying patterns in the image that downstream layers
can use.

To create a convolutional layer in Keras, call the
\href{https://keras.rstudio.com/reference/layer_conv_2d.html}{\texttt{layer\_conv\_2d}}
function, and specify the the number of filters in the layer
(\texttt{filters} parameter), the size of the filters
(\texttt{kernel\_size} parameter), and the activation function to use
(\texttt{activation} parameter).

Pooling layers are used to downsample intermediate feature maps in the
CNN. Keras has multiple options for pooling layers, but today we will
only use
\href{https://keras.rstudio.com/reference/layer_max_pooling_2d.html}{\texttt{layer\_max\_pooling\_2d}}
which takes a \texttt{pool\_size} argument for the size of the pooling
window.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{=} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_conv\_2d}\NormalTok{(}
    \CommentTok{\# }\AlertTok{TODO}\CommentTok{ use 32 filters of size 3x3 and relu activation.}
    \CommentTok{\# Don\textquotesingle{}t forget the \textasciigrave{}input\_shape\textasciigrave{} parameter}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_max\_pooling\_2d}\NormalTok{(}
    \CommentTok{\# }\AlertTok{TODO}\CommentTok{ use a window of size 2}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_conv\_2d}\NormalTok{(}
    \CommentTok{\# }\AlertTok{TODO}\CommentTok{ use 64 filters of size 3x3 and relu activation}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_max\_pooling\_2d}\NormalTok{(}
    \CommentTok{\# }\AlertTok{TODO}\CommentTok{ use a window of size 2}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_conv\_2d}\NormalTok{(}
    \CommentTok{\# }\AlertTok{TODO}\CommentTok{ use 64 filters of size 3x3 and relu activation}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_flatten}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}
    \CommentTok{\# }\AlertTok{TODO}\CommentTok{ use 64 units with relu activation}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}
    \CommentTok{\# }\AlertTok{TODO}\CommentTok{ use 10 units and softmax activation}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Let's take a look at what we've built so far:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

You can see that the output of every \texttt{layer\_conv\_2d} and
\texttt{layer\_max\_pooling\_2d} is a 3D tensor of shape
\texttt{(height,\ width,\ channels)}. For example, the output of the
first layer is a tensor of shape \texttt{(26,\ 26,\ 32)}. Note that the
width and height dimensions shrink as you go deeper in the network. The
number of channels is controlled by the \texttt{filters} parameter of
the convolutional layers. Also, as you can see, the
\texttt{(3,\ 3,\ 64)} outputs are flattened into vectors of shape
\(576=3\cdot 3\cdot 64\) before going through two dense layers.

\hypertarget{data-preprocessing}{%
\subsubsection{Data preprocessing}\label{data-preprocessing}}

Before training this model, we have to preprocess the data by scaling
the inputs. Recall that the inputs are integer arrays in which the
elements take values between 0 in 255. It is standard practice to scale
the inputs so that the elements take values between 0 and 1. This
typically helps the network train better.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Reshape and rescale train\_images and test\_images.}
\NormalTok{train\_images }\OtherTok{=} \FunctionTok{array\_reshape}\NormalTok{(train\_images, }\FunctionTok{c}\NormalTok{(}\DecValTok{60000}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{train\_images }\OtherTok{=}\NormalTok{ train\_images }\SpecialCharTok{/} \DecValTok{255}

\NormalTok{test\_images }\OtherTok{=} \FunctionTok{array\_reshape}\NormalTok{(test\_images, }\FunctionTok{c}\NormalTok{(}\DecValTok{10000}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{test\_images }\OtherTok{=}\NormalTok{ test\_images }\SpecialCharTok{/} \DecValTok{255}
\end{Highlighting}
\end{Shaded}

\hypertarget{compiletrain-and-evaluate-the-model}{%
\subsubsection{Compile,train and evaluate the
model}\label{compiletrain-and-evaluate-the-model}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# }\AlertTok{TODO}\CommentTok{ compile the model using the rmsprop optimizer,}
\CommentTok{\# the sparse categorical cross{-}entropy loss, and the accuracy metric.}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# }\AlertTok{TODO}\CommentTok{ Train the model for 5 epochs with a batch size of 64,}
\CommentTok{\# and use 20\% of the images as validation}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{evaluate}\NormalTok{(}
  \CommentTok{\# }\AlertTok{TODO}\CommentTok{ Evaluate the model on the test data}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The accuracy of our model on MNIST is quite good!

\hypertarget{exercise-2}{%
\subsection{Exercise 2}\label{exercise-2}}

In this exercise we are going to implement convolution on images,
without worrying about stride and padding, and test it with the Sobel
filter. There are two Sobel filters: \(G_x\) detects horizontal edges
and \(G_y\) detects vertical edges.

\begin{equation}
G_x=\begin{vmatrix}
-1 & -2 & -1 \\
0 & 0 & 0 \\
1 & 2 & 1
\end{vmatrix}
\qquad
G_y=\begin{vmatrix}
-1 & 0 & 1 \\
-2 & 0 & 2 \\
-1 & 0 & 1
\end{vmatrix}={G_x}^T
\end{equation}

Can you explain why and how these filters work?

In order to get the image \(E\) with the edges, we convolve \(G_x\) and
\(G_y\) with the input image \(I\), to obtain the degree of horizontal
and vertical ``borderness'' of each pixel. We then combine these values
(separately for each pixel) with an L2 norm:

\begin{equation}
E=\sqrt{(G_x*I)^2+(G_y*I)^2}
\end{equation}

As a reference, this is the result we want to obtain:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(OpenImageR)}

\CommentTok{\# we only keep the first channel, as the image is already black and white}
\NormalTok{img }\OtherTok{=} \FunctionTok{readImage}\NormalTok{(}\StringTok{"\textasciitilde{}/Pictures/einstein.jpg"}\NormalTok{)[,,}\DecValTok{1}\NormalTok{]}

\CommentTok{\# NB: you can resize the image to speed up subsequent operations}
\CommentTok{\# img = resizeImage(img, floor(ncol(img) / 4), floor(nrow(img) / 4))}

\CommentTok{\# define the two filters}
\NormalTok{sobel\_x }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{nrow =} \DecValTok{3}\NormalTok{)}
\NormalTok{sobel\_y }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{nrow =} \DecValTok{3}\NormalTok{)}

\NormalTok{apply\_sobel }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(conv\_fn) \{}
  \CommentTok{\# convolve both filters on the image}
\NormalTok{  conv\_x }\OtherTok{=} \FunctionTok{conv\_fn}\NormalTok{(img, sobel\_x)}
\NormalTok{  conv\_y }\OtherTok{=} \FunctionTok{conv\_fn}\NormalTok{(img, sobel\_y)}
  
  \CommentTok{\# combine the two convolutions}
\NormalTok{  conv }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{(conv\_x}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+}\NormalTok{ conv\_y}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
  
  \CommentTok{\# normalize maximum value to 1}
\NormalTok{  conv }\SpecialCharTok{/} \FunctionTok{max}\NormalTok{(conv)}
\NormalTok{\}}


\NormalTok{result }\OtherTok{=} \FunctionTok{apply\_sobel}\NormalTok{(convolution)}
\NormalTok{grid}\SpecialCharTok{::}\FunctionTok{grid.raster}\NormalTok{(result)}
\end{Highlighting}
\end{Shaded}

We now implement our version of convolutions. For an input matrix
\(\textbf{X}\) of size \(r(\textbf{X})\times c(\textbf{X})\) and a
kernel \(\textbf{K}\) of size \(r(\textbf{K})\times c(\textbf{K})\), the
result of the convolution is \(\textbf{Y}=\textbf{K}*\textbf{X}\) with
\(r(\textbf{Y})=r(\textbf{X})-r(\textbf{K})+1\),
\(c(\textbf{Y})=c(\textbf{X})-c(\textbf{K})+1\), and elements:

\begin{equation}
y_{ij}=\sum_{k=1}^{r(\textbf{K})}\sum_{l=1}^{c(\textbf{K})}x_{i+k-1,j+l-1}\cdot k_{kl}
\end{equation}

for \(1\leq i \leq r(\textbf{Y})\) and \(1\leq j \leq c(\textbf{Y})\).

You now have to implement a function that computes \(y_{ij}\) given the
image, the kernel, \(i\) and \(j\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{compute\_convolution\_at\_position }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(i, j, image, kernel) \{}
  \CommentTok{\# }\AlertTok{TODO}\CommentTok{ compute and return the convolution at row i and column j with the formula above}
\NormalTok{\}}


\NormalTok{our\_conv }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(image, kernel) \{}
\NormalTok{  result\_rows }\OtherTok{=}\NormalTok{ (}
    \CommentTok{\# }\AlertTok{TODO}\CommentTok{ compute the number of rows of the result}
\NormalTok{  )}
  
\NormalTok{  result\_cols }\OtherTok{=}\NormalTok{ (}
    \CommentTok{\# }\AlertTok{TODO}\CommentTok{ compute the number of columns of the result}
\NormalTok{  )}

  \CommentTok{\# perform the convolution}
\NormalTok{  vec }\OtherTok{=} \FunctionTok{apply}\NormalTok{(}\FunctionTok{expand.grid}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{result\_rows, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{result\_cols), }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(pos) \{}
    \FunctionTok{compute\_convolution\_at\_position}\NormalTok{(pos[}\DecValTok{1}\NormalTok{], pos[}\DecValTok{2}\NormalTok{], image, kernel)}
\NormalTok{  \})}
  
  \CommentTok{\# reshape to a matrix}
  \FunctionTok{matrix}\NormalTok{(vec, }\AttributeTok{nrow =}\NormalTok{ result\_rows, }\AttributeTok{ncol =}\NormalTok{ result\_cols)}
\NormalTok{\}}

\NormalTok{our\_result }\OtherTok{=} \FunctionTok{apply\_sobel}\NormalTok{(our\_conv)}
\NormalTok{grid}\SpecialCharTok{::}\FunctionTok{grid.raster}\NormalTok{(our\_result)}
\end{Highlighting}
\end{Shaded}

If you did everything correctly, this image should match the image
above.

\hypertarget{exercise-3}{%
\subsection{Exercise 3}\label{exercise-3}}

Recall that the convolution \(\textbf{Y}=\textbf{K}*\textbf{X}\) has
elements

\begin{equation}
y_{ij}=\sum_{k=1}^{r(\textbf{K})}\sum_{l=1}^{c(\textbf{K})}x_{i+k-1,j+l-1}\cdot k_{kl}
\end{equation}

Now consider \(\textbf{X}\) and \(\textbf{Y}\) to be the input and
output of a convolutional layer with filter \(\textbf{K}\). For
simplicity, we focus on a single channel; actual convolution layers in
CNN perform this operation several times with different learnable
filters.

Imagine this convolution is a hidden layer of the neural network, with
\(\textbf{X}\) being the input from the previous layer, and
\(\textbf{Y}\) the pre-activation output to the next layer. Then, we can
define the loss function in terms of \(\textbf{Y}\),
i.e.~\(\mathcal{L}=f(\textbf{Y})\), where \(f\) includes the activation,
all the following layers, and the classification/regression loss.

Show that:

\begin{equation}
\frac{\partial\mathcal{L}}{\partial k_{kl}}=\sum_{i=1}^{r(\textbf{Y})}\sum_{j=1}^{c(\textbf{Y})}\frac{\partial\mathcal{L}}{\partial y_{ij}}\cdot x_{i+k-1,j+l-1}
\end{equation}

Then show that

\begin{equation}
\frac{\partial\mathcal{L}}{\partial x_{ij}}=\sum_{k=L_k}^{U_k}\sum_{l=L_l}^{U_l}\frac{\partial\mathcal{L}}{\partial y_{ab}}k_{kl}
\end{equation}

With

\begin{align}
a &= i-k+1 \\
b &= j-l+1 \\
L_k &= \max(1, i - r(\textbf{X}) + r(\textbf{K})) \\
L_l &= \max(1, j - c(\textbf{X}) + c(\textbf{K})) \\
U_k &= \min(r(\textbf{K}), i) \\
U_l &= \min(c(\textbf{K}), j)
\end{align}

As you can see, the gradient of the input is obtained by convolving the
same filter with the gradient of the output, with some care at the
borders.

Hint: it is easier to analyze convolutions in one dimension with a small
example, then generalize the result to two dimensions and arbitrary
filter/image size.

Now, write a function that computes
\(\partial \mathcal{L}/\partial x_{ij}\), with
\(\mathcal{L}=\sum_{i,j}{y_{ij}}^2\) and \(\textbf{K}=G_x\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{conv\_gradient\_wrt\_input }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(dloss\_dy, kernel) \{}
\NormalTok{  image\_rows }\OtherTok{=}\NormalTok{ (}
    \CommentTok{\# }\AlertTok{TODO}\CommentTok{ compute the number of rows of the original image}
\NormalTok{  )}
  
\NormalTok{  image\_cols }\OtherTok{=}\NormalTok{ (}
    \CommentTok{\# }\AlertTok{TODO}\CommentTok{ compute the number of columns of the original image}
\NormalTok{  )}

\NormalTok{  vec }\OtherTok{=} \FunctionTok{apply}\NormalTok{(}\FunctionTok{expand.grid}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{image\_rows, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{image\_cols), }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(pos) \{}
\NormalTok{    i }\OtherTok{=}\NormalTok{ pos[}\DecValTok{1}\NormalTok{]}
\NormalTok{    j }\OtherTok{=}\NormalTok{ pos[}\DecValTok{2}\NormalTok{]}

    \CommentTok{\# }\AlertTok{TODO}\CommentTok{ compute and return the gradient at row i and column j}
\NormalTok{  \})}
  
  \FunctionTok{matrix}\NormalTok{(vec, }\AttributeTok{nrow =}\NormalTok{ image\_rows, }\AttributeTok{ncol =}\NormalTok{ image\_cols)}
\NormalTok{\}}


\NormalTok{result }\OtherTok{=} \FunctionTok{our\_conv}\NormalTok{(img, sobel\_x)}
\NormalTok{input\_gradient }\OtherTok{=} \FunctionTok{conv\_gradient\_wrt\_input}\NormalTok{(}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ result, sobel\_x)}
\NormalTok{grid}\SpecialCharTok{::}\FunctionTok{grid.raster}\NormalTok{((}
\NormalTok{  input\_gradient }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(input\_gradient)}
\NormalTok{) }\SpecialCharTok{/}\NormalTok{ (}
  \FunctionTok{max}\NormalTok{(input\_gradient) }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(input\_gradient)}
\NormalTok{))}
\end{Highlighting}
\end{Shaded}

We can verify this gradient is correct for a single pixel with finite
differences:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eps }\OtherTok{=} \FloatTok{1e{-}6}

\NormalTok{i }\OtherTok{=} \FunctionTok{floor}\NormalTok{(}\FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(img) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{))}
\NormalTok{j }\OtherTok{=} \FunctionTok{floor}\NormalTok{(}\FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FunctionTok{ncol}\NormalTok{(img) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{))}

\CommentTok{\# add epsilon to position i,j and convolve}
\NormalTok{img[i, j] }\OtherTok{=}\NormalTok{ img[i, j] }\SpecialCharTok{+}\NormalTok{ eps}
\NormalTok{conv\_pos }\OtherTok{=} \FunctionTok{our\_conv}\NormalTok{(img, sobel\_x)}

\CommentTok{\# remove epsilon to position i,j and convolve}
\NormalTok{img[i, j] }\OtherTok{=}\NormalTok{ img[i, j] }\SpecialCharTok{{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ eps}
\NormalTok{conv\_neg }\OtherTok{=} \FunctionTok{our\_conv}\NormalTok{(img, sobel\_x)}

\CommentTok{\# undo modification to the image}
\NormalTok{img[i, j] }\OtherTok{=}\NormalTok{ img[i, j] }\SpecialCharTok{+}\NormalTok{ eps}

\CommentTok{\# compute the difference of the losses}
\CommentTok{\# NB: we sum the differences to get a more accurate result}
\NormalTok{empirical\_gradient }\OtherTok{=} \FunctionTok{sum}\NormalTok{(conv\_pos}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{{-}}\NormalTok{ conv\_neg}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ eps)}

\CommentTok{\# compare empirical and analytical gradients }
\FunctionTok{c}\NormalTok{(empirical\_gradient, input\_gradient[i, j])}
\end{Highlighting}
\end{Shaded}

If you did everything correctly, these two numbers should be the same.

Now, can you guess what image \emph{maximizes} the loss we just defined?
We can find this through gradient \emph{ascent}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{maxim }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{runif}\NormalTok{(}\DecValTok{81}\NormalTok{), }\AttributeTok{nrow =} \DecValTok{9}\NormalTok{)}

\NormalTok{losses }\OtherTok{=} \FunctionTok{lapply}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{250}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(i) \{}
  \CommentTok{\# }\AlertTok{TODO}\CommentTok{ convolve \textasciigrave{}sobel\_x\textasciigrave{} with \textasciigrave{}maxim\textasciigrave{} and compute the loss}

  \CommentTok{\# }\AlertTok{TODO}\CommentTok{ compute the gradient of the loss, and modify \textasciigrave{}maxim\textasciigrave{} accordingly}
  
\NormalTok{  loss}
\NormalTok{\})}

\FunctionTok{plot}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(losses), losses)}
\NormalTok{grid}\SpecialCharTok{::}\FunctionTok{grid.raster}\NormalTok{(}
\NormalTok{  (maxim }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(maxim)) }\SpecialCharTok{/}\NormalTok{ (}\FunctionTok{max}\NormalTok{(maxim) }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(maxim)),}
  \AttributeTok{interpolate =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}


\end{document}
