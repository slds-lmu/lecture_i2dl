% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  a4paper,
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Lab 7},
  pdfauthor={Emilio Dorigatti},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{bbold}
\usepackage{tikz}
\usepackage[utf8]{inputenc}
\usetikzlibrary{snakes,arrows,shapes}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Lab 7}
\author{Emilio Dorigatti}
\date{2020-12-18}

\begin{document}
\maketitle

Welcome to the seventh lab, which is focused on convolutions and
convolutional neural networks. The first exercise shows how to train
CNNs with Keras, while the second exercise is about implementing
convolutions for black-and-white images. The third exercise is about
computing the gradients of the convolution operator.

\hypertarget{exercise-1}{%
\subsection{Exercise 1}\label{exercise-1}}

In this exercise, we will learn how to build CNNs in Keras to classify
images.

CNNs are a special type of neural network inspired by the structure of
the visual cortex in animals. They can be applied to a wide range of
tasks such as image recognition, time-series analysis, sentence
classification, etc. Two key features that differentiate CNNs from fully
connected nets are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Local connections: Each neuron in a convolutional layer is only
  connected to a subset of the neurons in the previous layer.
\item
  Shared weights: Each convolutional layer consists of multiple filters
  and each filter consists of multiple neurons. All the neurons in a
  given filter share the same weights but each of these neurons is
  connected to a different subset of the neurons in the previous layer.
\end{enumerate}

CNNs consistently outperform all other models in machine vision tasks
such as image recognition, object detection, etc.

\hypertarget{classifying-hand-written-digits}{%
\subsubsection{Classifying hand-written
digits}\label{classifying-hand-written-digits}}

We will be working with is the MNIST dataset (included in Keras). It
consists of 28x28 pixels, grayscale images of hand-written digits and
their associated labels (0 to 9). The training set contains 60,000
images and the test set contains 10,000. Let's load the data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(keras)}

\NormalTok{mnist }\OtherTok{=} \FunctionTok{dataset\_mnist}\NormalTok{()}

\CommentTok{\#train\_images and train\_labels form the training set}
\NormalTok{train\_images }\OtherTok{=}\NormalTok{ mnist}\SpecialCharTok{$}\NormalTok{train}\SpecialCharTok{$}\NormalTok{x}
\NormalTok{train\_labels }\OtherTok{=}\NormalTok{ mnist}\SpecialCharTok{$}\NormalTok{train}\SpecialCharTok{$}\NormalTok{y}

\CommentTok{\#test\_images and test\_labels from the test set}
\NormalTok{test\_images }\OtherTok{=}\NormalTok{ mnist}\SpecialCharTok{$}\NormalTok{test}\SpecialCharTok{$}\NormalTok{x}
\NormalTok{test\_labels }\OtherTok{=}\NormalTok{ mnist}\SpecialCharTok{$}\NormalTok{test}\SpecialCharTok{$}\NormalTok{y}
\end{Highlighting}
\end{Shaded}

The images are encoded as 3D arrays of integers from 0 to 255, and the
labels are 1D arrays of integers from 0 to 9.

\hypertarget{build-the-network}{%
\subsubsection{Build the network}\label{build-the-network}}

A CNN typically consists of a series of convolutional and pooling layers
followed by a few fully-connected layers. The convolutional layers
detect important visual patterns in the input, and the fully-connected
layers then classify the input based on the activations in the final
convolutional/pooling layer. Each convolutional layer consists of
multiple filters. When the CNN is trained, each filter in a layer
specializes in identifying patterns in the image that downstream layers
can use.

To create a convolutional layer in Keras, call the
\href{https://keras.rstudio.com/reference/layer_conv_2d.html}{\texttt{layer\_conv\_2d}}
function, and specify the the number of filters in the layer
(\texttt{filters} parameter), the size of the filters
(\texttt{kernel\_size} parameter), and the activation function to use
(\texttt{activation} parameter).

Pooling layers are used to downsample intermediate feature maps in the
CNN. Keras has multiple options for pooling layers, but today we will
only use
\href{https://keras.rstudio.com/reference/layer_max_pooling_2d.html}{\texttt{layer\_max\_pooling\_2d}}
which takes a \texttt{pool\_size} argument for the size of the pooling
window.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{=} \FunctionTok{keras\_model\_sequential}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_conv\_2d}\NormalTok{(}
    \DecValTok{32}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{activation =} \StringTok{"relu"}\NormalTok{, }\AttributeTok{input\_shape =} \FunctionTok{c}\NormalTok{(}\DecValTok{28}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_max\_pooling\_2d}\NormalTok{(}
    \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_conv\_2d}\NormalTok{(}
    \DecValTok{64}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{activation =} \StringTok{"relu"}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_max\_pooling\_2d}\NormalTok{(}
    \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_conv\_2d}\NormalTok{(}
    \DecValTok{64}\NormalTok{, }\FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{activation =} \StringTok{"relu"}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_flatten}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}
    \DecValTok{64}\NormalTok{, }\AttributeTok{activation =} \StringTok{"relu"}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{layer\_dense}\NormalTok{(}
    \DecValTok{10}\NormalTok{, }\AttributeTok{activation =} \StringTok{"softmax"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Let's take a look at what we've built so far:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Model: "sequential"
## ________________________________________________________________________________
## Layer (type)                        Output Shape                    Param #     
## ================================================================================
## conv2d_2 (Conv2D)                   (None, 26, 26, 32)              320         
## ________________________________________________________________________________
## max_pooling2d_1 (MaxPooling2D)      (None, 13, 13, 32)              0           
## ________________________________________________________________________________
## conv2d_1 (Conv2D)                   (None, 11, 11, 64)              18496       
## ________________________________________________________________________________
## max_pooling2d (MaxPooling2D)        (None, 5, 5, 64)                0           
## ________________________________________________________________________________
## conv2d (Conv2D)                     (None, 3, 3, 64)                36928       
## ________________________________________________________________________________
## flatten (Flatten)                   (None, 576)                     0           
## ________________________________________________________________________________
## dense_1 (Dense)                     (None, 64)                      36928       
## ________________________________________________________________________________
## dense (Dense)                       (None, 10)                      650         
## ================================================================================
## Total params: 93,322
## Trainable params: 93,322
## Non-trainable params: 0
## ________________________________________________________________________________
\end{verbatim}

You can see that the output of every \texttt{layer\_conv\_2d} and
\texttt{layer\_max\_pooling\_2d} is a 3D tensor of shape
\texttt{(height,\ width,\ channels)}. For example, the output of the
first layer is a tensor of shape \texttt{(26,\ 26,\ 32)}. Note that the
width and height dimensions shrink as you go deeper in the network. The
number of channels is controlled by the \texttt{filters} parameter of
the convolutional layers. Also, as you can see, the
\texttt{(3,\ 3,\ 64)} outputs are flattened into vectors of shape
\(576=3\cdot 3\cdot 64\) before going through two dense layers.

\hypertarget{data-preprocessing}{%
\subsubsection{Data preprocessing}\label{data-preprocessing}}

Before training this model, we have to preprocess the data by scaling
the inputs. Recall that the inputs are integer arrays in which the
elements take values between 0 in 255. It is standard practice to scale
the inputs so that the elements take values between 0 and 1. This
typically helps the network train better.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Reshape and rescale train\_images and test\_images.}
\NormalTok{train\_images }\OtherTok{=} \FunctionTok{array\_reshape}\NormalTok{(train\_images, }\FunctionTok{c}\NormalTok{(}\DecValTok{60000}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{train\_images }\OtherTok{=}\NormalTok{ train\_images }\SpecialCharTok{/} \DecValTok{255}

\NormalTok{test\_images }\OtherTok{=} \FunctionTok{array\_reshape}\NormalTok{(test\_images, }\FunctionTok{c}\NormalTok{(}\DecValTok{10000}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{28}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{test\_images }\OtherTok{=}\NormalTok{ test\_images }\SpecialCharTok{/} \DecValTok{255}
\end{Highlighting}
\end{Shaded}

\hypertarget{compiletrain-and-evaluate-the-model}{%
\subsubsection{Compile,train and evaluate the
model}\label{compiletrain-and-evaluate-the-model}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{compile}\NormalTok{(}
\NormalTok{  model,}
  \AttributeTok{loss =} \StringTok{"sparse\_categorical\_crossentropy"}\NormalTok{,}
  \AttributeTok{optimizer =} \StringTok{"rmsprop"}\NormalTok{,}
  \AttributeTok{metrics =} \FunctionTok{c}\NormalTok{(}\StringTok{"accuracy"}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fit}\NormalTok{(}
\NormalTok{  model,}
\NormalTok{  train\_images, train\_labels,}
  \AttributeTok{batch\_size =} \DecValTok{64}\NormalTok{,}
  \AttributeTok{epochs =} \DecValTok{5}\NormalTok{,}
  \AttributeTok{validation\_split =} \FloatTok{0.2}\NormalTok{,}
  \AttributeTok{verbose =} \DecValTok{0}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{evaluate}\NormalTok{(}
\NormalTok{  model, test\_images, test\_labels,}
  \AttributeTok{verbose =} \DecValTok{0}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $loss
## [1] 0.03047978
## 
## $accuracy
## [1] 0.9905
\end{verbatim}

The accuracy of our model on MNIST is quite good!

\hypertarget{exercise-2}{%
\subsection{Exercise 2}\label{exercise-2}}

In this exercise we are going to implement convolution on images,
without worrying about stride and padding, and test it with the Sobel
filter. There are two Sobel filters: \(G_x\) detects horizontal edges
and \(G_y\) detects vertical edges.

\begin{equation}
G_x=\begin{vmatrix}
-1 & -2 & -1 \\
0 & 0 & 0 \\
1 & 2 & 1
\end{vmatrix}
\qquad
G_y=\begin{vmatrix}
-1 & 0 & 1 \\
-2 & 0 & 2 \\
-1 & 0 & 1
\end{vmatrix}={G_x}^T
\end{equation}

Can you explain why and how these filters work?

In order to get the image \(E\) with the edges, we convolve \(G_x\) and
\(G_y\) with the input image \(I\), to obtain the degree of horizontal
and vertical ``borderness'' of each pixel. We then combine these values
(separately for each pixel) with an L2 norm:

\begin{equation}
E=\sqrt{(G_x*I)^2+(G_y*I)^2}
\end{equation}

As a reference, this is the result we want to obtain:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(OpenImageR)}

\CommentTok{\# we only keep the first channel, as the image is already black and white}
\NormalTok{img }\OtherTok{=} \FunctionTok{readImage}\NormalTok{(}\StringTok{"\textasciitilde{}/Pictures/einstein.jpg"}\NormalTok{)[,,}\DecValTok{1}\NormalTok{]}

\CommentTok{\# NB: you can resize the image to speed up subsequent operations}
\CommentTok{\# img = resizeImage(img, floor(ncol(img) / 4), floor(nrow(img) / 4))}

\CommentTok{\# define the two filters}
\NormalTok{sobel\_x }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{nrow =} \DecValTok{3}\NormalTok{)}
\NormalTok{sobel\_y }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{nrow =} \DecValTok{3}\NormalTok{)}

\NormalTok{apply\_sobel }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(conv\_fn) \{}
  \CommentTok{\# convolve both filters on the image}
\NormalTok{  conv\_x }\OtherTok{=} \FunctionTok{conv\_fn}\NormalTok{(img, sobel\_x)}
\NormalTok{  conv\_y }\OtherTok{=} \FunctionTok{conv\_fn}\NormalTok{(img, sobel\_y)}
  
  \CommentTok{\# combine the two convolutions}
\NormalTok{  conv }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{(conv\_x}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+}\NormalTok{ conv\_y}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
  
  \CommentTok{\# normalize maximum value to 1}
\NormalTok{  conv }\SpecialCharTok{/} \FunctionTok{max}\NormalTok{(conv)}
\NormalTok{\}}


\NormalTok{result }\OtherTok{=} \FunctionTok{apply\_sobel}\NormalTok{(convolution)}
\NormalTok{grid}\SpecialCharTok{::}\FunctionTok{grid.raster}\NormalTok{(result)}
\end{Highlighting}
\end{Shaded}

\includegraphics{lab7-solutions_files/figure-latex/unnamed-chunk-8-1.pdf}

We now implement our version of convolutions. For an input matrix
\(\textbf{X}\) of size \(r(\textbf{X})\times c(\textbf{X})\) and a
kernel \(\textbf{K}\) of size \(r(\textbf{K})\times c(\textbf{K})\), the
result of the convolution is \(\textbf{Y}=\textbf{K}*\textbf{X}\) with
\(r(\textbf{Y})=r(\textbf{X})-r(\textbf{K})+1\),
\(c(\textbf{Y})=c(\textbf{X})-c(\textbf{K})+1\), and elements:

\begin{equation}
y_{ij}=\sum_{k=1}^{r(\textbf{K})}\sum_{l=1}^{c(\textbf{K})}x_{i+k-1,j+l-1}\cdot k_{kl}
\end{equation}

for \(1\leq i \leq r(\textbf{Y})\) and \(1\leq j \leq c(\textbf{Y})\).

You now have to implement a function that computes \(y_{ij}\) given the
image, the kernel, \(i\) and \(j\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{compute\_convolution\_at\_position }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(i, j, image, kernel) \{}
\NormalTok{  result\_ij }\OtherTok{=} \FloatTok{0.0}
  \ControlFlowTok{for}\NormalTok{(k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(kernel)) \{}
    \ControlFlowTok{for}\NormalTok{(l }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{ncol}\NormalTok{(kernel)) \{}
\NormalTok{      result\_ij }\OtherTok{=}\NormalTok{ result\_ij }\SpecialCharTok{+}\NormalTok{ image[i }\SpecialCharTok{+}\NormalTok{ k }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{, j }\SpecialCharTok{+}\NormalTok{ l }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ kernel[k, l]}
\NormalTok{    \}}
\NormalTok{  \}}
\NormalTok{  result\_ij}
\NormalTok{\}}


\NormalTok{our\_conv }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(image, kernel) \{}
\NormalTok{  result\_rows }\OtherTok{=}\NormalTok{ (}
    \FunctionTok{nrow}\NormalTok{(image) }\SpecialCharTok{{-}} \FunctionTok{nrow}\NormalTok{(kernel) }\SpecialCharTok{+} \DecValTok{1}
\NormalTok{  )}
  
\NormalTok{  result\_cols }\OtherTok{=}\NormalTok{ (}
    \FunctionTok{ncol}\NormalTok{(image) }\SpecialCharTok{{-}} \FunctionTok{ncol}\NormalTok{(kernel) }\SpecialCharTok{+} \DecValTok{1}
\NormalTok{  )}

  \CommentTok{\# perform the convolution}
\NormalTok{  vec }\OtherTok{=} \FunctionTok{apply}\NormalTok{(}\FunctionTok{expand.grid}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{result\_rows, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{result\_cols), }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(pos) \{}
    \FunctionTok{compute\_convolution\_at\_position}\NormalTok{(pos[}\DecValTok{1}\NormalTok{], pos[}\DecValTok{2}\NormalTok{], image, kernel)}
\NormalTok{  \})}
  
  \CommentTok{\# reshape to a matrix}
  \FunctionTok{matrix}\NormalTok{(vec, }\AttributeTok{nrow =}\NormalTok{ result\_rows, }\AttributeTok{ncol =}\NormalTok{ result\_cols)}
\NormalTok{\}}

\NormalTok{our\_result }\OtherTok{=} \FunctionTok{apply\_sobel}\NormalTok{(our\_conv)}
\NormalTok{grid}\SpecialCharTok{::}\FunctionTok{grid.raster}\NormalTok{(our\_result)}
\end{Highlighting}
\end{Shaded}

\includegraphics{lab7-solutions_files/figure-latex/unnamed-chunk-9-1.pdf}

If you did everything correctly, this image should match the image
above.

\hypertarget{exercise-3}{%
\subsection{Exercise 3}\label{exercise-3}}

Recall that the convolution \(\textbf{Y}=\textbf{K}*\textbf{X}\) has
elements

\begin{equation}
y_{ij}=\sum_{k=1}^{r(\textbf{K})}\sum_{l=1}^{c(\textbf{K})}x_{i+k-1,j+l-1}\cdot k_{kl}
\end{equation}

Now consider \(\textbf{X}\) and \(\textbf{Y}\) to be the input and
output of a convolutional layer with filter \(\textbf{K}\). For
simplicity, we focus on a single channel; actual convolution layers in
CNN perform this operation several times with different learnable
filters.

Imagine this convolution is a hidden layer of the neural network, with
\(\textbf{X}\) being the input from the previous layer, and
\(\textbf{Y}\) the pre-activation output to the next layer. Then, we can
define the loss function in terms of \(\textbf{Y}\),
i.e.~\(\mathcal{L}=f(\textbf{Y})\), where \(f\) includes the activation,
all the following layers, and the classification/regression loss.

Show that:

\begin{equation}
\frac{\partial\mathcal{L}}{\partial k_{kl}}=\sum_{i=1}^{r(\textbf{Y})}\sum_{j=1}^{c(\textbf{Y})}\frac{\partial\mathcal{L}}{\partial y_{ij}}\cdot x_{i+k-1,j+l-1}
\end{equation}

Then show that

\begin{equation}
\frac{\partial\mathcal{L}}{\partial x_{ij}}=\sum_{k=L_k}^{U_k}\sum_{l=L_l}^{U_l}\frac{\partial\mathcal{L}}{\partial y_{ab}}k_{kl}
\end{equation}

With

\begin{align}
a &= i-k+1 \\
b &= j-l+1 \\
L_k &= \max(1, i - r(\textbf{X}) + r(\textbf{K})) \\
L_l &= \max(1, j - c(\textbf{X}) + c(\textbf{K})) \\
U_k &= \min(r(\textbf{K}), i) \\
U_l &= \min(c(\textbf{K}), j)
\end{align}

As you can see, the gradient of the input is obtained by convolving the
same filter with the gradient of the output, with some care at the
borders.

Hint: it is easier to analyze convolutions in one dimension with a small
example, then generalize the result to two dimensions and arbitrary
filter/image size.

Now, write a function that computes
\(\partial \mathcal{L}/\partial x_{ij}\), with
\(\mathcal{L}=\sum_{i,j}{y_{ij}}^2\) and \(\textbf{K}=G_x\).

\hypertarget{solution}{%
\paragraph{Solution}\label{solution}}

We start by showing that

\begin{equation}
\frac{\partial\mathcal{L}}{\partial k_{kl}}=\sum_{i=1}^{r(\textbf{Y})}\sum_{j=1}^{c(\textbf{Y})}\frac{\partial\mathcal{L}}{\partial y_{ij}}\cdot x_{i+k-1,j+l-1}
\end{equation}

The loss is a generic function of every \(y_{ij}\), therefore by the
chain rule we have

\begin{equation}
\frac{\partial\mathcal{L}}{\partial k_{kl}}=\sum_{i=1}^{r(\textbf{Y})}\sum_{j=1}^{c(\textbf{Y})}\frac{\partial\mathcal{L}}{\partial y_{ij}}\cdot\frac{\partial y_{ij}}{\partial k_{kl}}
\end{equation}

The element of the kernel is fixed (because \(k\) and \(l\) are given),
and each \(y_{ij}\) is influenced only once by \(k_{kl}\). This means
that we have to find which element of \(\textbf{X}\) is
\(\partial y_{ij}/\partial k_{kl}\). In the image below, \(\textbf{X}\)
is a vector of \(c(\textbf{X})=6\) elements, and \(\textbf{Y}\) was
obtained with a kernel of size \(c(\textbf{K})=3\), which result in
\(c(\textbf{Y})=4\). The arrows indicate which elements of
\(\textbf{X}\) were used to compute which element of \(\textbf{Y}\), and
the red arrows highlight the third element of the kernel, i.e.~\(k_3\).

\begin{figure}
\centering
\begin{tikzpicture}[>=latex',line join=bevel,]
\node (x1) at (27.0bp,90.0bp) [draw,ellipse] {$x_1$};
  \node (y1) at (99.0bp,18.0bp) [draw,ellipse] {$y_1$};
  \node (x2) at (99.0bp,90.0bp) [draw,ellipse] {$x_2$};
  \node (y2) at (171.0bp,18.0bp) [draw,ellipse] {$y_2$};
  \node (x3) at (171.0bp,90.0bp) [draw,ellipse] {$x_3$};
  \node (y3) at (243.0bp,18.0bp) [draw,ellipse] {$y_3$};
  \node (x4) at (243.0bp,90.0bp) [draw,ellipse] {$x_4$};
  \node (y4) at (315.0bp,18.0bp) [draw,ellipse] {$y_4$};
  \node (x5) at (315.0bp,90.0bp) [draw,ellipse] {$x_5$};
  \node (x6) at (387.0bp,90.0bp) [draw,ellipse] {$x_6$};
  \draw [->] (x1) ..controls (51.75bp,64.938bp) and (65.524bp,51.546bp)  .. (y1);
  \draw [->] (x2) ..controls (99.0bp,63.983bp) and (99.0bp,54.712bp)  .. (y1);
  \draw [->] (x2) ..controls (123.75bp,64.938bp) and (137.52bp,51.546bp)  .. (y2);
  \draw [red,->] (x3) ..controls (146.25bp,64.938bp) and (132.48bp,51.546bp)  .. (y1);
  \draw [->] (x3) ..controls (171.0bp,63.983bp) and (171.0bp,54.712bp)  .. (y2);
  \draw [->] (x3) ..controls (195.75bp,64.938bp) and (209.52bp,51.546bp)  .. (y3);
  \draw [red,->] (x4) ..controls (218.25bp,64.938bp) and (204.48bp,51.546bp)  .. (y2);
  \draw [->] (x4) ..controls (243.0bp,63.983bp) and (243.0bp,54.712bp)  .. (y3);
  \draw [->] (x4) ..controls (267.75bp,64.938bp) and (281.52bp,51.546bp)  .. (y4);
  \draw [red,->] (x5) ..controls (290.25bp,64.938bp) and (276.48bp,51.546bp)  .. (y3);
  \draw [->] (x5) ..controls (315.0bp,63.983bp) and (315.0bp,54.712bp)  .. (y4);
  \draw [red,->] (x6) ..controls (362.25bp,64.938bp) and (348.48bp,51.546bp)  .. (y4);
\end{tikzpicture}
\end{figure}

We can see that \(\partial y_i/\partial k_3=x_{i+3-1}\),
e.g.~\(\partial y_2/\partial k_3=x_{4}\), so that:

\begin{equation}
\frac{\partial\mathcal{L}}{\partial k_{i}}=\sum_{j=1}^{6}\frac{\partial\mathcal{L}}{\partial y_{j}}\cdot x_{i+j-1}
\end{equation}

This extends straightforwardly to several dimensions.

We now show that

\begin{equation}
\frac{\partial\mathcal{L}}{\partial x_{ij}}=\sum_{k=L_k}^{U_k}\sum_{l=L_l}^{U_l}\frac{\partial\mathcal{L}}{\partial y_{ab}}k_{kl}
\end{equation}

With

\begin{align}
a &= i-k+1 \\
b &= j-l+1 \\
L_k &= \max(1, i - r(\textbf{X}) + r(\textbf{K})) \\
L_l &= \max(1, j - c(\textbf{X}) + c(\textbf{K})) \\
U_k &= \min(r(\textbf{K}), i) \\
U_l &= \min(c(\textbf{K}), j)
\end{align}

Going back to the example in one dimension, we now highlight the
influence of a fixed element of the input, such as \(x_3\):

\begin{figure}
\centering
\begin{tikzpicture}[>=latex',line join=bevel,]
\node (x1) at (27.0bp,90.0bp) [draw,ellipse] {$x_1$};
  \node (y1) at (99.0bp,18.0bp) [draw,ellipse] {$y_1$};
  \node (x2) at (99.0bp,90.0bp) [draw,ellipse] {$x_2$};
  \node (y2) at (171.0bp,18.0bp) [draw,ellipse] {$y_2$};
  \node (x3) at (171.0bp,90.0bp) [draw,ellipse] {$x_3$};
  \node (y3) at (243.0bp,18.0bp) [draw,ellipse] {$y_3$};
  \node (x4) at (243.0bp,90.0bp) [draw,ellipse] {$x_4$};
  \node (y4) at (315.0bp,18.0bp) [draw,ellipse] {$y_4$};
  \node (x5) at (315.0bp,90.0bp) [draw,ellipse] {$x_5$};
  \node (x6) at (387.0bp,90.0bp) [draw,ellipse] {$x_6$};
  \draw [->] (x1) ..controls (51.75bp,64.938bp) and (65.524bp,51.546bp)  .. (y1);
  \draw [->] (x2) ..controls (99.0bp,63.983bp) and (99.0bp,54.712bp)  .. (y1);
  \draw [->] (x2) ..controls (123.75bp,64.938bp) and (137.52bp,51.546bp)  .. (y2);
  \draw [red,->] (x3) ..controls (146.25bp,64.938bp) and (132.48bp,51.546bp)  .. (y1);
  \definecolor{strokecol}{rgb}{0.0,0.0,0.0};
  \pgfsetstrokecolor{strokecol}
  \draw (147.43bp,67.334bp) node {$k_3$};
  \draw [red,->] (x3) ..controls (171.0bp,63.983bp) and (171.0bp,54.712bp)  .. (y2);
  \draw (180.0bp,64.197bp) node {$k_2$};
  \draw [red,->] (x3) ..controls (195.75bp,64.938bp) and (209.52bp,51.546bp)  .. (y3);
  \draw (194.57bp,67.334bp) node {$k_1$};
  \draw [->] (x4) ..controls (218.25bp,64.938bp) and (204.48bp,51.546bp)  .. (y2);
  \draw [->] (x4) ..controls (243.0bp,63.983bp) and (243.0bp,54.712bp)  .. (y3);
  \draw [->] (x4) ..controls (267.75bp,64.938bp) and (281.52bp,51.546bp)  .. (y4);
  \draw [->] (x5) ..controls (290.25bp,64.938bp) and (276.48bp,51.546bp)  .. (y3);
  \draw [->] (x5) ..controls (315.0bp,63.983bp) and (315.0bp,54.712bp)  .. (y4);
  \draw [->] (x6) ..controls (362.25bp,64.938bp) and (348.48bp,51.546bp)  .. (y4);
\end{tikzpicture}
\end{figure}

We also annotated which element of the kernel is used in every edge.

For brevity, let \(y_i^\prime=\partial\mathcal{L}/\partial y_i\). Then,
from the image we see that:

\begin{equation}
\frac{\partial\mathcal{L}}{\partial x_3}=y_3^\prime k_1+y_2^\prime k_2+y_1^\prime k_3=\sum_{i=1}^3 y_{3-i+1}^\prime k_i
\end{equation}

In particular, note that the indices increse on \(\textbf{K}\) and
decrease on \(\textbf{Y}\). Some care needs to be taken at the borders:

\begin{align*}
\frac{\partial\mathcal{L}}{\partial x_1} &= k_1y_1^\prime \\
\frac{\partial\mathcal{L}}{\partial x_2} &= k_2y_1^\prime+k_1y_3^\prime \\
\frac{\partial\mathcal{L}}{\partial x_5} &= k_2y_4^\prime+k_3y_3^\prime \\
\frac{\partial\mathcal{L}}{\partial x_6} &= k_3y_4^\prime
\end{align*}

In general, the first and last \(c(\textbf{K})-1\) elements will have
less summands, i.e.~when \(i<c(\textbf{K})\) and
\(i>c(\textbf{X})-c(\textbf{K})+1\). In the former case, we sum the
kernel elements from 1 to \(i\), and in the latter case we sum from
\(i-c(\textbf{X})+c(\textbf{K})\) to 3 (both ends included). As before,
this reasoning extends trivially to multiple dimensions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{conv\_gradient\_wrt\_input }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(dloss\_dy, kernel) \{}
\NormalTok{  image\_rows }\OtherTok{=}\NormalTok{ (}
    \FunctionTok{nrow}\NormalTok{(dloss\_dy) }\SpecialCharTok{+} \FunctionTok{nrow}\NormalTok{(kernel) }\SpecialCharTok{{-}} \DecValTok{1}
\NormalTok{  )}
  
\NormalTok{  image\_cols }\OtherTok{=}\NormalTok{ (}
    \FunctionTok{ncol}\NormalTok{(dloss\_dy) }\SpecialCharTok{+} \FunctionTok{ncol}\NormalTok{(kernel) }\SpecialCharTok{{-}} \DecValTok{1}
\NormalTok{  )}

\NormalTok{  vec }\OtherTok{=} \FunctionTok{apply}\NormalTok{(}\FunctionTok{expand.grid}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{image\_rows, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{image\_cols), }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(pos) \{}
\NormalTok{    i }\OtherTok{=}\NormalTok{ pos[}\DecValTok{1}\NormalTok{]}
\NormalTok{    j }\OtherTok{=}\NormalTok{ pos[}\DecValTok{2}\NormalTok{]}

\NormalTok{    lk }\OtherTok{=} \FunctionTok{max}\NormalTok{(}\DecValTok{1}\NormalTok{, i }\SpecialCharTok{{-}}\NormalTok{ image\_rows }\SpecialCharTok{+} \FunctionTok{nrow}\NormalTok{(kernel))}
\NormalTok{    ll }\OtherTok{=} \FunctionTok{max}\NormalTok{(}\DecValTok{1}\NormalTok{, j }\SpecialCharTok{{-}}\NormalTok{ image\_cols }\SpecialCharTok{+} \FunctionTok{ncol}\NormalTok{(kernel))}
\NormalTok{    uk }\OtherTok{=} \FunctionTok{min}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(kernel), i)}
\NormalTok{    ul }\OtherTok{=} \FunctionTok{min}\NormalTok{(}\FunctionTok{ncol}\NormalTok{(kernel), j)}
    
\NormalTok{    gradient\_ij }\OtherTok{=} \FloatTok{0.0}
    \ControlFlowTok{for}\NormalTok{(k }\ControlFlowTok{in}\NormalTok{ lk}\SpecialCharTok{:}\NormalTok{uk) \{}
      \ControlFlowTok{for}\NormalTok{(l }\ControlFlowTok{in}\NormalTok{ ll}\SpecialCharTok{:}\NormalTok{ul) \{}
\NormalTok{        a }\OtherTok{=}\NormalTok{ i }\SpecialCharTok{{-}}\NormalTok{ k }\SpecialCharTok{+} \DecValTok{1}
\NormalTok{        b }\OtherTok{=}\NormalTok{ j }\SpecialCharTok{{-}}\NormalTok{ l }\SpecialCharTok{+} \DecValTok{1}
        
\NormalTok{        gradient\_ij }\OtherTok{=}\NormalTok{ gradient\_ij }\SpecialCharTok{+}\NormalTok{ dloss\_dy[a, b] }\SpecialCharTok{*}\NormalTok{ kernel[k, l]}
\NormalTok{      \}}
\NormalTok{    \}}
\NormalTok{    gradient\_ij}
\NormalTok{  \})}
  
  \FunctionTok{matrix}\NormalTok{(vec, }\AttributeTok{nrow =}\NormalTok{ image\_rows, }\AttributeTok{ncol =}\NormalTok{ image\_cols)}
\NormalTok{\}}


\NormalTok{result }\OtherTok{=} \FunctionTok{our\_conv}\NormalTok{(img, sobel\_x)}
\NormalTok{input\_gradient }\OtherTok{=} \FunctionTok{conv\_gradient\_wrt\_input}\NormalTok{(}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ result, sobel\_x)}
\NormalTok{grid}\SpecialCharTok{::}\FunctionTok{grid.raster}\NormalTok{((}
\NormalTok{  input\_gradient }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(input\_gradient)}
\NormalTok{) }\SpecialCharTok{/}\NormalTok{ (}
  \FunctionTok{max}\NormalTok{(input\_gradient) }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(input\_gradient)}
\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{lab7-solutions_files/figure-latex/unnamed-chunk-10-1.pdf}

We can verify this gradient is correct for a single pixel with finite
differences:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eps }\OtherTok{=} \FloatTok{1e{-}6}

\NormalTok{i }\OtherTok{=} \FunctionTok{floor}\NormalTok{(}\FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(img) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{))}
\NormalTok{j }\OtherTok{=} \FunctionTok{floor}\NormalTok{(}\FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FunctionTok{ncol}\NormalTok{(img) }\SpecialCharTok{+} \DecValTok{1}\NormalTok{))}

\CommentTok{\# add epsilon to position i,j and convolve}
\NormalTok{img[i, j] }\OtherTok{=}\NormalTok{ img[i, j] }\SpecialCharTok{+}\NormalTok{ eps}
\NormalTok{conv\_pos }\OtherTok{=} \FunctionTok{our\_conv}\NormalTok{(img, sobel\_x)}

\CommentTok{\# remove epsilon to position i,j and convolve}
\NormalTok{img[i, j] }\OtherTok{=}\NormalTok{ img[i, j] }\SpecialCharTok{{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ eps}
\NormalTok{conv\_neg }\OtherTok{=} \FunctionTok{our\_conv}\NormalTok{(img, sobel\_x)}

\CommentTok{\# undo modification to the image}
\NormalTok{img[i, j] }\OtherTok{=}\NormalTok{ img[i, j] }\SpecialCharTok{+}\NormalTok{ eps}

\CommentTok{\# compute the difference of the losses}
\CommentTok{\# NB: we sum the differences to get a more accurate result}
\NormalTok{empirical\_gradient }\OtherTok{=} \FunctionTok{sum}\NormalTok{(conv\_pos}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{{-}}\NormalTok{ conv\_neg}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ eps)}

\CommentTok{\# compare empirical and analytical gradients }
\FunctionTok{c}\NormalTok{(empirical\_gradient, input\_gradient[i, j])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.972549 -0.972549
\end{verbatim}

If you did everything correctly, these two numbers should be the same.

Now, can you guess what image \emph{maximizes} the loss we just defined?
We can find this through gradient \emph{ascent}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{maxim }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{runif}\NormalTok{(}\DecValTok{81}\NormalTok{), }\AttributeTok{nrow =} \DecValTok{9}\NormalTok{)}

\NormalTok{losses }\OtherTok{=} \FunctionTok{lapply}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{250}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(i) \{}
\NormalTok{  conv }\OtherTok{=} \FunctionTok{our\_conv}\NormalTok{(maxim, sobel\_x)}
\NormalTok{  loss }\OtherTok{=} \FunctionTok{sum}\NormalTok{(conv}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}

\NormalTok{  grad }\OtherTok{=} \FunctionTok{conv\_gradient\_wrt\_input}\NormalTok{(}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ conv, sobel\_x)}
\NormalTok{  maxim }\OtherTok{\textless{}\textless{}{-}}\NormalTok{ maxim }\SpecialCharTok{+} \FloatTok{0.01} \SpecialCharTok{*}\NormalTok{ grad}
  
\NormalTok{  loss}
\NormalTok{\})}

\FunctionTok{plot}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(losses), losses)}
\NormalTok{grid}\SpecialCharTok{::}\FunctionTok{grid.raster}\NormalTok{(}
\NormalTok{  (maxim }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(maxim)) }\SpecialCharTok{/}\NormalTok{ (}\FunctionTok{max}\NormalTok{(maxim) }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(maxim)),}
  \AttributeTok{interpolate =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{lab7-solutions_files/figure-latex/unnamed-chunk-12-1.pdf}

\end{document}
