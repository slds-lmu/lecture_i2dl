% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  a4paper,
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Lab 3},
  pdfauthor={Emilio Dorigatti},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{bbold}
\usepackage{tikz}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Lab 3}
\author{Emilio Dorigatti}
\date{2020-11-20}

\begin{document}
\maketitle

Welcome to the third lab. The first exercise is an implementation of
gradient descent on a bivariate function. The second exercise is about
computing derivatives of the weights of a neural network, and the third
exercise combines the previous two.

\hypertarget{exercise-1}{%
\subsection{Exercise 1}\label{exercise-1}}

This exercise is about gradient descent. We will use the function
\(f(x_1, x_2)=(x_1-6)^2+x_2^2-x_1x_2\) as a running example:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use pen and paper to do three iterations of gradient descent:

  \begin{itemize}
  \tightlist
  \item
    Find the gradient of \(f\);
  \item
    Start from the point \(x_1=x_2=6\) and use a step size of \(1/2\)
    for the first step, \(1/3\) for the second step and \(1/4\) for the
    third step;
  \item
    What will happen if you keep going?
  \end{itemize}
\item
  Write a function that performs gradient descent:

  \begin{itemize}
  \tightlist
  \item
    For simplicity, we use a constant learning rate.
  \item
    Can you find a way to prematurely stop the optimization when you are
    close to the optimum?
  \end{itemize}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{func.value }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(x) \{}
  \CommentTok{\# }\AlertTok{TODO}\CommentTok{ compute the value of f at x}
\NormalTok{\}}

\NormalTok{func.gradient }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(x) \{}
  \FunctionTok{c}\NormalTok{(}
    \CommentTok{\# }\AlertTok{TODO}\CommentTok{ compute the gradienf of f at x}
\NormalTok{  )}
\NormalTok{\}}

\FunctionTok{func.value}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\FunctionTok{func.gradient}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Does it match what you computed?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gradient\_descent\_optimizer }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(x0, func, grad, max\_steps, alpha) \{}
  \CommentTok{\# x0 is the initial point}
  \CommentTok{\# func computes the value of the function at a given point}
  \CommentTok{\# grad computes the gradient of the function at a given point}
  \CommentTok{\# max\_steps is the maximum number of gradient descent steps}
  \CommentTok{\# alpha is the learning rate}
  
  \CommentTok{\# }\AlertTok{TODO}\CommentTok{ use a for loop to do gradient descent}
\NormalTok{\}}

\FunctionTok{gradient\_descent\_optimizer}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{), func.value, func.gradient, }\DecValTok{10}\NormalTok{, }\FloatTok{0.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Play a bit with the starting point and learning rate to get a feel for
its behavior; how close can you get to the minimum?

\hypertarget{exercise-2}{%
\subsection{Exercise 2}\label{exercise-2}}

This exercise is about computing gradients with the chain rule, with pen
and paper. We will work with a neural network with a single hidden layer
with two neurons and an output layer with one neuron (see Figure
\ref{fig:ex2net}).

\begin{figure}
\centering
\begin{tikzpicture}
\node[draw, circle] (b1) at (1, 4) {$1$ };
\node[draw, circle] (x1) at (0, 2) {$x_1$ };
\node[draw, circle] (x2) at (0, 0) { $x_2$ };
\node[draw, circle] (b2) at (3, 4) { $1$ };
\node[draw, circle] (z1) at (2, 2) { $z_1$ };
\node[draw, circle] (z2) at (2, 0) { $z_2$ };
\node[draw, circle] (y) at (4, 1) { $f$ };

\draw[->] (b1) -- (z1) node [near start,above,sloped] { $b_1$ };
\draw[->] (b1) -- (z2) node [near start,below,sloped] { $b_2$ };

\draw[->] (x1) -- (z1) node [near start,above,sloped] { $w_{11}$ };
\draw[->] (x1) -- (z2) node [near start,above,sloped] { $w_{12}$ };

\draw[->] (x2) -- (z1) node [near start,above,sloped] { $w_{21}$ };
\draw[->] (x2) -- (z2) node [near start,above,sloped] { $w_{22}$ };

\draw[->] (b2) -- (y) node [midway,above,sloped] { $c$ };
\draw[->] (z1) -- (y) node [midway,above,sloped] { $u_1$ };
\draw[->] (z2) -- (y) node [midway,above,sloped] { $u_2$ };
\end{tikzpicture}
\caption{Neural network used in Exercise 2.}
\label{fig:ex2net}
\end{figure}

The neurons in the hidden layer use the \(\tanh\) activation, while the
output neuron uses the sigmoid. The loss used in binary classification
is the \emph{binary cross-entropy}:

\[
\mathcal{L}(y, f_{out})=-y\log f_{out}-(1-y)\log(1-f_{out})
\]

where \(y\in\{0,1\}\) is the true label and \(f_{out}\in(0,1)\) is the
predicted probability that \(y=1\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute \(\partial\mathcal{L}(y, f_{out})/\partial f_{out}\)
\item
  Compute \(\partial f_{out}/\partial f_{in}\)
\item
  Show that \(\partial\sigma(x)/\partial x=\sigma(x)(1-\sigma(x))\)
\item
  Show that \(\partial\tanh(x)/\partial x=1-\tanh(x)^2\) (Hint:
  \(\tanh(x)=(e^x-e^{-x})(e^x+e^{-x})^{-1}\))
\item
  Compute \(\partial f_{in}/\partial c\)
\item
  Compute \(\partial f_{in}/\partial u_1\)
\item
  Compute \(\partial\mathcal{L}(y, f_{out})/\partial c\)
\item
  Compute \(\partial\mathcal{L}(y, f_{out})/\partial u_1\)
\item
  Compute \(\partial f_{in}/\partial z_{2,out}\)
\item
  Compute \(\partial z_{2,out}/\partial z_{2,in}\)
\item
  Compute \(\partial z_{2,in}/\partial b_2\)
\item
  Compute \(\partial z_{2,in}/\partial w_{12}\)
\item
  Compute \(\partial z_{2,in}/\partial x_1\)
\item
  Compute \(\partial\mathcal{L}(y, f_{out})/\partial b_2\)
\item
  Compute \(\partial\mathcal{L}(y, f_{out})/\partial w_{12}\)
\item
  Compute \(\partial\mathcal{L}(y, f_{out})/\partial x_1\)
\end{enumerate}

You will notice that there are lots of redundancies. We will see how to
improve these computations in the lecture and in the next lab. Luckily,
modern deep learning software computes gradients automatically for you.

\hypertarget{exercise-3}{%
\subsection{Exercise 3}\label{exercise-3}}

Now that we know how to do gradient descent and how to compute the
derivatives of the weights of a simple network, we can try to do these
steps together and train our first neural network! We will use the small
dataset with five points we studied in the first lab.

First, let's define the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data.x1 }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{);}
\NormalTok{data.x2 }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{);}
\NormalTok{data.y }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{);}
\end{Highlighting}
\end{Shaded}

Next, a function to compute the output of the network:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sigmoid }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(x) \{}
  \CommentTok{\# }\AlertTok{TODO}\CommentTok{ compute the sigmoid on the input}
\NormalTok{\}}

\NormalTok{nnet.predict }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(params) \{}
  \CommentTok{\# params is a vector, we first unpack for clarity}
\NormalTok{  b1 }\OtherTok{=}\NormalTok{ params[}\DecValTok{1}\NormalTok{]; b2 }\OtherTok{=}\NormalTok{ params[}\DecValTok{2}\NormalTok{];}
\NormalTok{  w11 }\OtherTok{=}\NormalTok{ params[}\DecValTok{3}\NormalTok{]; w12 }\OtherTok{=}\NormalTok{ params[}\DecValTok{4}\NormalTok{];}
\NormalTok{  w21 }\OtherTok{=}\NormalTok{ params[}\DecValTok{5}\NormalTok{]; w22 }\OtherTok{=}\NormalTok{ params[}\DecValTok{6}\NormalTok{];}
\NormalTok{  c }\OtherTok{=}\NormalTok{ params[}\DecValTok{7}\NormalTok{]; u1 }\OtherTok{=}\NormalTok{ params[}\DecValTok{8}\NormalTok{]; u2 }\OtherTok{=}\NormalTok{ params[}\DecValTok{9}\NormalTok{];}
  
  \CommentTok{\# }\AlertTok{TODO}\CommentTok{ compute and return the output of the network}
\NormalTok{\}}

\CommentTok{\# this should return the predictions for the five points in the datasets}
\NormalTok{params }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{9}\NormalTok{)}
\FunctionTok{nnet.predict}\NormalTok{(params)}
\end{Highlighting}
\end{Shaded}

Since gradient descent is done on the loss function, we need a function
to compute it:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nnet.loss }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(params) \{}
  \CommentTok{\# }\AlertTok{TODO}\CommentTok{ compute the predictions and return the average loss}
\NormalTok{\}}

\FunctionTok{nnet.loss}\NormalTok{(params)}
\end{Highlighting}
\end{Shaded}

Now, we need to compute the gradient of each parameter:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nnet.gradient }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(params) \{}
  \CommentTok{\# params is a vector, we first unpack for clarity}
\NormalTok{  b1 }\OtherTok{=}\NormalTok{ params[}\DecValTok{1}\NormalTok{]; b2 }\OtherTok{=}\NormalTok{ params[}\DecValTok{2}\NormalTok{];}
\NormalTok{  w11 }\OtherTok{=}\NormalTok{ params[}\DecValTok{3}\NormalTok{]; w12 }\OtherTok{=}\NormalTok{ params[}\DecValTok{4}\NormalTok{];}
\NormalTok{  w21 }\OtherTok{=}\NormalTok{ params[}\DecValTok{5}\NormalTok{]; w22 }\OtherTok{=}\NormalTok{ params[}\DecValTok{6}\NormalTok{];}
\NormalTok{  c }\OtherTok{=}\NormalTok{ params[}\DecValTok{7}\NormalTok{]; u1 }\OtherTok{=}\NormalTok{ params[}\DecValTok{8}\NormalTok{]; u2 }\OtherTok{=}\NormalTok{ params[}\DecValTok{9}\NormalTok{];}
  
  \CommentTok{\# }\AlertTok{TODO}\CommentTok{ compute all the partial derivatives}
  
  \CommentTok{\# return the derivatives in the same order as the parameters vector}
  \FunctionTok{c}\NormalTok{(}
\NormalTok{    dL\_db1, dL\_db2,}
\NormalTok{    dL\_dw11, dL\_dw12,}
\NormalTok{    dL\_dw21, dL\_dw22,}
\NormalTok{    dL\_dc, dL\_du1, dL\_du2}
\NormalTok{  )}
\NormalTok{\}}

\FunctionTok{nnet.gradient}\NormalTok{(params)}
\end{Highlighting}
\end{Shaded}

Finite differences are a useful way to check that the gradients are
computed correctly:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# first, compute the analytical gradient of the parameters}
\NormalTok{gradient }\OtherTok{=} \FunctionTok{nnet.gradient}\NormalTok{(params);}

\NormalTok{eps }\OtherTok{=} \FloatTok{1e{-}9}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{9}\NormalTok{) \{}
  \CommentTok{\# compute loss when subtracting eps to parameter i}
\NormalTok{  neg\_params }\OtherTok{=} \FunctionTok{c}\NormalTok{(params);}
\NormalTok{  neg\_params[i] }\OtherTok{=}\NormalTok{ neg\_params[i] }\SpecialCharTok{{-}}\NormalTok{ eps;}
\NormalTok{  neg\_value }\OtherTok{=} \FunctionTok{nnet.loss}\NormalTok{(neg\_params);}
  
  \CommentTok{\# compute loss when adding eps to parameter i}
\NormalTok{  pos\_params }\OtherTok{=} \FunctionTok{c}\NormalTok{(params);}
\NormalTok{  pos\_params[i] }\OtherTok{=}\NormalTok{ pos\_params[i] }\SpecialCharTok{+}\NormalTok{ eps;}
\NormalTok{  pos\_value }\OtherTok{=} \FunctionTok{nnet.loss}\NormalTok{(pos\_params);}

  \CommentTok{\# compute the "empirical" gradient of parameter i}
\NormalTok{  fdiff\_gradient }\OtherTok{=} \FunctionTok{mean}\NormalTok{((pos\_value }\SpecialCharTok{{-}}\NormalTok{ neg\_value) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ eps));}
  
  \CommentTok{\# error if difference is too large}
  \FunctionTok{stopifnot}\NormalTok{(}\FunctionTok{abs}\NormalTok{(gradient[i] }\SpecialCharTok{{-}}\NormalTok{ fdiff\_gradient) }\SpecialCharTok{\textless{}} \FloatTok{1e{-}5}\NormalTok{);}
\NormalTok{\}}

\FunctionTok{print}\NormalTok{(}\StringTok{"Gradients are correct!"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can finally train our network. Since the network is so small compared
to the dataset, the training procedure is very sensitive to the way the
weights are initialized and the step size used in gradient descent.

Try to play around with the learning rate and the random initialization
of the weights and find reliable values that make training successful in
most cases.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{min\_loss }\OtherTok{=} \DecValTok{10}
\NormalTok{best\_params }\OtherTok{=} \ConstantTok{NULL}

\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{) \{}
\NormalTok{  params }\OtherTok{=} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{9}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}
\NormalTok{  optimized\_params }\OtherTok{=} \FunctionTok{gradient\_descent\_optimizer}\NormalTok{(}
\NormalTok{    params, nnet.loss, nnet.gradient, }\AttributeTok{max\_steps =} \DecValTok{100}\NormalTok{, }\AttributeTok{alpha =} \DecValTok{1}
\NormalTok{  )}
\NormalTok{  final\_loss }\OtherTok{=} \FunctionTok{nnet.loss}\NormalTok{(optimized\_params)}
  \FunctionTok{cat}\NormalTok{(}\StringTok{"Loss"}\NormalTok{, final\_loss, }\FunctionTok{ifelse}\NormalTok{(final\_loss }\SpecialCharTok{\textless{}} \FloatTok{0.1}\NormalTok{, }\StringTok{"*"}\NormalTok{, }\StringTok{""}\NormalTok{), }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
  
  \ControlFlowTok{if}\NormalTok{(final\_loss }\SpecialCharTok{\textless{}}\NormalTok{ min\_loss) \{}
\NormalTok{    min\_loss }\OtherTok{=}\NormalTok{ final\_loss}
\NormalTok{    best\_params }\OtherTok{=}\NormalTok{ optimized\_params}
\NormalTok{  \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We can use the function in the previous lab to visualize the decision
boundary of the best network:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(scales)}
\FunctionTok{library}\NormalTok{(ggplot2)}

\NormalTok{data }\OtherTok{=} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{expand.grid}\NormalTok{(}
  \AttributeTok{x0 =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{1}\NormalTok{,}
  \AttributeTok{x1 =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1} \SpecialCharTok{/} \DecValTok{25}\NormalTok{),}
  \AttributeTok{x2 =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1} \SpecialCharTok{/} \DecValTok{25}\NormalTok{)}
\NormalTok{))}

\NormalTok{data.x1 }\OtherTok{=}\NormalTok{ data[,}\DecValTok{2}\NormalTok{]}
\NormalTok{data.x2 }\OtherTok{=}\NormalTok{ data[,}\DecValTok{3}\NormalTok{]}

\NormalTok{plot\_grid }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(predictions) \{}
  \CommentTok{\# plots the predicted value for each point on the grid;}
  \CommentTok{\# the predictions should have one column and}
  \CommentTok{\# the same number of rows (10,201) as the data}
\NormalTok{  df }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\FunctionTok{as.data.frame}\NormalTok{(data), }\AttributeTok{y =}\NormalTok{ predictions)}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{geom\_tile}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x1, }\AttributeTok{y =}\NormalTok{ x2, }\AttributeTok{fill =}\NormalTok{ y, }\AttributeTok{color =}\NormalTok{ y), df) }\SpecialCharTok{+}
    \FunctionTok{scale\_color\_gradient2}\NormalTok{(}\AttributeTok{low =} \FunctionTok{muted}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\DecValTok{70}\NormalTok{), }\AttributeTok{mid =} \StringTok{"white"}\NormalTok{,}
                         \AttributeTok{high =} \FunctionTok{muted}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\DecValTok{70}\NormalTok{), }\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{),}
                         \AttributeTok{midpoint =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{scale\_fill\_gradient2}\NormalTok{(}\AttributeTok{low =} \FunctionTok{muted}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\DecValTok{70}\NormalTok{), }\AttributeTok{mid =} \StringTok{"white"}\NormalTok{,}
                        \AttributeTok{high =} \FunctionTok{muted}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\DecValTok{70}\NormalTok{), }\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{),}
                        \AttributeTok{midpoint =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{), }\AttributeTok{y=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)))}
\NormalTok{\}}

\FunctionTok{plot\_grid}\NormalTok{(}\FunctionTok{nnet.predict}\NormalTok{(best\_params))}
\end{Highlighting}
\end{Shaded}

Also try to visualize the decision boundary of network with random
parameters:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_grid}\NormalTok{(}\FunctionTok{nnet.predict}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{9}\NormalTok{, }\AttributeTok{sd=}\DecValTok{5}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}


\end{document}
