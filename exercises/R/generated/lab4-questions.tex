% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  a4paper,
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Lab 4},
  pdfauthor={Emilio Dorigatti},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{bbold}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Lab 4}
\author{Emilio Dorigatti}
\date{2020-11-20}

\begin{document}
\maketitle

Welcome to the fourth lab. In this lab, we will derive the
backpropagation equations, code the training procedure, and test it on
our beloved dataset with five points.

\hypertarget{exercise-1}{%
\subsection{Exercise 1}\label{exercise-1}}

Consider a neural network with \(L\) layers and a loss function
\(\mathcal{L}(\textbf{y},\textbf{z}^{(L)}_{\cdot,out})\). Call the
output of the \(i\)-th unit of the \(\ell\)-th layer
\(\textbf{z}^{(\ell)}_{i,out}=\sigma^{(\ell)}(\textbf{z}^{(\ell)}_{i,in})\)
with
\(\textbf{z}^{(\ell)}_{i,in}=\sum_j\textbf{W}^{(\ell)}_{ji}\textbf{z}^{(\ell-1)}_{j,out}+\textbf{b}^{(\ell)}_{i}\)
its pre-activation output. Finally, consider
\(\delta^{(\ell)}_i=\partial\mathcal{L}(\textbf{y},\textbf{z}^{(L)}_{\cdot,out})/\partial\mathbf{z}^{(\ell)}_{i,in}\)
the gradient of the loss with respect to the pre-activation outputs of
layer \(\ell\).

Derive the back-propagation algorithm for a network with arbitrary
architecture. You might find the results of the previous lab a useful
reference, as well as chapter 5 of the book \emph{Mathematics for
Machine Learning} (\url{https://mml-book.github.io}).

\begin{enumerate}
\item Show that
\begin{align}
\delta^{(L)}_i &= \frac{\partial\mathcal{L}(\textbf{y},\textbf{z}^{(L)}_{\cdot,out})}{\partial\textbf{z}^{(L)}_{i,out}}
\cdot{\sigma^\prime}^{(L)}(\textbf{z}^{(L)}_{i,in})
\\
\label{eq:dldwji}
\frac{\partial \mathcal{L}(\textbf{y},\textbf{z}^{(L)}_{\cdot,out})}{\partial \textbf{W}^{(\ell)}_{ji}}&=\delta^{(\ell)}_i\cdot\textbf{z}^{(\ell-1)}_{j,out} \\
\frac{\partial \mathcal{L}(\textbf{y},\textbf{z}^{(L)}_{\cdot,out})}{\partial \textbf{b}^{(\ell)}_{i}}&=\delta^{(\ell)}_i \\
\label{eq:deltas}
\delta^{(\ell-1)}_i&=\left(\sum_k\delta^{(\ell)}_k\cdot\textbf{W}^{(\ell)}_{ik}\right)\cdot{\sigma^\prime}^{(\ell-1)}(\textbf{z}^{(\ell-1)}_{i,in})
\end{align}

\item Use vectorized operations (i.e., operations with vectors and matrices) to compute the gradients with respect to a single sample.

\item Extend the vectorized operations to handle data in batches, and show that:
\begin{align}
\Delta^{(L)}&=\nabla_{\textbf{Z}^{(L)}_{out}}\mathcal{L}(\textbf{Y},\textbf{Z}^{(L)}_{out})\odot{\sigma^\prime}^{(L)}(\textbf{Z}^{(L)}_{in}) \\
\nabla_{\textbf{W}^{(\ell)}}\mathcal{L}(\textbf{Y},\textbf{Z}^{(\ell)}_{out})&={\textbf{Z}^{(\ell-1)}_{out}}^T \cdot\Delta^{(\ell)} \\
\nabla_{\textbf{b}^{(\ell)}}\mathcal{L}(\textbf{Y},\textbf{Z}^{(L)}_{out})&=\sum_i {\Delta^{(\ell)}_i}^T \\
\Delta^{(\ell-1)}&=\Delta^{(\ell)}{\textbf{W}^{(\ell)}}^T\odot{\sigma^\prime}^{(\ell-1)}(\textbf{Z}^{(\ell-1)}_{in})
\end{align}

where $\Delta^{(\ell)}$, $\textbf{Y}$ and $\textbf{Z}^{(\ell)}_{out}$ are matrices whose $i$-th row contain the respective vectors $\delta$, $\textbf{y}$ and $\textbf{z}^{(\ell)}_{\cdot,out}$ for the $i$-th sample in the batch, and $\odot$ is the element-wise product.
\end{enumerate}

\hypertarget{exercise-2}{%
\subsection{Exercise 2}\label{exercise-2}}

In this exercise, we will code the backpropagation algorithm and apply
it to our five-points dataset.

First, let's define a function to quickly create a neural network with
layers of given size. It will use tanh activation in the hidden layers
and sigmoid for the output layer. Although we will use it for
classification, we use the mean squared error loss for a change.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# activations, losses, and their gradient}
\NormalTok{sigmoid }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(x) \{ }\DecValTok{1} \SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{x)) \}}
\NormalTok{sigmoid\_derivative }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(x) \{ }\FunctionTok{sigmoid}\NormalTok{(x) }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{sigmoid}\NormalTok{(x)) \}}
\NormalTok{tanh\_derivative }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(x) \{ }\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{tanh}\NormalTok{(x)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{ \}}
\NormalTok{mse }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(ytrue, ypred) \{ }\FunctionTok{mean}\NormalTok{((ytrue }\SpecialCharTok{{-}}\NormalTok{ ypred)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) \}}
\NormalTok{mse\_derivative }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(ytrue, ypred) \{ }\DecValTok{2} \SpecialCharTok{*}\NormalTok{ (ypred }\SpecialCharTok{{-}}\NormalTok{ ytrue) }\SpecialCharTok{/} \FunctionTok{length}\NormalTok{(ytrue) \}}


\NormalTok{nnet.new }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(layer\_sizes) \{}
  \CommentTok{\# all information about the network is stored in a list}
\NormalTok{  nnet }\OtherTok{=} \FunctionTok{list}\NormalTok{(}
    \AttributeTok{weights =} \FunctionTok{list}\NormalTok{(),}
    \AttributeTok{biases =} \FunctionTok{list}\NormalTok{(),}
    \AttributeTok{activations =} \FunctionTok{list}\NormalTok{(),}
    \AttributeTok{activations\_derivatives =} \FunctionTok{list}\NormalTok{(),}
    \AttributeTok{loss =}\NormalTok{ mse,}
    \AttributeTok{loss\_derivative =}\NormalTok{ mse\_derivative}
\NormalTok{  )}
  
  \CommentTok{\# create random weight matrices and bias vectors}
\NormalTok{  last\_size }\OtherTok{=}\NormalTok{ layer\_sizes[}\DecValTok{1}\NormalTok{]  }\CommentTok{\# the first element is the number of inputs}
  \ControlFlowTok{for}\NormalTok{(l }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(layer\_sizes)) \{}
\NormalTok{    this\_size }\OtherTok{=}\NormalTok{ layer\_sizes[l]}
    
    \CommentTok{\# weights are initialize using the the famous "Glorot" initialization}
\NormalTok{    b }\OtherTok{=} \FunctionTok{sqrt}\NormalTok{(}\DecValTok{6} \SpecialCharTok{/}\NormalTok{ (this\_size }\SpecialCharTok{+}\NormalTok{ last\_size))}
\NormalTok{    nnet}\SpecialCharTok{$}\NormalTok{weights[[l }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{]] }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}
      \FunctionTok{runif}\NormalTok{(last\_size }\SpecialCharTok{*}\NormalTok{ this\_size, }\SpecialCharTok{{-}}\NormalTok{b, b), }\AttributeTok{ncol =}\NormalTok{ this\_size}
\NormalTok{    )}
    
    \CommentTok{\# biases are initialized to zero}
\NormalTok{    nnet}\SpecialCharTok{$}\NormalTok{biases[[l }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{]] }\OtherTok{=} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, this\_size)}
    
    \CommentTok{\# set the activation}
\NormalTok{    nnet}\SpecialCharTok{$}\NormalTok{activations[[l }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{]] }\OtherTok{=}\NormalTok{ tanh}
\NormalTok{    nnet}\SpecialCharTok{$}\NormalTok{activations\_derivatives[[l }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{]] }\OtherTok{=}\NormalTok{ tanh\_derivative}
    
\NormalTok{    last\_size }\OtherTok{=}\NormalTok{ this\_size}
\NormalTok{  \}}
  
  \CommentTok{\# change the output activation to sigmoid}
\NormalTok{  nnet}\SpecialCharTok{$}\NormalTok{activations[[}\FunctionTok{length}\NormalTok{(nnet}\SpecialCharTok{$}\NormalTok{activations)]] }\OtherTok{=}\NormalTok{ sigmoid}
\NormalTok{  nnet}\SpecialCharTok{$}\NormalTok{activations\_derivatives[[}\FunctionTok{length}\NormalTok{(nnet}\SpecialCharTok{$}\NormalTok{activations)]] }\OtherTok{=}\NormalTok{ sigmoid\_derivative}
  
\NormalTok{  nnet}
\NormalTok{\}}

\NormalTok{nnet }\OtherTok{=} \FunctionTok{nnet.new}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Let us now write the forward pass:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nnet.predict }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(nnet, data.x) \{}
  \CommentTok{\# data.x is a matrix with samples on rows}
  \CommentTok{\# }\AlertTok{TODO}\CommentTok{ perform the forward pass and return the predictions}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

As in the previous labs, let us visualize the output for a randomly
initialized network:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(scales)}
\FunctionTok{library}\NormalTok{(ggplot2)}

\NormalTok{grid }\OtherTok{=} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{x1 =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1} \SpecialCharTok{/} \DecValTok{25}\NormalTok{), }\AttributeTok{x2 =} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1} \SpecialCharTok{/} \DecValTok{25}\NormalTok{)))}
\NormalTok{plot\_grid }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(predictions) \{}
  \CommentTok{\# plots the predicted value for each point on the grid;}
  \CommentTok{\# the predictions should have one column and}
  \CommentTok{\# the same number of rows (10,201) as the data}
\NormalTok{  df }\OtherTok{=} \FunctionTok{cbind}\NormalTok{(}\FunctionTok{as.data.frame}\NormalTok{(grid), }\AttributeTok{y =}\NormalTok{ predictions)}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{geom\_tile}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x1, }\AttributeTok{y =}\NormalTok{ x2, }\AttributeTok{fill =}\NormalTok{ y, }\AttributeTok{color =}\NormalTok{ y), df) }\SpecialCharTok{+}
    \FunctionTok{scale\_color\_gradient2}\NormalTok{(}\AttributeTok{low =} \FunctionTok{muted}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\DecValTok{70}\NormalTok{), }\AttributeTok{mid =} \StringTok{"white"}\NormalTok{,}
                         \AttributeTok{high =} \FunctionTok{muted}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\DecValTok{70}\NormalTok{), }\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{),}
                         \AttributeTok{midpoint =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{scale\_fill\_gradient2}\NormalTok{(}\AttributeTok{low =} \FunctionTok{muted}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\DecValTok{70}\NormalTok{), }\AttributeTok{mid =} \StringTok{"white"}\NormalTok{,}
                        \AttributeTok{high =} \FunctionTok{muted}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\DecValTok{70}\NormalTok{), }\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{),}
                        \AttributeTok{midpoint =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{), }\AttributeTok{y=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)))}
\NormalTok{\}}

\CommentTok{\# run this a few times to see what different random networks predict}
\NormalTok{nnet }\OtherTok{=} \FunctionTok{nnet.new}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\FunctionTok{plot\_grid}\NormalTok{(}\FunctionTok{nnet.predict}\NormalTok{(nnet, grid))}
\end{Highlighting}
\end{Shaded}

Now, we code backpropagation to compute the gradients. Use the
vectorized formulas in Equations 5-8 to make your code much faster.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nnet.gradients }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(nnet, x, y) \{}
  \CommentTok{\# x is be a matrix with samples on rows}
  \CommentTok{\# y is a vector with the labels}
  
\NormalTok{  n\_layers }\OtherTok{=} \FunctionTok{length}\NormalTok{(nnet}\SpecialCharTok{$}\NormalTok{weights)}
\NormalTok{  activations }\OtherTok{=} \FunctionTok{list}\NormalTok{(x)}
\NormalTok{  pre\_activations }\OtherTok{=} \FunctionTok{list}\NormalTok{(x)}
  \CommentTok{\# }\AlertTok{TODO}\CommentTok{ perform the forward pass, storing all activations}
\NormalTok{  loss }\OtherTok{=}\NormalTok{ nnet}\SpecialCharTok{$}\FunctionTok{loss}\NormalTok{(y, activations[[}\FunctionTok{length}\NormalTok{(activations)]])}
  
\NormalTok{  weights\_gradients }\OtherTok{=} \FunctionTok{list}\NormalTok{()}
\NormalTok{  biases\_gradients }\OtherTok{=} \FunctionTok{list}\NormalTok{()}
  \CommentTok{\# }\AlertTok{TODO}\CommentTok{ compute the gradients}
  
  \CommentTok{\# make sure the gradients have the correct size}
  \ControlFlowTok{for}\NormalTok{(l }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_layers) \{}
    \FunctionTok{stopifnot}\NormalTok{(}\FunctionTok{dim}\NormalTok{(nnet}\SpecialCharTok{$}\NormalTok{weights[[l]]) }\SpecialCharTok{==} \FunctionTok{dim}\NormalTok{(weights\_gradients[[l]]))}
    \FunctionTok{stopifnot}\NormalTok{(}\FunctionTok{length}\NormalTok{(nnet}\SpecialCharTok{$}\NormalTok{biases[[l]]) }\SpecialCharTok{==} \FunctionTok{length}\NormalTok{(biases\_gradients[[l]]))}
\NormalTok{  \}}
  
  \CommentTok{\# return gradients as a list}
  \FunctionTok{list}\NormalTok{(}
    \AttributeTok{loss =}\NormalTok{ loss,}
    \AttributeTok{weights\_gradients =}\NormalTok{ weights\_gradients,}
    \AttributeTok{biases\_gradients =}\NormalTok{ biases\_gradients}
\NormalTok{  )}
\NormalTok{\}}

\NormalTok{data.x }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}
  \DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{,}
  \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}
\NormalTok{), }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{)}
\NormalTok{data.y }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\FunctionTok{nnet.gradients}\NormalTok{(nnet, data.x, data.y)}
\end{Highlighting}
\end{Shaded}

We now need to implement gradient descent:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nnet.gradient\_descent\_step }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(nnet, gradients, learning\_rate) \{}
  \CommentTok{\# }\AlertTok{TODO}\CommentTok{ perform one step of gradient descent,}
  \CommentTok{\# modifying the parameters of the network}
\NormalTok{  nnet  }\CommentTok{\# return the modified parameters}
\NormalTok{\}}


\NormalTok{nnet.train }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(nnet, x, y, n\_epochs, learning\_rate) \{}
\NormalTok{  losses }\OtherTok{=} \FunctionTok{list}\NormalTok{()}
  
  \CommentTok{\# }\AlertTok{TODO}\CommentTok{ iterate over the dataset for the given number of epochs and}
  \CommentTok{\# modify the weights at each epoch.}
  \CommentTok{\# use all the data to compute the gradients}
  
  \FunctionTok{list}\NormalTok{(}
    \AttributeTok{losses =} \FunctionTok{unlist}\NormalTok{(losses),}
    \AttributeTok{nnet =}\NormalTok{ nnet}
\NormalTok{  )}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Finally, let us train the network on the small dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data.x }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}
  \DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{,}
  \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}
\NormalTok{), }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{)}
\NormalTok{data.y }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)}

\NormalTok{nnet }\OtherTok{=} \FunctionTok{nnet.new}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{result }\OtherTok{=} \FunctionTok{nnet.train}\NormalTok{(nnet, data.x, data.y, }\DecValTok{2500}\NormalTok{, }\FloatTok{0.25}\NormalTok{)}
\FunctionTok{nnet.predict}\NormalTok{(result}\SpecialCharTok{$}\NormalTok{nnet, data.x)}
\end{Highlighting}
\end{Shaded}

By plotting the loss after each parameter update, we can be sure that
the network converged:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(result}\SpecialCharTok{$}\NormalTok{losses)}
\end{Highlighting}
\end{Shaded}

And the decision boundary of the network is:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_grid}\NormalTok{(}\FunctionTok{nnet.predict}\NormalTok{(result}\SpecialCharTok{$}\NormalTok{nnet, grid))}
\end{Highlighting}
\end{Shaded}

Try to train a few randomly initialized network to discover different
decision boundaries. Try to modify the learning rate and see how it
affects the convergence speed. Finally, try different ways to initialize
the weights and note how the trainability of the network is affected.

\end{document}
