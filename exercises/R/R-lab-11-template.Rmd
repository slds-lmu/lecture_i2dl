---
title: "Deep Learning Lab 10 | Summer Term 2025"
author: "Emanuel Sommer, Prof. Dr. David RÃ¼gamer"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE, echo = TRUE)
```

## Exercise 1

In this exercise, we will implement a denoising autoencoder (DAE). We will use the MNIST dataset for this task. The autoencoder will be trained on noisy images and should be able to remove the noise from the images.

```{r}
library(tensorflow)
cat("This code was tested on tensorflow version", tf$version$GIT_VERSION)
```

As usual, we start by loading and pre-processing the dataset.

```{r}
library(keras3)

mnist = dataset_mnist()
x_train = mnist$train$x / 255
y_train = to_categorical(mnist$train$y)
input_size = 28 * 28
dim(x_train) <- c(nrow(x_train), input_size)
```

First let's look at some original images and their noisy versions with varying noise factors (NF).

```{r}
# Define noise factors
noise_fcts <- c(0.0, 0.1, 0.5, 1.0)

# Define the function to add Gaussian noise
add_gaussian_noise <- function(x, noise_fct) {
  #!hwbegin TODO add Gaussian noise to the input image and scale the noise by noise_fct
  x_noisy <- x + noise_fct * rnorm(length(x), mean = 0, sd = 1)
  #!hwend
  x_noisy
}
# Create a list to hold plots
plot_list <- list()

# Generate plots
library(ggplot2)
library(patchwork)
for (i in seq_along(noise_fcts)) {
  noise_fct <- noise_fcts[i]
  for (j in 1:length(x_train)) {
    if (j > 5) {
      break
    }
    x <- x_train[j,]
    x_noisy <- add_gaussian_noise(x, noise_fct)
    x_noisy <- matrix(x_noisy, nrow = 28, ncol = 28, byrow = T)
    x_noisy <- x_noisy[,28:1]
    
    p <- ggplot(data = as.data.frame(as.table(x_noisy)), aes(Var1, Var2, fill = Freq)) +
      geom_tile() +
      scale_fill_gradient(low = "black", high = "white") +
      theme_void() +
      theme(legend.position = "none") +
      ggtitle(paste("NF =", noise_fct)) +
      coord_fixed()
    
    plot_list <- append(plot_list, list(p))
  }
}

# Combine plots using patchwork
combined_plot <- wrap_plots(plot_list, ncol = 5)

# Display the combined plot
print(combined_plot)

```

Let's first construct a fully connected autoencoder and train it.

```{r}
input <- keras_input(shape = list(input_size))
encoder <- keras_model_sequential(input_shape = c(input_size)) |> 
  layer_dense(units = 128, activation = 'relu') |>
  layer_dense(units = 64, activation = 'relu')

decoder = keras_model_sequential(input_shape = c(64)) |>
  layer_dense(units = 128, activation = 'relu') |>
  layer_dense(units = input_size, activation = 'sigmoid')

output <- input |>
  #!hwbegin TODO add a layer that adds Gaussian noise to the input to make it a denoising autoencoder
  layer_gaussian_noise(stddev = 0.5) |>
  #!hwend
  encoder() |>
  decoder()

autoencoder <- keras_model(inputs = input, outputs = output)

autoencoder |> compile(optimizer = 'adam', loss = 'mse')

autoencoder |> fit(x_train, x_train, epochs = 10, batch_size = 256, shuffle = TRUE)
```

Now let's visualize some reconstructed images.

```{r}
# feed some images through the autoencoder and display the original and reconstructed images
n_images <- 5
original_images <- x_train[1:n_images,]
reconstructed_images <- autoencoder |> predict(original_images)

# visualize them with patchwork and ggplot2
plot_list <- list()

for (i in 1:n_images) {
  original_image <- matrix(original_images[i,], nrow = 28, ncol = 28, byrow = T)
  original_image <- original_image[,28:1]
  reconstructed_image <- matrix(reconstructed_images[i,], nrow = 28, ncol = 28, byrow = T)
  reconstructed_image <- reconstructed_image[,28:1]
  
  p1 <- ggplot(data = as.data.frame(as.table(original_image)), aes(Var1, Var2, fill = Freq)) +
    geom_tile() +
    scale_fill_gradient(low = "black", high = "white") +
    theme_void() +
    theme(legend.position = "none") +
    ggtitle("Orig.") +
    coord_fixed()
  
  p2 <- ggplot(data = as.data.frame(as.table(reconstructed_image)), aes(Var1, Var2, fill = Freq)) +
    geom_tile() +
    scale_fill_gradient(low = "black", high = "white") +
    theme_void() +
    theme(legend.position = "none") +
    ggtitle("Rec.") +
    coord_fixed()
  
  plot_list <- append(plot_list, list(p1 + p2))
}

combined_plot <- wrap_plots(plot_list, nrow = 5)
print(combined_plot)
```

The reconstructions are pretty good. Let's try to be much more parameter efficient by using a convolutional autoencoder. Check out the parameter count of the fully connected DAE with the `summary()` function (218,192).

```{r}
input <- keras_input(shape = list(input_size))
encoder <- keras_model_sequential(input_shape = c(input_size)) |>
  layer_reshape(target_shape = c(28, 28, 1)) |>
  layer_conv_2d(filters = 16, kernel_size = 3, activation = 'relu', padding = 'same') |>
  layer_max_pooling_2d(pool_size = c(2, 2), padding = 'same') |>
  layer_conv_2d(filters = 32, kernel_size = 3, activation = 'relu', padding = 'same') |>
  layer_max_pooling_2d(pool_size = c(2, 2), padding = 'same') |>
  layer_conv_2d(filters = 64, kernel_size = 3, activation = 'relu', padding = 'same') |>
  layer_max_pooling_2d(pool_size = c(2, 2), padding = 'same')

decoder <- keras_model_sequential(input_shape = c(4, 4, 64)) |>
  layer_conv_2d_transpose(filters = 32, kernel_size = 2, activation = 'relu', padding = 'valid', strides = 2) |>
  layer_conv_2d_transpose(filters = 16, kernel_size = 2, activation = 'relu', padding = 'valid', strides = 2) |>
  layer_conv_2d_transpose(filters = 1, kernel_size = 2, activation = 'sigmoid', padding = 'valid', strides = 2) |>
  layer_cropping_2d(cropping = 2) |>
  layer_flatten()

output <- input |> 
  #!hwbegin TODO add a layer that adds Gaussian noise to the input to make it a denoising autoencoder
  layer_gaussian_noise(stddev = 0.5) |>
  #!hwend
  encoder() |> 
  decoder()

conv_autoencoder <- keras_model(inputs = input, outputs = output)
# print(summary(conv_autoencoder))

conv_autoencoder |> compile(optimizer = 'adam', loss = 'mse')
conv_autoencoder |> fit(x_train, x_train, epochs = 10, batch_size = 256, shuffle = TRUE)
```

Again, let's visualize some reconstructed images.

```{r}
# feed some images through the autoencoder and display the original and reconstructed images
n_images <- 5
original_images <- x_train[1:n_images,]
reconstructed_images <- conv_autoencoder |> predict(original_images)

# visualize them with patchwork and ggplot2
plot_list <- list()

for (i in 1:n_images) {
  original_image <- matrix(original_images[i,], nrow = 28, ncol = 28, byrow = T)
  original_image <- original_image[,28:1]
  reconstructed_image <- matrix(reconstructed_images[i,], nrow = 28, ncol = 28, byrow = T)
  reconstructed_image <- reconstructed_image[,28:1]
  
  p1 <- ggplot(data = as.data.frame(as.table(original_image)), aes(Var1, Var2, fill = Freq)) +
    geom_tile() +
    scale_fill_gradient(low = "black", high = "white") +
    theme_void() +
    theme(legend.position = "none") +
    ggtitle("Orig.") +
    coord_fixed()
  
  p2 <- ggplot(data = as.data.frame(as.table(reconstructed_image)), aes(Var1, Var2, fill = Freq)) +
    geom_tile() +
    scale_fill_gradient(low = "black", high = "white") +
    theme_void() +
    theme(legend.position = "none") +
    ggtitle("Rec.") +
    coord_fixed()
  
  plot_list <- append(plot_list, list(p1 + p2))
}

combined_plot <- wrap_plots(plot_list, nrow = 5)
print(combined_plot)
```
The results are competitive with the fully connected autoencoder, but the convolutional autoencoder is much more parameter efficient (about 7 times less parameters)! As an exercise give a short explanation why the convolutional autoencoder is more parameter efficient than the fully connected autoencoder.


## Exercise 2
In this exercise we will get acquainted with the KL divergence for normal distributions. First, let $p(x)=\mathcal{N}(\mu_1,\sigma_1^2)$ and $q(x)=\mathcal{N}(\mu_2,\sigma_2^2)$ and show that

\begin{equation}
\text{KL}(q||p)
= \mathbb{E}_{x\sim q}\left[\log\frac{q(x)}{p(x)}\right]
=\log\frac{\sigma_1}{\sigma_2}+\frac{\sigma_2^2+(\mu_1-\mu_2)^2}{2\sigma_1^2} -\frac 1 2
\end{equation}

Now, consider a variaitonal autoencoder that takes a vector as input $\textbf{x}$ and transforms it into a mean vector $\mu(\textbf{x})$ and a variance vector $\sigma(\textbf{x})^2$. From these, we derive the latent code $\textbf{z}\sim q(\textbf{z})=\mathcal{N}(\mu(\textbf{x}),\text{diag}(\sigma(\textbf{x})^2))$, i.e. a multivariate Gaussian in $d$ dimensions with a given mean vector and diagonal covariance matrix. The prior distribution for $\textbf{z}$ is another $d$-dimensional multivariate Gaussian $p=\mathcal{N}(\textbf{0},\textbf{1})$.

Now show that:

\begin{equation}
\text{KL}(q||p)= -\frac 1 2 \sum_{i=1}^d \left(1+\log\sigma_i(\textbf{x})^2-\sigma_i(\textbf{x})^2 - \mu_i(\textbf{x})^2 \right)
\end{equation}

Hint: start by showing that $p$ and $q$ can be factorized into a product of independent Gaussian components, one for each dimension, then apply the formula for the KL divergence for the univariate case.

<!--#!solutionbegin-->
### Solution
We analyze each term separately:

\begin{align}
\mathbb{E}_{x\sim q}\left[\log q(x)\right]
&= -\frac 1 2 \log(2\pi\sigma_2^2)+\mathbb{E}\left[-\frac{1}{2\sigma_2^2}\left(x-\mu_2\right)^2\right] \\
&= -\frac 1 2 \log(2\pi\sigma_2^2) -\frac{1}{2\sigma_2^2}\left(\mathbb{E}[x^2]-2\mu_2\mathbb{E}[x]+\mu_2^2\right) \\
&= -\frac 1 2 \log(2\pi\sigma_2^2) -\frac{1}{2\sigma_2^2}\left(\sigma_2^2+\mu_2^2-2\mu_2^2+\mu_2^2\right) \\
&= -\frac 1 2(1+\log(2\pi\sigma_2^2))
\end{align}

and

\begin{align}
\mathbb{E}_{x\sim q}\left[\log p(x)\right]
&= -\frac 1 2 \log(2\pi\sigma_1^2)+\mathbb{E}\left[-\frac{1}{2\sigma_1^2}\left(x-\mu_1\right)^2\right] \\
&= -\frac 1 2 \log(2\pi\sigma_1^2) -\frac{1}{2\sigma_1^2}\left(\mathbb{E}[x^2]-2\mu_1\mathbb{E}[x]+\mu_1^2\right) \\
&= -\frac 1 2 \log(2\pi\sigma_1^2) -\frac{1}{2\sigma_1^2}\left(\sigma_2^2+\mu_2^2-2\mu_1\mu_2+\mu_1^2\right) \\
&= -\frac 1 2 \log(2\pi\sigma_1^2) -\frac{\sigma_2^2+(\mu_1-\mu_2)^2}{2\sigma_1^2}
\end{align}

Now taking the difference:


\begin{align}
\text{KL}(q||p)
&=\mathbb{E}_{x\sim q}\left[\log q(x)\right] - \mathbb{E}_{x\sim q}\left[\log p(x)\right] \\
&=-\frac 1 2(1+\log(2\pi\sigma_2^2))+\frac 1 2 \log(2\pi\sigma_1^2) +\frac{\sigma_2^2+(\mu_1-\mu_2)^2}{2\sigma_1^2} \\
&=-\frac 1 2\left[1+\log(2\pi\sigma_2^2)-\log(2\pi\sigma_1^2)-\frac{\sigma_2^2+(\mu_1-\mu_2)^2}{\sigma_1^2}\right] \\
&=-\frac 1 2\left[1+\log\frac{\sigma_2^2}{\sigma_1^2}-\frac{\sigma_2^2+(\mu_1-\mu_2)^2}{\sigma_1^2} \right] \\
&= \log\frac{\sigma_1}{\sigma_2}+\frac{\sigma_2^2+(\mu_1-\mu_2)^2}{2\sigma_1^2} -\frac 1 2
\end{align}

Moving to the second question, the expression for $p$ can be factorized as follows:

\begin{align}
q(\textbf{z})
&=
  (2\pi)^{-d/2}
  \det(\text{diag}(\sigma(\textbf{x})^2))^{-1/2}
  \exp\left(-\frac 1 2
    (\textbf{z}-\mu(\textbf{x}))^T
    \text{diag}(\sigma(\textbf{x})^2)^{-1}
    (\textbf{z}-\mu(\textbf{x}))
  \right) \\
&=
  (2\pi)^{-d/2}
  \left(\prod_i \sigma_i(\textbf{x})^2\right)^{-1/2}
  \exp\left(
    -\frac 1 2 \sum_i \sigma_i(\textbf{x})^{-2}(z_i-\mu_i(\textbf{x}))^2
  \right) \\
&=
  \prod_{i=1}^d
  (2\pi\sigma_i(\textbf{x})^2)^{-1/2}
  \exp\left(
    -\frac{1}{2\sigma_i(\textbf{x})^2} (z_i-\mu_i(\textbf{x}))^2
  \right) \\
&=
  \prod_{i=1}^d
  \mathcal{N}(z_i|\mu_i(\textbf{x})^2, \sigma_i(\textbf{x})^2)
\end{align}

Where we made use of some convenient properties of diagonal matrices, namely that their determinant is the product of the elements on the diagonal, and that their inverse is again diagonal with the elements replaced by their reciprocal.

Now since the mean of $p$ is zero and the covariance is the identity matrix, we have:

\begin{equation}
p(\textbf{z})=\prod_{i=1}^d\mathcal{N}(z_i|0, 1)
\end{equation}

We now plug these into the formula for the KL divergence to get:

\begin{align}
\text{KL}(q||p)
&= \mathbb{E}_{x\sim q}\left[\log q(x)\right] - \mathbb{E}_{x\sim q}\left[\log p(x)\right] \\
&= \mathbb{E}\left[\log\prod_{i=1}^d q(x)\right] - \mathbb{E}\left[\log\prod_{i=1}^d p_i(x)\right] \\
&= \sum_{i=1}^d \mathbb{E}\left[\log q_i(x)\right] - \sum_i \mathbb{E}\left[\log p_i(x)\right] \\
&= \sum_{i=1}^d \mathbb{E}\left[\log\frac{q_i(x)}{p_i(x)}\right] \\
&= \sum_{i=1}^d \left(\log\frac{1}{\sigma_i(\textbf{x})}+\frac{\sigma_i(\textbf{x})^2+\mu(\textbf{x})^2}{2} -\frac 1 2\right) \\
&= -\frac 1 2 \sum_{i=1}^d \left(1+\log\sigma_i(\textbf{x})^2-\sigma_i(\textbf{x})^2 - \mu_i(\textbf{x})^2 \right)
\end{align}

<!--#!solutionend-->

## Exercise 3 (optional)

In this exercise we are going to implement variational autoencoders (VAEs) on the MNIST dataset. In doing this, we will learn some advanced features of Keras that allow us to extend its capabilities beyond what is available out of the box. Specifically, we will see how to create custom layers that can be integrated seamlessly with the other components, and how to create custom callbacks that monitor and alter the training process.

We have already loaded an preprocessed the data in exercise 1.

In a VAE, the encoder outputs mean and variance of a multivariate Gaussian distribution of the latent codes. Nothing prevents you from using a more complicated distribution in the same framework, but this is the usual choice. The expected log likelihood is then approximated by decoding a single sample from this distribution. Moreover, since we need the model to be differentiable end-to-end, sampling from the latent codes is re-formulated via the reparametrization trick.

Keras does not provide a layer that can perform this sampling operation, but it can be easily extended to include such a layer. This involves creating a `Layer` class that implements these main three methods:

 1. `build(input_shape)`: in this method, you create the parameters necessary for the layer to operate.
 2. `call(x)`: this method contains the computational logic of the layer, and computes the output from the input `x` of the previous layer.
 3. `initialize(<arguments>)`: this initializes the layer with the arguments passed to the constructor.

In practice, these functions operate with Tensorflow variables, although it is not immediately clear; `build` creates them, and `call` creates the part of the computational graph responsible to compute the output. A look at the [documentation](https://keras3.posit.co/articles/making_new_layers_and_models_via_subclassing.html#the-layer-class-the-combination-of-state-weights-and-some-computation) is advised.

For example, here is how to implement a normal dense layer:


```{r}
our_dense <- Layer("OurDense",
  initialize = function(activation, units = 32, ...) {
    self$units <- as.integer(units)
    self$activation <- activation
    super$initialize(...)
  },
  
  build = function(input_shape) {
    self$w <- self$add_weight(
      shape = shape(tail(input_shape, 1), self$units),
      initializer = "random_normal",
      trainable = TRUE
    )
    self$b <- self$add_weight(
      shape = shape(self$units),
      initializer = "zeros",
      trainable = TRUE
    )
  },

  call = function(inputs) {
    self$activation(op_matmul(inputs, self$w) + self$b)
  }
)
```

Now we can use this as any other Keras layer:

```{r}
model <- keras_model_sequential(input_shape = tf$TensorShape(as.integer(784))) |>
  our_dense(units = 32, activation = activation_relu) |>
  our_dense(units = 10, activation = activation_softmax)

model |> compile(
  loss = 'categorical_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy')
)

model |> fit(
  x_train, y_train,
  epochs = 4
)
```


We now create a custom layer that performs sampling for a VAE. Until now, we have used `keras_model_sequential` to build our neural networks, but Keras can also build networks that are a general directed acyclic graph. In this graph, every node is a whole layer, and a layer can process data from several other layers, and can send its output to an arbitrary number of other layers. We will use this to separate the creation of mean and standard deviation for the latent code, and write our custom layer to take both of them as input. Moreover, for numerical reasons, the encoder network will actually predict the logarithm of the standard deviation.

Let us start from the custom layer.

```{r}
# this parameter re-weighs the KL divergence
# later it will be clear why it is needed
# this is actually a tensorflow variable
kl_weight = tf$Variable(1.0)

layer_sampling <- Layer("SamplingLayer",
  initialize = function() {
    super$initialize()
  },
  build = function(input_shape) {},

  call = function(x, mask = NULL) {
    # here `x` is a list of two matrices,
    # the mean and log variance of the latent codes.
    # (samples are on rows and latent dimensions on columns)
    mu = x[[1]]
    log_var = x[[2]]

    kl_loss = (
      #!hwbegin TODO compute the KL divergence according to equation 2\n# some useful functions are k_sum, k_exp and k_square
      -op_mean(op_sum(1 + log_var - op_square(mu) - op_exp(log_var), axis=-1)) / 2
      #!hwend
    )

    self$add_loss(
      kl_weight * kl_loss / 784  # scale the KL to a magnitude comparable to the MSE
    )

    # generate the random noise
    eps = random_normal(op_shape(mu))

    #!hwbegin TODO compute the latent code using the reparametrization trick
    mu + eps * op_exp(log_var / 2)
    #!hwend
  },
)
```

This encoder has a hidden layer of size 512. The output of the latter is connected to two separate layers that will predict mean and (the logarithm of the) standard deviation of the latent code for the input samples. We now connect these two layers to the sampling layer we implemented above, and add a few more dense layers to reconstruct the example.

We now create the model, starting from the encoder.

```{r}
latent_dim = 32L
hidden_dim = 512L
sample_dim = 784L

input  = keras_input(shape = list(sample_dim))

# common encoder layers
encoder <- our_dense(units = hidden_dim, activation = activation_relu)
ll = input |> encoder()

# separate heads for mean and standard deviation
latent_mean = ll |> our_dense(units = latent_dim, activation = activation_linear)
latent_log_std = ll |> our_dense(units = latent_dim, activation = activation_linear)
```

This encoder has two hidden layers of size 256 and 64. The output of the latter is connected to two separate layers that will predict mean and (the logarithm of the) standard deviation of the latent code for the input samples. We now connect these two layers to the sampling layer we implemented above, and add a few more dense layers to reconstruct the example.

```{r}
# note that the input is a list of two layers
latent_code = layer_sampling(list(latent_mean, latent_log_std))

# decoder layers
output = latent_code |>
  our_dense(units = hidden_dim, activation = activation_relu) |>
  our_dense(units = sample_dim, activation = activation_linear)
```

We can create a Keras model with arbitrarily complicated topologies by appropriately connecting layers, as we just did. We only need to tell Keras which layers are input layers and which are output. Although we did not do it here, we can also have several input layers and/or several output layers, each with its own loss. This is useful when, for example, you want to create a network that takes as input an picture and a sound, and outputs the location of the objects in the picture that does that sound, along with a textual description of what is going on.

We now create, compile and fit the model as usual.

```{r}
model = keras_model(input, output)

model |> compile(
  #!hwbegin TODO use the mean squared error loss and the adam optimizer
  loss = 'mse',
  optimizer = 'adam',
  #!hwend
)

model |> fit(
  #!hwbegin TODO fit for two epochs on the training dataset
  x_train, x_train, epochs = 2
  #!hwend
)
```

Let us check the reconstruction of a digit:

```{r}
reconstruction = model |> predict(x_train[2,,drop=FALSE])
reconstruction = pmin(1, pmax(reconstruction, 0))

dim(reconstruction) <- c(28, 28)

plot(as.raster(reconstruction))
```

It is already quite good. Now try to remove the division of the KL by 784, train again and visualize the result.

You should see a gray blob that looks a bit like the average of many digits. This phenomenon is named _mode collapse_, i.e. the distribution of the generator collapsed to a single mode that covers the entire dataset, instead of (at least) one mode for every digit. In VAEs, this is typically caused by a KL term that is very strong at the beginning of training, and dominates the reconstruction loss. The optimizer will focus most of its efforts to reduce this term, ending up in a poor local minimum.

A popular method to deal with this issue is _KL annealing_. It consists in training the network without the KL regularizer for some time, then slowly increasing the weight of the KL. This procedure allows the network to first learn how to perform good reconstructions, then to adjust the latent code to conform to a Normal distribution without erasing progress on the reconstruction.

In order to implement this method, we need to change the variable `kl_weight` during training. It is possible to do this using a custom Keras callback. In case you do not remember, a callback provides a specific functionality at certain stages of the training process. In some previous labs, we used the early stopping callback to prevent overfitting.

Creating a custom callback is very similar to creating a custom layer (see the documentation [here](https://keras.rstudio.com/articles/training_callbacks.html)):

```{r}
KlAnnealingCallback <- Callback("KlAnnealingCallback",
  initialize = function(epoch_start, duration_epochs, variable) {
    self$epoch_start = epoch_start
    self$duration_epochs = duration_epochs
    self$variable = variable
  },

  # no kl at the beginning of training
   on_train_begin = function(logs = list()) {
    self$variable$assign(0.0)
  },

  # update the weight at the beginning of every epoch
  on_epoch_begin = function(epoch, logs = list()) {
    epoch = epoch + 1  # it starts from zero
    if(epoch < self$epoch_start) {
      message("epoch ", epoch, " is lower than starting epoch ", self$epoch_start)
    }
    else {
      new_weight = (
        #!hwbegin TODO linearly interpolate the new weight
        pmin(1, (epoch - self$epoch_start) / self$duration_epochs)
        #!hwend
      )

      self$variable$assign(new_weight)
      message("kl weight is ", new_weight, " at epoch ", epoch)
    }
  }
)

#!hwbegin TODO create the VAE as before
input = keras_input(shape = list(sample_dim))

ll = input |> our_dense(units = hidden_dim, activation = activation_relu)

latent_mean = ll |> our_dense(units = latent_dim, activation = activation_linear)
latent_log_std = ll |> our_dense(units = latent_dim, activation = activation_linear)

latent_code = layer_sampling(list(latent_mean, latent_log_std))

output = latent_code |>
  our_dense(units = hidden_dim, activation = activation_relu) |>
  our_dense(units = sample_dim, activation = activation_linear)
#!hwend

model = keras_model(input, output)

# here we isolate the encoder into its own model
# later we will use it for encoding samples
encoder = keras_model(input, latent_code)

model |> compile(
  loss = 'mse',
  optimizer = 'adam',
)

model |> fit(
  x_train, x_train, epochs = 20,
  callbacks = list(
    KlAnnealingCallback(epoch_start = 4, duration_epochs = 8, variable = kl_weight)
  )
)
```


```{r}
reconstruction = model |> predict(x_train[2,,drop=FALSE])
reconstruction = pmin(1, pmax(reconstruction, 0))

dim(reconstruction) <- c(28, 28)

plot(as.raster(reconstruction))
```
